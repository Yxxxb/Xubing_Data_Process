{
  "summary": {
    "total_repositories": 44,
    "total_skipped": 6,
    "successful": 40,
    "failed": 4,
    "statistics": {
      "avg_files_per_repo": 98.8,
      "avg_lines_per_repo": 61788.85,
      "avg_files_changed_per_pr": 7.9743589743589745,
      "avg_lines_changed_per_pr": 205.74358974358975,
      "repos_with_swebench": 44,
      "repos_with_patch": 43,
      "repos_with_issue_comments": 18,
      "total_issue_comments": 33,
      "avg_issue_comments_per_repo": 0.65,
      "repos_with_problem_statement": 7
    }
  },
  "repositories": [
    {
      "tar_file_name": "Ahmkel#Keras-Project-Template#pull#5",
      "repo_name": "Ahmkel#Keras-Project-Template#pull#5",
      "success": true,
      "error": null,
      "commit": {
        "sha": "f82098c9bceef04e24f6e62422eead6e20d6145b",
        "message": "Merge pull request #4 from Sucran/master\n\nsolve the issue #3",
        "author": {
          "name": "Ahmed Hamada Mohamed Kamel El-Hinidy",
          "email": "Ahmed.El-Hinidy.2014@ieee.org",
          "date": "2018-04-17T20:25:57Z"
        },
        "html_url": "https://github.com/Ahmkel/Keras-Project-Template/commit/f82098c9bceef04e24f6e62422eead6e20d6145b",
        "api_url": "https://api.github.com/repos/Ahmkel/Keras-Project-Template/commits/f82098c9bceef04e24f6e62422eead6e20d6145b"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Ahmkel#Keras-Project-Template#pull#5",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Ahmkel#Keras-Project-Template#pull#5.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Ahmkel#Keras-Project-Template#pull#5/source_code"
      },
      "pr": {
        "number": 5,
        "title": "use dotmap to replace bunch",
        "body": "Hi, @Ahmkel , I find a potential problem with this template, if we want this template more useful ~\r\nhere is the json file in our template \r\n```json\r\n{\r\n  \"exp_name\": \"simple_mnist\",\r\n  \"num_epochs\": 20,\r\n  \"learning_rate\": 0.001,\r\n  \"optimizer\": \"adam\",\r\n  \"batch_size\": 64,\r\n  \"validation_split\":0.25,\r\n  \"verbose_training\": true,\r\n  \"checkpoint_monitor\": \"val_loss\",\r\n  \"checkpoint_mode\": \"min\",\r\n  \"checkpoint_save_best_only\": true,\r\n  \"checkpoint_save_weights_only\": true,\r\n  \"checkpoint_verbose\": true,\r\n  \"tensorboard_write_graph\": true\r\n}\r\n```\r\nsometimes more and more paramters may need to write into this json file, and it may becomes like this\r\n```json\r\n{\r\n  \"exp_name\": \"simple_mnist\",\r\n  \"num_epochs\": 20,\r\n  \"learning_rate\": 0.001,\r\n  \"optimizer\": \"adam\",\r\n  \"batch_size\": 64,\r\n  \"validation_split\":0.25,\r\n  \"verbose_training\": true,\r\n  \"checkpoint_monitor\": \"val_loss\",\r\n  \"checkpoint_mode\": \"min\",\r\n  \"checkpoint_save_best_only\": true,\r\n  \"checkpoint_save_weights_only\": true,\r\n  \"checkpoint_verbose\": true,\r\n  \"tensorboard_write_graph\": true,\r\n \"learning_rate\": 0.001,\r\n  \"optimizer\": \"adam\",\r\n  \"batch_size\": 64,\r\n  \"validation_split\":0.25,\r\n\"tensorboard_write_graph\": true,\r\n  \"validation_split\":0.25,\r\n  \"verbose_training\": true,\r\n  \"checkpoint_monitor\": \"val_loss\",\r\n  \"checkpoint_mode\": \"min\",\r\n\"tensorboard_write_graph\": true\r\n}\r\n``` \r\njson file will become long and messy, and we can write it in another way actually, like this\r\n```json\r\n{\r\n  \"exp\":{\r\n    \"name\": \"simple_mnist\"\r\n  },\r\n  \"model\":{\r\n    \"large_kernel_size\": 10,\r\n    \"target_size\": 224,\r\n    \"learning_rate\": 1e-4,\r\n    \"momentum\": 0.9,\r\n    \"decay\":5e-4\r\n  },\r\n  \"trainer\":{\r\n    \"num_epochs\": 100,\r\n    \"batch_size\": 20,\r\n    \"step_per_epoch\": 125,\r\n    \"dev_step\": 25,\r\n    \"verbose_training\": true\r\n  },\r\n  \"callbacks\":{\r\n    \"checkpoint_monitor\": \"val_loss\",\r\n    \"checkpoint_mode\": \"min\",\r\n    \"checkpoint_save_best_only\": true,\r\n    \"checkpoint_save_weights_only\": true,\r\n    \"checkpoint_verbose\": 1,\r\n    \"tensorboard_write_graph\": true\r\n  }\r\n```\r\n\r\nHoweverï¼Œ the problem is we can not access the `name` parameters for `config.exp.name`ï¼Œsince the Bunch lib do not support automatic child Bunch creation, and correct one is `config.exp['name']`, which is ugly.\r\n\r\nSo, may be we can change the Bunch lib to other same functional lib to implement  a dot-accessible dictionary,\r\n\r\nand I find a lib call DotMap, you can check this\r\n\r\n[https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary](url)\r\n\r\nI recommend to use DotMap instead.",
        "state": "closed",
        "created_at": "2018-04-18T07:54:26Z",
        "updated_at": "2018-05-15T03:57:10Z",
        "merged_at": "2018-05-15T03:56:01Z",
        "html_url": "https://github.com/Ahmkel/Keras-Project-Template/pull/5",
        "user": "Sucran",
        "additions": 41,
        "deletions": 35,
        "changed_files": 5,
        "commits": 2
      },
      "swebench": {
        "instance_id": "Ahmkel_Keras-Project-Template-5",
        "repo": "/Ahmkel/Keras-Project-Template",
        "base_commit": "f82098c9bceef04e24f6e62422eead6e20d6145b",
        "problem_statement": {},
        "edit_files": [
          "configs/simple_mnist_config.json",
          "main.py",
          "models/simple_mnist_model.py",
          "trainers/simple_mnist_trainer.py",
          "utils/config.py"
        ],
        "oracle_files": [
          "{\n  \"exp_name\": \"simple_mnist\",\n  \"num_epochs\": 20,\n  \"learning_rate\": 0.001,\n  \"optimizer\": \"adam\",\n  \"batch_size\": 64,\n  \"validation_split\":0.25,\n  \"verbose_training\": true,\n  \"checkpoint_monitor\": \"val_loss\",\n  \"checkpoint_mode\": \"min\",\n  \"checkpoint_save_best_only\": true,\n  \"checkpoint_save_weights_only\": true,\n  \"checkpoint_verbose\": true,\n  \"tensorboard_write_graph\": true\n  \n}\n",
          "from data_loader.simple_mnist_data_loader import SimpleMnistDataLoader\nfrom models.simple_mnist_model import SimpleMnistModel\nfrom trainers.simple_mnist_trainer import SimpleMnistModelTrainer\nfrom utils.config import process_config\nfrom utils.dirs import create_dirs\nfrom utils.utils import get_args\n\n\ndef main():\n    # capture the config path from the run arguments\n    # then process the json configuration file\n    try:\n        args = get_args()\n        config = process_config(args.config)\n    except:\n        print(\"missing or invalid arguments\")\n        exit(0)\n\n    # create the experiments dirs\n    create_dirs([config.tensorboard_log_dir, config.checkpoint_dir])\n\n    print('Create the data generator.')\n    data_loader = SimpleMnistDataLoader(config)\n\n    print('Create the model.')\n    model = SimpleMnistModel(config)\n\n    print('Create the trainer')\n    trainer = SimpleMnistModelTrainer(model.model, data_loader.get_train_data(), config)\n\n    print('Start training the model.')\n    trainer.train()\n\n\nif __name__ == '__main__':\n    main()\n",
          "from base.base_model import BaseModel\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense\n\n\nclass SimpleMnistModel(BaseModel):\n    def __init__(self, config):\n        super(SimpleMnistModel, self).__init__(config)\n        self.build_model()\n\n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Dense(32, activation='relu', input_shape=(28 * 28,)))\n        self.model.add(Dense(16, activation='relu'))\n        self.model.add(Dense(10, activation='softmax'))\n\n        self.model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=self.config.optimizer,\n            metrics=['acc'],\n        )\n",
          "from base.base_trainer import BaseTrain\nimport os\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\n\n\nclass SimpleMnistModelTrainer(BaseTrain):\n    def __init__(self, model, data, config):\n        super(SimpleMnistModelTrainer, self).__init__(model, data, config)\n        self.callbacks = []\n        self.loss = []\n        self.acc = []\n        self.val_loss = []\n        self.val_acc = []\n        self.init_callbacks()\n\n    def init_callbacks(self):\n        self.callbacks.append(\n            ModelCheckpoint(\n                filepath=os.path.join(self.config.checkpoint_dir, '%s-{epoch:02d}-{val_loss:.2f}.hdf5' % self.config.exp_name),\n                monitor=self.config.checkpoint_monitor,\n                mode=self.config.checkpoint_mode,\n                save_best_only=self.config.checkpoint_save_best_only,\n                save_weights_only=self.config.checkpoint_save_weights_only,\n                verbose=self.config.checkpoint_verbose,\n            )\n        )\n\n        self.callbacks.append(\n                TensorBoard(\n                    log_dir=self.config.tensorboard_log_dir,\n                    write_graph=self.config.tensorboard_write_graph,\n                )\n            )\n\n        if hasattr(self.config,\"comet_api_key\"):\n            from comet_ml import Experiment\n            experiment = Experiment(api_key=self.config.comet_api_key, project_name=self.config.exp_name)\n            experiment.disable_mp()\n            experiment.log_multiple_params(self.config)\n            self.callbacks.append(experiment.get_keras_callback())\n\n    def train(self):\n        history = self.model.fit(\n            self.data[0], self.data[1],\n            epochs=self.config.num_epochs,\n            verbose=self.config.verbose_training,\n            batch_size=self.config.batch_size,\n            validation_split=self.config.validation_split,\n            callbacks=self.callbacks,\n        )\n        self.loss.extend(history.history['loss'])\n        self.acc.extend(history.history['acc'])\n        self.val_loss.extend(history.history['val_loss'])\n        self.val_acc.extend(history.history['val_acc'])\n",
          "import json\nfrom bunch import Bunch\nimport os\nimport time\n\n\ndef get_config_from_json(json_file):\n    \"\"\"\n    Get the config from a json file\n    :param json_file:\n    :return: config(namespace) or config(dictionary)\n    \"\"\"\n    # parse the configurations from the config json file provided\n    with open(json_file, 'r') as config_file:\n        config_dict = json.load(config_file)\n\n    # convert the dictionary to a namespace using bunch lib\n    config = Bunch(config_dict)\n\n    return config, config_dict\n\n\ndef process_config(json_file):\n    config, _ = get_config_from_json(json_file)\n    config.tensorboard_log_dir = os.path.join(\"experiments\", time.strftime(\"%Y-%m-%d/\",time.localtime()), config.exp_name, \"logs/\")\n    config.checkpoint_dir = os.path.join(\"experiments\", time.strftime(\"%Y-%m-%d/\",time.localtime()), config.exp_name, \"checkpoints/\")\n    return config\n"
        ],
        "test_patch": "",
        "patch_preview": "From 2ac63d61ac55506bca8b4ce199ce6463f510c47b Mon Sep 17 00:00:00 2001\nFrom: Richard <1095279138475@tianchi.com>\nDate: Wed, 18 Apr 2018 15:53:06 +0800\nSubject: [PATCH] use dotmap to replace bunch\n\n---\n configs/simple_mnist_config.json | 35 ++++++++++++++++++++------------\n main.py                          |  3 +--\n models/simple_mnist_model.py     |  2 +-\n trainers/simple_mnist_trainer.py | 24 +++++++++++-----------\n utils/config.py                  |  8 ++++----\n 5 files changed, 40 insertions("
      },
      "patch": {
        "length": 6178,
        "files_changed": 5,
        "lines_added": 40,
        "lines_deleted": 32,
        "net_change": 8,
        "changed_files": [
          {
            "file": "configs/simple_mnist_config.json",
            "added": 22,
            "deleted": 13
          },
          {
            "file": "main.py",
            "added": 1,
            "deleted": 2
          },
          {
            "file": "models/simple_mnist_model.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "trainers/simple_mnist_trainer.py",
            "added": 12,
            "deleted": 12
          },
          {
            "file": "utils/config.py",
            "added": 4,
            "deleted": 4
          }
        ]
      },
      "issue_comments": [
        {
          "id": 388718228,
          "body": "used your changes in my PR because I liked the hierarchical config: https://github.com/Ahmkel/Keras-Project-Template/pull/6",
          "user": "jsleep",
          "created_at": "2018-05-14T07:11:08Z",
          "html_url": "https://github.com/Ahmkel/Keras-Project-Template/pull/5#issuecomment-388718228"
        },
        {
          "id": 389035173,
          "body": "Hi!\r\nSorry for the late response. I agree, it's a very good idea.\r\nThanks.",
          "user": "Ahmkel",
          "created_at": "2018-05-15T03:56:39Z",
          "html_url": "https://github.com/Ahmkel/Keras-Project-Template/pull/5#issuecomment-389035173"
        },
        {
          "id": 386380385,
          "body": "Actually this seems like a good idea. +1.",
          "user": "bhavul",
          "created_at": "2018-05-03T17:50:20Z",
          "html_url": "https://github.com/Ahmkel/Keras-Project-Template/pull/5#issuecomment-386380385"
        }
      ],
      "issue_comments_count": 3,
      "code_statistics": {
        "total_files": 23,
        "total_lines": 1902,
        "total_bytes": 174379,
        "python_files": 17,
        "python_lines": 226,
        "file_extensions": {
          ".py": 17,
          "": 1,
          ".md": 1,
          ".jpg": 1,
          ".PNG": 1,
          ".json": 1,
          ".out": 1
        },
        "largest_files": [
          {
            "path": "experiment_note/temp.out",
            "size": 78081,
            "lines": 940,
            "extension": ".out"
          },
          {
            "path": "figures/Tensorboard_demo.PNG",
            "size": 48491,
            "lines": 223,
            "extension": ".PNG"
          },
          {
            "path": "LICENSE",
            "size": 11357,
            "lines": 201,
            "extension": ""
          },
          {
            "path": "README.md",
            "size": 6575,
            "lines": 172,
            "extension": ".md"
          },
          {
            "path": "figures/ProjectArchitecture.jpg",
            "size": 22177,
            "lines": 124,
            "extension": ".jpg"
          },
          {
            "path": "trainers/simple_mnist_trainer.py",
            "size": 2115,
            "lines": 54,
            "extension": ".py"
          },
          {
            "path": "main.py",
            "size": 1052,
            "lines": 36,
            "extension": ".py"
          },
          {
            "path": "utils/config.py",
            "size": 871,
            "lines": 27,
            "extension": ".py"
          },
          {
            "path": "base/base_model.py",
            "size": 877,
            "lines": 25,
            "extension": ".py"
          },
          {
            "path": "models/simple_mnist_model.py",
            "size": 676,
            "lines": 21,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 23,
        "files_changed_count": 5,
        "files_changed_ratio": 0.21739130434782608,
        "total_lines_in_repo": 1902,
        "lines_added": 40,
        "lines_deleted": 32,
        "net_lines_changed": 8,
        "lines_changed_ratio": 0.03785488958990536,
        "pr_body_length": 2563,
        "commit_message_length": 60,
        "python_file_count": 17,
        "python_line_count": 226
      }
    },
    {
      "tar_file_name": "CoderWanFeng#python-office#pull#18",
      "repo_name": "CoderWanFeng#python-office#pull#18",
      "success": false,
      "error": "object of type 'NoneType' has no len()",
      "commit": {
        "sha": "af4b0227d8dfe9575b51ab659d535b8ab0fa0957",
        "message": "Merge pull request #15 from heyWFeng/master\n\nadd setup guide",
        "author": {
          "name": "ç¨‹åºå‘˜æ™šæž«",
          "email": "40364228+CoderWanFeng@users.noreply.github.com",
          "date": "2022-05-07T05:36:11Z"
        },
        "html_url": "https://github.com/CoderWanFeng/python-office/commit/af4b0227d8dfe9575b51ab659d535b8ab0fa0957",
        "api_url": "https://api.github.com/repos/CoderWanFeng/python-office/commits/af4b0227d8dfe9575b51ab659d535b8ab0fa0957"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/CoderWanFeng#python-office#pull#18",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/CoderWanFeng#python-office#pull#18.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/CoderWanFeng#python-office#pull#18/source_code"
      },
      "pr": {
        "number": 18,
        "title": "add pdf password",
        "body": null,
        "state": "closed",
        "created_at": "2022-05-09T11:48:23Z",
        "updated_at": "2022-05-09T11:48:40Z",
        "merged_at": "2022-05-09T11:48:40Z",
        "html_url": "https://github.com/CoderWanFeng/python-office/pull/18",
        "user": "heyWFeng",
        "additions": 115,
        "deletions": 56,
        "changed_files": 9,
        "commits": 1
      },
      "swebench": {
        "instance_id": "CoderWanFeng_python-office-18",
        "repo": "/CoderWanFeng/python-office",
        "base_commit": "af4b0227d8dfe9575b51ab659d535b8ab0fa0957",
        "problem_statement": {},
        "edit_files": [
          "README.md",
          "office/pdf.py",
          "office/tools.py",
          "service/__init__.py",
          "service/pdf/__init__.py",
          "service/pdf/add_watermark_service.py",
          "setup.cfg",
          "utils/__init__.py",
          "utils/progress.py"
        ],
        "oracle_files": [
          "# Pythonè‡ªåŠ¨åŒ–åŠžå…¬å­¦ä¹ æŒ‡å—\n\n\n![å›¾ç‰‡](https://f10.baidu.com/it/u=442371958,171656815&fm=30&app=106&f=JPEG&access=215967316?w=640&h=201&s=D923707E86D40D7216227510020080DA)\n\n\n\nå¤§å®¶å¥½ï¼Œè¿™é‡Œæ˜¯æ³•å­¦é™¢æ¯•ä¸šçš„ç¨‹åºå‘˜æ™šæž«ï¼Œä¸“æ³¨äºŽPythonè‡ªåŠ¨åŒ–åŠžå…¬çŸ¥è¯†åˆ†äº«ã€‚\n> è‡ªåŠ¨åŒ–åŠžå…¬ï¼Œå­¦ä¹ äº¤æµç¾¤[ðŸ‘‰ã€æˆ³æˆ‘åŠ å…¥ã€‘](http://www.python4office.cn/wechat-group/)\n\næœ€è¿‘Pypiå®˜ç½‘å‘å¸ƒäº†ä¸€ä¸ªPythonè‡ªåŠ¨åŒ–åŠžå…¬çš„ç¥žå™¨ï¼špython-officeï¼Œå†…å«æ‰€æœ‰Pythonè‡ªåŠ¨åŒ–åŠžå…¬çš„ç¬¬ä¸‰æ–¹åº“ï¼Œå¯ä»¥å¸®åŠ©å°ç™½ã€å¿«é€Ÿä½¿ç”¨ã€‘Pythonè‡ªåŠ¨åŒ–åŠžå…¬ã€‚\n\n\n\n# ä¸‹è½½å’Œä½¿ç”¨\n\næœ¬é¡¹ç›®æ—¨åœ¨å¼€å‘ä¸€ä¸ªç¬¬ä¸‰æ–¹åº“ï¼špython-officeï¼Œå¯ä»¥å¸®åŠ©éœ€è¦è¿›è¡ŒPythonè‡ªåŠ¨åŒ–åŠžå…¬çš„æœ‹å‹ï¼Œ**å°¤å…¶æ˜¯å°ç™½ï¼Œ**é€šè¿‡ä¸‹åˆ—æ–¹å¼ï¼Œä¸€é”®å®‰è£…å®Œæˆè¿›è¡ŒPythonè‡ªåŠ¨åŒ–åŠžå…¬çš„å¼€å‘çŽ¯å¢ƒã€‚\n> ç›®å‰é¡¹ç›®å·²ä¸Šçº¿Pythonå®˜ç½‘ï¼š[ä¼ é€é—¨](https://pypi.org/project/python-office)\n\n0. å®‰è£…è¿™ä¸ªåº“ä¹‹å‰ï¼Œä½ çš„ç”µè„‘ä¸Šï¼Œéœ€è¦æœ‰pythonçŽ¯å¢ƒ\næ²¡æœ‰çš„åŒå­¦ï¼Œè¯·ç§»æ­¥è¿™ä¸ª6åˆ†é’Ÿçš„å®‰è£…è§†é¢‘ï¼š[æ¥ï¼Œæ‰‹æŠŠæ‰‹å¸¦ä½ æ­å»ºPythonçŽ¯å¢ƒ](https://www.bilibili.com/video/BV1Q44y1u7rV)\n\n1. å®‰è£…å¥½åŽï¼Œæ‰“å¼€pycharmçš„terminalï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼Œå³å¯è‡ªåŠ¨å®‰è£…\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple python-office -U\n```\n\nä½œç”¨ï¼š\n- ä¸€é”®æ­å»ºæ‰€æœ‰Python + è‡ªåŠ¨åŒ–åŠžå…¬çš„ç¼–ç¨‹çŽ¯å¢ƒã€‚\n- ä¸€è¡Œä»£ç ï¼Œè§£å†³å¤§éƒ¨åˆ†è‡ªåŠ¨åŒ–åŠžå…¬çš„é—®é¢˜ï¼Œä¸éœ€è¦å°ç™½å­¦ä¹ PythonçŸ¥è¯†ï¼Œè‡ªå·±è‹¦å“ˆå“ˆçš„å†™ä»£ç \n\n\n# åŠŸèƒ½æ–‡æ¡£ï¼ˆæŒç»­æ›´æ–°ï¼‰\n\næœ¬éƒ¨åˆ†å†…å®¹ï¼Œæ—¨åœ¨ä»‹ç»python-officeçš„æ‰€æœ‰å·²æœ‰åŠŸèƒ½ï¼ŒæŒç»­æ›´æ–°ä¸­\n### ä¸€ã€Wordæ“ä½œ\n\n#### 1ã€wordè½¬pdf\n[python-officeåº“ï¼šåªè¦2è¡ŒPythonä»£ç ï¼Œå®žçŽ°Wordæ‰¹é‡è½¬æ¢PDF](https://mp.weixin.qq.com/s/6SM_66BjCIzUkkRWrDe5pQ)\n```python\nimport office # å¯¼å…¥python-office\n\npath = '.'  # pathè¿™é‡Œï¼Œå¡«å†™ä½ å­˜æ”¾wordæ–‡ä»¶çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼šC:/app/workbook\noffice.word.docx2pdf(path=path) # ç¨‹åºå°±å¯ä»¥è‡ªåŠ¨å°†è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰wordæ–‡æ¡£ï¼Œè‡ªåŠ¨è½¬æ¢æˆpdfæ–‡æ¡£äº†\n```\n### äºŒã€PDFæ“ä½œ\n#### 1ã€pdfæ·»åŠ æ°´å°\n[ä¸€è¡ŒPythonä»£ç ï¼Œç»™PDFæ–‡ä»¶æ·»åŠ æ°´å°ï¼Œå¿«é€Ÿè€Œä¸”å…è´¹~python-officeè‡ªåŠ¨åŒ–åŠžå…¬ï¼ŒYYDS](https://mp.weixin.qq.com/s/yJDs5RoytRL5hl-ybXkZOA)\n```python\nimport office  # å¯¼å…¥python-office\n\noffice.pdf.add_watermark() # ä¸éœ€è¦å¯¹ä»£ç è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œç›´æŽ¥è¿è¡Œ\n```\n### ä¸‰ã€å›¾ç‰‡æ“ä½œ\n\n#### 1ã€æ–‡å­—ç”Ÿæˆè¯äº‘\n[é€†å¤©ï¼1 è¡Œä»£ç å°±å¯ä»¥ç”Ÿæˆå¯è§†åŒ–è¯äº‘ï¼Œpython-officeè‡ªåŠ¨åŒ–åŠžå…¬å‘å¸ƒæ–°åŠŸèƒ½ï¼](https://mp.weixin.qq.com/s/ifmt7MDleACNQKxk77EeNA)\n```python\nimport office  # å¯¼å…¥python-office\n\noffice.image.txt2wordcloud(filename='yes-minister.txt', \n                            color='black', \n                            result_file=\"your_wordcloud.png\")\n\nå‚æ•°è¯´æ˜Ž\nfilenameï¼šè¿™é‡Œå¡«å†™ä½ éœ€è¦ç”Ÿæˆè¯äº‘çš„txtæ–‡æ¡£çš„è·¯å¾„ï¼Œä¾‹å¦‚ï¼šd:/work/ciyun.txt\ncolorï¼šè¿™ä¸ªæ˜¯è¯äº‘çš„åº•è‰²ï¼Œå¯ä»¥è®¾ç½®ä»»ä½•é¢œè‰²ï¼›å¯ä»¥ä¸å¡«ï¼Œé»˜è®¤æ˜¯ç™½è‰²åº•è‰²ã€‚\nresult_fileï¼šç”Ÿæˆè¯äº‘å›¾ç‰‡çš„åå­—ï¼Œæ ¼å¼å¿…é¡»æ˜¯pngï¼›å¯ä»¥ä¸å¡«ï¼Œé»˜è®¤æ˜¯ï¼šyour_wordcloud.png\n```\n\n#### 2ã€å›¾ç‰‡æ·»åŠ æ°´å°\n```python\nimport office\n\noffice.image.add_watermark('anaconda2.jpg','å…¬ä¼—å·ï¼šç¨‹åºå‘˜æ™šæž«')\n```\n### å››ã€æ–‡ä»¶å’Œæ–‡ä»¶å¤¹æ“ä½œ\n#### 1ã€æ‰¹é‡é‡å‘½åæ–‡ä»¶å’Œæ–‡ä»¶å¤¹\n```python\nimport office\npath = 'D:\\\\QMDownload\\\\'\noffice.file.replace4filename(path=path,del_content='æ–°å»ºæ–‡ä»¶å¤¹',replace_content='5')\n\n```\n### äº”ã€è§†é¢‘æ“ä½œ\n#### 1ã€ä»Žè§†é¢‘é‡Œæå–MP3\n```python\nimport office\npath = 'D:\\\\QMDownload\\\\'\noffice.video.video2mp3(path=path,mp3_name='result')\n```\n# ç›®å‰åŒ…å«çš„ç¬¬ä¸‰æ–¹åº“æœ‰\n\n\n#### Excel\n\n- xlrdï¼šè¯»å–excel\n\n- xlwtï¼šå†™å…¥Excel\n\n- xlutilsï¼šè°ƒæ•´Excelçš„æ ¼å¼\n\n- xlwingsï¼š[xlwingsåº“ | Excelä¸ŽPythonçš„å®Œç¾Žç»“åˆï¼ˆé™„ä½¿ç”¨æ–‡æ¡£ï¼‰](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247492034&idx=1&sn=b677b3f285b1426c0c83dbba7708a5d7&chksm=eaf540f7dd82c9e1ff2bfa197580f5e88c4d45ad1c18e9c9ef534d7b3e5ae006dca62c3546bf&scene=21#wechat_redirect)\n\n- openpyxlï¼šçµæ´»å¤„ç†Excelçš„æ•°æ®\n\n- xlswriterï¼šåŠŸèƒ½æ‰©å±•åº“\n\n- pandasï¼š[ç³»ç»Ÿæ€§çš„å­¦ä¼š Pandasï¼Œ çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247495847&idx=1&sn=056789b0e560c014d8f9530fbf63d584&chksm=eaf55192dd82d884f69c48d657e3f76654a6cb5f9e9a4a70780be69320fd525e0fe3773c543c&scene=21#wechat_redirect)\n\n- pyxllï¼šä¸€ä¸ªå¼ºå¤§çš„æ’ä»¶åº“\n\n  \n\n#### Word\n\n- python-docxï¼š[Python-Docxåº“ | Wordä¸ŽPythonçš„å®Œç¾Žç»“åˆï¼ˆé™„ä½¿ç”¨æ–‡æ¡£ï¼‰](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247491631&idx=1&sn=c169f107acfb03b2f37661a4b6f50587&chksm=eaf5411add82c80c59af213553db3020d0b5a439b84dcb21086258a6a9b2de2719df0390e32a&scene=21#wechat_redirect)\n\n#### PPT\n\n- python-pptxï¼š[python-pptxåº“ | PPTä¸ŽPythonçš„å®Œç¾Žç»“åˆï¼ˆé™„ä½¿ç”¨æ–‡æ¡£ï¼‰](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247492263&idx=1&sn=2d7f601b34913415238b7a232acba13c&chksm=eaf54392dd82ca844a6fc653e3492bdac12d96a332d305f05ea15d01c916e5f7f81fa3decae3&scene=21#wechat_redirect)\n\n#### PDF\n\n- PyPDF2ï¼š[PyPDF2åº“ | PDFä¸ŽPythonçš„å®Œç¾Žç»“åˆï¼ˆé™„ä½¿ç”¨æ–‡æ¡£ï¼‰](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247492209&idx=1&sn=55152c540a1c927bb9fcb79005327b29&chksm=eaf54344dd82ca5295e6e2d1e11712f97118871f6639d593826200f1bce45b98c0c03d494de7&scene=21#wechat_redirect)\n- å¾…å®Œå–„\n\n#### OCR\n\n- easyocrï¼šå›¾ç‰‡è¯†åˆ«åº“ï¼Œæ”¯æŒ80+è¯­è¨€\n- å¾…å®Œå–„\n\n#### çˆ¬è™«\n\n- scrapyï¼šä¸€é”®å¼€å¯çˆ¬è™«ï¼Œçˆ¬å–å…¨ç«™èµ„æº\n\n#### ç½‘ç«™å¼€å‘\n\n- djangoï¼š[æ·±åº¦ç›˜ç‚¹ | å²ä¸Šæœ€å…¨Pythonç½‘ç«™å¼€å‘åº“ï¼ˆ37ä¸ªï¼‰](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247494188&idx=1&sn=3e0d887d9588399e4c6035dd7916f8fc&chksm=eaf54b19dd82c20f9ae7bf3f5a7f9606d456b85e63f31ebe41d6938ed77c88f438a6b08cdab7&scene=21#wechat_redirect)\n- flaskï¼šä¸€é”®ç”Ÿæˆç½‘ç«™\n\n#### æ•°æ®åˆ†æž & æ•°æ®å¯è§†åŒ–\n\n- pandas\n- numpy\n- matplotlibï¼š[278é¡µPDFï¼šã€ŠPythonæ•°æ®åˆ†æžåŸºç¡€ã€‹ï¼Œ0åŸºç¡€å…¥é—¨ä¸“ç”¨~](http://mp.weixin.qq.com/s?__biz=MzI2Nzg5MjgyNg==&mid=2247496126&idx=3&sn=b4bb4d3551e6486baa1b70ef72414a8e&chksm=eaf5508bdd82d99db0bd1b6ed9307328bc6954de87b5f26ef5ae222b2e4fd7c500890a20dd7e&scene=21#wechat_redirect)\n\n\n\n\n# åŠ å…¥é¡¹ç›®\n\n#### é¡¹ç›®ä»‹ç»\n\næœ¬é¡¹ç›®æ—¨åœ¨æ‰“åŒ…æ‰€æœ‰Python + è‡ªåŠ¨åŒ–åŠžå…¬çš„æŠ€æœ¯ï¼Œæ–¹ä¾¿å¤§å®¶çš„è‡ªåŠ¨åŒ–åŠžå…¬ä½¿ç”¨ã€‚\næ¬¢è¿Žå¤§å®¶æäº¤PRï¼ˆpull requestï¼‰ï¼Œä¸€èµ·æ¥ä¸°å¯Œè¿™ä¸ªé¡¹ç›®ï¼\n> â€œ\n>\n> æ¬¢è¿Žæ„Ÿå…´è¶£çš„æœ‹å‹ï¼Œé€šè¿‡æäº¤PRçš„æ–¹å¼ï¼Œå‚ä¸Žè¯¥é¡¹ç›®çš„æ›´æ–°ä¸Žç»´æŠ¤ï¼Œæˆ‘æ¯å¤©ä¸‹åˆmergeä¸€æ¬¡ã€‚æºç åœ°å€å¦‚ä¸‹\n\n- Giteeåœ°å€ï¼š[https://gitee.com/CoderWanFeng/python-office](https://gitee.com/CoderWanFeng/python-office)\n\n- GitHubåœ°å€ï¼š[https://github.com/CoderWanFeng/python-office](https://github.com/CoderWanFeng/python-office)\n\n\n# ç‰ˆæœ¬è¯´æ˜Ž\n\n| ç‰ˆæœ¬å· | ç‰ˆæœ¬ä¿¡æ¯            | å‘å¸ƒæ—¥æœŸ  |\n| ------ | ------------------- | --------- |\n| 0.0.1  | initï¼šé¡¹ç›®åˆå§‹åŒ–          | 2022-4-19 |\n| 0.0.2  | initï¼šæ·»åŠ åŸºç¡€åº“          | 2022-4-21 |\n| 0.0.3  | initï¼šmatplotlibå’Œeasyocr | 2022-4-24 |\n| 0.0.4  | initï¼šä¿®æ”¹é…ç½®æ–‡ä»¶ä¸ºsetup.cfg | 2022-4-24 |\n| 0.0.5  | initï¼šå‘å¸ƒwheelæ–‡ä»¶ | 2022-4-24 |\n| 0.0.6  | addï¼šwordæ‰¹é‡è½¬pdf | 2022-4-24 |\n| 0.0.7  | patchï¼šwordæ‰¹é‡è½¬pdf | 2022-4-24 |\n| 0.0.8  | addï¼šå•ä¸ªpdfæ·»åŠ æ°´å° | 2022-4-25 |\n| 0.0.9  | patchï¼šå› ä¸ºå®‰è£…åŒ…å¤ªå¤§ï¼ŒåŽ»æŽ‰matplotlibï¼›æ·»åŠ é¡¹ç›®äº¤æµç¾¤ | 2022-4-25 |\n| 0.0.10  | addï¼štxtæ–‡æœ¬è½¬è¯äº‘åŠŸèƒ½ | 2022-4-28 |\n| 0.0.11  | updateï¼šwordæ‰¹é‡è½¬pdf | 2022-5-1 |\n| 0.0.12  | addï¼šé‡å‘½åæŒ‡å®šè·¯å¾„ä¸‹çš„æ–‡ä»¶/æ–‡ä»¶å¤¹ | 2022-5-4 |\n| 0.0.13  | updateï¼šå› ä¸ºc++14çš„åŽŸå› ï¼ŒåŽ»æŽ‰wordcloudåº“ï¼Œéœ€è¦çš„åŒå­¦ï¼Œè‡ªè¡Œinstallå³å¯ | 2022-5-6 |\n| 0.0.14  | addï¼šç»™å›¾ç‰‡åŠ æ°´å°ã€ç”ŸæˆäºŒç»´ç ã€æå–éŸ³é¢‘ã€ç¿»è¯‘ | 2022-5-6 |\n\n> å…³äºŽç‰ˆæœ¬æ›´æ–°ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œè¯·ç§ä¿¡å¾®åš@[ç¨‹åºå‘˜æ™šæž«](http://www.python4office.cn/weibo-qaq/)\n>\n> æˆ–è€…ï¼Œæ¬¢è¿Žæœ‰å­¦ä¹ /å®šåˆ¶åŠŸèƒ½/åŠ å…¥é¡¹ç›®éœ€æ±‚çš„åŒå­¦ï¼Œç›´æŽ¥åŠ å…¥æˆ‘ä»¬çš„é¡¹ç›®äº¤æµç¾¤ðŸ‘‰[ç‚¹æˆ‘ç›´è¾¾](http://www.python4office.cn/images/2-free-group.jpg)\n\n## å‚è€ƒèµ„æ–™ \n- å…³äºŽsetup.pyçš„å‚æ•°è¯´æ˜Ž\n    - https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/#install-requires\n- å¦‚ä½•ä½¿ç”¨setup.cfg\n    - https://zhuanlan.zhihu.com/p/261579357\n- æ‰“åŒ…pip\n    - https://mp.weixin.qq.com/s/zzD4pxNMFd0ZuWqXlVFdAg",
          "# -*- coding: utf-8 -*-\nimport reportlab\nimport os\nfrom PyPDF2 import PdfFileWriter, PdfFileReader\nfrom reportlab.pdfgen import canvas\nfrom reportlab.pdfbase.ttfonts import TTFont\nfrom reportlab.pdfbase.pdfmetrics import registerFont\nfrom fpdf import FPDF\n\n\ndef create_watermark(content):\n    \"\"\"åˆ›å»ºPDFæ°´å°æ¨¡æ¿\n    \"\"\"\n    # åˆ›å»ºä¸€ä¸ªPDFæ–‡ä»¶æ¥ä½œä¸ºä¸€ä¸ªæ°´å°æ–‡ä»¶\n    c = canvas.Canvas('watermark.pdf')\n    reportlab.pdfbase.pdfmetrics.registerFont(\n        reportlab.pdfbase.ttfonts.TTFont('simfang', 'C:/Windows/Fonts/simfang.ttf'))\n    c.setFont('simfang', 20)\n    c.saveState()\n    c.translate(305, 505)\n    c.rotate(45)\n    c.drawCentredString(0, 0, content)\n    c.restoreState()\n    c.save()\n    pdf_watermark = PdfFileReader('watermark.pdf')\n    return pdf_watermark\n\n\ndef pdf_add_watermark(pdf_file_in, pdf_file_mark, pdf_file_out):\n    # print(pdf_file_out)\n    pdf_output = PdfFileWriter()\n    input_stream = open(pdf_file_in, 'rb')\n    pdf_input = PdfFileReader(input_stream, strict=False)\n    # èŽ·å–PDFæ–‡ä»¶çš„é¡µæ•°\n    if pdf_input.getIsEncrypted():\n        print(\"æ–‡ä»¶å·²è¢«åŠ å¯†\")\n        PDF_Passwd = input(\"è¯·è¾“å…¥PDFå¯†ç ï¼š\")\n        # å°è¯•ç”¨ç©ºå¯†ç è§£å¯†\n        try:\n            pdf_input.decrypt(PDF_Passwd)\n        except Exception:\n            print(f\"å°è¯•ç”¨å¯†ç {PDF_Passwd}è§£å¯†å¤±è´¥.\")\n            return False\n    pageNum = pdf_input.getNumPages()\n    # è¯»å…¥æ°´å°pdfæ–‡ä»¶\n    # print(pdf_file_mark)\n    mark_stream = open(pdf_file_mark, mode='rb')\n    pdf_watermark = PdfFileReader(mark_stream, strict=False)\n    # ç»™æ¯ä¸€é¡µæ‰“æ°´å°\n    for i in range(pageNum):\n        page = pdf_input.getPage(i)\n        page.mergePage(pdf_watermark.getPage(0))\n        page.compressContentStreams()  # åŽ‹ç¼©å†…å®¹\n        pdf_output.addPage(page)\n    pdf_output.write(open(pdf_file_out, 'wb'))\n\n\ndef add_watermark():\n    pdf_file_in = input(\"è¯·è¾“å…¥éœ€è¦æ·»åŠ æ°´å°çš„æ–‡ä»¶ä½ç½®ï¼š\")  # éœ€è¦æ·»åŠ æ°´å°çš„æ–‡ä»¶\n    Watermark_Str = input(\"è¯·è¾“å…¥éœ€è¦æ·»åŠ çš„æ°´å°å†…å®¹ï¼š\")\n    print('=' * 20)\n    print('æ­£åœ¨æŒ‰è¦æ±‚ï¼Œç»™ä½ çš„PDFæ–‡ä»¶æ·»åŠ æ°´å°ï¼Œè¯·è®©ç¨‹åºé£žä¸€ä¼šå„¿~')\n    print('=' * 20)\n    pdf_file_mark = 'watermark.pdf'  # æ°´å°æ–‡ä»¶\n    create_watermark(str(Watermark_Str))\n    pdf_file_out = 'æ·»åŠ äº†æ°´å°çš„æ–‡ä»¶.pdf'  # æ·»åŠ PDFæ°´å°åŽçš„æ–‡ä»¶\n    pdf_add_watermark(pdf_file_in, pdf_file_mark, pdf_file_out)\n    print(\"æ°´å°æ·»åŠ ç»“æŸï¼Œè¯·æ‰“å¼€ç”µè„‘ä¸Šçš„è¿™ä¸ªä½ç½®ï¼ŒæŸ¥çœ‹ç»“æžœæ–‡ä»¶ï¼š{path}\".format(path=os.getcwd()))\n\n\n# txtè½¬pdf\ndef txt2pdf():\n    pdf = FPDF()\n    pdf.add_page()  # Add a page\n    pdf.set_font(\"Arial\", size=15)  # set style and size of font\n    f = open(\n        'D:\\\\workplace\\\\code\\\\BaiduNetdiskWorkspace\\\\personal\\\\linux\\\\workplace\\\\pro\\\\git\\\\gitee\\\\python-office\\\\test\\\\allpackages.txt',\n        \"r\")  # open the text file in read mode\n    # insert the texts in pdf\n    for x in f:\n        pdf.cell(50, 5, txt=x, ln=1, align='C')\n    # pdf.output(\"path where you want to store pdf file\\\\file_name.pdf\")\n    pdf.output(\"game_notes.pdf\")\n",
          "# import the library\nfrom translate import Translator\nimport qrcode\n\n\ndef transtools(to_lang, content):\n    # specifying the language\n    translator = Translator(to_lang)\n    # typing the message\n    translation = translator.translate(content)\n    # print the translated message\n    print(translation)\n\n\ndef qrcodetools(url):\n    # Creating object\n    # version: defines size of image from integer(1 to 40), box_size = size of each box in pixels, border = thickness of the border.\n    qr = qrcode.QRCode(version=1, box_size=10, border=5)\n    # add_date :  pass the input text\n    qr.add_data(url)\n    # converting into image\n    qr.make(fit=True)\n    # specify the foreground and background color for the img\n    img = qr.make_image(fill='black', back_color='white')\n    # store the image\n    img.save('qrcode_img.png')\n",
          "from . import image",
          "",
          "",
          "[metadata]\nname = python-office\nversion = 0.0.14\ndescription = python for office\nlong_description = file: README.md\nlong_description_content_type = text/markdown\nurl = http://www.python4office.cn/python-office/profile/\nauthor = CoderWanFeng\nauthor_email = 1957875073@qq.com\nlicense = MIT\nlicense_file = LICENSE\nplatforms = any\n\nproject_urls = \n\tBug Tracker = https://gitee.com/CoderWanFeng/python-office/issues\n\tDocumentation = https://gitee.com/CoderWanFeng/python-office/blob/master/README.md\n\tSource Code = https://gitee.com/CoderWanFeng/python-office/\n\n[options]\npackages = find:\ninstall_requires = \n\tpandas\n\txlrd\n\txlwt\n\txlutils\n\txlwings\n\topenpyxl\n\tpython-docx\n\tpython-pptx\n\tPyPDF2\n\tscrapy\n\tdjango\n\tflask\n\teasyocr\n\treportlab\n\trich >= 9.13.0\n\tmoviepy\n\tfpdf\n\tqrcode\n\ttranslate\npython_requires = >=3.6\ninclude_package_data = True\nzip_safe = False\n\n",
          "",
          ""
        ],
        "test_patch": "",
        "patch_preview": "From 7419f8cb4f3e81cf8be6c1d3ac8bcf5fecf98743 Mon Sep 17 00:00:00 2001\nFrom: CoderWanFeng <1529577833@qq.com>\nDate: Mon, 9 May 2022 19:47:53 +0800\nSubject: [PATCH] add pdf password\n\n---\n README.md                            |  9 ++++\n office/pdf.py                        | 80 +++++++++-------------------\n office/tools.py                      | 11 ++++\n service/__init__.py                  |  3 +-\n service/pdf/__init__.py              |  1 +\n service/pdf/add_watermark_service.py | 53 ++++++++++++"
      },
      "patch": {
        "length": 9037,
        "files_changed": 9,
        "lines_added": 115,
        "lines_deleted": 56,
        "net_change": 59,
        "changed_files": [
          {
            "file": "README.md",
            "added": 9,
            "deleted": 0
          },
          {
            "file": "office/pdf.py",
            "added": 26,
            "deleted": 54
          },
          {
            "file": "office/tools.py",
            "added": 11,
            "deleted": 0
          },
          {
            "file": "service/__init__.py",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "service/pdf/__init__.py",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "service/pdf/add_watermark_service.py",
            "added": 53,
            "deleted": 0
          },
          {
            "file": "setup.cfg",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "utils/__init__.py",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "utils/progress.py",
            "added": 11,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 26,
        "total_lines": 1685,
        "total_bytes": 125655,
        "python_files": 16,
        "python_lines": 429,
        "file_extensions": {
          ".txt": 1,
          "": 1,
          ".cfg": 1,
          ".py": 16,
          ".sh": 1,
          ".md": 5,
          ".xmind": 1
        },
        "largest_files": [
          {
            "path": "Documentation/python-office è‡ªåŠ¨åŒ–åŠžå…¬.xmind",
            "size": 79531,
            "lines": 472,
            "extension": ".xmind"
          },
          {
            "path": "LICENSE",
            "size": 11357,
            "lines": 201,
            "extension": ""
          },
          {
            "path": "README.md",
            "size": 8411,
            "lines": 195,
            "extension": ".md"
          },
          {
            "path": "allpackages.txt",
            "size": 1797,
            "lines": 99,
            "extension": ".txt"
          },
          {
            "path": "Documentation/function-list-001.md",
            "size": 2838,
            "lines": 96,
            "extension": ".md"
          },
          {
            "path": "service/image/add_watermark_service.py",
            "size": 2730,
            "lines": 92,
            "extension": ".py"
          },
          {
            "path": "office/pdf.py",
            "size": 3085,
            "lines": 83,
            "extension": ".py"
          },
          {
            "path": "office/image.py",
            "size": 2262,
            "lines": 66,
            "extension": ".py"
          },
          {
            "path": "Documentation/txt2wordcloud.md",
            "size": 2915,
            "lines": 56,
            "extension": ".md"
          },
          {
            "path": "Documentation/pdf-add-water-mark.md",
            "size": 2376,
            "lines": 54,
            "extension": ".md"
          }
        ]
      }
    },
    {
      "tar_file_name": "Delgan#loguru#pull#1240",
      "repo_name": "Delgan#loguru#pull#1240",
      "success": true,
      "error": null,
      "commit": {
        "sha": "fddcd4556c01bf7add84a265ee79a41be06647c7",
        "message": "Add unit test ensuring \"lazy\" args are called exactly once",
        "author": {
          "name": "Delgan",
          "email": "delgan.py@gmail.com",
          "date": "2024-11-26T11:26:08Z"
        },
        "html_url": "https://github.com/Delgan/loguru/commit/fddcd4556c01bf7add84a265ee79a41be06647c7",
        "api_url": "https://api.github.com/repos/Delgan/loguru/commits/fddcd4556c01bf7add84a265ee79a41be06647c7"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Delgan#loguru#pull#1240",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Delgan#loguru#pull#1240.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Delgan#loguru#pull#1240/source_code"
      },
      "pr": {
        "number": 1240,
        "title": "Fix \"ValueError\" using Loguru with Cython (when missing stack frame)",
        "body": "Fix #88.\r\nFix #680.\r\n\r\nIn Cython, calling `logger.info()` used to cause a `ValueError(\"call stack is not deep enough\")` which happens because of Cython optimizations.\r\n\r\nAs a workaround, this PR wraps the call into a `try / except` clause and generate \"dummy\" information if the frame failed to be retrieve:\r\n- module name -> `None`\r\n- file name -> `\"<unknown>\"`\r\n- function name -> `\"<unknown>\"`\r\n- line no -> `0`\r\n\r\nNot ideal, but certainly better than if Loguru didn't work at all.\r\n\r\nExample of output:\r\n\r\n```\r\n2024-11-26 15:58:48.985 | INFO | None:<unknown>:0 - Message from Cython!\r\n```",
        "state": "closed",
        "created_at": "2024-11-26T16:33:06Z",
        "updated_at": "2024-11-29T22:45:36Z",
        "merged_at": "2024-11-26T16:35:10Z",
        "html_url": "https://github.com/Delgan/loguru/pull/1240",
        "user": "Delgan",
        "additions": 62,
        "deletions": 24,
        "changed_files": 8,
        "commits": 1
      },
      "swebench": {
        "instance_id": "Delgan_loguru-1240",
        "repo": "/Delgan/loguru",
        "base_commit": "fddcd4556c01bf7add84a265ee79a41be06647c7",
        "problem_statement": {
          "title": "call stack is not deep enough (Cython)",
          "body": "Hi @Delgan \r\n\r\nWhen using loguru 0.2.5 inside python script it works as expected, but when i compile this file to an executable using Cython i get this error and program terminates :\r\n **call stack is not deep enough**\r\n\r\nMore info of the error : \r\n```\r\nTraceback (most recent call last):\r\n  File \"test_layer.py\", line 66, in init test_layer\r\n    logger.warning('just test')\r\n  File \"/usr/local/lib/python3.5/dist-packages/loguru/_logger.py\", line 1406, in log_function\r\n    frame = get_frame(_self._depth + 1)\r\nValueError: call stack is not deep enough\r\n```\r\nDo you have any idea why ?\r\n\r\nI tried increasing the execution depth limit inside the script that i import loguru by using sys.setrecursionlimit(3000) but still issue remains..\r\n\r\nOS: raspbian stretch lite\r\nCython-0.29.7\r\n\r\n\r\n"
        },
        "edit_files": [
          "CHANGELOG.rst",
          "docs/resources/recipes.rst",
          "loguru/_logger.py",
          "tests/conftest.py",
          "tests/test_activation.py",
          "tests/test_add_option_filter.py",
          "tests/test_formatting.py",
          "tests/test_opt.py"
        ],
        "oracle_files": [
          "`Unreleased`_\n=============\n\n- Fix possible ``RuntimeError`` when removing all handlers with ``logger.remove()`` due to thread-safety issue (`#1183 <https://github.com/Delgan/loguru/issues/1183>`_, thanks `@jeremyk <https://github.com/jeremyk>`_).\n- Fix ``diagnose=True`` option of exception formatting not working as expected with Python 3.13 (`#1235 <https://github.com/Delgan/loguru/issues/1235>`_, thanks `@etianen <https://github.com/etianen>`_).\n- Fix non-standard level names not fully compatible with ``logging.Formatter()`` (`#1231 <https://github.com/Delgan/loguru/issues/1231>`_, thanks `@yechielb2000 <https://github.com/yechielb2000>`_).\n- Fix inability to display a literal ``\"\\\"`` immediately before color markups (`#988 <https://github.com/Delgan/loguru/issues/988>`_).\n- Fix possible infinite recursion when an exception is raised from a ``__repr__``  method decorated with ``logger.catch()`` (`#1044 <https://github.com/Delgan/loguru/issues/1044>`_).\n- Improve performance of ``datetime`` formatting while logging messages (`#1201 <https://github.com/Delgan/loguru/issues/1201>`_, thanks `@trim21 <https://github.com/trim21>`_).\n- Reduce startup time in the presence of installed but unused ``IPython`` third-party library (`#1001 <https://github.com/Delgan/loguru/issues/1001>`_, thanks `@zakstucke <https://github.com/zakstucke>`_).\n\n\n`0.7.2`_ (2023-09-11)\n=====================\n\n- Add support for formatting of ``ExceptionGroup`` errors (`#805 <https://github.com/Delgan/loguru/issues/805>`_).\n- Fix possible ``RuntimeError`` when using ``multiprocessing.set_start_method()`` after importing the ``logger`` (`#974 <https://github.com/Delgan/loguru/issues/974>`_).\n- Fix formatting of possible ``__notes__`` attached to an ``Exception`` (`#980 <https://github.com/Delgan/loguru/issues/980>`_).\n\n\n`0.7.1`_ (2023-09-04)\n=====================\n\n- Add a new ``context`` optional argument to ``logger.add()`` specifying ``multiprocessing`` context (like ``\"spawn\"`` or ``\"fork\"``) to be used internally instead of the default one (`#851 <https://github.com/Delgan/loguru/issues/851>`_).\n- Add support for true colors on Windows using ANSI/VT console when available (`#934 <https://github.com/Delgan/loguru/issues/934>`_, thanks `@tunaflsh <https://github.com/tunaflsh>`_).\n- Fix possible deadlock when calling ``logger.complete()`` with concurrent logging of an asynchronous sink (`#906 <https://github.com/Delgan/loguru/issues/906>`_).\n- Fix file possibly rotating too early or too late when re-starting an application around midnight (`#894 <https://github.com/Delgan/loguru/issues/894>`_).\n- Fix inverted ``\"<hide>\"`` and ``\"<strike>\"`` color tags (`#943 <https://github.com/Delgan/loguru/pull/943>`_, thanks `@tunaflsh <https://github.com/tunaflsh>`_).\n- Fix possible untraceable errors raised when logging non-unpicklable ``Exception`` instances while using ``enqueue=True`` (`#329 <https://github.com/Delgan/loguru/issues/329>`_).\n- Fix possible errors raised when logging non-picklable ``Exception`` instances while using ``enqueue=True`` (`#342 <https://github.com/Delgan/loguru/issues/342>`_, thanks `@ncoudene <https://github.com/ncoudene>`_).\n- Fix missing seconds and microseconds when formatting timezone offset that requires such accuracy (`#961 <https://github.com/Delgan/loguru/issues/961>`_).\n- Raise ``ValueError`` if an attempt to use nanosecond precision for time formatting is detected (`#855 <https://github.com/Delgan/loguru/issues/855>`_).\n\n\n`0.7.0`_ (2023-04-10)\n=====================\n\n- Update ``InterceptHandler`` recipe to make it compatible with Python 3.11 (`#654 <https://github.com/Delgan/loguru/issues/654>`_).\n- Add a new ``watch`` optional argument to file sinks in order to automatically re-create possibly deleted or changed file (`#471 <https://github.com/Delgan/loguru/issues/471>`_).\n- Make ``patch()`` calls cumulative instead of overriding the possibly existing patching function (`#462 <https://github.com/Delgan/loguru/issues/462>`_).\n- Make sinks added with ``enqueue=True`` and ``catch=False`` still process logged messages in case of internal exception (`#833 <https://github.com/Delgan/loguru/issues/833>`_).\n- Avoid possible deadlocks caused by re-using the logger inside a sink, a signal handler or a ``__del__`` method. Since the logger is not re-entrant, such misuse will be detected and will now generate a ``RuntimeError`` (`#712 <https://github.com/Delgan/loguru/issues/712>`_, thanks `@jacksmith15 <https://github.com/jacksmith15>`_).\n- Fix file sink rotation using an aware ``datetime.time`` for which the timezone was ignored (`#697 <https://github.com/Delgan/loguru/issues/697>`_).\n- Fix logs colorization not automatically enabled for Jupyter Notebook and Google Colab (`#494 <https://github.com/Delgan/loguru/issues/494>`_).\n- Fix logs colorization not automatically enabled for Github Actions and others CI platforms (`#604 <https://github.com/Delgan/loguru/issues/604>`_).\n- Fix ``logger.complete()`` possibly hanging forever when ``enqueue=True`` and ``catch=False`` if internal thread killed due to ``Exception`` raised by sink (`#647 <https://github.com/Delgan/loguru/issues/647>`_).\n- Fix incompatibility with ``freezegun`` library used to simulate time (`#600 <https://github.com/Delgan/loguru/issues/600>`_).\n- Raise exception if ``logger.catch()`` is used to wrap a class instead of a function to avoid unexpected behavior (`#623 <https://github.com/Delgan/loguru/issues/623>`_).\n\n\n`0.6.0`_ (2022-01-29)\n=====================\n\n- Remove internal use of ``pickle.loads()`` to fix the (finally rejected) security vulnerability referenced as `CVE-2022-0329 <https://nvd.nist.gov/vuln/detail/CVE-2022-0329>`_ (`#563 <https://github.com/Delgan/loguru/issues/563>`_).\n- Modify coroutine sink to make it discard log messages when ``loop=None`` and no event loop is running (due to internally using ``asyncio.get_running_loop()`` in place of ``asyncio.get_event_loop()``).\n- Remove the possibility to add a coroutine sink with ``enqueue=True`` if ``loop=None`` and no event loop is running.\n- Change default encoding of file sink to be ``utf8`` instead of ``locale.getpreferredencoding()`` (`#339 <https://github.com/Delgan/loguru/issues/339>`_).\n- Prevent non-ascii characters to be escaped while logging JSON message with ``serialize=True`` (`#575 <https://github.com/Delgan/loguru/pull/575>`_, thanks `@ponponon <https://github.com/ponponon>`_).\n- Fix ``flake8`` errors and improve code readability (`#353 <https://github.com/Delgan/loguru/issues/353>`_, thanks `@AndrewYakimets <https://github.com/AndrewYakimets>`_).\n\n\n`0.5.3`_ (2020-09-20)\n=====================\n\n- Fix child process possibly hanging at exit while combining ``enqueue=True`` with third party library like ``uwsgi`` (`#309 <https://github.com/Delgan/loguru/issues/309>`_, thanks `@dstlmrk <https://github.com/dstlmrk>`_).\n- Fix possible exception during formatting of non-string messages (`#331 <https://github.com/Delgan/loguru/issues/331>`_).\n\n\n`0.5.2`_ (2020-09-06)\n=====================\n\n- Fix ``AttributeError`` within handlers using ``serialize=True`` when calling ``logger.exception()`` outside of the context of an exception (`#296 <https://github.com/Delgan/loguru/issues/296>`_).\n- Fix error while logging an exception containing a non-picklable ``value`` to a handler with ``enqueue=True`` (`#298 <https://github.com/Delgan/loguru/issues/298>`_).\n- Add support for async callable classes (with ``__call__`` method) used as sinks (`#294 <https://github.com/Delgan/loguru/pull/294>`_, thanks `@jessekrubin <https://github.com/jessekrubin>`_).\n\n\n`0.5.1`_ (2020-06-12)\n=====================\n\n- Modify the way the ``extra`` dict is used by ``LogRecord`` in order to prevent possible ``KeyError`` with standard ``logging`` handlers (`#271 <https://github.com/Delgan/loguru/issues/271>`_).\n- Add a new ``default`` optional argument to ``logger.catch()``, it should be the returned value by the decorated function in case an error occurred (`#272 <https://github.com/Delgan/loguru/issues/272>`_).\n- Fix ``ValueError`` when using ``serialize=True`` in combination with ``logger.catch()`` or ``logger.opt(record=True)`` due to circular reference of the ``record`` dict (`#286 <https://github.com/Delgan/loguru/issues/286>`_).\n\n\n`0.5.0`_ (2020-05-17)\n=====================\n\n- Remove the possibility to modify the severity ``no`` of levels once they have been added in order to prevent surprising behavior (`#209 <https://github.com/Delgan/loguru/issues/209>`_).\n- Add better support for \"structured logging\" by automatically adding ``**kwargs`` to the ``extra`` dict besides using these arguments to format the message. This behavior can be disabled by setting the new ``.opt(capture=False)`` parameter (`#2 <https://github.com/Delgan/loguru/issues/2>`_).\n- Add a new ``onerror`` optional argument to ``logger.catch()``, it should be a function which will be called when an exception occurs in order to customize error handling (`#224 <https://github.com/Delgan/loguru/issues/224>`_).\n- Add a new ``exclude`` optional argument to ``logger.catch()``, is should be a type of exception to be purposefully ignored and propagated to the caller without being logged (`#248 <https://github.com/Delgan/loguru/issues/248>`_).\n- Modify ``complete()`` to make it callable from non-asynchronous functions, it can thus be used if ``enqueue=True`` to make sure all messages have been processed (`#231 <https://github.com/Delgan/loguru/issues/231>`_).\n- Fix possible deadlocks on Linux when ``multiprocessing.Process()`` collides with ``enqueue=True`` or ``threading`` (`#231 <https://github.com/Delgan/loguru/issues/231>`_).\n- Fix ``compression`` function not executable concurrently due to file renaming (to resolve conflicts) being performed after and not before it (`#243 <https://github.com/Delgan/loguru/issues/243>`_).\n- Fix the filter function listing files for ``retention`` being too restrictive, it now matches files based on the pattern ``\"basename(.*).ext(.*)\"`` (`#229 <https://github.com/Delgan/loguru/issues/229>`_).\n- Fix the impossibility to ``remove()`` a handler if an exception is raised while the sink' ``stop()`` function is called (`#237 <https://github.com/Delgan/loguru/issues/237>`_).\n- Fix file sink left in an unstable state if an exception occurred during ``retention`` or ``compression`` process (`#238 <https://github.com/Delgan/loguru/issues/238>`_).\n- Fix situation where changes made to ``record[\"message\"]`` were unexpectedly ignored when ``opt(colors=True)``, causing \"out-of-date\" ``message`` to be logged due to implementation details (`#221 <https://github.com/Delgan/loguru/issues/221>`_).\n- Fix possible exception if a stream having an ``isatty()`` method returning ``True`` but not being compatible with ``colorama`` is used on Windows (`#249 <https://github.com/Delgan/loguru/issues/249>`_).\n- Fix exceptions occurring in coroutine sinks never retrieved and hence causing warnings (`#227 <https://github.com/Delgan/loguru/issues/227>`_).\n\n\n`0.4.1`_ (2020-01-19)\n=====================\n\n- Deprecate the ``ansi`` parameter of ``.opt()`` in favor of ``colors`` which is a name more appropriate.\n- Prevent unrelated files and directories to be incorrectly collected thus causing errors during the ``retention`` process (`#195 <https://github.com/Delgan/loguru/issues/195>`_, thanks `@gazpachoking <https://github.com/gazpachoking>`_).\n- Strip color markups contained in ``record[\"message\"]`` when logging with ``.opt(ansi=True)`` instead of leaving them as is (`#198 <https://github.com/Delgan/loguru/issues/198>`_).\n- Ignore color markups contained in ``*args`` and ``**kwargs`` when logging with ``.opt(ansi=True)``, leave them as is instead of trying to use them to colorize the message which could cause undesirable errors (`#197 <https://github.com/Delgan/loguru/issues/197>`_).\n\n\n`0.4.0`_ (2019-12-02)\n=====================\n\n- Add support for coroutine functions used as sinks and add the new ``logger.complete()`` asynchronous method to ``await`` them (`#171 <https://github.com/Delgan/loguru/issues/171>`_).\n- Add a way to filter logs using one level per module in the form of a ``dict`` passed to the ``filter`` argument (`#148 <https://github.com/Delgan/loguru/issues/148>`_).\n- Add type hints to annotate the public methods using a ``.pyi`` stub file (`#162 <https://github.com/Delgan/loguru/issues/162>`_).\n- Add support for ``copy.deepcopy()`` of the ``logger`` allowing multiple independent loggers with separate set of handlers (`#72 <https://github.com/Delgan/loguru/issues/72>`_).\n- Add the possibility to convert ``datetime`` to UTC before formatting (in logs and filenames) by adding ``\"!UTC\"`` at the end of the time format specifier (`#128 <https://github.com/Delgan/loguru/issues/128>`_).\n- Add the level ``name`` as the first argument of namedtuple returned by the ``.level()`` method.\n- Remove ``class`` objects from the list of supported sinks and restrict usage of ``**kwargs`` in ``.add()`` to file sink only. User is in charge of instantiating sink and wrapping additional keyword arguments if needed, before passing it to the ``.add()`` method.\n- Rename the ``logger.configure()`` keyword argument ``patch`` to ``patcher`` so it better matches the signature of ``logger.patch()``.\n- Fix incompatibility with ``multiprocessing`` on Windows by entirely refactoring the internal structure of the ``logger`` so it can be inherited by child processes along with added handlers (`#108 <https://github.com/Delgan/loguru/issues/108>`_).\n- Fix ``AttributeError`` while using a file sink on some distributions (like Alpine Linux) missing the ``os.getxattr`` and ``os.setxattr`` functions (`#158 <https://github.com/Delgan/loguru/pull/158>`_, thanks `@joshgordon <https://github.com/joshgordon>`_).\n- Fix values wrongly displayed for keyword arguments during exception formatting with ``diagnose=True`` (`#144 <https://github.com/Delgan/loguru/issues/144>`_).\n- Fix logging messages wrongly chopped off at the end while using standard ``logging.Handler`` sinks with ``.opt(raw=True)`` (`#136 <https://github.com/Delgan/loguru/issues/136>`_).\n- Fix potential errors during rotation if destination file exists due to large resolution clock on Windows (`#179 <https://github.com/Delgan/loguru/issues/179>`_).\n- Fix an error using a ``filter`` function \"by name\" while receiving a log with ``record[\"name\"]`` equals to ``None``.\n- Fix incorrect record displayed while handling errors (if ``catch=True``) occurring because of non-picklable objects (if ``enqueue=True``).\n- Prevent hypothetical ``ImportError`` if a Python installation is missing the built-in ``distutils`` module (`#118 <https://github.com/Delgan/loguru/issues/118>`_).\n- Raise ``TypeError`` instead of ``ValueError`` when a ``logger`` method is called with argument of invalid type.\n- Raise ``ValueError`` if the built-in ``format()`` and ``filter()`` functions are respectively used as ``format`` and ``filter`` arguments of the ``add()`` method. This helps the user to understand the problem, as such a mistake can quite easily occur (`#177 <https://github.com/Delgan/loguru/issues/177>`_).\n- Remove inheritance of some record dict attributes to ``str`` (for ``\"level\"``, ``\"file\"``, ``\"thread\"`` and ``\"process\"``).\n- Give a name to the worker thread used when ``enqueue=True`` (`#174 <https://github.com/Delgan/loguru/pull/174>`_, thanks `@t-mart <https://github.com/t-mart>`_).\n\n\n`0.3.2`_ (2019-07-21)\n=====================\n\n- Fix exception during import when executing Python with ``-s`` and ``-S`` flags causing ``site.USER_SITE`` to be missing (`#114 <https://github.com/Delgan/loguru/issues/114>`_).\n\n\n`0.3.1`_ (2019-07-13)\n=====================\n\n- Fix ``retention`` and ``rotation`` issues when file sink initialized with ``delay=True`` (`#113 <https://github.com/Delgan/loguru/issues/113>`_).\n- Fix ``\"sec\"`` no longer recognized as a valid duration unit for file ``rotation`` and ``retention`` arguments.\n- Ensure stack from the caller is displayed while formatting exception of a function decorated with ``@logger.catch`` when ``backtrace=False``.\n- Modify datetime used to automatically rename conflicting file when rotating (it happens if file already exists because ``\"{time}\"`` not presents in filename) so it's based on the file creation time rather than the current time.\n\n\n`0.3.0`_ (2019-06-29)\n=====================\n\n- Remove all dependencies previously needed by ``loguru`` (on Windows platform, it solely remains ``colorama`` and ``win32-setctime``).\n- Add a new ``logger.patch()`` method which can be used to modify the record dict on-the-fly before it's being sent to the handlers.\n- Modify behavior of sink option ``backtrace`` so it only extends the stacktrace upward, the display of variables values is now controlled with the new ``diagnose`` argument (`#49 <https://github.com/Delgan/loguru/issues/49>`_).\n- Change behavior of ``rotation`` option in file sinks: it is now based on the file creation time rather than the current time, note that proper support may differ depending on your platform (`#58 <https://github.com/Delgan/loguru/issues/58>`_).\n- Raise errors on unknowns color tags rather than silently ignoring them (`#57 <https://github.com/Delgan/loguru/issues/57>`_).\n- Add the possibility to auto-close color tags by using ``</>`` (e.g. ``<yellow>message</>``).\n- Add coloration of exception traceback even if ``diagnose`` and ``backtrace`` options are ``False``.\n- Add a way to limit the depth of formatted exceptions traceback by setting the conventional ``sys.tracebacklimit`` variable (`#77 <https://github.com/Delgan/loguru/issues/77>`_).\n- Add ``__repr__`` value to the ``logger`` for convenient debugging (`#84 <https://github.com/Delgan/loguru/issues/84>`_).\n- Remove colors tags mixing directives (e.g. ``<red,blue>``) for simplification.\n- Make the ``record[\"exception\"]`` attribute unpackable as a ``(type, value, traceback)`` tuple.\n- Fix error happening in some rare circumstances because ``frame.f_globals`` dict did not contain ``\"__name__\"`` key and hence prevented Loguru to retrieve the module's name. From now, ``record[\"name\"]`` will be equal to ``None`` in such case (`#62 <https://github.com/Delgan/loguru/issues/62>`_).\n- Fix logging methods not being serializable with ``pickle`` and hence raising exception while being passed to some ``multiprocessing`` functions (`#102 <https://github.com/Delgan/loguru/issues/102>`_).\n- Fix exception stack trace not colorizing source code lines on Windows.\n- Fix possible ``AttributeError`` while formatting exceptions within a ``celery`` task (`#52 <https://github.com/Delgan/loguru/issues/52>`_).\n- Fix ``logger.catch`` decorator not working with generator and coroutine functions (`#75 <https://github.com/Delgan/loguru/issues/75>`_).\n- Fix ``record[\"path\"]`` case being normalized for no necessary reason (`#85 <https://github.com/Delgan/loguru/issues/85>`_).\n- Fix some Windows terminal emulators (mintty) not correctly detected as supporting colors, causing ansi codes to be automatically stripped (`#104 <https://github.com/Delgan/loguru/issues/104>`_).\n- Fix handler added with ``enqueue=True`` stopping working if exception was raised in sink although ``catch=True``.\n- Fix thread-safety of ``enable()`` and ``disable()`` being called during logging.\n- Use Tox to run tests (`#41 <https://github.com/Delgan/loguru/issues/41>`_).\n\n\n`0.2.5`_ (2019-01-20)\n=====================\n\n- Modify behavior of sink option ``backtrace=False`` so it doesn't extend traceback upward automatically (`#30 <https://github.com/Delgan/loguru/issues/30>`_).\n- Fix import error on some platforms using Python 3.5 with limited ``localtime()`` support (`#33 <https://github.com/Delgan/loguru/issues/33>`_).\n- Fix incorrect time formatting of locale month using ``MMM`` and ``MMMM`` tokens (`#34 <https://github.com/Delgan/loguru/pull/34>`_, thanks `@nasyxx <https://github.com/nasyxx>`_).\n- Fix race condition permitting writing on a stopped handler.\n\n\n`0.2.4`_ (2018-12-26)\n=====================\n\n- Fix adding handler while logging which was not thread-safe (`#22 <https://github.com/Delgan/loguru/issues/22>`_).\n\n\n`0.2.3`_ (2018-12-16)\n=====================\n\n- Add support for PyPy.\n- Add support for Python 3.5.\n- Fix incompatibility with ``awscli`` by downgrading required ``colorama`` dependency version (`#12 <https://github.com/Delgan/loguru/issues/12>`_).\n\n\n`0.2.2`_ (2018-12-12)\n=====================\n\n- Deprecate ``logger.start()`` and ``logger.stop()`` methods in favor of ``logger.add()`` and ``logger.remove()`` (`#3 <https://github.com/Delgan/loguru/issues/3>`_).\n- Fix ignored formatting while using ``logging.Handler`` sinks (`#4 <https://github.com/Delgan/loguru/issues/4>`_).\n- Fix impossibility to set empty environment variable color on Windows (`#7 <https://github.com/Delgan/loguru/issues/7>`_).\n\n\n`0.2.1`_ (2018-12-08)\n=====================\n\n- Fix typo preventing README to be correctly displayed on PyPI.\n\n\n`0.2.0`_ (2018-12-08)\n=====================\n\n- Remove the ``parser`` and refactor it into the ``logger.parse()`` method.\n- Remove the ``notifier`` and its dependencies (``pip install notifiers`` should be used instead).\n\n\n`0.1.0`_ (2018-12-07)\n=====================\n\n- Add logger.\n- Add notifier.\n- Add parser.\n\n\n`0.0.1`_ (2017-09-04)\n=====================\n\nInitial release.\n\n\n.. _Unreleased: https://github.com/delgan/loguru/compare/0.7.2...master\n.. _0.7.2: https://github.com/delgan/loguru/releases/tag/0.7.2\n.. _0.7.1: https://github.com/delgan/loguru/releases/tag/0.7.1\n.. _0.7.0: https://github.com/delgan/loguru/releases/tag/0.7.0\n.. _0.6.0: https://github.com/delgan/loguru/releases/tag/0.6.0\n.. _0.5.3: https://github.com/delgan/loguru/releases/tag/0.5.3\n.. _0.5.2: https://github.com/delgan/loguru/releases/tag/0.5.2\n.. _0.5.1: https://github.com/delgan/loguru/releases/tag/0.5.1\n.. _0.5.0: https://github.com/delgan/loguru/releases/tag/0.5.0\n.. _0.4.1: https://github.com/delgan/loguru/releases/tag/0.4.1\n.. _0.4.0: https://github.com/delgan/loguru/releases/tag/0.4.0\n.. _0.3.2: https://github.com/delgan/loguru/releases/tag/0.3.2\n.. _0.3.1: https://github.com/delgan/loguru/releases/tag/0.3.1\n.. _0.3.0: https://github.com/delgan/loguru/releases/tag/0.3.0\n.. _0.2.5: https://github.com/delgan/loguru/releases/tag/0.2.5\n.. _0.2.4: https://github.com/delgan/loguru/releases/tag/0.2.4\n.. _0.2.3: https://github.com/delgan/loguru/releases/tag/0.2.3\n.. _0.2.2: https://github.com/delgan/loguru/releases/tag/0.2.2\n.. _0.2.1: https://github.com/delgan/loguru/releases/tag/0.2.1\n.. _0.2.0: https://github.com/delgan/loguru/releases/tag/0.2.0\n.. _0.1.0: https://github.com/delgan/loguru/releases/tag/0.1.0\n.. _0.0.1: https://github.com/delgan/loguru/releases/tag/0.0.1\n",
          "Code snippets and recipes for ``loguru``\n========================================\n\n.. highlight:: python3\n\n.. |print| replace:: :func:`print()`\n.. |open| replace:: :func:`open()`\n.. |sys.__stdout__| replace:: :data:`sys.__stdout__`\n.. |sys.stdout| replace:: :data:`sys.stdout`\n.. |sys.stderr| replace:: :data:`sys.stderr`\n.. |warnings| replace:: :mod:`warnings`\n.. |warnings.showwarning| replace:: :func:`warnings.showwarning`\n.. |warnings.warn| replace:: :func:`warnings.warn`\n.. |contextlib.redirect_stdout| replace:: :func:`contextlib.redirect_stdout`\n.. |copy.deepcopy| replace:: :func:`copy.deepcopy`\n.. |os.fork| replace:: :func:`os.fork`\n.. |os.umask| replace:: :func:`os.umask`\n.. |multiprocessing| replace:: :mod:`multiprocessing`\n.. |pickle| replace:: :mod:`pickle`\n.. |traceback| replace:: :mod:`traceback`\n.. |Thread| replace:: :class:`~threading.Thread`\n.. |Process| replace:: :class:`~multiprocessing.Process`\n.. |Pool| replace:: :class:`~multiprocessing.pool.Pool`\n.. |Pool.map| replace:: :meth:`~multiprocessing.pool.Pool.map`\n.. |Pool.apply| replace:: :meth:`~multiprocessing.pool.Pool.apply`\n.. |sys.stdout.reconfigure| replace:: :meth:`sys.stdout.reconfigure() <io.TextIOWrapper.reconfigure>`\n.. |UnicodeEncodeError| replace:: :exc:`UnicodeEncodeError`\n\n.. |add| replace:: :meth:`~loguru._logger.Logger.add()`\n.. |remove| replace:: :meth:`~loguru._logger.Logger.remove()`\n.. |enable| replace:: :meth:`~loguru._logger.Logger.enable()`\n.. |disable| replace:: :meth:`~loguru._logger.Logger.disable()`\n.. |bind| replace:: :meth:`~loguru._logger.Logger.bind()`\n.. |patch| replace:: :meth:`~loguru._logger.Logger.patch()`\n.. |opt| replace:: :meth:`~loguru._logger.Logger.opt()`\n.. |log| replace:: :meth:`~loguru._logger.Logger.log()`\n.. |level| replace:: :meth:`~loguru._logger.Logger.level()`\n.. |configure| replace:: :meth:`~loguru._logger.Logger.configure()`\n.. |complete| replace:: :meth:`~loguru._logger.Logger.complete()`\n\n.. _`unicode`: https://docs.python.org/3/howto/unicode.html\n\n.. |if-name-equals-main| replace:: ``if __name__ == \"__main__\":``\n.. _if-name-equals-main: https://docs.python.org/3/library/__main__.html#idiomatic-usage\n\n.. |logot| replace:: ``logot``\n.. _logot: https://logot.readthedocs.io/\n\n.. |pytest| replace:: ``pytest``\n.. _pytest: https://docs.pytest.org/en/latest/\n\n.. |stackprinter| replace:: ``stackprinter``\n.. _stackprinter: https://github.com/cknd/stackprinter\n\n.. |zmq| replace:: ``zmq``\n.. _zmq: https://github.com/zeromq/pyzmq\n\n.. _`GH#88`: https://github.com/Delgan/loguru/issues/88\n.. _`GH#132`: https://github.com/Delgan/loguru/issues/132\n\n\nSecurity considerations when using Loguru\n-----------------------------------------\n\nFirstly, if you use |pickle| to load log messages (e.g. from the network), make sure the source is trustable or sign the data to verify its authenticity before deserializing it. If you do not take these precautions, malicious code could be executed by an attacker. You can read more details in this article: `Whatâ€™s so dangerous about pickles? <https://intoli.com/blog/dangerous-pickles/>`_\n\n.. code::\n\n    import hashlib\n    import hmac\n    import pickle\n\n    def client(connection):\n        data = pickle.dumps(\"Log message\")\n        digest =  hmac.digest(b\"secret-shared-key\", data, hashlib.sha1)\n        connection.send(digest + b\" \" + data)\n\n    def server(connection):\n        expected_digest, data = connection.read().split(b\" \", 1)\n        data_digest = hmac.digest(b\"secret-shared-key\", data, hashlib.sha1)\n        if not hmac.compare_digest(data_digest, expected_digest):\n            print(\"Integrity error\")\n        else:\n            message = pickle.loads(data)\n            logger.info(message)\n\n\nYou should also avoid logging a message that could be maliciously hand-crafted by an attacker. Calling ``logger.debug(message, value)`` is roughly equivalent to calling ``print(message.format(value))`` and the same safety rules apply. In particular, an attacker could force printing of assumed hidden variables of your application. Here is an article explaining the possible vulnerability: `Be Careful with Python's New-Style String Format <https://lucumr.pocoo.org/2016/12/29/careful-with-str-format/>`_.\n\n.. code::\n\n    SECRET_KEY = 'Y0UC4NTS33Th1S!'\n\n    class SomeValue:\n        def __init__(self, value):\n            self.value = value\n\n    # If user types \"{value.__init__.__globals__[SECRET_KEY]}\" then the secret key is displayed.\n    message = \"[Custom message] \" + input()\n    logger.info(message, value=SomeValue(10))\n\n\nAnother danger due to external input is the possibility of a log injection attack. Consider that you may need to escape user values before logging them: `Is your Python code vulnerable to log injection? <https://dev.arie.bovenberg.net/blog/is-your-python-code-vulnerable-to-log-injection/>`_\n\n.. code::\n\n    logger.add(\"file.log\", format=\"{level} {message}\")\n\n    # If value is \"Josh logged in.\\nINFO User James\" then there will appear to be two log entries.\n    username = external_data()\n    logger.info(\"User \" + username + \" logged in.\")\n\n\nNote that by default, Loguru will display the value of existing variables when an ``Exception`` is logged. This is very useful for debugging but could lead to credentials appearing in log files. Make sure to turn it off in production (or set the ``LOGURU_DIAGNOSE=NO`` environment variable).\n\n.. code::\n\n    logger.add(\"out.log\", diagnose=False)\n\n\nAnother thing you should consider is to change the access permissions of your log file. Loguru creates files using the built-in |open| function, which means by default they might be read by a different user than the owner. If this is not desirable, be sure to modify the default access rights.\n\n.. code::\n\n    def opener(file, flags):\n        return os.open(file, flags, 0o600)\n\n    logger.add(\"combined.log\", opener=opener)\n\n\nAvoiding logs to be printed twice on the terminal\n-------------------------------------------------\n\nThe logger is pre-configured for convenience with a default handler which writes messages to |sys.stderr|. You should |remove| it first if you plan to |add| another handler logging messages to the console, otherwise you may end up with duplicated logs.\n\n.. code::\n\n    logger.remove()  # Remove all handlers added so far, including the default one.\n    logger.add(sys.stderr, level=\"WARNING\")\n\n\nChanging the level of an existing handler\n-----------------------------------------\n\nOnce a handler has been added, it is actually not possible to update it. This is a deliberate choice in order to keep the Loguru's API minimal. Several solutions are possible, tough, if you need to change the configured ``level`` of a handler. Chose the one that best fits your use case.\n\nThe most straightforward workaround is to |remove| your handler and then re-|add| it with the updated ``level`` parameter. To do so, you have to keep a reference to the identifier number returned while adding a handler::\n\n    handler_id = logger.add(sys.stderr, level=\"WARNING\")\n\n    logger.info(\"Logging 'WARNING' or higher messages only\")\n\n    ...\n\n    logger.remove(handler_id)  # For the default handler, it's actually '0'.\n    logger.add(sys.stderr, level=\"DEBUG\")\n\n    logger.debug(\"Logging 'DEBUG' messages too\")\n\n\nAlternatively, you can combine the |bind| method with the ``filter`` argument to provide a function dynamically filtering logs based on their level::\n\n    def my_filter(record):\n        if record[\"extra\"].get(\"warn_only\"):  # \"warn_only\" is bound to the logger and set to 'True'\n            return record[\"level\"].no >= logger.level(\"WARNING\").no\n        return True  # Fallback to default 'level' configured while adding the handler\n\n\n    logger.add(sys.stderr, filter=my_filter, level=\"DEBUG\")\n\n    # Use this logger first, debug messages are filtered out\n    logger = logger.bind(warn_only=True)\n    logger.warn(\"Initialization in progress\")\n\n    # Then you can use this one to log all messages\n    logger = logger.bind(warn_only=False)\n    logger.debug(\"Back to debug messages\")\n\n\nFinally, more advanced control over handler's level can be achieved by using a callable object as the ``filter``::\n\n    class MyFilter:\n\n        def __init__(self, level):\n            self.level = level\n\n        def __call__(self, record):\n            levelno = logger.level(self.level).no\n            return record[\"level\"].no >= levelno\n\n    my_filter = MyFilter(\"WARNING\")\n    logger.add(sys.stderr, filter=my_filter, level=0)\n\n    logger.warning(\"OK\")\n    logger.debug(\"NOK\")\n\n    my_filter.level = \"DEBUG\"\n    logger.debug(\"OK\")\n\n\nConfiguring Loguru to be used by a library or an application\n------------------------------------------------------------\n\nA clear distinction must be made between the use of Loguru within a library or an application.\n\nIn case of an application, you can add handlers from anywhere in your code. It's advised though to configure the logger from within a |if-name-equals-main|_ block inside the entry file of your script.\n\nHowever, if your work is intended to be used as a library, you usually should not add any handler. This is user responsibility to configure logging according to its preferences, and it's better not to interfere with that. Indeed, since Loguru is based on a single common logger, handlers added by a library will also receive user logs, which is generally not desirable.\n\nBy default, a third-library should not emit logs except if specifically requested. For this reason, there exist the |disable| and |enable| methods. Make sure to first call ``logger.disable(\"mylib\")``. This avoids library logs to be mixed with those of the user. The user can always call ``logger.enable(\"mylib\")`` if he wants to access the logs of your library.\n\nIf you would like to ease logging configuration for your library users, it is advised to provide a function like ``configure_logger()`` in charge of adding the desired handlers. This will allow the user to activate the logging only if he needs to.\n\nTo summarize, let's look at this hypothetical package (none of the listed files are required, it all depends on how you plan your project to be used):\n\n.. code:: text\n\n    mypackage\n    â”œâ”€â”€ __init__.py\n    â”œâ”€â”€ __main__.py\n    â”œâ”€â”€ main.py\n    â””â”€â”€ mymodule.py\n\nFiles relate to Loguru as follows:\n\n* File ``__init__.py``:\n\n    * It is the entry point when your project is used as a library (``import mypackage``).\n    * It should contain ``logger.disable(\"mypackage\")`` unconditionally at the top level.\n    * It should not call ``logger.add()`` as it modifies handlers configuration.\n\n* File ``__main__.py``:\n\n    * It is the entry point when your project is used as an application (``python -m mypackage``).\n    * It can contain logging configuration unconditionally at the top level.\n\n* File ``main.py``:\n\n    * It is the entry point when your project is used as a script (``python mypackage/main.py``).\n    * It can contain logging configuration inside an ``if __name__ == \"__main__\":`` block.\n\n* File ``mymodule.py``:\n\n    * It is an internal module used by your project.\n    * It can use the ``logger`` simply by importing it.\n    * It does not need to configure anything.\n\n\n\nSending and receiving log messages across network or processes\n--------------------------------------------------------------\n\nIt is possible to transmit logs between different processes and even between different computer if needed. Once the connection is established between the two Python programs, this requires serializing the logging record in one side while re-constructing the message on the other hand.\n\nThis can be achieved using a custom sink for the client and |patch| for the server.\n\n.. code::\n\n    # client.py\n    import sys\n    import socket\n    import struct\n    import time\n    import pickle\n\n    from loguru import logger\n\n\n    class SocketHandler:\n\n        def __init__(self, host, port):\n            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.sock.connect((host, port))\n\n        def write(self, message):\n            record = message.record\n            data = pickle.dumps(record)\n            slen = struct.pack(\">L\", len(data))\n            self.sock.send(slen + data)\n\n    logger.configure(handlers=[{\"sink\": SocketHandler('localhost', 9999)}])\n\n    while 1:\n        time.sleep(1)\n        logger.info(\"Sending message from the client\")\n\n\n.. code::\n\n    # server.py\n    import socketserver\n    import pickle\n    import struct\n\n    from loguru import logger\n\n\n    class LoggingStreamHandler(socketserver.StreamRequestHandler):\n\n        def handle(self):\n            while True:\n                chunk = self.connection.recv(4)\n                if len(chunk) < 4:\n                    break\n                slen = struct.unpack('>L', chunk)[0]\n                chunk = self.connection.recv(slen)\n                while len(chunk) < slen:\n                    chunk = chunk + self.connection.recv(slen - len(chunk))\n                record = pickle.loads(chunk)\n                level, message = record[\"level\"].no, record[\"message\"]\n                logger.patch(lambda record: record.update(record)).log(level, message)\n\n    server = socketserver.TCPServer(('localhost', 9999), LoggingStreamHandler)\n    server.serve_forever()\n\n\nKeep in mind though that `pickling is unsafe <https://intoli.com/blog/dangerous-pickles/>`_, use this with care.\n\nAnother possibility is to use a third party library like |zmq|_ for example.\n\n.. code::\n\n    # client.py\n    import zmq\n    from zmq.log.handlers import PUBHandler\n    from logging import Formatter\n    from loguru import logger\n\n    socket = zmq.Context().socket(zmq.PUB)\n    socket.connect(\"tcp://127.0.0.1:12345\")\n    handler = PUBHandler(socket)\n    handler.setFormatter(Formatter(\"%(message)s\"))\n    logger.add(handler)\n\n    logger.info(\"Logging from client\")\n\n\n.. code::\n\n    # server.py\n    import sys\n    import zmq\n    from loguru import logger\n\n    socket = zmq.Context().socket(zmq.SUB)\n    socket.bind(\"tcp://127.0.0.1:12345\")\n    socket.subscribe(\"\")\n\n    logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": \"{message}\"}])\n\n    while True:\n        _, message = socket.recv_multipart()\n        logger.info(message.decode(\"utf8\").strip())\n\n\n\nResolving ``UnicodeEncodeError`` and other encoding issues\n----------------------------------------------------------\n\nWhen you write a log message, the handler may need to encode the received `unicode`_ string to a specific sequence of bytes. The ``encoding`` used to perform this operation varies depending on the sink type and your environment. Problem may occur if you try to write a character which is not supported by the handler ``encoding``. In such case, it's likely that Python will raise an |UnicodeEncodeError|.\n\nFor example, this may happen while printing to the terminal::\n\n    print(\"å¤©\")\n    # UnicodeEncodeError: 'charmap' codec can't encode character '\\u5929' in position 0: character maps to <undefined>\n\nA similar error may occur while writing to a file which has not been opened using an appropriate encoding. Most common problem happen while logging to standard output or to a file on Windows. So, how to avoid such error? Simply by properly configuring your handler so that it can process any kind of unicode string.\n\nIf you are encountering this error while logging to ``stdout``, you have several options:\n\n* Use |sys.stderr| instead of |sys.stdout| (the former will escape faulty characters rather than raising exception)\n* Set the :envvar:`PYTHONIOENCODING` environment variable to ``utf-8``\n* Call |sys.stdout.reconfigure| with ``encoding='utf-8'`` and / or ``errors='backslashreplace'``\n\nIf you are using a file sink, you can configure the ``errors`` or ``encoding`` parameter while adding the handler like ``logger.add(\"file.log\", encoding=\"utf8\")`` for example.  All additional ``**kwargs`` argument are passed to the built-in |open| function.\n\nFor other types of handlers, you have to check if there is a way to parametrize encoding or fallback policy.\n\n\nLogging entry and exit of functions with a decorator\n----------------------------------------------------\n\nIn some cases, it might be useful to log entry and exit values of a function. Although Loguru doesn't provide such feature out of the box, it can be easily implemented by using Python decorators::\n\n    import functools\n    from loguru import logger\n\n\n    def logger_wraps(*, entry=True, exit=True, level=\"DEBUG\"):\n\n        def wrapper(func):\n            name = func.__name__\n\n            @functools.wraps(func)\n            def wrapped(*args, **kwargs):\n                logger_ = logger.opt(depth=1)\n                if entry:\n                    logger_.log(level, \"Entering '{}' (args={}, kwargs={})\", name, args, kwargs)\n                result = func(*args, **kwargs)\n                if exit:\n                    logger_.log(level, \"Exiting '{}' (result={})\", name, result)\n                return result\n\n            return wrapped\n\n        return wrapper\n\nYou could then use it like this::\n\n    @logger_wraps()\n    def foo(a, b, c):\n        logger.info(\"Inside the function\")\n        return a * b * c\n\n    def bar():\n        foo(2, 4, c=8)\n\n    bar()\n\n\nWhich would result in::\n\n    2019-04-07 11:08:44.198 | DEBUG    | __main__:bar:30 - Entering 'foo' (args=(2, 4), kwargs={'c': 8})\n    2019-04-07 11:08:44.198 | INFO     | __main__:foo:26 - Inside the function\n    2019-04-07 11:08:44.198 | DEBUG    | __main__:bar:30 - Exiting 'foo' (result=64)\n\n\nHere is another simple example to record timing of a function::\n\n    def timeit(func):\n\n        def wrapped(*args, **kwargs):\n            start = time.time()\n            result = func(*args, **kwargs)\n            end = time.time()\n            logger.debug(\"Function '{}' executed in {:f} s\", func.__name__, end - start)\n            return result\n\n        return wrapped\n\n\nUsing logging function based on custom added levels\n---------------------------------------------------\n\nAfter adding a new level, it's habitually used with the |log| function::\n\n    logger.level(\"foobar\", no=33, icon=\"ðŸ¤–\", color=\"<blue>\")\n\n    logger.log(\"foobar\", \"A message\")\n\n\nFor convenience, one can assign a new logging function which automatically uses the custom added level::\n\n    from functools import partialmethod\n\n    logger.__class__.foobar = partialmethod(logger.__class__.log, \"foobar\")\n\n    logger.foobar(\"A message\")\n\n\nThe new method need to be added only once and will be usable across all your files importing the ``logger``. Assigning the method to ``logger.__class__`` rather than ``logger`` directly ensures that it stays available even after calling ``logger.bind()``, ``logger.patch()`` and ``logger.opt()`` (because these functions return a new ``logger`` instance).\n\n\nSetting permissions on created log files\n----------------------------------------\n\nTo set desired permissions on created log files, use the ``opener`` argument to pass in a custom opener with permissions octal::\n\n    def opener(file, flags):\n        return os.open(file, flags, 0o600)  # read/write by owner only\n\n    logger.add(\"foo.log\", rotation=\"100 kB\", opener=opener)\n\nWhen using an opener argument, all created log files including ones created during rotation will use the initially provided opener.\n\nNote that the provided mode will be masked out by the OS `umask <https://en.wikipedia.org/wiki/Umask>`_ value (describing which bits are *not* to be set when creating a file or directory). This value is conventionally equals to ``0o022``, which means specifying a ``0o666`` mode will result in a ``0o666 - 0o022 = 0o644`` file permission in this case (which is actually the default). It is possible to change the umask value by first calling |os.umask|, but this needs to be done with careful consideration, as it changes the value globally and can cause security issues.\n\n\nPreserving an ``opt()`` parameter for the whole module\n------------------------------------------------------\n\nSupposing you wish to color each of your log messages without having to call ``logger.opt(colors=True)`` every time, you can add this at the very beginning of your module::\n\n    logger = logger.opt(colors=True)\n\n    logger.info(\"It <green>works</>!\")\n\nHowever, it should be noted that it's not possible to chain |opt| calls, using this method again will reset the ``colors`` option to its default value (which is ``False``). For this reason, it is also necessary to patch the |opt| method so that all subsequent calls continue to use the desired value::\n\n    from functools import partial\n\n    logger = logger.opt(colors=True)\n    logger.opt = partial(logger.opt, colors=True)\n\n    logger.opt(raw=True).info(\"It <green>still</> works!\\n\")\n\n\nSerializing log messages using a custom function\n------------------------------------------------\n\nEach handler added with ``serialize=True`` will create messages by converting the logging record to a valid JSON string. Depending on the sink for which the messages are intended, it may be useful to make changes to the generated string. Instead of using the ``serialize`` parameter, you can implement your own serialization function and use it directly in your sink::\n\n    def serialize(record):\n        subset = {\"timestamp\": record[\"time\"].timestamp(), \"message\": record[\"message\"]}\n        return json.dumps(subset)\n\n    def sink(message):\n        serialized = serialize(message.record)\n        print(serialized)\n\n    logger.add(sink)\n\n\nIf you need to send structured logs to a file (or any kind of sink in general), a similar result can be obtained by using a custom ``format`` function::\n\n    def formatter(record):\n        # Note this function returns the string to be formatted, not the actual message to be logged\n        record[\"extra\"][\"serialized\"] = serialize(record)\n        return \"{extra[serialized]}\\n\"\n\n    logger.add(\"file.log\", format=formatter)\n\n\nYou can also use |patch| for this, so the serialization function will be called only once in case you want to use it in multiple sinks::\n\n    def patching(record):\n        record[\"extra\"][\"serialized\"] = serialize(record)\n\n    logger = logger.patch(patching)\n\n    # Note that if \"format\" is not a function, possible exception will be appended to the message\n    logger.add(sys.stderr, format=\"{extra[serialized]}\")\n    logger.add(\"file.log\", format=\"{extra[serialized]}\")\n\n\nRotating log file based on both size and time\n---------------------------------------------\n\nThe ``rotation`` argument of file sinks accept size or time limits but not both for simplification reasons. However, it is possible to create a custom function to support more advanced scenarios::\n\n    import datetime\n\n    class Rotator:\n        def __init__(self, *, size, at):\n            now = datetime.datetime.now()\n\n            self._size_limit = size\n            self._time_limit = now.replace(hour=at.hour, minute=at.minute, second=at.second)\n\n            if now >= self._time_limit:\n                # The current time is already past the target time so it would rotate already.\n                # Add one day to prevent an immediate rotation.\n                self._time_limit += datetime.timedelta(days=1)\n\n        def should_rotate(self, message, file):\n            file.seek(0, 2)\n            if file.tell() + len(message) > self._size_limit:\n                return True\n            excess = message.record[\"time\"].timestamp() - self._time_limit.timestamp()\n            if excess >= 0:\n                elapsed_days = datetime.timedelta(seconds=excess).days\n                self._time_limit += datetime.timedelta(days=elapsed_days + 1)\n                return True\n            return False\n\n    # Rotate file if over 500 MB or at midnight every day\n    rotator = Rotator(size=5e+8, at=datetime.time(0, 0, 0))\n    logger.add(\"file.log\", rotation=rotator.should_rotate)\n\n\nAdapting colors and format of logged messages dynamically\n---------------------------------------------------------\n\nIt is possible to customize the colors of your logs thanks to several :ref:`markup tags <color>`. Those are used to configure the ``format`` of your handler. By creating a appropriate formatting function, you can easily define colors depending on the logged message.\n\nFor example, if you want to associate each module with a unique color::\n\n    from collections import defaultdict\n    from random import choice\n\n    colors = [\"blue\", \"cyan\", \"green\", \"magenta\", \"red\", \"yellow\"]\n    color_per_module = defaultdict(lambda: choice(colors))\n\n    def formatter(record):\n        color_tag = color_per_module[record[\"name\"]]\n        return \"<\" + color_tag + \">[{name}]</> <bold>{message}</>\\n{exception}\"\n\n    logger.add(sys.stderr, format=formatter)\n\n\nIf you need to dynamically colorize the ``record[\"message\"]``, make sure that the color tags appear in the returned format instead of modifying the message::\n\n    def rainbow(text):\n        colors = [\"red\", \"yellow\", \"green\", \"cyan\", \"blue\", \"magenta\"]\n        chars = (\"<{}>{}</>\".format(colors[i % len(colors)], c) for i, c in enumerate(text))\n        return \"\".join(chars)\n\n    def formatter(record):\n        rainbow_message = rainbow(record[\"message\"])\n        # Prevent '{}' in message (if any) to be incorrectly parsed during formatting\n        escaped = rainbow_message.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        return \"<b>{time}</> \" + escaped + \"\\n{exception}\"\n\n    logger.add(sys.stderr, format=formatter)\n\n\nDynamically formatting messages to properly align values with padding\n---------------------------------------------------------------------\n\nThe default formatter is unable to vertically align log messages because the length of ``{name}``, ``{function}`` and ``{line}`` are not fixed.\n\nOne workaround consists of using padding with some maximum value that should suffice most of the time. For this purpose, you can use Python's string formatting directives, like in this example::\n\n    fmt = \"{time} | {level: <8} | {name: ^15} | {function: ^15} | {line: >3} | {message}\"\n    logger.add(sys.stderr, format=fmt)\n\nHere, ``<``, ``^`` and ``>`` will left, center, and right-align the respective keys, and pad them to a maximum length.\n\nOther solutions are possible by using a formatting function or class. For example, it is possible to dynamically adjust the padding length based on previously encountered values::\n\n    class Formatter:\n\n        def __init__(self):\n            self.padding = 0\n            self.fmt = \"{time} | {level: <8} | {name}:{function}:{line}{extra[padding]} | {message}\\n{exception}\"\n\n        def format(self, record):\n            length = len(\"{name}:{function}:{line}\".format(**record))\n            self.padding = max(self.padding, length)\n            record[\"extra\"][\"padding\"] = \" \" * (self.padding - length)\n            return self.fmt\n\n    formatter = Formatter()\n\n    logger.remove()\n    logger.add(sys.stderr, format=formatter.format)\n\n\nCustomizing the formatting of exceptions\n----------------------------------------\n\nLoguru will automatically add the traceback of occurring exception while using ``logger.exception()`` or ``logger.opt(exception=True)``::\n\n    def inverse(x):\n        try:\n            1 / x\n        except ZeroDivisionError:\n            logger.exception(\"Oups...\")\n\n    if __name__ == \"__main__\":\n        inverse(0)\n\n.. code-block:: none\n\n    2019-11-15 10:01:13.703 | ERROR    | __main__:inverse:8 - Oups...\n    Traceback (most recent call last):\n    File \"foo.py\", line 6, in inverse\n        1 / x\n    ZeroDivisionError: division by zero\n\nIf the handler is added with ``backtrace=True``, the traceback is extended to see where the exception came from:\n\n.. code-block:: none\n\n    2019-11-15 10:11:32.829 | ERROR    | __main__:inverse:8 - Oups...\n    Traceback (most recent call last):\n      File \"foo.py\", line 16, in <module>\n        inverse(0)\n    > File \"foo.py\", line 6, in inverse\n        1 / x\n    ZeroDivisionError: division by zero\n\nIf the handler is added with ``diagnose=True``, then the traceback is annotated to see what caused the problem:\n\n.. code-block:: none\n\n    Traceback (most recent call last):\n\n    File \"foo.py\", line 6, in inverse\n        1 / x\n            â”” 0\n\n    ZeroDivisionError: division by zero\n\nIt is possible to further personalize the formatting of exception by adding an handler with a custom ``format`` function. For example, supposing you want to format errors using the |stackprinter|_ library::\n\n    import stackprinter\n\n    def format(record):\n        format_ = \"{time} {message}\\n\"\n\n        if record[\"exception\"] is not None:\n            record[\"extra\"][\"stack\"] = stackprinter.format(record[\"exception\"])\n            format_ += \"{extra[stack]}\\n\"\n\n        return format_\n\n    logger.add(sys.stderr, format=format)\n\n.. code-block:: none\n\n    2019-11-15T10:46:18.059964+0100 Oups...\n    File foo.py, line 17, in inverse\n        15   def inverse(x):\n        16       try:\n    --> 17           1 / x\n        18       except ZeroDivisionError:\n        ..................................................\n        x = 0\n        ..................................................\n\n    ZeroDivisionError: division by zero\n\n\nDisplaying a stacktrace without using the error context\n-------------------------------------------------------\n\nIt may be useful in some cases to display the traceback at the time your message is logged, while no exceptions have been raised. Although this feature is not built-in into Loguru as it is more related to debugging than logging, it is possible to |patch| your logger and then display the stacktrace as needed (using the |traceback| module)::\n\n    import traceback\n\n    def add_traceback(record):\n        extra = record[\"extra\"]\n        if extra.get(\"with_traceback\", False):\n            extra[\"traceback\"] = \"\\n\" + \"\".join(traceback.format_stack())\n        else:\n            extra[\"traceback\"] = \"\"\n\n    logger = logger.patch(add_traceback)\n    logger.add(sys.stderr, format=\"{time} - {message}{extra[traceback]}\")\n\n    logger.info(\"No traceback\")\n    logger.bind(with_traceback=True).info(\"With traceback\")\n\nHere is another example that demonstrates how to prefix the logged message with the full call stack::\n\n    import traceback\n    from itertools import takewhile\n\n    def tracing_formatter(record):\n        # Filter out frames coming from Loguru internals\n        frames = takewhile(lambda f: \"/loguru/\" not in f.filename, traceback.extract_stack())\n        stack = \" > \".join(\"{}:{}:{}\".format(f.filename, f.name, f.lineno) for f in frames)\n        record[\"extra\"][\"stack\"] = stack\n        return \"{level} | {extra[stack]} - {message}\\n{exception}\"\n\n    def foo():\n        logger.info(\"Deep call\")\n\n    def bar():\n        foo()\n\n    logger.remove()\n    logger.add(sys.stderr, format=tracing_formatter)\n\n    bar()\n    # Output: \"INFO | script.py:<module>:23 > script.py:bar:18 > script.py:foo:15 - Deep call\"\n\n\nManipulating newline terminator to write multiple logs on the same line\n-----------------------------------------------------------------------\n\nYou can temporarily log a message on a continuous line by combining the use of |bind|, |opt| and a custom ``format`` function. This is especially useful if you want to illustrate a step-by-step process in progress, for example::\n\n    def formatter(record):\n        end = record[\"extra\"].get(\"end\", \"\\n\")\n        return \"[{time}] {message}\" + end + \"{exception}\"\n\n    logger.add(sys.stderr, format=formatter)\n    logger.add(\"foo.log\", mode=\"w\")\n\n    logger.bind(end=\"\").debug(\"Progress: \")\n\n    for _ in range(5):\n        logger.opt(raw=True).debug(\".\")\n\n    logger.opt(raw=True).debug(\"\\n\")\n\n    logger.info(\"Done\")\n\n.. code-block:: none\n\n    [2020-03-26T22:47:01.708016+0100] Progress: .....\n    [2020-03-26T22:47:01.709031+0100] Done\n\nNote, however, that you may encounter difficulties depending on the sinks you use. Logging is not always appropriate for this type of end-user message.\n\n\nCapturing standard ``stdout``, ``stderr`` and ``warnings``\n----------------------------------------------------------\n\nThe use of logging should be privileged over |print|, yet, it may happen that you don't have plain control over code executed in your application. If you wish to capture standard output, you can suppress |sys.stdout| (and |sys.stderr|) with a custom stream object using |contextlib.redirect_stdout|. You have to take care of first removing the default handler, and not adding a new stdout sink once redirected or that would cause dead lock (you may use |sys.__stdout__| instead)::\n\n    import contextlib\n    import sys\n    from loguru import logger\n\n    class StreamToLogger:\n\n        def __init__(self, level=\"INFO\"):\n            self._level = level\n\n        def write(self, buffer):\n            for line in buffer.rstrip().splitlines():\n                logger.opt(depth=1).log(self._level, line.rstrip())\n\n        def flush(self):\n            pass\n\n    logger.remove()\n    logger.add(sys.__stdout__)\n\n    stream = StreamToLogger()\n    with contextlib.redirect_stdout(stream):\n        print(\"Standard output is sent to added handlers.\")\n\n\nYou may also capture warnings emitted by your application by replacing |warnings.showwarning|::\n\n    import warnings\n    from loguru import logger\n\n    showwarning_ = warnings.showwarning\n\n    def showwarning(message, *args, **kwargs):\n        logger.opt(depth=2).warning(message)\n        showwarning_(message, *args, **kwargs)\n\n    warnings.showwarning = showwarning\n\n\nAlternatively, if you want to emit warnings based on logged messages, you can simply use |warnings.warn| as a sink::\n\n\n    logger.add(warnings.warn, format=\"{message}\", filter=lambda record: record[\"level\"].name == \"WARNING\")\n\n\nCircumventing modules whose ``__name__`` value is absent\n--------------------------------------------------------\n\nLoguru makes use of the global variable ``__name__`` to determine from where the logged message is coming from. However, it may happen in very specific situation (like some Dask distributed environment) that this value is not set. In such case, Loguru will use ``None`` to make up for the lack of the value. This implies that if you want to |disable| messages coming from such special module, you have to explicitly call ``logger.disable(None)``.\n\nSimilar considerations should be taken into account while dealing with the ``filter`` attribute. As ``__name__`` is missing, Loguru will assign the ``None`` value to the ``record[\"name\"]`` entry. It also means that once formatted in your log messages, the ``{name}`` token will be equals to ``\"None\"``. This can be worked around by manually overriding the ``record[\"name\"]`` value using |patch| from inside the faulty module::\n\n    # If Loguru fails to retrieve the proper \"name\" value, assign it manually\n    logger = logger.patch(lambda record: record.update(name=\"my_module\"))\n\nYou probably should not worry about all of this except if you noticed that your code is subject to this behavior.\n\n\nInteroperability with ``tqdm`` iterations\n-----------------------------------------\n\nTrying to use the Loguru's ``logger`` during an iteration wrapped by the ``tqdm`` library may disturb the displayed progress bar. As a workaround, one can use the ``tqdm.write()`` function instead of writings logs directly to ``sys.stderr``::\n\n    import time\n\n    from loguru import logger\n    from tqdm import tqdm\n\n    logger.remove()\n    logger.add(lambda msg: tqdm.write(msg, end=\"\"), colorize=True)\n\n    logger.info(\"Initializing\")\n\n    for x in tqdm(range(100)):\n        logger.info(\"Iterating #{}\", x)\n        time.sleep(0.1)\n\n\nYou may encounter problems with colorization of your logs after importing ``tqdm`` using Spyder on Windows. This issue is discussed in `GH#132`_. You can easily circumvent the problem by calling ``colorama.deinit()`` right after your import.\n\n\nUsing Loguru's ``logger`` within a Cython module\n------------------------------------------------\n\nLoguru and Cython do not interoperate very well. This is because Loguru (and logging generally) heavily relies on Python stack frames while Cython, being an alternative Python implementation, try to get rid of these frames for optimization reasons.\n\nCalling the ``logger`` from code compiled with Cython may raise this kind of exception::\n\n    ValueError: call stack is not deep enough\n\nThis error happens when Loguru tries to access a stack frame which has been suppressed by Cython. There is no way for Loguru to retrieve contextual information of the logged message, but there exists a workaround that will at least prevent your application to crash::\n\n    # Add this at the start of your file\n    logger = logger.opt(depth=-1)\n\nNote that logged messages should be displayed correctly, but function name and other information will be incorrect. This issue is discussed in `GH#88`_.\n\n\nCreating independent loggers with separate set of handlers\n----------------------------------------------------------\n\nLoguru is fundamentally designed to be usable with exactly one global ``logger`` object dispatching logging messages to the configured handlers. In some circumstances, it may be useful to have specific messages logged to specific handlers.\n\nFor example, supposing you want to split your logs in two files based on an arbitrary identifier, you can achieve that by combining |bind| and ``filter``::\n\n    from loguru import logger\n\n    def task_A():\n        logger_a = logger.bind(task=\"A\")\n        logger_a.info(\"Starting task A\")\n        do_something()\n        logger_a.success(\"End of task A\")\n\n    def task_B():\n        logger_b = logger.bind(task=\"B\")\n        logger_b.info(\"Starting task B\")\n        do_something_else()\n        logger_b.success(\"End of task B\")\n\n    logger.add(\"file_A.log\", filter=lambda record: record[\"extra\"][\"task\"] == \"A\")\n    logger.add(\"file_B.log\", filter=lambda record: record[\"extra\"][\"task\"] == \"B\")\n\n    task_A()\n    task_B()\n\nThat way, ``\"file_A.log\"`` and ``\"file_B.log\"`` will only contains logs from respectively the ``task_A()`` and ``task_B()`` function.\n\nNow, supposing that you have a lot of these tasks. It may be a bit cumbersome to configure every handlers like this. Most importantly, it may unnecessarily slow down your application as each log will need to be checked by the ``filter`` function of each handler. In such case, it is recommended to rely on the |copy.deepcopy| built-in method that will create an independent ``logger`` object. If you add a handler to a deep copied ``logger``, it will not be shared with others functions using the original ``logger``::\n\n    import copy\n    from loguru import logger\n\n    def task(task_id, logger):\n        logger.info(\"Starting task {}\", task_id)\n        do_something(task_id)\n        logger.success(\"End of task {}\", task_id)\n\n    logger.remove()\n\n    for task_id in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n        logger_ = copy.deepcopy(logger)\n        logger_.add(\"file_%s.log\" % task_id)\n        task(task_id, logger_)\n\nNote that you may encounter errors if you try to copy a ``logger`` to which non-picklable handlers have been added. For this reason, it is generally advised to remove all handlers before calling ``copy.deepcopy(logger)``.\n\n\nCompatibility with ``multiprocessing`` using ``enqueue`` argument\n-----------------------------------------------------------------\n\nOn Linux, thanks to |os.fork| there is no pitfall while using the ``logger`` inside another process started by the |multiprocessing| module. The child process will automatically inherit added handlers, the ``enqueue=True`` parameter is optional but is recommended as it would avoid concurrent access of your sink::\n\n    # Linux implementation\n    import multiprocessing\n    from loguru import logger\n\n    def my_process():\n        logger.info(\"Executing function in child process\")\n        logger.complete()\n\n    if __name__ == \"__main__\":\n        logger.add(\"file.log\", enqueue=True)\n\n        process = multiprocessing.Process(target=my_process)\n        process.start()\n        process.join()\n\n        logger.info(\"Done\")\n\nThings get a little more complicated on Windows. Indeed, this operating system does not support forking, so Python has to use an alternative method to create sub-processes called \"spawning\". This procedure requires the whole file where the child process is created to be reloaded from scratch. This does not interoperate very well with Loguru, causing handlers to be added twice without any synchronization or, on the contrary, not being added at all (depending on ``add()`` and ``remove()`` being called inside or outside the ``__main__`` branch). For this reason, the ``logger`` object need to be explicitly passed as an initializer argument of your child process::\n\n    # Windows implementation\n    import multiprocessing\n    from loguru import logger\n\n    def my_process(logger_):\n        logger_.info(\"Executing function in child process\")\n        logger_.complete()\n\n    if __name__ == \"__main__\":\n        logger.remove()  # Default \"sys.stderr\" sink is not picklable\n        logger.add(\"file.log\", enqueue=True)\n\n        process = multiprocessing.Process(target=my_process, args=(logger, ))\n        process.start()\n        process.join()\n\n        logger.info(\"Done\")\n\nWindows requires the added sinks to be picklable or otherwise will raise an error while creating the child process. Many stream objects like standard output and file descriptors are not picklable. In such case, the ``enqueue=True`` argument is required as it will allow the child process to only inherit the queue object where logs are sent.\n\nThe |multiprocessing| library is also commonly used to start a pool of workers using for example |Pool.map| or |Pool.apply|. Again, it will work flawlessly on Linux, but it will require some tinkering on Windows. You will probably not be able to pass the ``logger`` as an argument for your worker functions because it needs to be picklable, but although handlers added using ``enqueue=True`` are \"inheritable\", they are not \"picklable\". Instead, you will need to make use of the ``initializer`` and ``initargs`` parameters while creating the |Pool| object in a way allowing your workers to access the shared ``logger``. You can either assign it to a class attribute or override the global logger of your child processes:\n\n.. code::\n\n    # workers_a.py\n    class Worker:\n\n        _logger = None\n\n        @staticmethod\n        def set_logger(logger_):\n            Worker._logger = logger_\n\n        def work(self, x):\n            self._logger.info(\"Square rooting {}\", x)\n            return x**0.5\n\n\n.. code::\n\n    # workers_b.py\n    from loguru import logger\n\n    def set_logger(logger_):\n        global logger\n        logger = logger_\n\n    def work(x):\n        logger.info(\"Square rooting {}\", x)\n        return x**0.5\n\n\n.. code::\n\n    # main.py\n    from multiprocessing import Pool\n    from loguru import logger\n    import workers_a\n    import workers_b\n\n    if __name__ == \"__main__\":\n        logger.remove()\n        logger.add(\"file.log\", enqueue=True)\n\n        worker = workers_a.Worker()\n        with Pool(4, initializer=worker.set_logger, initargs=(logger, )) as pool:\n            results = pool.map(worker.work, [1, 10, 100])\n\n        with Pool(4, initializer=workers_b.set_logger, initargs=(logger, )) as pool:\n            results = pool.map(workers_b.work, [1, 10, 100])\n\n        logger.info(\"Done\")\n\nIndependently of the operating system, note that the process in which a handler is added with ``enqueue=True`` is in charge of the queue internally used. This means that you should avoid to ``.remove()`` such handler from the parent process is any child is likely to continue using it. More importantly, note that a |Thread| is started internally to consume the queue. Therefore, it is recommended to call |complete| before leaving |Process| to make sure the queue is left in a stable state.\n\nAnother thing to keep in mind when dealing with multiprocessing is the fact that handlers created with ``enqueue=True`` create a queue internally in the default multiprocessing context. If they are passed through to a subprocesses instantiated within a different context (e.g. with ``multiprocessing.get_context(\"spawn\")`` on linux, where the default context is ``\"fork\"``) it will most likely result in crashing the subprocess. This is also noted in the `python multiprocessing docs <https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods>`_. To prevent any problems, you should specify the context to be used by Loguru while adding the handler. This can be done by passing the ``context`` argument to the ``add()`` method::\n\n    import multiprocessing\n    from loguru import logger\n    import workers_a\n\n    if __name__ == \"__main__\":\n        context = multiprocessing.get_context(\"spawn\")\n\n        logger.remove()\n        logger.add(\"file.log\", enqueue=True, context=context)\n\n        worker = workers_a.Worker()\n        with context.Pool(4, initializer=worker.set_logger, initargs=(logger, )) as pool:\n            results = pool.map(worker.work, [1, 10, 100])\n\n\n.. _recipes-testing:\n\nTesting logging\n---------------\n\nLogging calls can be tested using |logot|_, a high-level log testing library with built-in support for Loguru::\n\n    from logot import Logot, logged\n\n    def test_something(logot: Logot) -> None:\n        do_something()\n        logot.assert_logged(logged.info(\"Something was done\"))\n\nEnable Loguru log capture in your |pytest|_ configuration:\n\n.. code:: toml\n\n   [tool.pytest.ini_options]\n   logot_capturer = \"logot.loguru.LoguruCapturer\"\n\n.. seealso::\n\n    See `using logot with Loguru <https://logot.readthedocs.io/latest/integrations/loguru.html>`_ for more information\n    about `configuring pytest <https://logot.readthedocs.io/latest/integrations/loguru.html#enabling-for-pytest>`_\n    and `configuring unittest <https://logot.readthedocs.io/latest/integrations/loguru.html#enabling-for-unittest>`_.\n\n.. note::\n\n    When migrating an existing project from standard :mod:`logging`, it can be useful to migrate your existing test\n    cases too. See :ref:`migrating assertLogs() <migration-assert-logs>` and :ref:`migrating caplog <migration-caplog>`\n    for more information.\n",
          "\"\"\"Core logging functionalities of the `Loguru` library.\n\n.. References and links rendered by Sphinx are kept here as \"module documentation\" so that they can\n   be used in the ``Logger`` docstrings but do not pollute ``help(logger)`` output.\n\n.. |Logger| replace:: :class:`~Logger`\n.. |add| replace:: :meth:`~Logger.add()`\n.. |remove| replace:: :meth:`~Logger.remove()`\n.. |complete| replace:: :meth:`~Logger.complete()`\n.. |catch| replace:: :meth:`~Logger.catch()`\n.. |bind| replace:: :meth:`~Logger.bind()`\n.. |contextualize| replace:: :meth:`~Logger.contextualize()`\n.. |patch| replace:: :meth:`~Logger.patch()`\n.. |opt| replace:: :meth:`~Logger.opt()`\n.. |log| replace:: :meth:`~Logger.log()`\n.. |level| replace:: :meth:`~Logger.level()`\n.. |enable| replace:: :meth:`~Logger.enable()`\n.. |disable| replace:: :meth:`~Logger.disable()`\n\n.. |Any| replace:: :obj:`~typing.Any`\n.. |str| replace:: :class:`str`\n.. |int| replace:: :class:`int`\n.. |bool| replace:: :class:`bool`\n.. |tuple| replace:: :class:`tuple`\n.. |namedtuple| replace:: :func:`namedtuple<collections.namedtuple>`\n.. |list| replace:: :class:`list`\n.. |dict| replace:: :class:`dict`\n.. |str.format| replace:: :meth:`str.format()`\n.. |Path| replace:: :class:`pathlib.Path`\n.. |match.groupdict| replace:: :meth:`re.Match.groupdict()`\n.. |Handler| replace:: :class:`logging.Handler`\n.. |sys.stderr| replace:: :data:`sys.stderr`\n.. |sys.exc_info| replace:: :func:`sys.exc_info()`\n.. |time| replace:: :class:`datetime.time`\n.. |datetime| replace:: :class:`datetime.datetime`\n.. |timedelta| replace:: :class:`datetime.timedelta`\n.. |open| replace:: :func:`open()`\n.. |logging| replace:: :mod:`logging`\n.. |signal| replace:: :mod:`signal`\n.. |contextvars| replace:: :mod:`contextvars`\n.. |multiprocessing| replace:: :mod:`multiprocessing`\n.. |Thread.run| replace:: :meth:`Thread.run()<threading.Thread.run()>`\n.. |Exception| replace:: :class:`Exception`\n.. |AbstractEventLoop| replace:: :class:`AbstractEventLoop<asyncio.AbstractEventLoop>`\n.. |asyncio.get_running_loop| replace:: :func:`asyncio.get_running_loop()`\n.. |asyncio.run| replace:: :func:`asyncio.run()`\n.. |loop.run_until_complete| replace::\n    :meth:`loop.run_until_complete()<asyncio.loop.run_until_complete()>`\n.. |loop.create_task| replace:: :meth:`loop.create_task()<asyncio.loop.create_task()>`\n\n.. |logger.trace| replace:: :meth:`logger.trace()<Logger.trace()>`\n.. |logger.debug| replace:: :meth:`logger.debug()<Logger.debug()>`\n.. |logger.info| replace:: :meth:`logger.info()<Logger.info()>`\n.. |logger.success| replace:: :meth:`logger.success()<Logger.success()>`\n.. |logger.warning| replace:: :meth:`logger.warning()<Logger.warning()>`\n.. |logger.error| replace:: :meth:`logger.error()<Logger.error()>`\n.. |logger.critical| replace:: :meth:`logger.critical()<Logger.critical()>`\n\n.. |file-like object| replace:: ``file-like object``\n.. _file-like object: https://docs.python.org/3/glossary.html#term-file-object\n.. |callable| replace:: ``callable``\n.. _callable: https://docs.python.org/3/library/functions.html#callable\n.. |coroutine function| replace:: ``coroutine function``\n.. _coroutine function: https://docs.python.org/3/glossary.html#term-coroutine-function\n.. |re.Pattern| replace:: ``re.Pattern``\n.. _re.Pattern: https://docs.python.org/3/library/re.html#re-objects\n.. |multiprocessing.Context| replace:: ``multiprocessing.Context``\n.. _multiprocessing.Context:\n   https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n\n.. |better_exceptions| replace:: ``better_exceptions``\n.. _better_exceptions: https://github.com/Qix-/better-exceptions\n\n.. |loguru-config| replace:: ``loguru-config``\n.. _loguru-config: https://github.com/erezinman/loguru-config\n\n.. _Pendulum: https://pendulum.eustace.io/docs/#tokens\n\n.. _@Qix-: https://github.com/Qix-\n.. _@erezinman: https://github.com/erezinman\n.. _@sdispater: https://github.com/sdispater\n\n.. _formatting directives: https://docs.python.org/3/library/string.html#format-string-syntax\n.. _reentrant: https://en.wikipedia.org/wiki/Reentrancy_(computing)\n\"\"\"\n\nimport builtins\nimport contextlib\nimport functools\nimport logging\nimport re\nimport sys\nimport threading\nimport warnings\nfrom collections import namedtuple\nfrom inspect import isclass, iscoroutinefunction, isgeneratorfunction\nfrom multiprocessing import current_process, get_context\nfrom multiprocessing.context import BaseContext\nfrom os.path import basename, splitext\nfrom threading import current_thread\n\nfrom . import _asyncio_loop, _colorama, _defaults, _filters\nfrom ._better_exceptions import ExceptionFormatter\nfrom ._colorizer import Colorizer\nfrom ._contextvars import ContextVar\nfrom ._datetime import aware_now\nfrom ._error_interceptor import ErrorInterceptor\nfrom ._file_sink import FileSink\nfrom ._get_frame import get_frame\nfrom ._handler import Handler\nfrom ._locks_machinery import create_logger_lock\nfrom ._recattrs import RecordException, RecordFile, RecordLevel, RecordProcess, RecordThread\nfrom ._simple_sinks import AsyncSink, CallableSink, StandardSink, StreamSink\n\nif sys.version_info >= (3, 6):\n    from os import PathLike\nelse:\n    from pathlib import PurePath as PathLike\n\n\nLevel = namedtuple(\"Level\", [\"name\", \"no\", \"color\", \"icon\"])  # noqa: PYI024\n\nstart_time = aware_now()\n\ncontext = ContextVar(\"loguru_context\", default={})\n\n\nclass Core:\n    def __init__(self):\n        levels = [\n            Level(\n                \"TRACE\",\n                _defaults.LOGURU_TRACE_NO,\n                _defaults.LOGURU_TRACE_COLOR,\n                _defaults.LOGURU_TRACE_ICON,\n            ),\n            Level(\n                \"DEBUG\",\n                _defaults.LOGURU_DEBUG_NO,\n                _defaults.LOGURU_DEBUG_COLOR,\n                _defaults.LOGURU_DEBUG_ICON,\n            ),\n            Level(\n                \"INFO\",\n                _defaults.LOGURU_INFO_NO,\n                _defaults.LOGURU_INFO_COLOR,\n                _defaults.LOGURU_INFO_ICON,\n            ),\n            Level(\n                \"SUCCESS\",\n                _defaults.LOGURU_SUCCESS_NO,\n                _defaults.LOGURU_SUCCESS_COLOR,\n                _defaults.LOGURU_SUCCESS_ICON,\n            ),\n            Level(\n                \"WARNING\",\n                _defaults.LOGURU_WARNING_NO,\n                _defaults.LOGURU_WARNING_COLOR,\n                _defaults.LOGURU_WARNING_ICON,\n            ),\n            Level(\n                \"ERROR\",\n                _defaults.LOGURU_ERROR_NO,\n                _defaults.LOGURU_ERROR_COLOR,\n                _defaults.LOGURU_ERROR_ICON,\n            ),\n            Level(\n                \"CRITICAL\",\n                _defaults.LOGURU_CRITICAL_NO,\n                _defaults.LOGURU_CRITICAL_COLOR,\n                _defaults.LOGURU_CRITICAL_ICON,\n            ),\n        ]\n        self.levels = {level.name: level for level in levels}\n        self.levels_ansi_codes = {\n            **{name: Colorizer.ansify(level.color) for name, level in self.levels.items()},\n            None: \"\",\n        }\n\n        # Cache used internally to quickly access level attributes based on their name or severity.\n        # It can also contain integers as keys, it serves to avoid calling \"isinstance()\" repeatedly\n        # when \"logger.log()\" is used.\n        self.levels_lookup = {\n            name: (name, name, level.no, level.icon) for name, level in self.levels.items()\n        }\n\n        self.handlers_count = 0\n        self.handlers = {}\n\n        self.extra = {}\n        self.patcher = None\n\n        self.min_level = float(\"inf\")\n        self.enabled = {}\n        self.activation_list = []\n        self.activation_none = True\n\n        self.thread_locals = threading.local()\n        self.lock = create_logger_lock()\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"thread_locals\"] = None\n        state[\"lock\"] = None\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.thread_locals = threading.local()\n        self.lock = create_logger_lock()\n\n\nclass Logger:\n    \"\"\"An object to dispatch logging messages to configured handlers.\n\n    The |Logger| is the core object of ``loguru``, every logging configuration and usage pass\n    through a call to one of its methods. There is only one logger, so there is no need to retrieve\n    one before usage.\n\n    Once the ``logger`` is imported, it can be used to write messages about events happening in your\n    code. By reading the output logs of your application, you gain a better understanding of the\n    flow of your program and you more easily track and debug unexpected behaviors.\n\n    Handlers to which the logger sends log messages are added using the |add| method. Note that you\n    can use the |Logger| right after import as it comes pre-configured (logs are emitted to\n    |sys.stderr| by default). Messages can be logged with different severity levels and they can be\n    formatted using curly braces (it uses |str.format| under the hood).\n\n    When a message is logged, a \"record\" is associated with it. This record is a dict which contains\n    information about the logging context: time, function, file, line, thread, level... It also\n    contains the ``__name__`` of the module, this is why you don't need named loggers.\n\n    You should not instantiate a |Logger| by yourself, use ``from loguru import logger`` instead.\n    \"\"\"\n\n    def __init__(self, core, exception, depth, record, lazy, colors, raw, capture, patchers, extra):\n        self._core = core\n        self._options = (exception, depth, record, lazy, colors, raw, capture, patchers, extra)\n\n    def __repr__(self):\n        return \"<loguru.logger handlers=%r>\" % list(self._core.handlers.values())\n\n    def add(\n        self,\n        sink,\n        *,\n        level=_defaults.LOGURU_LEVEL,\n        format=_defaults.LOGURU_FORMAT,\n        filter=_defaults.LOGURU_FILTER,\n        colorize=_defaults.LOGURU_COLORIZE,\n        serialize=_defaults.LOGURU_SERIALIZE,\n        backtrace=_defaults.LOGURU_BACKTRACE,\n        diagnose=_defaults.LOGURU_DIAGNOSE,\n        enqueue=_defaults.LOGURU_ENQUEUE,\n        context=_defaults.LOGURU_CONTEXT,\n        catch=_defaults.LOGURU_CATCH,\n        **kwargs\n    ):\n        r\"\"\"Add a handler sending log messages to a sink adequately configured.\n\n        Parameters\n        ----------\n        sink : |file-like object|_, |str|, |Path|, |callable|_, |coroutine function|_ or |Handler|\n            An object in charge of receiving formatted logging messages and propagating them to an\n            appropriate endpoint.\n        level : |int| or |str|, optional\n            The minimum severity level from which logged messages should be sent to the sink.\n        format : |str| or |callable|_, optional\n            The template used to format logged messages before being sent to the sink.\n        filter : |callable|_, |str| or |dict|, optional\n            A directive optionally used to decide for each logged message whether it should be sent\n            to the sink or not.\n        colorize : |bool|, optional\n            Whether the color markups contained in the formatted message should be converted to ansi\n            codes for terminal coloration, or stripped otherwise. If ``None``, the choice is\n            automatically made based on the sink being a tty or not.\n        serialize : |bool|, optional\n            Whether the logged message and its records should be first converted to a JSON string\n            before being sent to the sink.\n        backtrace : |bool|, optional\n            Whether the exception trace formatted should be extended upward, beyond the catching\n            point, to show the full stacktrace which generated the error.\n        diagnose : |bool|, optional\n            Whether the exception trace should display the variables values to eases the debugging.\n            This should be set to ``False`` in production to avoid leaking sensitive data.\n        enqueue : |bool|, optional\n            Whether the messages to be logged should first pass through a multiprocessing-safe queue\n            before reaching the sink. This is useful while logging to a file through multiple\n            processes. This also has the advantage of making logging calls non-blocking.\n        context : |multiprocessing.Context| or |str|, optional\n            A context object or name that will be used for all tasks involving internally the\n            |multiprocessing| module, in particular when ``enqueue=True``. If ``None``, the default\n            context is used.\n        catch : |bool|, optional\n            Whether errors occurring while sink handles logs messages should be automatically\n            caught. If ``True``, an exception message is displayed on |sys.stderr| but the exception\n            is not propagated to the caller, preventing your app to crash.\n        **kwargs\n            Additional parameters that are only valid to configure a coroutine or file sink (see\n            below).\n\n\n        If and only if the sink is a coroutine function, the following parameter applies:\n\n        Parameters\n        ----------\n        loop : |AbstractEventLoop|, optional\n            The event loop in which the asynchronous logging task will be scheduled and executed. If\n            ``None``, the loop used is the one returned by |asyncio.get_running_loop| at the time of\n            the logging call (task is discarded if there is no loop currently running).\n\n\n        If and only if the sink is a file path, the following parameters apply:\n\n        Parameters\n        ----------\n        rotation : |str|, |int|, |time|, |timedelta| or |callable|_, optional\n            A condition indicating whenever the current logged file should be closed and a new one\n            started.\n        retention : |str|, |int|, |timedelta| or |callable|_, optional\n            A directive filtering old files that should be removed during rotation or end of\n            program.\n        compression : |str| or |callable|_, optional\n            A compression or archive format to which log files should be converted at closure.\n        delay : |bool|, optional\n            Whether the file should be created as soon as the sink is configured, or delayed until\n            first logged message. It defaults to ``False``.\n        watch : |bool|, optional\n            Whether or not the file should be watched and re-opened when deleted or changed (based\n            on its device and inode properties) by an external program. It defaults to ``False``.\n        mode : |str|, optional\n            The opening mode as for built-in |open| function. It defaults to ``\"a\"`` (open the\n            file in appending mode).\n        buffering : |int|, optional\n            The buffering policy as for built-in |open| function. It defaults to ``1`` (line\n            buffered file).\n        encoding : |str|, optional\n            The file encoding as for built-in |open| function. It defaults to ``\"utf8\"``.\n        **kwargs\n            Others parameters are passed to the built-in |open| function.\n\n        Returns\n        -------\n        :class:`int`\n            An identifier associated with the added sink and which should be used to\n            |remove| it.\n\n        Raises\n        ------\n        ValueError\n            If any of the arguments passed to configure the sink is invalid.\n\n        Notes\n        -----\n        Extended summary follows.\n\n        .. _sink:\n\n        .. rubric:: The sink parameter\n\n        The ``sink`` handles incoming log messages and proceed to their writing somewhere and\n        somehow. A sink can take many forms:\n\n        - A |file-like object|_ like ``sys.stderr`` or ``open(\"file.log\", \"w\")``. Anything with\n          a ``.write()`` method is considered as a file-like object. Custom handlers may also\n          implement ``flush()`` (called after each logged message), ``stop()`` (called at sink\n          termination) and ``complete()`` (awaited by the eponymous method).\n        - A file path as |str| or |Path|. It can be parametrized with some additional parameters,\n          see below.\n        - A |callable|_ (such as a simple function) like ``lambda msg: print(msg)``. This\n          allows for logging procedure entirely defined by user preferences and needs.\n        - A asynchronous |coroutine function|_ defined with the ``async def`` statement. The\n          coroutine object returned by such function will be added to the event loop using\n          |loop.create_task|. The tasks should be awaited before ending the loop by using\n          |complete|.\n        - A built-in |Handler| like ``logging.StreamHandler``. In such a case, the `Loguru` records\n          are automatically converted to the structure expected by the |logging| module.\n\n        Note that the logging functions are not `reentrant`_. This means you should avoid using\n        the ``logger`` inside any of your sinks or from within |signal| handlers. Otherwise, you\n        may face deadlock if the module's sink was not explicitly disabled.\n\n        .. _message:\n\n        .. rubric:: The logged message\n\n        The logged message passed to all added sinks is nothing more than a string of the\n        formatted log, to which a special attribute is associated: the ``.record`` which is a dict\n        containing all contextual information possibly needed (see below).\n\n        Logged messages are formatted according to the ``format`` of the added sink. This format\n        is usually a string containing braces fields to display attributes from the record dict.\n\n        If fine-grained control is needed, the ``format`` can also be a function which takes the\n        record as parameter and return the format template string. However, note that in such a\n        case, you should take care of appending the line ending and exception field to the returned\n        format, while ``\"\\n{exception}\"`` is automatically appended for convenience if ``format`` is\n        a string.\n\n        The ``filter`` attribute can be used to control which messages are effectively passed to the\n        sink and which one are ignored. A function can be used, accepting the record as an\n        argument, and returning ``True`` if the message should be logged, ``False`` otherwise. If\n        a string is used, only the records with the same ``name`` and its children will be allowed.\n        One can also pass a ``dict`` mapping module names to minimum required level. In such case,\n        each log record will search for it's closest parent in the ``dict`` and use the associated\n        level as the filter. The ``dict`` values can be ``int`` severity, ``str`` level name or\n        ``True`` and ``False`` to respectively authorize and discard all module logs\n        unconditionally. In order to set a default level, the ``\"\"`` module name should be used as\n        it is the parent of all modules (it does not suppress global ``level`` threshold, though).\n\n        Note that while calling a logging method, the keyword arguments (if any) are automatically\n        added to the ``extra`` dict for convenient contextualization (in addition to being used for\n        formatting).\n\n        .. _levels:\n\n        .. rubric:: The severity levels\n\n        Each logged message is associated with a severity level. These levels make it possible to\n        prioritize messages and to choose the verbosity of the logs according to usages. For\n        example, it allows to display some debugging information to a developer, while hiding it to\n        the end user running the application.\n\n        The ``level`` attribute of every added sink controls the minimum threshold from which log\n        messages are allowed to be emitted. While using the ``logger``, you are in charge of\n        configuring the appropriate granularity of your logs. It is possible to add even more custom\n        levels by using the |level| method.\n\n        Here are the standard levels with their default severity value, each one is associated with\n        a logging method of the same name:\n\n        +----------------------+------------------------+------------------------+\n        | Level name           | Severity value         | Logger method          |\n        +======================+========================+========================+\n        | ``TRACE``            | 5                      | |logger.trace|         |\n        +----------------------+------------------------+------------------------+\n        | ``DEBUG``            | 10                     | |logger.debug|         |\n        +----------------------+------------------------+------------------------+\n        | ``INFO``             | 20                     | |logger.info|          |\n        +----------------------+------------------------+------------------------+\n        | ``SUCCESS``          | 25                     | |logger.success|       |\n        +----------------------+------------------------+------------------------+\n        | ``WARNING``          | 30                     | |logger.warning|       |\n        +----------------------+------------------------+------------------------+\n        | ``ERROR``            | 40                     | |logger.error|         |\n        +----------------------+------------------------+------------------------+\n        | ``CRITICAL``         | 50                     | |logger.critical|      |\n        +----------------------+------------------------+------------------------+\n\n        .. _record:\n\n        .. rubric:: The record dict\n\n        The record is just a Python dict, accessible from sinks by ``message.record``. It contains\n        all contextual information of the logging call (time, function, file, line, level, etc.).\n\n        Each of the record keys can be used in the handler's ``format`` so the corresponding value\n        is properly displayed in the logged message (e.g. ``\"{level}\"`` will return ``\"INFO\"``).\n        Some records' values are objects with two or more attributes. These can be formatted with\n        ``\"{key.attr}\"`` (``\"{key}\"`` would display one by default).\n\n        Note that you can use any `formatting directives`_ available in Python's ``str.format()``\n        method (e.g. ``\"{key: >3}\"`` will right-align and pad to a width of 3 characters). This is\n        particularly useful for time formatting (see below).\n\n        +------------+---------------------------------+----------------------------+\n        | Key        | Description                     | Attributes                 |\n        +============+=================================+============================+\n        | elapsed    | The time elapsed since the      | See |timedelta|            |\n        |            | start of the program            |                            |\n        +------------+---------------------------------+----------------------------+\n        | exception  | The formatted exception if any, | ``type``, ``value``,       |\n        |            | ``None`` otherwise              | ``traceback``              |\n        +------------+---------------------------------+----------------------------+\n        | extra      | The dict of attributes          | None                       |\n        |            | bound by the user (see |bind|)  |                            |\n        +------------+---------------------------------+----------------------------+\n        | file       | The file where the logging call | ``name`` (default),        |\n        |            | was made                        | ``path``                   |\n        +------------+---------------------------------+----------------------------+\n        | function   | The function from which the     | None                       |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | level      | The severity used to log the    | ``name`` (default),        |\n        |            | message                         | ``no``, ``icon``           |\n        +------------+---------------------------------+----------------------------+\n        | line       | The line number in the source   | None                       |\n        |            | code                            |                            |\n        +------------+---------------------------------+----------------------------+\n        | message    | The logged message (not yet     | None                       |\n        |            | formatted)                      |                            |\n        +------------+---------------------------------+----------------------------+\n        | module     | The module where the logging    | None                       |\n        |            | call was made                   |                            |\n        +------------+---------------------------------+----------------------------+\n        | name       | The ``__name__`` where the      | None                       |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | process    | The process in which the        | ``name``, ``id`` (default) |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | thread     | The thread in which the         | ``name``, ``id`` (default) |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | time       | The aware local time when the   | See |datetime|             |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n\n        .. _time:\n\n        .. rubric:: The time formatting\n\n        To use your favorite time representation, you can set it directly in the time formatter\n        specifier of your handler format, like for example ``format=\"{time:HH:mm:ss} {message}\"``.\n        Note that this datetime represents your local time, and it is also made timezone-aware,\n        so you can display the UTC offset to avoid ambiguities.\n\n        The time field can be formatted using more human-friendly tokens. These constitute a subset\n        of the one used by the `Pendulum`_ library of `@sdispater`_. To escape a token, just add\n        square brackets around it, for example ``\"[YY]\"`` would display literally ``\"YY\"``.\n\n        If you prefer to display UTC rather than local time, you can add ``\"!UTC\"`` at the very end\n        of the time format, like ``{time:HH:mm:ss!UTC}``. Doing so will convert the ``datetime``\n        to UTC before formatting.\n\n        If no time formatter specifier is used, like for example if ``format=\"{time} {message}\"``,\n        the default one will use ISO 8601.\n\n        +------------------------+---------+----------------------------------------+\n        |                        | Token   | Output                                 |\n        +========================+=========+========================================+\n        | Year                   | YYYY    | 2000, 2001, 2002 ... 2012, 2013        |\n        |                        +---------+----------------------------------------+\n        |                        | YY      | 00, 01, 02 ... 12, 13                  |\n        +------------------------+---------+----------------------------------------+\n        | Quarter                | Q       | 1 2 3 4                                |\n        +------------------------+---------+----------------------------------------+\n        | Month                  | MMMM    | January, February, March ...           |\n        |                        +---------+----------------------------------------+\n        |                        | MMM     | Jan, Feb, Mar ...                      |\n        |                        +---------+----------------------------------------+\n        |                        | MM      | 01, 02, 03 ... 11, 12                  |\n        |                        +---------+----------------------------------------+\n        |                        | M       | 1, 2, 3 ... 11, 12                     |\n        +------------------------+---------+----------------------------------------+\n        | Day of Year            | DDDD    | 001, 002, 003 ... 364, 365             |\n        |                        +---------+----------------------------------------+\n        |                        | DDD     | 1, 2, 3 ... 364, 365                   |\n        +------------------------+---------+----------------------------------------+\n        | Day of Month           | DD      | 01, 02, 03 ... 30, 31                  |\n        |                        +---------+----------------------------------------+\n        |                        | D       | 1, 2, 3 ... 30, 31                     |\n        +------------------------+---------+----------------------------------------+\n        | Day of Week            | dddd    | Monday, Tuesday, Wednesday ...         |\n        |                        +---------+----------------------------------------+\n        |                        | ddd     | Mon, Tue, Wed ...                      |\n        |                        +---------+----------------------------------------+\n        |                        | d       | 0, 1, 2 ... 6                          |\n        +------------------------+---------+----------------------------------------+\n        | Days of ISO Week       | E       | 1, 2, 3 ... 7                          |\n        +------------------------+---------+----------------------------------------+\n        | Hour                   | HH      | 00, 01, 02 ... 23, 24                  |\n        |                        +---------+----------------------------------------+\n        |                        | H       | 0, 1, 2 ... 23, 24                     |\n        |                        +---------+----------------------------------------+\n        |                        | hh      | 01, 02, 03 ... 11, 12                  |\n        |                        +---------+----------------------------------------+\n        |                        | h       | 1, 2, 3 ... 11, 12                     |\n        +------------------------+---------+----------------------------------------+\n        | Minute                 | mm      | 00, 01, 02 ... 58, 59                  |\n        |                        +---------+----------------------------------------+\n        |                        | m       | 0, 1, 2 ... 58, 59                     |\n        +------------------------+---------+----------------------------------------+\n        | Second                 | ss      | 00, 01, 02 ... 58, 59                  |\n        |                        +---------+----------------------------------------+\n        |                        | s       | 0, 1, 2 ... 58, 59                     |\n        +------------------------+---------+----------------------------------------+\n        | Fractional Second      | S       | 0 1 ... 8 9                            |\n        |                        +---------+----------------------------------------+\n        |                        | SS      | 00, 01, 02 ... 98, 99                  |\n        |                        +---------+----------------------------------------+\n        |                        | SSS     | 000 001 ... 998 999                    |\n        |                        +---------+----------------------------------------+\n        |                        | SSSS... | 000[0..] 001[0..] ... 998[0..] 999[0..]|\n        |                        +---------+----------------------------------------+\n        |                        | SSSSSS  | 000000 000001 ... 999998 999999        |\n        +------------------------+---------+----------------------------------------+\n        | AM / PM                | A       | AM, PM                                 |\n        +------------------------+---------+----------------------------------------+\n        | Timezone               | Z       | -07:00, -06:00 ... +06:00, +07:00      |\n        |                        +---------+----------------------------------------+\n        |                        | ZZ      | -0700, -0600 ... +0600, +0700          |\n        |                        +---------+----------------------------------------+\n        |                        | zz      | EST CST ... MST PST                    |\n        +------------------------+---------+----------------------------------------+\n        | Seconds timestamp      | X       | 1381685817, 1234567890.123             |\n        +------------------------+---------+----------------------------------------+\n        | Microseconds timestamp | x       | 1234567890123                          |\n        +------------------------+---------+----------------------------------------+\n\n        .. _file:\n\n        .. rubric:: The file sinks\n\n        If the sink is a |str| or a |Path|, the corresponding file will be opened for writing logs.\n        The path can also contain a special ``\"{time}\"`` field that will be formatted with the\n        current date at file creation. The file is closed at sink stop, i.e. when the application\n        ends or the handler is removed.\n\n        The ``rotation`` check is made before logging each message. If there is already an existing\n        file with the same name that the file to be created, then the existing file is renamed by\n        appending the date to its basename to prevent file overwriting. This parameter accepts:\n\n        - an |int| which corresponds to the maximum file size in bytes before that the current\n          logged file is closed and a new one started over.\n        - a |timedelta| which indicates the frequency of each new rotation.\n        - a |time| which specifies the hour when the daily rotation should occur.\n        - a |str| for human-friendly parametrization of one of the previously enumerated types.\n          Examples: ``\"100 MB\"``, ``\"0.5 GB\"``, ``\"1 month 2 weeks\"``, ``\"4 days\"``, ``\"10h\"``,\n          ``\"monthly\"``, ``\"18:00\"``, ``\"sunday\"``, ``\"w0\"``, ``\"monday at 12:00\"``, ...\n        - a |callable|_ which will be invoked before logging. It should accept two arguments: the\n          logged message and the file object, and it should return ``True`` if the rotation should\n          happen now, ``False`` otherwise.\n\n        The ``retention`` occurs at rotation or at sink stop if rotation is ``None``. Files\n        resulting from previous sessions or rotations are automatically collected from disk. A file\n        is selected if it matches the pattern ``\"basename(.*).ext(.*)\"`` (possible time fields are\n        beforehand replaced with ``.*``) based on the configured sink. Afterwards, the list is\n        processed to determine files to be retained. This parameter accepts:\n\n        - an |int| which indicates the number of log files to keep, while older files are deleted.\n        - a |timedelta| which specifies the maximum age of files to keep.\n        - a |str| for human-friendly parametrization of the maximum age of files to keep.\n          Examples: ``\"1 week, 3 days\"``, ``\"2 months\"``, ...\n        - a |callable|_ which will be invoked before the retention process. It should accept the\n          list of log files as argument and process to whatever it wants (moving files, removing\n          them, etc.).\n\n        The ``compression`` happens at rotation or at sink stop if rotation is ``None``. This\n        parameter accepts:\n\n        - a |str| which corresponds to the compressed or archived file extension. This can be one\n          of: ``\"gz\"``, ``\"bz2\"``, ``\"xz\"``, ``\"lzma\"``, ``\"tar\"``, ``\"tar.gz\"``, ``\"tar.bz2\"``,\n          ``\"tar.xz\"``, ``\"zip\"``.\n        - a |callable|_ which will be invoked before file termination. It should accept the path of\n          the log file as argument and process to whatever it wants (custom compression, network\n          sending, removing it, etc.).\n\n        Either way, if you use a custom function designed according to your preferences, you must be\n        very careful not to use the ``logger`` within your function. Otherwise, there is a risk that\n        your program hang because of a deadlock.\n\n        .. _color:\n\n        .. rubric:: The color markups\n\n        To add colors to your logs, you just have to enclose your format string with the appropriate\n        tags (e.g. ``<red>some message</red>``). These tags are automatically removed if the sink\n        doesn't support ansi codes. For convenience, you can use ``</>`` to close the last opening\n        tag without repeating its name (e.g. ``<red>another message</>``).\n\n        The special tag ``<level>`` (abbreviated with ``<lvl>``) is transformed according to\n        the configured color of the logged message level.\n\n        Tags which are not recognized will raise an exception during parsing, to inform you about\n        possible misuse. If you wish to display a markup tag literally, you can escape it by\n        prepending a ``\\`` like for example ``\\<blue>``. To prevent the escaping to occur, you can\n        simply double the ``\\`` (e.g. ``\\\\<blue>`` will print a literal ``\\`` before colored text).\n        If, for some reason, you need to escape a string programmatically, note that the regex used\n        internally to parse markup tags is ``r\"(\\\\*)(</?(?:[fb]g\\s)?[^<>\\s]*>)\"``.\n\n        Note that when logging a message with ``opt(colors=True)``, color tags present in the\n        formatting arguments (``args`` and ``kwargs``) are completely ignored. This is important if\n        you need to log strings containing markups that might interfere with the color tags (in this\n        case, do not use f-string).\n\n        Here are the available tags (note that compatibility may vary depending on terminal):\n\n        +------------------------------------+--------------------------------------+\n        | Color (abbr)                       | Styles (abbr)                        |\n        +====================================+======================================+\n        | Black (k)                          | Bold (b)                             |\n        +------------------------------------+--------------------------------------+\n        | Blue (e)                           | Dim (d)                              |\n        +------------------------------------+--------------------------------------+\n        | Cyan (c)                           | Normal (n)                           |\n        +------------------------------------+--------------------------------------+\n        | Green (g)                          | Italic (i)                           |\n        +------------------------------------+--------------------------------------+\n        | Magenta (m)                        | Underline (u)                        |\n        +------------------------------------+--------------------------------------+\n        | Red (r)                            | Strike (s)                           |\n        +------------------------------------+--------------------------------------+\n        | White (w)                          | Reverse (v)                          |\n        +------------------------------------+--------------------------------------+\n        | Yellow (y)                         | Blink (l)                            |\n        +------------------------------------+--------------------------------------+\n        |                                    | Hide (h)                             |\n        +------------------------------------+--------------------------------------+\n\n        Usage:\n\n        +-----------------+-------------------------------------------------------------------+\n        | Description     | Examples                                                          |\n        |                 +---------------------------------+---------------------------------+\n        |                 | Foreground                      | Background                      |\n        +=================+=================================+=================================+\n        | Basic colors    | ``<red>``, ``<r>``              | ``<GREEN>``, ``<G>``            |\n        +-----------------+---------------------------------+---------------------------------+\n        | Light colors    | ``<light-blue>``, ``<le>``      | ``<LIGHT-CYAN>``, ``<LC>``      |\n        +-----------------+---------------------------------+---------------------------------+\n        | 8-bit colors    | ``<fg 86>``, ``<fg 255>``       | ``<bg 42>``, ``<bg 9>``         |\n        +-----------------+---------------------------------+---------------------------------+\n        | Hex colors      | ``<fg #00005f>``, ``<fg #EE1>`` | ``<bg #AF5FD7>``, ``<bg #fff>`` |\n        +-----------------+---------------------------------+---------------------------------+\n        | RGB colors      | ``<fg 0,95,0>``                 | ``<bg 72,119,65>``              |\n        +-----------------+---------------------------------+---------------------------------+\n        | Stylizing       | ``<bold>``, ``<b>``,  ``<underline>``, ``<u>``                    |\n        +-----------------+-------------------------------------------------------------------+\n\n        .. _env:\n\n        .. rubric:: The environment variables\n\n        The default values of sink parameters can be entirely customized. This is particularly\n        useful if you don't like the log format of the pre-configured sink.\n\n        Each of the |add| default parameter can be modified by setting the ``LOGURU_[PARAM]``\n        environment variable. For example on Linux: ``export LOGURU_FORMAT=\"{time} - {message}\"``\n        or ``export LOGURU_DIAGNOSE=NO``.\n\n        The default levels' attributes can also be modified by setting the ``LOGURU_[LEVEL]_[ATTR]``\n        environment variable. For example, on Windows: ``setx LOGURU_DEBUG_COLOR \"<blue>\"``\n        or ``setx LOGURU_TRACE_ICON \"ðŸš€\"``. If you use the ``set`` command, do not include quotes\n        but escape special symbol as needed, e.g. ``set LOGURU_DEBUG_COLOR=^<blue^>``.\n\n        If you want to disable the pre-configured sink, you can set the ``LOGURU_AUTOINIT``\n        variable to ``False``.\n\n        On Linux, you will probably need to edit the ``~/.profile`` file to make this persistent. On\n        Windows, don't forget to restart your terminal for the change to be taken into account.\n\n        Examples\n        --------\n        >>> logger.add(sys.stdout, format=\"{time} - {level} - {message}\", filter=\"sub.module\")\n\n        >>> logger.add(\"file_{time}.log\", level=\"TRACE\", rotation=\"100 MB\")\n\n        >>> def debug_only(record):\n        ...     return record[\"level\"].name == \"DEBUG\"\n        ...\n        >>> logger.add(\"debug.log\", filter=debug_only)  # Other levels are filtered out\n\n        >>> def my_sink(message):\n        ...     record = message.record\n        ...     update_db(message, time=record[\"time\"], level=record[\"level\"])\n        ...\n        >>> logger.add(my_sink)\n\n        >>> level_per_module = {\n        ...     \"\": \"DEBUG\",\n        ...     \"third.lib\": \"WARNING\",\n        ...     \"anotherlib\": False\n        ... }\n        >>> logger.add(lambda m: print(m, end=\"\"), filter=level_per_module, level=0)\n\n        >>> async def publish(message):\n        ...     await api.post(message)\n        ...\n        >>> logger.add(publish, serialize=True)\n\n        >>> from logging import StreamHandler\n        >>> logger.add(StreamHandler(sys.stderr), format=\"{message}\")\n\n        >>> class RandomStream:\n        ...     def __init__(self, seed, threshold):\n        ...         self.threshold = threshold\n        ...         random.seed(seed)\n        ...     def write(self, message):\n        ...         if random.random() > self.threshold:\n        ...             print(message)\n        ...\n        >>> stream_object = RandomStream(seed=12345, threshold=0.25)\n        >>> logger.add(stream_object, level=\"INFO\")\n        \"\"\"\n        with self._core.lock:\n            handler_id = self._core.handlers_count\n            self._core.handlers_count += 1\n\n        error_interceptor = ErrorInterceptor(catch, handler_id)\n\n        if colorize is None and serialize:\n            colorize = False\n\n        if isinstance(sink, (str, PathLike)):\n            path = sink\n            name = \"'%s'\" % path\n\n            if colorize is None:\n                colorize = False\n\n            wrapped_sink = FileSink(path, **kwargs)\n            kwargs = {}\n            encoding = wrapped_sink.encoding\n            terminator = \"\\n\"\n            exception_prefix = \"\"\n        elif hasattr(sink, \"write\") and callable(sink.write):\n            name = getattr(sink, \"name\", None) or repr(sink)\n\n            if colorize is None:\n                colorize = _colorama.should_colorize(sink)\n\n            if colorize is True and _colorama.should_wrap(sink):\n                stream = _colorama.wrap(sink)\n            else:\n                stream = sink\n\n            wrapped_sink = StreamSink(stream)\n            encoding = getattr(sink, \"encoding\", None)\n            terminator = \"\\n\"\n            exception_prefix = \"\"\n        elif isinstance(sink, logging.Handler):\n            name = repr(sink)\n\n            if colorize is None:\n                colorize = False\n\n            wrapped_sink = StandardSink(sink)\n            encoding = getattr(sink, \"encoding\", None)\n            terminator = \"\"\n            exception_prefix = \"\\n\"\n        elif iscoroutinefunction(sink) or iscoroutinefunction(\n            getattr(sink, \"__call__\", None)  # noqa: B004\n        ):\n            name = getattr(sink, \"__name__\", None) or repr(sink)\n\n            if colorize is None:\n                colorize = False\n\n            loop = kwargs.pop(\"loop\", None)\n\n            # The worker thread needs an event loop, it can't create a new one internally because it\n            # has to be accessible by the user while calling \"complete()\", instead we use the global\n            # one when the sink is added. If \"enqueue=False\" the event loop is dynamically retrieved\n            # at each logging call, which is much more convenient. However, coroutine can't access\n            # running loop in Python 3.5.2 and earlier versions, see python/asyncio#452.\n            if enqueue and loop is None:\n                try:\n                    loop = _asyncio_loop.get_running_loop()\n                except RuntimeError as e:\n                    raise ValueError(\n                        \"An event loop is required to add a coroutine sink with `enqueue=True`, \"\n                        \"but none has been passed as argument and none is currently running.\"\n                    ) from e\n\n            coro = sink if iscoroutinefunction(sink) else sink.__call__\n            wrapped_sink = AsyncSink(coro, loop, error_interceptor)\n            encoding = \"utf8\"\n            terminator = \"\\n\"\n            exception_prefix = \"\"\n        elif callable(sink):\n            name = getattr(sink, \"__name__\", None) or repr(sink)\n\n            if colorize is None:\n                colorize = False\n\n            wrapped_sink = CallableSink(sink)\n            encoding = \"utf8\"\n            terminator = \"\\n\"\n            exception_prefix = \"\"\n        else:\n            raise TypeError(\"Cannot log to objects of type '%s'\" % type(sink).__name__)\n\n        if kwargs:\n            raise TypeError(\"add() got an unexpected keyword argument '%s'\" % next(iter(kwargs)))\n\n        if filter is None:\n            filter_func = None\n        elif filter == \"\":\n            filter_func = _filters.filter_none\n        elif isinstance(filter, str):\n            parent = filter + \".\"\n            length = len(parent)\n            filter_func = functools.partial(_filters.filter_by_name, parent=parent, length=length)\n        elif isinstance(filter, dict):\n            level_per_module = {}\n            for module, level_ in filter.items():\n                if module is not None and not isinstance(module, str):\n                    raise TypeError(\n                        \"The filter dict contains an invalid module, \"\n                        \"it should be a string (or None), not: '%s'\" % type(module).__name__\n                    )\n                if level_ is False:\n                    levelno_ = False\n                elif level_ is True:\n                    levelno_ = 0\n                elif isinstance(level_, str):\n                    try:\n                        levelno_ = self.level(level_).no\n                    except ValueError:\n                        raise ValueError(\n                            \"The filter dict contains a module '%s' associated to a level name \"\n                            \"which does not exist: '%s'\" % (module, level_)\n                        ) from None\n                elif isinstance(level_, int):\n                    levelno_ = level_\n                else:\n                    raise TypeError(\n                        \"The filter dict contains a module '%s' associated to an invalid level, \"\n                        \"it should be an integer, a string or a boolean, not: '%s'\"\n                        % (module, type(level_).__name__)\n                    )\n                if levelno_ < 0:\n                    raise ValueError(\n                        \"The filter dict contains a module '%s' associated to an invalid level, \"\n                        \"it should be a positive integer, not: '%d'\" % (module, levelno_)\n                    )\n                level_per_module[module] = levelno_\n            filter_func = functools.partial(\n                _filters.filter_by_level, level_per_module=level_per_module\n            )\n        elif callable(filter):\n            if filter == builtins.filter:\n                raise ValueError(\n                    \"The built-in 'filter()' function cannot be used as a 'filter' parameter, \"\n                    \"this is most likely a mistake (please double-check the arguments passed \"\n                    \"to 'logger.add()').\"\n                )\n            filter_func = filter\n        else:\n            raise TypeError(\n                \"Invalid filter, it should be a function, a string or a dict, not: '%s'\"\n                % type(filter).__name__\n            )\n\n        if isinstance(level, str):\n            levelno = self.level(level).no\n        elif isinstance(level, int):\n            levelno = level\n        else:\n            raise TypeError(\n                \"Invalid level, it should be an integer or a string, not: '%s'\"\n                % type(level).__name__\n            )\n\n        if levelno < 0:\n            raise ValueError(\n                \"Invalid level value, it should be a positive integer, not: %d\" % levelno\n            )\n\n        if isinstance(format, str):\n            try:\n                formatter = Colorizer.prepare_format(format + terminator + \"{exception}\")\n            except ValueError as e:\n                raise ValueError(\n                    \"Invalid format, color markups could not be parsed correctly\"\n                ) from e\n            is_formatter_dynamic = False\n        elif callable(format):\n            if format == builtins.format:\n                raise ValueError(\n                    \"The built-in 'format()' function cannot be used as a 'format' parameter, \"\n                    \"this is most likely a mistake (please double-check the arguments passed \"\n                    \"to 'logger.add()').\"\n                )\n            formatter = format\n            is_formatter_dynamic = True\n        else:\n            raise TypeError(\n                \"Invalid format, it should be a string or a function, not: '%s'\"\n                % type(format).__name__\n            )\n\n        if not isinstance(encoding, str):\n            encoding = \"ascii\"\n\n        if isinstance(context, str):\n            context = get_context(context)\n        elif context is not None and not isinstance(context, BaseContext):\n            raise TypeError(\n                \"Invalid context, it should be a string or a multiprocessing context, \"\n                \"not: '%s'\" % type(context).__name__\n            )\n\n        with self._core.lock:\n            exception_formatter = ExceptionFormatter(\n                colorize=colorize,\n                encoding=encoding,\n                diagnose=diagnose,\n                backtrace=backtrace,\n                hidden_frames_filename=self.catch.__code__.co_filename,\n                prefix=exception_prefix,\n            )\n\n            handler = Handler(\n                name=name,\n                sink=wrapped_sink,\n                levelno=levelno,\n                formatter=formatter,\n                is_formatter_dynamic=is_formatter_dynamic,\n                filter_=filter_func,\n                colorize=colorize,\n                serialize=serialize,\n                enqueue=enqueue,\n                multiprocessing_context=context,\n                id_=handler_id,\n                error_interceptor=error_interceptor,\n                exception_formatter=exception_formatter,\n                levels_ansi_codes=self._core.levels_ansi_codes,\n            )\n\n            handlers = self._core.handlers.copy()\n            handlers[handler_id] = handler\n\n            self._core.min_level = min(self._core.min_level, levelno)\n            self._core.handlers = handlers\n\n        return handler_id\n\n    def remove(self, handler_id=None):\n        \"\"\"Remove a previously added handler and stop sending logs to its sink.\n\n        Parameters\n        ----------\n        handler_id : |int| or ``None``\n            The id of the sink to remove, as it was returned by the |add| method. If ``None``, all\n            handlers are removed. The pre-configured handler is guaranteed to have the index ``0``.\n\n        Raises\n        ------\n        ValueError\n            If ``handler_id`` is not ``None`` but there is no active handler with such id.\n\n        Examples\n        --------\n        >>> i = logger.add(sys.stderr, format=\"{message}\")\n        >>> logger.info(\"Logging\")\n        Logging\n        >>> logger.remove(i)\n        >>> logger.info(\"No longer logging\")\n        \"\"\"\n        if not (handler_id is None or isinstance(handler_id, int)):\n            raise TypeError(\n                \"Invalid handler id, it should be an integer as returned \"\n                \"by the 'add()' method (or None), not: '%s'\" % type(handler_id).__name__\n            )\n\n        with self._core.lock:\n            if handler_id is not None and handler_id not in self._core.handlers:\n                raise ValueError(\"There is no existing handler with id %d\" % handler_id) from None\n\n            if handler_id is None:\n                handler_ids = list(self._core.handlers)\n            else:\n                handler_ids = [handler_id]\n\n            for handler_id in handler_ids:\n                handlers = self._core.handlers.copy()\n                handler = handlers.pop(handler_id)\n\n                # This needs to be done first in case \"stop()\" raises an exception\n                levelnos = (h.levelno for h in handlers.values())\n                self._core.min_level = min(levelnos, default=float(\"inf\"))\n                self._core.handlers = handlers\n\n                handler.stop()\n\n    def complete(self):\n        \"\"\"Wait for the end of enqueued messages and asynchronous tasks scheduled by handlers.\n\n        This method proceeds in two steps: first it waits for all logging messages added to handlers\n        with ``enqueue=True`` to be processed, then it returns an object that can be awaited to\n        finalize all logging tasks added to the event loop by coroutine sinks.\n\n        It can be called from non-asynchronous code. This is especially recommended when the\n        ``logger`` is utilized with ``multiprocessing`` to ensure messages put to the internal\n        queue have been properly transmitted before leaving a child process.\n\n        The returned object should be awaited before the end of a coroutine executed by\n        |asyncio.run| or |loop.run_until_complete| to ensure all asynchronous logging messages are\n        processed. The function |asyncio.get_running_loop| is called beforehand, only tasks\n        scheduled in the same loop that the current one will be awaited by the method.\n\n        Returns\n        -------\n        :term:`awaitable`\n            An awaitable object which ensures all asynchronous logging calls are completed when\n            awaited.\n\n        Examples\n        --------\n        >>> async def sink(message):\n        ...     await asyncio.sleep(0.1)  # IO processing...\n        ...     print(message, end=\"\")\n        ...\n        >>> async def work():\n        ...     logger.info(\"Start\")\n        ...     logger.info(\"End\")\n        ...     await logger.complete()\n        ...\n        >>> logger.add(sink)\n        1\n        >>> asyncio.run(work())\n        Start\n        End\n\n        >>> def process():\n        ...     logger.info(\"Message sent from the child\")\n        ...     logger.complete()\n        ...\n        >>> logger.add(sys.stderr, enqueue=True)\n        1\n        >>> process = multiprocessing.Process(target=process)\n        >>> process.start()\n        >>> process.join()\n        Message sent from the child\n        \"\"\"\n        tasks = []\n\n        with self._core.lock:\n            handlers = self._core.handlers.copy()\n            for handler in handlers.values():\n                handler.complete_queue()\n                tasks.extend(handler.tasks_to_complete())\n\n        class AwaitableCompleter:\n            def __await__(self):\n                for task in tasks:\n                    yield from task.__await__()\n\n        return AwaitableCompleter()\n\n    def catch(\n        self,\n        exception=Exception,\n        *,\n        level=\"ERROR\",\n        reraise=False,\n        onerror=None,\n        exclude=None,\n        default=None,\n        message=\"An error has been caught in function '{record[function]}', \"\n        \"process '{record[process].name}' ({record[process].id}), \"\n        \"thread '{record[thread].name}' ({record[thread].id}):\"\n    ):\n        \"\"\"Return a decorator to automatically log possibly caught error in wrapped function.\n\n        This is useful to ensure unexpected exceptions are logged, the entire program can be\n        wrapped by this method. This is also very useful to decorate |Thread.run| methods while\n        using threads to propagate errors to the main logger thread.\n\n        Note that the visibility of variables values (which uses the great |better_exceptions|_\n        library from `@Qix-`_) depends on the ``diagnose`` option of each configured sink.\n\n        The returned object can also be used as a context manager.\n\n        Parameters\n        ----------\n        exception : |Exception|, optional\n            The type of exception to intercept. If several types should be caught, a tuple of\n            exceptions can be used too.\n        level : |str| or |int|, optional\n            The level name or severity with which the message should be logged.\n        reraise : |bool|, optional\n            Whether the exception should be raised again and hence propagated to the caller.\n        onerror : |callable|_, optional\n            A function that will be called if an error occurs, once the message has been logged.\n            It should accept the exception instance as it sole argument.\n        exclude : |Exception|, optional\n            A type of exception (or a tuple of types) that will be purposely ignored and hence\n            propagated to the caller without being logged.\n        default : |Any|, optional\n            The value to be returned by the decorated function if an error occurred without being\n            re-raised.\n        message : |str|, optional\n            The message that will be automatically logged if an exception occurs. Note that it will\n            be formatted with the ``record`` attribute.\n\n        Returns\n        -------\n        :term:`decorator` / :term:`context manager`\n            An object that can be used to decorate a function or as a context manager to log\n            exceptions possibly caught.\n\n        Examples\n        --------\n        >>> @logger.catch\n        ... def f(x):\n        ...     100 / x\n        ...\n        >>> def g():\n        ...     f(10)\n        ...     f(0)\n        ...\n        >>> g()\n        ERROR - An error has been caught in function 'g', process 'Main' (367), thread 'ch1' (1398):\n        Traceback (most recent call last):\n          File \"program.py\", line 12, in <module>\n            g()\n            â”” <function g at 0x7f225fe2bc80>\n        > File \"program.py\", line 10, in g\n            f(0)\n            â”” <function f at 0x7f225fe2b9d8>\n          File \"program.py\", line 6, in f\n            100 / x\n                  â”” 0\n        ZeroDivisionError: division by zero\n\n        >>> with logger.catch(message=\"Because we never know...\"):\n        ...    main()  # No exception, no logs\n\n        >>> # Use 'onerror' to prevent the program exit code to be 0 (if 'reraise=False') while\n        >>> # also avoiding the stacktrace to be duplicated on stderr (if 'reraise=True').\n        >>> @logger.catch(onerror=lambda _: sys.exit(1))\n        ... def main():\n        ...     1 / 0\n        \"\"\"\n        if callable(exception) and (\n            not isclass(exception) or not issubclass(exception, BaseException)\n        ):\n            return self.catch()(exception)\n\n        logger = self\n\n        class Catcher:\n            def __init__(self, from_decorator):\n                self._from_decorator = from_decorator\n\n            def __enter__(self):\n                return None\n\n            def __exit__(self, type_, value, traceback_):\n                if type_ is None:\n                    return None\n\n                # We must prevent infinite recursion in case \"logger.catch()\" handles an exception\n                # that occurs while logging another exception. This can happen for example when\n                # the exception formatter calls \"repr(obj)\" while the \"__repr__\" method is broken\n                # but decorated with \"logger.catch()\". In such a case, we ignore the catching\n                # mechanism and just let the exception be thrown (that way, the formatter will\n                # rightly assume the object is unprintable).\n                if getattr(logger._core.thread_locals, \"already_logging_exception\", False):\n                    return False\n\n                if not issubclass(type_, exception):\n                    return False\n\n                if exclude is not None and issubclass(type_, exclude):\n                    return False\n\n                from_decorator = self._from_decorator\n                _, depth, _, *options = logger._options\n\n                if from_decorator:\n                    depth += 1\n\n                catch_options = [(type_, value, traceback_), depth, True, *options]\n\n                logger._core.thread_locals.already_logging_exception = True\n                try:\n                    logger._log(level, from_decorator, catch_options, message, (), {})\n                finally:\n                    logger._core.thread_locals.already_logging_exception = False\n\n                if onerror is not None:\n                    onerror(value)\n\n                return not reraise\n\n            def __call__(self, function):\n                if isclass(function):\n                    raise TypeError(\n                        \"Invalid object decorated with 'catch()', it must be a function, \"\n                        \"not a class (tried to wrap '%s')\" % function.__name__\n                    )\n\n                catcher = Catcher(True)\n\n                if iscoroutinefunction(function):\n\n                    async def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return await function(*args, **kwargs)\n                        return default\n\n                elif isgeneratorfunction(function):\n\n                    def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return (yield from function(*args, **kwargs))\n                        return default\n\n                else:\n\n                    def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return function(*args, **kwargs)\n                        return default\n\n                functools.update_wrapper(catch_wrapper, function)\n                return catch_wrapper\n\n        return Catcher(False)\n\n    def opt(\n        self,\n        *,\n        exception=None,\n        record=False,\n        lazy=False,\n        colors=False,\n        raw=False,\n        capture=True,\n        depth=0,\n        ansi=False\n    ):\n        r\"\"\"Parametrize a logging call to slightly change generated log message.\n\n        Note that it's not possible to chain |opt| calls, the last one takes precedence over the\n        others as it will \"reset\" the options to their default values.\n\n        Parameters\n        ----------\n        exception : |bool|, |tuple| or |Exception|, optional\n            If it does not evaluate as ``False``, the passed exception is formatted and added to the\n            log message. It could be an |Exception| object or a ``(type, value, traceback)`` tuple,\n            otherwise the exception information is retrieved from |sys.exc_info|.\n        record : |bool|, optional\n            If ``True``, the record dict contextualizing the logging call can be used to format the\n            message by using ``{record[key]}`` in the log message.\n        lazy : |bool|, optional\n            If ``True``, the logging call attribute to format the message should be functions which\n            will be called only if the level is high enough. This can be used to avoid expensive\n            functions if not necessary.\n        colors : |bool|, optional\n            If ``True``, logged message will be colorized according to the markups it possibly\n            contains.\n        raw : |bool|, optional\n            If ``True``, the formatting of each sink will be bypassed and the message will be sent\n            as is.\n        capture : |bool|, optional\n            If ``False``, the ``**kwargs`` of logged message will not automatically populate\n            the ``extra`` dict (although they are still used for formatting).\n        depth : |int|, optional\n            Specify which stacktrace should be used to contextualize the logged message. This is\n            useful while using the logger from inside a wrapped function to retrieve worthwhile\n            information.\n        ansi : |bool|, optional\n            Deprecated since version 0.4.1: the ``ansi`` parameter will be removed in Loguru 1.0.0,\n            it is replaced by ``colors`` which is a more appropriate name.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but transforming logged message adequately before\n            sending.\n\n        Examples\n        --------\n        >>> try:\n        ...     1 / 0\n        ... except ZeroDivisionError:\n        ...    logger.opt(exception=True).debug(\"Exception logged with debug level:\")\n        ...\n        [18:10:02] DEBUG in '<module>' - Exception logged with debug level:\n        Traceback (most recent call last, catch point marked):\n        > File \"<stdin>\", line 2, in <module>\n        ZeroDivisionError: division by zero\n\n        >>> logger.opt(record=True).info(\"Current line is: {record[line]}\")\n        [18:10:33] INFO in '<module>' - Current line is: 1\n\n        >>> logger.opt(lazy=True).debug(\"If sink <= DEBUG: {x}\", x=lambda: math.factorial(2**5))\n        [18:11:19] DEBUG in '<module>' - If sink <= DEBUG: 263130836933693530167218012160000000\n\n        >>> logger.opt(colors=True).warning(\"We got a <red>BIG</red> problem\")\n        [18:11:30] WARNING in '<module>' - We got a BIG problem\n\n        >>> logger.opt(raw=True).debug(\"No formatting\\n\")\n        No formatting\n\n        >>> logger.opt(capture=False).info(\"Displayed but not captured: {value}\", value=123)\n        [18:11:41] Displayed but not captured: 123\n\n        >>> def wrapped():\n        ...     logger.opt(depth=1).info(\"Get parent context\")\n        ...\n        >>> def func():\n        ...     wrapped()\n        ...\n        >>> func()\n        [18:11:54] DEBUG in 'func' - Get parent context\n        \"\"\"\n        if ansi:\n            colors = True\n            warnings.warn(\n                \"The 'ansi' parameter is deprecated, please use 'colors' instead\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        args = self._options[-2:]\n        return Logger(self._core, exception, depth, record, lazy, colors, raw, capture, *args)\n\n    def bind(__self, **kwargs):  # noqa: N805\n        \"\"\"Bind attributes to the ``extra`` dict of each logged message record.\n\n        This is used to add custom context to each logging call.\n\n        Parameters\n        ----------\n        **kwargs\n            Mapping between keys and values that will be added to the ``extra`` dict.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which sends record with the customized ``extra``\n            dict.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[ip]} - {message}\")\n        >>> class Server:\n        ...     def __init__(self, ip):\n        ...         self.ip = ip\n        ...         self.logger = logger.bind(ip=ip)\n        ...     def call(self, message):\n        ...         self.logger.info(message)\n        ...\n        >>> instance_1 = Server(\"192.168.0.200\")\n        >>> instance_2 = Server(\"127.0.0.1\")\n        >>> instance_1.call(\"First instance\")\n        192.168.0.200 - First instance\n        >>> instance_2.call(\"Second instance\")\n        127.0.0.1 - Second instance\n        \"\"\"\n        *options, extra = __self._options\n        return Logger(__self._core, *options, {**extra, **kwargs})\n\n    @contextlib.contextmanager\n    def contextualize(__self, **kwargs):  # noqa: N805\n        \"\"\"Bind attributes to the context-local ``extra`` dict while inside the ``with`` block.\n\n        Contrary to |bind| there is no ``logger`` returned, the ``extra`` dict is modified in-place\n        and updated globally. Most importantly, it uses |contextvars| which means that\n        contextualized values are unique to each threads and asynchronous tasks.\n\n        The ``extra`` dict will retrieve its initial state once the context manager is exited.\n\n        Parameters\n        ----------\n        **kwargs\n            Mapping between keys and values that will be added to the context-local ``extra`` dict.\n\n        Returns\n        -------\n        :term:`context manager` / :term:`decorator`\n            A context manager (usable as a decorator too) that will bind the attributes once entered\n            and restore the initial state of the ``extra`` dict while exited.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{message} | {extra}\")\n        1\n        >>> def task():\n        ...     logger.info(\"Processing!\")\n        ...\n        >>> with logger.contextualize(task_id=123):\n        ...     task()\n        ...\n        Processing! | {'task_id': 123}\n        >>> logger.info(\"Done.\")\n        Done. | {}\n        \"\"\"\n        with __self._core.lock:\n            new_context = {**context.get(), **kwargs}\n            token = context.set(new_context)\n\n        try:\n            yield\n        finally:\n            with __self._core.lock:\n                context.reset(token)\n\n    def patch(self, patcher):\n        \"\"\"Attach a function to modify the record dict created by each logging call.\n\n        The ``patcher`` may be used to update the record on-the-fly before it's propagated to the\n        handlers. This allows the \"extra\" dict to be populated with dynamic values and also permits\n        advanced modifications of the record emitted while logging a message. The function is called\n        once before sending the log message to the different handlers.\n\n        It is recommended to apply modification on the ``record[\"extra\"]`` dict rather than on the\n        ``record`` dict itself, as some values are used internally by `Loguru`, and modify them may\n        produce unexpected results.\n\n        The logger can be patched multiple times. In this case, the functions are called in the\n        same order as they are added.\n\n        Parameters\n        ----------\n        patcher: |callable|_\n            The function to which the record dict will be passed as the sole argument. This function\n            is in charge of updating the record in-place, the function does not need to return any\n            value, the modified record object will be re-used.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which records are passed through the ``patcher``\n            function before being sent to the added handlers.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[utc]} {message}\")\n        >>> logger = logger.patch(lambda record: record[\"extra\"].update(utc=datetime.utcnow())\n        >>> logger.info(\"That's way, you can log messages with time displayed in UTC\")\n\n        >>> def wrapper(func):\n        ...     @functools.wraps(func)\n        ...     def wrapped(*args, **kwargs):\n        ...         logger.patch(lambda r: r.update(function=func.__name__)).info(\"Wrapped!\")\n        ...         return func(*args, **kwargs)\n        ...     return wrapped\n\n        >>> def recv_record_from_network(pipe):\n        ...     record = pickle.loads(pipe.read())\n        ...     level, message = record[\"level\"], record[\"message\"]\n        ...     logger.patch(lambda r: r.update(record)).log(level, message)\n        \"\"\"\n        *options, patchers, extra = self._options\n        return Logger(self._core, *options, [*patchers, patcher], extra)\n\n    def level(self, name, no=None, color=None, icon=None):\n        r\"\"\"Add, update or retrieve a logging level.\n\n        Logging levels are defined by their ``name`` to which a severity ``no``, an ansi ``color``\n        tag and an ``icon`` are associated and possibly modified at run-time. To |log| to a custom\n        level, you should necessarily use its name, the severity number is not linked back to levels\n        name (this implies that several levels can share the same severity).\n\n        To add a new level, its ``name`` and its ``no`` are required. A ``color`` and an ``icon``\n        can also be specified or will be empty by default.\n\n        To update an existing level, pass its ``name`` with the parameters to be changed. It is not\n        possible to modify the ``no`` of a level once it has been added.\n\n        To retrieve level information, the ``name`` solely suffices.\n\n        Parameters\n        ----------\n        name : |str|\n            The name of the logging level.\n        no : |int|\n            The severity of the level to be added or updated.\n        color : |str|\n            The color markup of the level to be added or updated.\n        icon : |str|\n            The icon of the level to be added or updated.\n\n        Returns\n        -------\n        ``Level``\n            A |namedtuple| containing information about the level.\n\n        Raises\n        ------\n        ValueError\n            If there is no level registered with such ``name``.\n\n        Examples\n        --------\n        >>> level = logger.level(\"ERROR\")\n        >>> print(level)\n        Level(name='ERROR', no=40, color='<red><bold>', icon='âŒ')\n        >>> logger.add(sys.stderr, format=\"{level.no} {level.icon} {message}\")\n        1\n        >>> logger.level(\"CUSTOM\", no=15, color=\"<blue>\", icon=\"@\")\n        Level(name='CUSTOM', no=15, color='<blue>', icon='@')\n        >>> logger.log(\"CUSTOM\", \"Logging...\")\n        15 @ Logging...\n        >>> logger.level(\"WARNING\", icon=r\"/!\\\\\")\n        Level(name='WARNING', no=30, color='<yellow><bold>', icon='/!\\\\\\\\')\n        >>> logger.warning(\"Updated!\")\n        30 /!\\\\ Updated!\n        \"\"\"\n        if not isinstance(name, str):\n            raise TypeError(\n                \"Invalid level name, it should be a string, not: '%s'\" % type(name).__name__\n            )\n\n        if no is color is icon is None:\n            try:\n                return self._core.levels[name]\n            except KeyError:\n                raise ValueError(\"Level '%s' does not exist\" % name) from None\n\n        if name not in self._core.levels:\n            if no is None:\n                raise ValueError(\n                    \"Level '%s' does not exist, you have to create it by specifying a level no\"\n                    % name\n                )\n            old_color, old_icon = \"\", \" \"\n        elif no is not None:\n            raise ValueError(\"Level '%s' already exists, you can't update its severity no\" % name)\n        else:\n            _, no, old_color, old_icon = self.level(name)\n\n        if color is None:\n            color = old_color\n\n        if icon is None:\n            icon = old_icon\n\n        if not isinstance(no, int):\n            raise TypeError(\n                \"Invalid level no, it should be an integer, not: '%s'\" % type(no).__name__\n            )\n\n        if no < 0:\n            raise ValueError(\"Invalid level no, it should be a positive integer, not: %d\" % no)\n\n        ansi = Colorizer.ansify(color)\n        level = Level(name, no, color, icon)\n\n        with self._core.lock:\n            self._core.levels[name] = level\n            self._core.levels_ansi_codes[name] = ansi\n            self._core.levels_lookup[name] = (name, name, no, icon)\n            for handler in self._core.handlers.values():\n                handler.update_format(name)\n\n        return level\n\n    def disable(self, name):\n        \"\"\"Disable logging of messages coming from ``name`` module and its children.\n\n        Developers of library using `Loguru` should absolutely disable it to avoid disrupting\n        users with unrelated logs messages.\n\n        Note that in some rare circumstances, it is not possible for `Loguru` to\n        determine the module's ``__name__`` value. In such situation, ``record[\"name\"]`` will be\n        equal to ``None``, this is why ``None`` is also a valid argument.\n\n        Parameters\n        ----------\n        name : |str| or ``None``\n            The name of the parent module to disable.\n\n        Examples\n        --------\n        >>> logger.info(\"Allowed message by default\")\n        [22:21:55] Allowed message by default\n        >>> logger.disable(\"my_library\")\n        >>> logger.info(\"While publishing a library, don't forget to disable logging\")\n        \"\"\"\n        self._change_activation(name, False)\n\n    def enable(self, name):\n        \"\"\"Enable logging of messages coming from ``name`` module and its children.\n\n        Logging is generally disabled by imported library using `Loguru`, hence this function\n        allows users to receive these messages anyway.\n\n        To enable all logs regardless of the module they are coming from, an empty string ``\"\"`` can\n        be passed.\n\n        Parameters\n        ----------\n        name : |str| or ``None``\n            The name of the parent module to re-allow.\n\n        Examples\n        --------\n        >>> logger.disable(\"__main__\")\n        >>> logger.info(\"Disabled, so nothing is logged.\")\n        >>> logger.enable(\"__main__\")\n        >>> logger.info(\"Re-enabled, messages are logged.\")\n        [22:46:12] Re-enabled, messages are logged.\n        \"\"\"\n        self._change_activation(name, True)\n\n    def configure(self, *, handlers=None, levels=None, extra=None, patcher=None, activation=None):\n        \"\"\"Configure the core logger.\n\n        It should be noted that ``extra`` values set using this function are available across all\n        modules, so this is the best way to set overall default values.\n\n        To load the configuration directly from a file, such as JSON or YAML, it is also possible to\n        use the |loguru-config|_ library developed by `@erezinman`_.\n\n        Parameters\n        ----------\n        handlers : |list| of |dict|, optional\n            A list of each handler to be added. The list should contain dicts of params passed to\n            the |add| function as keyword arguments. If not ``None``, all previously added\n            handlers are first removed.\n        levels : |list| of |dict|, optional\n            A list of each level to be added or updated. The list should contain dicts of params\n            passed to the |level| function as keyword arguments. This will never remove previously\n            created levels.\n        extra : |dict|, optional\n            A dict containing additional parameters bound to the core logger, useful to share\n            common properties if you call |bind| in several of your files modules. If not ``None``,\n            this will remove previously configured ``extra`` dict.\n        patcher : |callable|_, optional\n            A function that will be applied to the record dict of each logged messages across all\n            modules using the logger. It should modify the dict in-place without returning anything.\n            The function is executed prior to the one possibly added by the |patch| method. If not\n            ``None``, this will replace previously configured ``patcher`` function.\n        activation : |list| of |tuple|, optional\n            A list of ``(name, state)`` tuples which denotes which loggers should be enabled (if\n            ``state`` is ``True``) or disabled (if ``state`` is ``False``). The calls to |enable|\n            and |disable| are made accordingly to the list order. This will not modify previously\n            activated loggers, so if you need a fresh start prepend your list with ``(\"\", False)``\n            or ``(\"\", True)``.\n\n        Returns\n        -------\n        :class:`list` of :class:`int`\n            A list containing the identifiers of added sinks (if any).\n\n        Examples\n        --------\n        >>> logger.configure(\n        ...     handlers=[\n        ...         dict(sink=sys.stderr, format=\"[{time}] {message}\"),\n        ...         dict(sink=\"file.log\", enqueue=True, serialize=True),\n        ...     ],\n        ...     levels=[dict(name=\"NEW\", no=13, icon=\"Â¤\", color=\"\")],\n        ...     extra={\"common_to_all\": \"default\"},\n        ...     patcher=lambda record: record[\"extra\"].update(some_value=42),\n        ...     activation=[(\"my_module.secret\", False), (\"another_library.module\", True)],\n        ... )\n        [1, 2]\n\n        >>> # Set a default \"extra\" dict to logger across all modules, without \"bind()\"\n        >>> extra = {\"context\": \"foo\"}\n        >>> logger.configure(extra=extra)\n        >>> logger.add(sys.stderr, format=\"{extra[context]} - {message}\")\n        >>> logger.info(\"Context without bind\")\n        >>> # => \"foo - Context without bind\"\n        >>> logger.bind(context=\"bar\").info(\"Suppress global context\")\n        >>> # => \"bar - Suppress global context\"\n        \"\"\"\n        if handlers is not None:\n            self.remove()\n        else:\n            handlers = []\n\n        if levels is not None:\n            for params in levels:\n                self.level(**params)\n\n        if patcher is not None:\n            with self._core.lock:\n                self._core.patcher = patcher\n\n        if extra is not None:\n            with self._core.lock:\n                self._core.extra.clear()\n                self._core.extra.update(extra)\n\n        if activation is not None:\n            for name, state in activation:\n                if state:\n                    self.enable(name)\n                else:\n                    self.disable(name)\n\n        return [self.add(**params) for params in handlers]\n\n    def _change_activation(self, name, status):\n        if not (name is None or isinstance(name, str)):\n            raise TypeError(\n                \"Invalid name, it should be a string (or None), not: '%s'\" % type(name).__name__\n            )\n\n        with self._core.lock:\n            enabled = self._core.enabled.copy()\n\n            if name is None:\n                for n in enabled:\n                    if n is None:\n                        enabled[n] = status\n                self._core.activation_none = status\n                self._core.enabled = enabled\n                return\n\n            if name != \"\":\n                name += \".\"\n\n            activation_list = [\n                (n, s) for n, s in self._core.activation_list if n[: len(name)] != name\n            ]\n\n            parent_status = next((s for n, s in activation_list if name[: len(n)] == n), None)\n            if parent_status != status and not (name == \"\" and status is True):\n                activation_list.append((name, status))\n\n                def modules_depth(x):\n                    return x[0].count(\".\")\n\n                activation_list.sort(key=modules_depth, reverse=True)\n\n            for n in enabled:\n                if n is not None and (n + \".\")[: len(name)] == name:\n                    enabled[n] = status\n\n            self._core.activation_list = activation_list\n            self._core.enabled = enabled\n\n    @staticmethod\n    def parse(file, pattern, *, cast={}, chunk=2**16):  # noqa: B006\n        \"\"\"Parse raw logs and extract each entry as a |dict|.\n\n        The logging format has to be specified as the regex ``pattern``, it will then be\n        used to parse the ``file`` and retrieve each entry based on the named groups present\n        in the regex.\n\n        Parameters\n        ----------\n        file : |str|, |Path| or |file-like object|_\n            The path of the log file to be parsed, or an already opened file object.\n        pattern : |str| or |re.Pattern|_\n            The regex to use for logs parsing, it should contain named groups which will be included\n            in the returned dict.\n        cast : |callable|_ or |dict|, optional\n            A function that should convert in-place the regex groups parsed (a dict of string\n            values) to more appropriate types. If a dict is passed, it should be a mapping between\n            keys of parsed log dict and the function that should be used to convert the associated\n            value.\n        chunk : |int|, optional\n            The number of bytes read while iterating through the logs, this avoids having to load\n            the whole file in memory.\n\n        Yields\n        ------\n        :class:`dict`\n            The dict mapping regex named groups to matched values, as returned by |match.groupdict|\n            and optionally converted according to ``cast`` argument.\n\n        Examples\n        --------\n        >>> reg = r\"(?P<lvl>[0-9]+): (?P<msg>.*)\"    # If log format is \"{level.no} - {message}\"\n        >>> for e in logger.parse(\"file.log\", reg):  # A file line could be \"10 - A debug message\"\n        ...     print(e)                             # => {'lvl': '10', 'msg': 'A debug message'}\n\n        >>> caster = dict(lvl=int)                   # Parse 'lvl' key as an integer\n        >>> for e in logger.parse(\"file.log\", reg, cast=caster):\n        ...     print(e)                             # => {'lvl': 10, 'msg': 'A debug message'}\n\n        >>> def cast(groups):\n        ...     if \"date\" in groups:\n        ...         groups[\"date\"] = datetime.strptime(groups[\"date\"], \"%Y-%m-%d %H:%M:%S\")\n        ...\n        >>> with open(\"file.log\") as file:\n        ...     for log in logger.parse(file, reg, cast=cast):\n        ...         print(log[\"date\"], log[\"something_else\"])\n        \"\"\"\n        if isinstance(file, (str, PathLike)):\n\n            @contextlib.contextmanager\n            def opener():\n                with open(str(file)) as fileobj:\n                    yield fileobj\n\n        elif hasattr(file, \"read\") and callable(file.read):\n\n            @contextlib.contextmanager\n            def opener():\n                yield file\n\n        else:\n            raise TypeError(\n                \"Invalid file, it should be a string path or a file object, not: '%s'\"\n                % type(file).__name__\n            )\n\n        if isinstance(cast, dict):\n\n            def cast_function(groups):\n                for key, converter in cast.items():\n                    if key in groups:\n                        groups[key] = converter(groups[key])\n\n        elif callable(cast):\n            cast_function = cast\n        else:\n            raise TypeError(\n                \"Invalid cast, it should be a function or a dict, not: '%s'\" % type(cast).__name__\n            )\n\n        try:\n            regex = re.compile(pattern)\n        except TypeError:\n            raise TypeError(\n                \"Invalid pattern, it should be a string or a compiled regex, not: '%s'\"\n                % type(pattern).__name__\n            ) from None\n\n        with opener() as fileobj:\n            matches = Logger._find_iter(fileobj, regex, chunk)\n\n            for match in matches:\n                groups = match.groupdict()\n                cast_function(groups)\n                yield groups\n\n    @staticmethod\n    def _find_iter(fileobj, regex, chunk):\n        buffer = fileobj.read(0)\n\n        while True:\n            text = fileobj.read(chunk)\n            buffer += text\n            matches = list(regex.finditer(buffer))\n\n            if not text:\n                yield from matches\n                break\n\n            if len(matches) > 1:\n                end = matches[-2].end()\n                buffer = buffer[end:]\n                yield from matches[:-1]\n\n    def _log(self, level, from_decorator, options, message, args, kwargs):\n        core = self._core\n\n        if not core.handlers:\n            return\n\n        try:\n            level_id, level_name, level_no, level_icon = core.levels_lookup[level]\n        except (KeyError, TypeError):\n            if isinstance(level, str):\n                raise ValueError(\"Level '%s' does not exist\" % level) from None\n            if not isinstance(level, int):\n                raise TypeError(\n                    \"Invalid level, it should be an integer or a string, not: '%s'\"\n                    % type(level).__name__\n                ) from None\n            if level < 0:\n                raise ValueError(\n                    \"Invalid level value, it should be a positive integer, not: %d\" % level\n                ) from None\n            cache = (None, \"Level %d\" % level, level, \" \")\n            level_id, level_name, level_no, level_icon = cache\n            core.levels_lookup[level] = cache\n\n        if level_no < core.min_level:\n            return\n\n        (exception, depth, record, lazy, colors, raw, capture, patchers, extra) = options\n\n        frame = get_frame(depth + 2)\n\n        try:\n            name = frame.f_globals[\"__name__\"]\n        except KeyError:\n            name = None\n\n        try:\n            if not core.enabled[name]:\n                return\n        except KeyError:\n            enabled = core.enabled\n            if name is None:\n                status = core.activation_none\n                enabled[name] = status\n                if not status:\n                    return\n            else:\n                dotted_name = name + \".\"\n                for dotted_module_name, status in core.activation_list:\n                    if dotted_name[: len(dotted_module_name)] == dotted_module_name:\n                        if status:\n                            break\n                        enabled[name] = False\n                        return\n                enabled[name] = True\n\n        current_datetime = aware_now()\n\n        code = frame.f_code\n        file_path = code.co_filename\n        file_name = basename(file_path)\n        thread = current_thread()\n        process = current_process()\n        elapsed = current_datetime - start_time\n\n        if exception:\n            if isinstance(exception, BaseException):\n                type_, value, traceback = (type(exception), exception, exception.__traceback__)\n            elif isinstance(exception, tuple):\n                type_, value, traceback = exception\n            else:\n                type_, value, traceback = sys.exc_info()\n            exception = RecordException(type_, value, traceback)\n        else:\n            exception = None\n\n        log_record = {\n            \"elapsed\": elapsed,\n            \"exception\": exception,\n            \"extra\": {**core.extra, **context.get(), **extra},\n            \"file\": RecordFile(file_name, file_path),\n            \"function\": code.co_name,\n            \"level\": RecordLevel(level_name, level_no, level_icon),\n            \"line\": frame.f_lineno,\n            \"message\": str(message),\n            \"module\": splitext(file_name)[0],\n            \"name\": name,\n            \"process\": RecordProcess(process.ident, process.name),\n            \"thread\": RecordThread(thread.ident, thread.name),\n            \"time\": current_datetime,\n        }\n\n        if lazy:\n            args = [arg() for arg in args]\n            kwargs = {key: value() for key, value in kwargs.items()}\n\n        if capture and kwargs:\n            log_record[\"extra\"].update(kwargs)\n\n        if record:\n            if \"record\" in kwargs:\n                raise TypeError(\n                    \"The message can't be formatted: 'record' shall not be used as a keyword \"\n                    \"argument while logger has been configured with '.opt(record=True)'\"\n                )\n            kwargs.update(record=log_record)\n\n        if colors:\n            if args or kwargs:\n                colored_message = Colorizer.prepare_message(message, args, kwargs)\n            else:\n                colored_message = Colorizer.prepare_simple_message(str(message))\n            log_record[\"message\"] = colored_message.stripped\n        elif args or kwargs:\n            colored_message = None\n            log_record[\"message\"] = message.format(*args, **kwargs)\n        else:\n            colored_message = None\n\n        if core.patcher:\n            core.patcher(log_record)\n\n        for patcher in patchers:\n            patcher(log_record)\n\n        for handler in core.handlers.values():\n            handler.emit(log_record, level_id, from_decorator, raw, colored_message)\n\n    def trace(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'TRACE'``.\"\"\"\n        __self._log(\"TRACE\", False, __self._options, __message, args, kwargs)\n\n    def debug(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'DEBUG'``.\"\"\"\n        __self._log(\"DEBUG\", False, __self._options, __message, args, kwargs)\n\n    def info(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'INFO'``.\"\"\"\n        __self._log(\"INFO\", False, __self._options, __message, args, kwargs)\n\n    def success(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'SUCCESS'``.\"\"\"\n        __self._log(\"SUCCESS\", False, __self._options, __message, args, kwargs)\n\n    def warning(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'WARNING'``.\"\"\"\n        __self._log(\"WARNING\", False, __self._options, __message, args, kwargs)\n\n    def error(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'ERROR'``.\"\"\"\n        __self._log(\"ERROR\", False, __self._options, __message, args, kwargs)\n\n    def critical(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``'CRITICAL'``.\"\"\"\n        __self._log(\"CRITICAL\", False, __self._options, __message, args, kwargs)\n\n    def exception(__self, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log an ``'ERROR'```` message while also capturing the currently handled exception.\"\"\"\n        options = (True,) + __self._options[1:]\n        __self._log(\"ERROR\", False, options, __message, args, kwargs)\n\n    def log(__self, __level, __message, *args, **kwargs):  # noqa: N805\n        r\"\"\"Log ``message.format(*args, **kwargs)`` with severity ``level``.\"\"\"\n        __self._log(__level, False, __self._options, __message, args, kwargs)\n\n    def start(self, *args, **kwargs):\n        \"\"\"Add a handler sending log messages to a sink adequately configured.\n\n        Deprecated function, use |add| instead.\n\n        Warnings\n        --------\n        .. deprecated:: 0.2.2\n          ``start()`` will be removed in Loguru 1.0.0, it is replaced by ``add()`` which is a less\n          confusing name.\n        \"\"\"\n        warnings.warn(\n            \"The 'start()' method is deprecated, please use 'add()' instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.add(*args, **kwargs)\n\n    def stop(self, *args, **kwargs):\n        \"\"\"Remove a previously added handler and stop sending logs to its sink.\n\n        Deprecated function, use |remove| instead.\n\n        Warnings\n        --------\n        .. deprecated:: 0.2.2\n          ``stop()`` will be removed in Loguru 1.0.0, it is replaced by ``remove()`` which is a less\n          confusing name.\n        \"\"\"\n        warnings.warn(\n            \"The 'stop()' method is deprecated, please use 'remove()' instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.remove(*args, **kwargs)\n",
          "import asyncio\nimport builtins\nimport contextlib\nimport datetime\nimport io\nimport logging\nimport os\nimport pathlib\nimport sys\nimport threading\nimport time\nimport traceback\nimport warnings\nfrom typing import NamedTuple\n\nimport freezegun\nimport pytest\n\nimport loguru\n\nif sys.version_info < (3, 5, 3):\n\n    def run(coro):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        res = loop.run_until_complete(coro)\n        loop.close()\n        asyncio.set_event_loop(None)\n        return res\n\n    asyncio.run = run\nelif sys.version_info < (3, 7):\n\n    def run(coro):\n        loop = asyncio.new_event_loop()\n        res = loop.run_until_complete(coro)\n        loop.close()\n        asyncio.set_event_loop(None)\n        return res\n\n    asyncio.run = run\n\nif sys.version_info < (3, 6):\n\n    @pytest.fixture\n    def tmp_path(tmp_path):\n        return pathlib.Path(str(tmp_path))\n\n\nif sys.version_info >= (3, 6):\n    from pytest_mypy_plugins.item import YamlTestItem\n\n    def _fix_positional_only_args(item: YamlTestItem):\n        \"\"\"Remove forward-slash marker from the expected output for Python 3.6.\"\"\"\n        for output in item.expected_output:\n            output.message = output.message.replace(\", /\", \"\")\n            # Also patch the \"severity\" attribute because there is a parsing bug in the plugin.\n            output.severity = output.severity.replace(\", /\", \"\")\n\n    def _add_mypy_config(item: YamlTestItem):\n        \"\"\"Add some extra options to the mypy configuration for Python 3.7+.\"\"\"\n        item.additional_mypy_config += \"\\n\".join(\n            [\n                \"show_error_codes = false\",\n                \"force_uppercase_builtins = true\",\n                \"force_union_syntax = true\",\n            ]\n        )\n\n    def pytest_collection_modifyitems(config, items):\n        \"\"\"Modify the tests to ensure they produce the same output regardless of Python version.\"\"\"\n        for item in items:\n            if not isinstance(item, YamlTestItem):\n                continue\n\n            if sys.version_info >= (3, 7):\n                _add_mypy_config(item)\n            else:\n                _fix_positional_only_args(item)\n\n\n@contextlib.contextmanager\ndef new_event_loop_context():\n    loop = asyncio.new_event_loop()\n    try:\n        yield loop\n    finally:\n        loop.close()\n\n\n@contextlib.contextmanager\ndef set_event_loop_context(loop):\n    asyncio.set_event_loop(loop)\n    try:\n        yield\n    finally:\n        asyncio.set_event_loop(None)\n\n\ndef parse(text, *, strip=False, strict=True):\n    parser = loguru._colorizer.AnsiParser()\n    parser.feed(text)\n    tokens = parser.done(strict=strict)\n\n    if strip:\n        return parser.strip(tokens)\n    return parser.colorize(tokens, \"\")\n\n\ndef check_dir(dir, *, files=None, size=None):\n    actual_files = set(dir.iterdir())\n    seen = set()\n    if size is not None:\n        assert len(actual_files) == size\n    if files is not None:\n        assert len(actual_files) == len(files)\n        for name, content in files:\n            filepath = dir / name\n            assert filepath in actual_files\n            assert filepath not in seen\n            if content is not None:\n                assert filepath.read_text() == content\n            seen.add(filepath)\n\n\nclass StubStream(io.StringIO):\n    def fileno(self):\n        return 1\n\n\nclass StreamIsattyTrue(StubStream):\n    def isatty(self):\n        return True\n\n\nclass StreamIsattyFalse(StubStream):\n    def isatty(self):\n        return False\n\n\nclass StreamIsattyException(StubStream):\n    def isatty(self):\n        raise RuntimeError\n\n\nclass StreamFilenoException(StreamIsattyTrue):\n    def fileno(self):\n        raise RuntimeError\n\n\n@contextlib.contextmanager\ndef default_threading_excepthook():\n    if not hasattr(threading, \"excepthook\"):\n        yield\n        return\n\n    # Pytest added \"PytestUnhandledThreadExceptionWarning\", we need to\n    # remove it temporarily for some tests checking exceptions in threads.\n\n    def excepthook(args):\n        print(\"Exception in thread:\", file=sys.stderr, flush=True)\n        traceback.print_exception(\n            args.exc_type, args.exc_value, args.exc_traceback, file=sys.stderr\n        )\n\n    old_excepthook = threading.excepthook\n    threading.excepthook = excepthook\n    yield\n    threading.excepthook = old_excepthook\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef check_env_variables():\n    for var in os.environ:\n        if var.startswith(\"LOGURU_\"):\n            warnings.warn(\n                \"A Loguru environment variable has been detected \"\n                \"and may interfere with the tests: '%s'\" % var,\n                RuntimeWarning,\n                stacklevel=1,\n            )\n\n\n@pytest.fixture(autouse=True)\ndef reset_logger():\n    def reset():\n        loguru.logger.remove()\n        loguru.logger.__init__(\n            loguru._logger.Core(), None, 0, False, False, False, False, True, [], {}\n        )\n        loguru._logger.context.set({})\n\n    reset()\n    yield\n    reset()\n\n\n@pytest.fixture\ndef writer():\n    def w(message):\n        w.written.append(message)\n\n    w.written = []\n    w.read = lambda: \"\".join(w.written)\n    w.clear = lambda: w.written.clear()\n\n    return w\n\n\n@pytest.fixture\ndef sink_with_logger():\n    class SinkWithLogger:\n        def __init__(self, logger):\n            self.logger = logger\n            self.out = \"\"\n\n        def write(self, message):\n            self.logger.info(message)\n            self.out += message\n\n    return SinkWithLogger\n\n\n@pytest.fixture\ndef freeze_time(monkeypatch):\n    ctimes = {}\n    freezegun_localtime = freezegun.api.fake_localtime\n    builtins_open = builtins.open\n\n    fakes = {\"zone\": \"UTC\", \"offset\": 0, \"include_tm_zone\": True}\n\n    def fake_localtime(t=None):\n        fix_struct = os.name == \"nt\" and sys.version_info < (3, 6)\n\n        struct_time_attributes = [\n            (\"tm_year\", int),\n            (\"tm_mon\", int),\n            (\"tm_mday\", int),\n            (\"tm_hour\", int),\n            (\"tm_min\", int),\n            (\"tm_sec\", int),\n            (\"tm_wday\", int),\n            (\"tm_yday\", int),\n            (\"tm_isdst\", int),\n            (\"tm_zone\", str),\n            (\"tm_gmtoff\", int),\n        ]\n\n        if not fakes[\"include_tm_zone\"]:\n            struct_time_attributes = struct_time_attributes[:-2]\n            struct_time = NamedTuple(\"struct_time\", struct_time_attributes)._make\n        elif fix_struct:\n            struct_time = NamedTuple(\"struct_time\", struct_time_attributes)._make\n        else:\n            struct_time = time.struct_time\n\n        struct = freezegun_localtime(t)\n        override = {\"tm_zone\": fakes[\"zone\"], \"tm_gmtoff\": fakes[\"offset\"]}\n        attributes = []\n\n        for attribute, _ in struct_time_attributes:\n            if attribute in override:\n                value = override[attribute]\n            else:\n                value = getattr(struct, attribute)\n            attributes.append(value)\n\n        return struct_time(attributes)\n\n    def patched_open(filepath, *args, **kwargs):\n        if not os.path.exists(filepath):\n            tz = datetime.timezone(datetime.timedelta(seconds=fakes[\"offset\"]), name=fakes[\"zone\"])\n            ctimes[filepath] = datetime.datetime.now().replace(tzinfo=tz).timestamp()\n        return builtins_open(filepath, *args, **kwargs)\n\n    @contextlib.contextmanager\n    def freeze_time(date, timezone=(\"UTC\", 0), *, include_tm_zone=True):\n        # Freezegun does not behave very well with UTC and timezones, see spulec/freezegun#348.\n        # In particular, \"now(tz=utc)\" does not return the converted datetime.\n        # For this reason, we re-implement date parsing here to properly handle aware date using\n        # the optional \"tz_offset\" argument.\n        if isinstance(date, str):\n            for accepted_format in [\"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d %H:%M:%S.%f\"]:\n                try:\n                    date = datetime.datetime.strptime(date, accepted_format)\n                    break\n                except ValueError:\n                    pass\n\n        if not isinstance(date, datetime.datetime) or date.tzinfo is not None:\n            raise ValueError(\"Unsupported date provided\")\n\n        zone, offset = timezone\n        tz_offset = datetime.timedelta(seconds=offset)\n        tzinfo = datetime.timezone(tz_offset, zone)\n        date = date.replace(tzinfo=tzinfo)\n\n        with monkeypatch.context() as context:\n            context.setitem(fakes, \"zone\", zone)\n            context.setitem(fakes, \"offset\", offset)\n            context.setitem(fakes, \"include_tm_zone\", include_tm_zone)\n\n            context.setattr(loguru._file_sink, \"get_ctime\", ctimes.__getitem__)\n            context.setattr(loguru._file_sink, \"set_ctime\", ctimes.__setitem__)\n            context.setattr(builtins, \"open\", patched_open)\n\n            # Freezegun does not permit to override timezone name.\n            context.setattr(freezegun.api, \"fake_localtime\", fake_localtime)\n\n            with freezegun.freeze_time(date, tz_offset=tz_offset) as frozen:\n                yield frozen\n\n    return freeze_time\n\n\n@contextlib.contextmanager\ndef make_logging_logger(name, handler, fmt=\"%(message)s\", level=\"DEBUG\"):\n    logging_logger = logging.getLogger(name)\n    logging_logger.setLevel(level)\n    formatter = logging.Formatter(fmt)\n\n    handler.setLevel(level)\n    handler.setFormatter(formatter)\n    logging_logger.addHandler(handler)\n\n    try:\n        yield logging_logger\n    finally:\n        logging_logger.removeHandler(handler)\n\n\n@pytest.fixture\ndef f_globals_name_absent(monkeypatch):\n    getframe_ = loguru._get_frame.load_get_frame_function()\n\n    def patched_getframe(*args, **kwargs):\n        frame = getframe_(*args, **kwargs)\n        frame.f_globals.pop(\"__name__\", None)\n        return frame\n\n    with monkeypatch.context() as context:\n        context.setattr(loguru._logger, \"get_frame\", patched_getframe)\n        yield\n",
          "import pytest\n\nfrom loguru import logger\n\n\n@pytest.mark.parametrize(\n    (\"name\", \"should_log\"),\n    [\n        (\"\", False),\n        (\"tests\", False),\n        (\"test\", True),\n        (\"testss\", True),\n        (\"tests.\", True),\n        (\"tests.test_activation\", False),\n        (\"tests.test_activation.\", True),\n        (\"test_activation\", True),\n        (\".\", True),\n    ],\n)\ndef test_disable(writer, name, should_log):\n    logger.add(writer, format=\"{message}\")\n    logger.disable(name)\n    logger.debug(\"message\")\n    result = writer.read()\n\n    if should_log:\n        assert result == \"message\\n\"\n    else:\n        assert result == \"\"\n\n\n@pytest.mark.parametrize(\n    (\"name\", \"should_log\"),\n    [\n        (\"\", True),\n        (\"tests\", True),\n        (\"test\", False),\n        (\"testss\", False),\n        (\"tests.\", False),\n        (\"tests.test_activation\", True),\n        (\"tests.test_activation.\", False),\n        (\"test_activation\", False),\n        (\".\", False),\n    ],\n)\ndef test_enable(writer, name, should_log):\n    logger.add(writer, format=\"{message}\")\n    logger.disable(\"\")\n    logger.enable(name)\n    logger.debug(\"message\")\n    result = writer.read()\n\n    if should_log:\n        assert result == \"message\\n\"\n    else:\n        assert result == \"\"\n\n\ndef test_log_before_enable(writer):\n    logger.add(writer, format=\"{message}\")\n    logger.disable(\"\")\n    logger.debug(\"nope\")\n    logger.enable(\"tests\")\n    logger.debug(\"yes\")\n    result = writer.read()\n    assert result == \"yes\\n\"\n\n\ndef test_log_before_disable(writer):\n    logger.add(writer, format=\"{message}\")\n    logger.enable(\"\")\n    logger.debug(\"yes\")\n    logger.disable(\"tests\")\n    logger.debug(\"nope\")\n    result = writer.read()\n    assert result == \"yes\\n\"\n\n\ndef test_multiple_activations():\n    def n():\n        return len(logger._core.activation_list)\n\n    assert n() == 0\n    logger.enable(\"\")\n    assert n() == 0\n    logger.disable(\"\")\n    assert n() == 1\n    logger.enable(\"foo\")\n    assert n() == 2\n    logger.enable(\"foo.bar\")\n    assert n() == 2\n    logger.disable(\"foo\")\n    assert n() == 1\n    logger.disable(\"foo.bar\")\n    assert n() == 1\n    logger.enable(\"foo.bar\")\n    assert n() == 2\n    logger.disable(\"foo.bar.baz\")\n    assert n() == 3\n    logger.disable(\"foo.baz\")\n    assert n() == 3\n    logger.disable(\"foo.baz.bar\")\n    assert n() == 3\n    logger.enable(\"foo.baz.bar\")\n    assert n() == 4\n    logger.enable(\"\")\n    assert n() == 0\n\n\ndef test_log_before_enable_f_globals_name_absent(writer, f_globals_name_absent):\n    logger.add(writer, format=\"{message}\")\n    logger.disable(None)\n    logger.debug(\"nope\")\n    logger.enable(None)\n    logger.debug(\"yes\")\n    result = writer.read()\n    assert result == \"yes\\n\"\n\n\ndef test_log_before_disable_f_globals_name_absent(writer, f_globals_name_absent):\n    logger.add(writer, format=\"{message}\")\n    logger.enable(None)\n    logger.debug(\"yes\")\n    logger.disable(None)\n    logger.debug(\"nope\")\n    result = writer.read()\n    assert result == \"yes\\n\"\n\n\ndef test_f_globals_name_absent_with_others(writer, f_globals_name_absent):\n    logger.add(writer, format=\"{message}\")\n    logger.info(\"1\")\n    logger.enable(None)\n    logger.disable(\"foobar\")\n    logger.enable(\"foo.bar\")\n    logger.disable(None)\n    logger.info(\"2\")\n    logger.enable(\"foobar\")\n    logger.enable(None)\n    logger.info(\"3\")\n    assert writer.read() == \"1\\n3\\n\"\n\n\n@pytest.mark.parametrize(\"name\", [42, [], object()])\ndef test_invalid_enable_name(name):\n    with pytest.raises(TypeError):\n        logger.enable(name)\n\n\n@pytest.mark.parametrize(\"name\", [42, [], object()])\ndef test_invalid_disable_name(name):\n    with pytest.raises(TypeError):\n        logger.disable(name)\n",
          "import re\n\nimport pytest\n\nfrom loguru import logger\n\n\n@pytest.mark.parametrize(\n    \"filter\",\n    [\n        None,\n        \"\",\n        \"tests\",\n        \"tests.test_add_option_filter\",\n        (lambda r: True),\n        (lambda r: r[\"level\"].name == \"DEBUG\"),\n        {},\n        {\"\": \"DEBUG\"},\n        {\"tests\": True},\n        {\"tests.test_add_option_filter\": 10},\n        {\"\": \"WARNING\", \"tests\": 0},\n        {\"tests.test_add_option_filter\": 5, \"tests\": False},\n        {\"tests.test_add_option_filter.foobar\": False},\n        {\"tests.\": False},\n        {\"tests.test_add_option_filter.\": False},\n    ],\n)\ndef test_filtered_in(filter, writer):\n    logger.add(writer, filter=filter, format=\"{message}\")\n    logger.debug(\"Test Filter\")\n    assert writer.read() == \"Test Filter\\n\"\n\n\n@pytest.mark.parametrize(\n    \"filter\",\n    [\n        \"test\",\n        \"testss\",\n        \"tests.\",\n        \"tests.test_add_option_filter.\",\n        \".\",\n        (lambda r: False),\n        (lambda r: r[\"level\"].no != 10),\n        {\"\": False},\n        {\"\": True, \"tests\": 50},\n        {\"tests.test_add_option_filter\": False},\n        {\"tests\": \"WARNING\"},\n        {\"tests\": 5, \"tests.test_add_option_filter\": 40},\n        {\"\": 100, \"tests.test_add_option_filter.foobar\": True},\n    ],\n)\ndef test_filtered_out(filter, writer):\n    logger.add(writer, filter=filter, format=\"{message}\")\n    logger.debug(\"Test Filter\")\n    assert writer.read() == \"\"\n\n\n@pytest.mark.parametrize(\n    \"filter\",\n    [\n        None,\n        lambda _: True,\n        {},\n        {None: 0},\n        {\"\": False},\n        {\"tests\": False, None: True},\n        {\"unrelated\": 100},\n        {None: \"INFO\", \"\": \"WARNING\"},\n    ],\n)\ndef test_filtered_in_f_globals_name_absent(writer, filter, f_globals_name_absent):\n    logger.add(writer, filter=filter, format=\"{message}\", catch=False)\n    logger.info(\"It's ok\")\n    assert writer.read() == \"It's ok\\n\"\n\n\n@pytest.mark.parametrize(\n    \"filter\",\n    [\n        \"tests\",\n        \"\",\n        lambda _: False,\n        {None: False},\n        {\"\": 0, None: \"WARNING\"},\n        {None: 100, \"tests\": True},\n    ],\n)\ndef test_filtered_out_f_globals_name_absent(writer, filter, f_globals_name_absent):\n    logger.add(writer, filter=filter, format=\"{message}\", catch=False)\n    logger.info(\"It's not ok\")\n    assert writer.read() == \"\"\n\n\n@pytest.mark.parametrize(\"filter\", [-1, 3.4, object()])\ndef test_invalid_filter(writer, filter):\n    with pytest.raises(TypeError):\n        logger.add(writer, filter=filter)\n\n\n@pytest.mark.parametrize(\"filter\", [{\"foo\": None}, {\"foo\": 2.5}, {\"a\": \"DEBUG\", \"b\": object()}])\ndef test_invalid_filter_dict_level_types(writer, filter):\n    with pytest.raises(TypeError):\n        logger.add(writer, filter=filter)\n\n\n@pytest.mark.parametrize(\"filter\", [{1: \"DEBUG\"}, {object(): 10}])\ndef test_invalid_filter_dict_module_types(writer, filter):\n    with pytest.raises(TypeError):\n        logger.add(writer, filter=filter)\n\n\n@pytest.mark.parametrize(\"filter\", [{\"foo\": \"UNKNOWN_LEVEL\"}, {\"tests.test_add_option_filter\": \"\"}])\ndef test_invalid_filter_dict_values_unknown_level(writer, filter):\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"The filter dict contains a module '[^']*' associated to \"\n            \"a level name which does not exist: '[^']*'\"\n        ),\n    ):\n        logger.add(writer, filter=filter)\n\n\ndef test_invalid_filter_dict_values_wrong_integer_value(writer):\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"The filter dict contains a module '[^']*' associated to an invalid level, \"\n            \"it should be a positive integer, not: '[^']*'\"\n        ),\n    ):\n        logger.add(writer, filter={\"tests\": -1})\n\n\ndef test_filter_dict_with_custom_level(writer):\n    logger.level(\"MY_LEVEL\", 6, color=\"\", icon=\"\")\n    logger.add(writer, level=0, filter={\"tests\": \"MY_LEVEL\"}, format=\"{message}\")\n    logger.log(3, \"No\")\n    logger.log(9, \"Yes\")\n    assert writer.read() == \"Yes\\n\"\n\n\ndef test_invalid_filter_builtin(writer):\n    with pytest.raises(\n        ValueError,\n        match=re.escape(\n            \"The built-in 'filter()' function cannot be used as a 'filter' parameter, this is \"\n            \"most likely a mistake (please double-check the arguments passed to 'logger.add()'\"\n        ),\n    ):\n        logger.add(writer, filter=filter)\n",
          "import os\nimport re\n\nimport pytest\n\nfrom loguru import logger\n\n\n@pytest.mark.parametrize(\n    (\"format\", \"validator\"),\n    [\n        (\"{name}\", lambda r: r == \"tests.test_formatting\"),\n        (\"{time}\", lambda r: re.fullmatch(r\"\\d+-\\d+-\\d+T\\d+:\\d+:\\d+[.,]\\d+[+-]\\d{4}\", r)),\n        (\"{elapsed}\", lambda r: re.fullmatch(r\"\\d:\\d{2}:\\d{2}\\.\\d{6}\", r)),\n        (\"{elapsed.seconds}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{line}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{level}\", lambda r: r == \"DEBUG\"),\n        (\"{level.name}\", lambda r: r == \"DEBUG\"),\n        (\"{level.no}\", lambda r: r == \"10\"),\n        (\"{level.icon}\", lambda r: r == \"ðŸž\"),\n        (\"{file}\", lambda r: r == \"test_formatting.py\"),\n        (\"{file.name}\", lambda r: r == \"test_formatting.py\"),\n        (\"{file.path}\", lambda r: os.path.normcase(r) == os.path.normcase(__file__)),\n        (\"{function}\", lambda r: r == \"test_log_formatters\"),\n        (\"{module}\", lambda r: r == \"test_formatting\"),\n        (\"{thread}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{thread.id}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{thread.name}\", lambda r: isinstance(r, str) and r != \"\"),\n        (\"{process}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{process.id}\", lambda r: re.fullmatch(r\"\\d+\", r)),\n        (\"{process.name}\", lambda r: isinstance(r, str) and r != \"\"),\n        (\"{message}\", lambda r: r == \"Message\"),\n        (\"%s {{a}} å¤© {{1}} %d\", lambda r: r == \"%s {a} å¤© {1} %d\"),\n    ],\n)\n@pytest.mark.parametrize(\"use_log_function\", [False, True])\ndef test_log_formatters(format, validator, writer, use_log_function):\n    message = \"Message\"\n\n    logger.add(writer, format=format)\n\n    if use_log_function:\n        logger.log(\"DEBUG\", message)\n    else:\n        logger.debug(message)\n\n    result = writer.read().rstrip(\"\\n\")\n    assert validator(result)\n\n\n@pytest.mark.parametrize(\n    (\"format\", \"validator\"),\n    [\n        (\"{time}.log\", lambda r: re.fullmatch(r\"\\d+-\\d+-\\d+_\\d+-\\d+-\\d+\\_\\d+.log\", r)),\n        (\"%s_{{a}}_å¤©_{{1}}_%d\", lambda r: r == \"%s_{a}_å¤©_{1}_%d\"),\n    ],\n)\n@pytest.mark.parametrize(\"part\", [\"file\", \"dir\", \"both\"])\ndef test_file_formatters(tmp_path, format, validator, part):\n    if part == \"file\":\n        file = tmp_path.joinpath(format)\n    elif part == \"dir\":\n        file = tmp_path.joinpath(format, \"log.log\")\n    elif part == \"both\":\n        file = tmp_path.joinpath(format, format)\n\n    logger.add(file)\n    logger.debug(\"Message\")\n\n    files = [f for f in tmp_path.glob(\"**/*\") if f.is_file()]\n\n    assert len(files) == 1\n\n    file = files[0]\n\n    if part == \"file\":\n        assert validator(file.name)\n    elif part == \"dir\":\n        assert file.name == \"log.log\"\n        assert validator(file.parent.name)\n    elif part == \"both\":\n        assert validator(file.name)\n        assert validator(file.parent.name)\n\n\n@pytest.mark.parametrize(\n    (\"message\", \"args\", \"kwargs\", \"expected\"),\n    [\n        (\"{1, 2, 3} - {0} - {\", [], {}, \"{1, 2, 3} - {0} - {\"),\n        (\"{} + {} = {}\", [1, 2, 3], {}, \"1 + 2 = 3\"),\n        (\"{a} + {b} = {c}\", [], dict(a=1, b=2, c=3), \"1 + 2 = 3\"),\n        (\"{0} + {two} = {1}\", [1, 3], dict(two=2, nope=4), \"1 + 2 = 3\"),\n        (\n            \"{self} or {message} or {level}\",\n            [],\n            dict(self=\"a\", message=\"b\", level=\"c\"),\n            \"a or b or c\",\n        ),\n        (\"{:.2f}\", [1], {}, \"1.00\"),\n        (\"{0:0{three}d}\", [5], dict(three=3), \"005\"),\n        (\"{{nope}} {my_dict} {}\", [\"{{!}}\"], dict(my_dict={\"a\": 1}), \"{nope} {'a': 1} {{!}}\"),\n    ],\n)\n@pytest.mark.parametrize(\"use_log_function\", [False, True])\ndef test_log_formatting(writer, message, args, kwargs, expected, use_log_function):\n    logger.add(writer, format=\"{message}\", colorize=False)\n\n    if use_log_function:\n        logger.log(10, message, *args, **kwargs)\n    else:\n        logger.debug(message, *args, **kwargs)\n\n    assert writer.read() == expected + \"\\n\"\n\n\ndef test_f_globals_name_absent(writer, f_globals_name_absent):\n    logger.add(writer, format=\"{name} {message}\", colorize=False)\n    logger.info(\"Foobar\")\n    assert writer.read() == \"None Foobar\\n\"\n\n\ndef test_extra_formatting(writer):\n    logger.configure(extra={\"test\": \"my_test\", \"dict\": {\"a\": 10}})\n    logger.add(writer, format=\"{extra[test]} -> {extra[dict]} -> {message}\")\n    logger.debug(\"level: {name}\", name=\"DEBUG\")\n    assert writer.read() == \"my_test -> {'a': 10} -> level: DEBUG\\n\"\n\n\ndef test_kwargs_in_extra_dict():\n    extra_dicts = []\n    messages = []\n\n    def sink(message):\n        extra_dicts.append(message.record[\"extra\"])\n        messages.append(str(message))\n\n    logger.add(sink, format=\"{message}\")\n    logger.info(\"A\")\n    logger.info(\"B\", foo=123)\n    logger.bind(merge=True).info(\"C\", other=False)\n    logger.bind(override=False).info(\"D\", override=True)\n    logger.info(\"Formatted kwargs: {foobar}\", foobar=123)\n    logger.info(\"Ignored args: {}\", 456)\n    logger.info(\"Both: {foobar} {}\", 456, foobar=123)\n    logger.opt(lazy=True).info(\"Lazy: {lazy}\", lazy=lambda: 789)\n\n    assert messages == [\n        \"A\\n\",\n        \"B\\n\",\n        \"C\\n\",\n        \"D\\n\",\n        \"Formatted kwargs: 123\\n\",\n        \"Ignored args: 456\\n\",\n        \"Both: 123 456\\n\",\n        \"Lazy: 789\\n\",\n    ]\n\n    assert extra_dicts == [\n        {},\n        {\"foo\": 123},\n        {\"merge\": True, \"other\": False},\n        {\"override\": True},\n        {\"foobar\": 123},\n        {},\n        {\"foobar\": 123},\n        {\"lazy\": 789},\n    ]\n\n\ndef test_non_string_message(writer):\n    logger.add(writer, format=\"{message}\")\n\n    logger.info(1)\n    logger.info({})\n    logger.info(b\"test\")\n\n    assert writer.read() == \"1\\n{}\\nb'test'\\n\"\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_non_string_message_is_str_in_record(writer, colors):\n    output = \"\"\n\n    def sink(message):\n        nonlocal output\n        assert isinstance(message.record[\"message\"], str)\n        output += message\n\n    def format(record):\n        assert isinstance(record[\"message\"], str)\n        return \"[{message}]\\n\"\n\n    logger.add(sink, format=format, catch=False)\n    logger.opt(colors=colors).info(123)\n    assert output == \"[123]\\n\"\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_missing_positional_field_during_formatting(writer, colors):\n    logger.add(writer)\n\n    with pytest.raises(IndexError):\n        logger.opt(colors=colors).info(\"Foo {} {}\", 123)\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_missing_named_field_during_formatting(writer, colors):\n    logger.add(writer)\n\n    with pytest.raises(KeyError):\n        logger.opt(colors=colors).info(\"Foo {bar}\", baz=123)\n\n\ndef test_not_formattable_message(writer):\n    logger.add(writer)\n\n    with pytest.raises(AttributeError):\n        logger.info(123, baz=456)\n\n\ndef test_not_formattable_message_with_colors(writer):\n    logger.add(writer)\n\n    with pytest.raises(TypeError):\n        logger.opt(colors=True).info(123, baz=456)\n\n\ndef test_invalid_color_markup(writer):\n    with pytest.raises(\n        ValueError, match=\"^Invalid format, color markups could not be parsed correctly$\"\n    ):\n        logger.add(writer, format=\"<red>Not closed tag\", colorize=True)\n",
          "import sys\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom loguru import logger\n\nfrom .conftest import parse\n\n\ndef test_record(writer):\n    logger.add(writer, format=\"{message}\")\n\n    logger.opt(record=True).debug(\"1\")\n    logger.opt(record=True).debug(\"2 {record[level]}\")\n    logger.opt(record=True).log(11, \"3 {0} {a} {record[level].no}\", 4, a=5)\n\n    assert writer.read() == \"1\\n2 DEBUG\\n3 4 5 11\\n\"\n\n\ndef test_record_in_kwargs_too(writer):\n    logger.add(writer, catch=False)\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"^The message can't be formatted: 'record' shall not be used as a keyword argument \"\n            r\"while logger has been configured with '\\.opt\\(record=True\\)'$\"\n        ),\n    ):\n        logger.opt(record=True).info(\"Foo {record}\", record=123)\n\n\ndef test_record_not_in_extra():\n    extra = None\n\n    def sink(message):\n        nonlocal extra\n        extra = message.record[\"extra\"]\n\n    logger.add(sink, catch=False)\n\n    logger.opt(record=True).info(\"Test\")\n\n    assert extra == {}\n\n\ndef test_kwargs_in_extra_of_record():\n    message = None\n\n    def sink(message_):\n        nonlocal message\n        message = message_\n\n    logger.add(sink, format=\"{message}\", catch=False)\n\n    logger.opt(record=True).info(\"Test {record[extra][foo]}\", foo=123)\n\n    assert message == \"Test 123\\n\"\n    assert message.record[\"extra\"] == {\"foo\": 123}\n\n\ndef test_exception_boolean(writer):\n    logger.add(writer, format=\"{level.name}: {message}\")\n\n    try:\n        1 / 0  # noqa: B018\n    except Exception:\n        logger.opt(exception=True).debug(\"Error {0} {record}\", 1, record=\"test\")\n\n    lines = writer.read().strip().splitlines()\n\n    assert lines[0] == \"DEBUG: Error 1 test\"\n    assert lines[-1] == \"ZeroDivisionError: division by zero\"\n\n\ndef test_exception_exc_info(writer):\n    logger.add(writer, format=\"{message}\")\n\n    try:\n        1 / 0  # noqa: B018\n    except Exception:\n        exc_info = sys.exc_info()\n\n    logger.opt(exception=exc_info).debug(\"test\")\n\n    lines = writer.read().strip().splitlines()\n\n    assert lines[0] == \"test\"\n    assert lines[-1] == \"ZeroDivisionError: division by zero\"\n\n\ndef test_exception_class(writer):\n    logger.add(writer, format=\"{message}\")\n\n    try:\n        1 / 0  # noqa: B018\n    except Exception:\n        _, exc_class, _ = sys.exc_info()\n\n    logger.opt(exception=exc_class).debug(\"test\")\n\n    lines = writer.read().strip().splitlines()\n\n    assert lines[0] == \"test\"\n    assert lines[-1] == \"ZeroDivisionError: division by zero\"\n\n\ndef test_exception_log_function(writer):\n    logger.add(writer, format=\"{level.no} {message}\")\n\n    try:\n        1 / 0  # noqa: B018\n    except Exception:\n        logger.opt(exception=True).log(50, \"Error\")\n\n    lines = writer.read().strip().splitlines()\n\n    assert lines[0] == \"50 Error\"\n    assert lines[-1] == \"ZeroDivisionError: division by zero\"\n\n\ndef test_lazy(writer):\n    counter = 0\n\n    def laziness():\n        nonlocal counter\n        counter += 1\n        return counter\n\n    logger.add(writer, level=10, format=\"{level.no} => {message}\")\n\n    logger.opt(lazy=True).log(10, \"1: {lazy}\", lazy=laziness)\n    logger.opt(lazy=True).log(5, \"2: {0}\", laziness)\n\n    logger.remove()\n\n    logger.opt(lazy=True).log(20, \"3: {}\", laziness)\n\n    i = logger.add(writer, level=15, format=\"{level.no} => {message}\")\n    logger.add(writer, level=20, format=\"{level.no} => {message}\")\n\n    logger.log(17, \"4: {}\", counter)\n    logger.opt(lazy=True).log(14, \"5: {lazy}\", lazy=lambda: counter)\n\n    logger.remove(i)\n\n    logger.opt(lazy=True).log(16, \"6: {0}\", lambda: counter)\n\n    logger.opt(lazy=True).info(\"7: {}\", laziness)\n    logger.debug(\"7: {}\", counter)\n\n    assert writer.read() == \"10 => 1: 1\\n17 => 4: 1\\n20 => 7: 2\\n\"\n\n\ndef test_lazy_function_executed_only_once(writer):\n    counter = 0\n\n    def laziness():\n        nonlocal counter\n        counter += 1\n        return counter\n\n    logger.add(writer, level=10, format=\"{level.name} => {message}\")\n\n    logger.opt(lazy=True).info(\"1: {lazy} {lazy}\", lazy=laziness)\n    logger.opt(lazy=True).info(\"2: {0} {0}\", laziness)\n\n    assert writer.read() == \"INFO => 1: 1 1\\nINFO => 2: 2 2\\n\"\n\n\ndef test_logging_within_lazy_function(writer):\n    logger.add(writer, level=20, format=\"{message}\")\n\n    def laziness():\n        logger.trace(\"Nope\")\n        logger.warning(\"Yes Warn\")\n\n    logger.opt(lazy=True).trace(\"No\", laziness)\n\n    assert writer.read() == \"\"\n\n    logger.opt(lazy=True).info(\"Yes\", laziness)\n\n    assert writer.read() == \"Yes Warn\\nYes\\n\"\n\n\ndef test_depth(writer):\n    logger.add(writer, format=\"{function} : {message}\")\n\n    def a():\n        logger.opt(depth=1).debug(\"Test 1\")\n        logger.opt(depth=0).debug(\"Test 2\")\n        logger.opt(depth=1).log(10, \"Test 3\")\n\n    a()\n\n    logger.remove()\n\n    assert writer.read() == \"test_depth : Test 1\\na : Test 2\\ntest_depth : Test 3\\n\"\n\n\ndef test_capture(writer):\n    logger.add(writer, format=\"{message} {extra}\")\n    logger.opt(capture=False).info(\"No {}\", 123, no=False)\n    logger.opt(capture=False).info(\"Formatted: {fmt}\", fmt=456)\n    logger.opt(capture=False).info(\"Formatted bis: {} {fmt}\", 123, fmt=456)\n    assert writer.read() == \"No 123 {}\\nFormatted: 456 {}\\nFormatted bis: 123 456 {}\\n\"\n\n\ndef test_colors(writer):\n    logger.add(writer, format=\"<red>a</red> {message}\", colorize=True)\n    logger.opt(colors=True).debug(\"<blue>b</blue>\")\n    logger.opt(colors=True).log(20, \"<y>c</y>\")\n\n    assert writer.read() == parse(\n        \"<red>a</red> <blue>b</blue>\\n\" \"<red>a</red> <y>c</y>\\n\", strip=False\n    )\n\n\ndef test_colors_not_colorize(writer):\n    logger.add(writer, format=\"<red>a</red> {message}\", colorize=False)\n    logger.opt(colors=True).debug(\"<blue>b</blue>\")\n    assert writer.read() == parse(\"<red>a</red> <blue>b</blue>\\n\", strip=True)\n\n\ndef test_colors_doesnt_color_unrelated(writer):\n    logger.add(writer, format=\"{message} {extra[trap]}\", colorize=True)\n    logger.bind(trap=\"<red>B</red>\").opt(colors=True).debug(\"<red>A</red>\")\n    assert writer.read() == parse(\"<red>A</red>\", strip=False) + \" <red>B</red>\\n\"\n\n\ndef test_colors_doesnt_strip_unrelated(writer):\n    logger.add(writer, format=\"{message} {extra[trap]}\", colorize=False)\n    logger.bind(trap=\"<red>B</red>\").opt(colors=True).debug(\"<red>A</red>\")\n    assert writer.read() == parse(\"<red>A</red>\", strip=True) + \" <red>B</red>\\n\"\n\n\ndef test_colors_doesnt_raise_unrelated_colorize(writer):\n    logger.add(writer, format=\"{message} {extra[trap]}\", colorize=True, catch=False)\n    logger.bind(trap=\"</red>\").opt(colors=True).debug(\"A\")\n    assert writer.read() == \"A </red>\\n\"\n\n\ndef test_colors_doesnt_raise_unrelated_not_colorize(writer):\n    logger.add(writer, format=\"{message} {extra[trap]}\", colorize=False, catch=False)\n    logger.bind(trap=\"</red>\").opt(colors=True).debug(\"A\")\n    assert writer.read() == \"A </red>\\n\"\n\n\ndef test_colors_doesnt_raise_unrelated_colorize_dynamic(writer):\n    logger.add(writer, format=lambda x: \"{message} {extra[trap]}\", colorize=True, catch=False)\n    logger.bind(trap=\"</red>\").opt(colors=True).debug(\"A\")\n    assert writer.read() == \"A </red>\"\n\n\ndef test_colors_doesnt_raise_unrelated_not_colorize_dynamic(writer):\n    logger.add(writer, format=lambda x: \"{message} {extra[trap]}\", colorize=False, catch=False)\n    logger.bind(trap=\"</red>\").opt(colors=True).debug(\"A\")\n    assert writer.read() == \"A </red>\"\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_within_record(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger_ = logger.bind(start=\"<red>\", end=\"</red>\")\n    logger_.opt(colors=True, record=True).debug(\"{record[extra][start]}B{record[extra][end]}\")\n    assert writer.read() == \"<red>B</red>\\n\"\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_nested(writer, colorize):\n    logger.add(writer, format=\"(<red>[{message}]</red>)\", colorize=colorize)\n    logger.opt(colors=True).debug(\"A<green>B</green>C<blue>D</blue>E\")\n    assert writer.read() == parse(\n        \"(<red>[A<green>B</green>C<blue>D</blue>E]</red>)\\n\", strip=not colorize\n    )\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_stripped_in_message_record(colorize):\n    message = None\n\n    def sink(msg):\n        nonlocal message\n        message = msg.record[\"message\"]\n\n    logger.add(sink, colorize=colorize)\n    logger.opt(colors=True).debug(\"<red>Test</red>\")\n    assert message == \"Test\"\n\n\n@pytest.mark.parametrize(\"message\", [\"<red>\", \"</red>\", \"X </red> <red> Y\"])\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_invalid_markup_in_message(writer, message, colorize):\n    logger.add(writer, format=\"<red>{message}</red>\", colorize=colorize, catch=False)\n    with pytest.raises(\n        ValueError,\n        match='(Closing|Opening) tag \"[^\"]*\" has no corresponding (opening|closing) tag',\n    ):\n        logger.opt(colors=True).debug(message)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_args(writer, colorize):\n    logger.add(writer, format=\"=> {message} <=\", colorize=colorize)\n    logger.opt(colors=True).debug(\"the {0}test{end}\", \"<red>\", end=\"</red>\")\n    assert writer.read() == \"=> the <red>test</red> <=\\n\"\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_level(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger.level(\"DEBUG\", color=\"<green>\")\n    logger.opt(colors=True).debug(\"a <level>level</level> b\")\n    assert writer.read() == parse(\"a <green>level</green> b\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_double_message(writer, colorize):\n    logger.add(\n        writer, format=\"<red><b>{message}...</b> - <c>...{message}</c></red>\", colorize=colorize\n    )\n    logger.opt(colors=True).debug(\"<g>foo</g> bar <g>baz</g>\")\n\n    assert writer.read() == parse(\n        \"<red><b><g>foo</g> bar <g>baz</g>...</b> - <c>...<g>foo</g> bar <g>baz</g></c></red>\\n\",\n        strip=not colorize,\n    )\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_multiple_calls(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger.opt(colors=True).debug(\"a <red>foo</red> b\")\n    logger.opt(colors=True).debug(\"a <red>foo</red> b\")\n    assert writer.read() == parse(\"a <red>foo</red> b\\na <red>foo</red> b\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_multiple_calls_level_color_changed(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger.level(\"INFO\", color=\"<blue>\")\n    logger.opt(colors=True).info(\"a <level>foo</level> b\")\n    logger.level(\"INFO\", color=\"<red>\")\n    logger.opt(colors=True).info(\"a <level>foo</level> b\")\n    assert writer.read() == parse(\"a <blue>foo</blue> b\\na <red>foo</red> b\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_dynamic_formatter(writer, colorize):\n    logger.add(writer, format=lambda r: \"<red>{message}</red>\", colorize=colorize)\n    logger.opt(colors=True).debug(\"<b>a</b> <y>b</y>\")\n    assert writer.read() == parse(\"<red><b>a</b> <y>b</y></red>\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_format_specs(writer, colorize):\n    fmt = \"<g>{level.no:03d} {message:} {message!s:} {{nope}} {extra[a][b]!r}</g>\"\n    logger.add(writer, colorize=colorize, format=fmt)\n    logger.bind(a={\"b\": \"c\"}).opt(colors=True).debug(\"<g>{X}</g>\")\n    assert writer.read() == parse(\"<g>010 <g>{X}</g> {X} {nope} 'c'</g>\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_message_specs(writer, colorize):\n    logger.add(writer, colorize=colorize, format=\"<g>{message}</g>\")\n    logger.opt(colors=True).debug(\"{} <b>A</b> {{nope}} {key:03d} {let!r}\", 1, key=10, let=\"c\")\n    logger.opt(colors=True).debug(\"<b>{0:0{1}d}</b>\", 2, 4)\n    assert writer.read() == parse(\n        \"<g>1 <b>A</b> {nope} 010 'c'</g>\\n<g><b>0002</b></g>\\n\", strip=not colorize\n    )\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colored_string_used_as_spec(writer, colorize):\n    logger.add(writer, colorize=colorize, format=\"{level.no:{message}} <red>{message}</red>\")\n    logger.opt(colors=True).log(30, \"03d\")\n    assert writer.read() == parse(\"030 <red>03d</red>\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colored_string_getitem(writer, colorize):\n    logger.add(writer, colorize=colorize, format=\"<red>{message[0]}</red>\")\n    logger.opt(colors=True).info(\"ABC\")\n    assert writer.read() == parse(\"<red>A</red>\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_without_formatting_args(writer, colorize):\n    string = \"{} This { should } not } raise {\"\n    logger.add(writer, colorize=colorize, format=\"{message}\")\n    logger.opt(colors=True).info(string)\n    assert writer.read() == string + \"\\n\"\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_recursion_depth_exceeded_in_format(writer, colorize):\n    with pytest.raises(\n        ValueError, match=\"^Invalid format, color markups could not be parsed correctly$\"\n    ):\n        logger.add(writer, format=\"{message:{message:{message:}}}\", colorize=colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_recursion_depth_exceeded_in_message(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n\n    with pytest.raises(ValueError, match=\"Max string recursion exceeded\"):\n        logger.opt(colors=True).info(\"{foo:{foo:{foo:}}}\", foo=123)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_auto_indexing(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger.opt(colors=True).info(\"<red>{}</red> <green>{}</green>\", \"foo\", \"bar\")\n    assert writer.read() == parse(\"<red>foo</red> <green>bar</green>\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_colors_with_manual_indexing(writer, colorize):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n    logger.opt(colors=True).info(\"<red>{1}</red> <green>{0}</green>\", \"foo\", \"bar\")\n    assert writer.read() == parse(\"<red>bar</red> <green>foo</green>\\n\", strip=not colorize)\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\n@pytest.mark.parametrize(\"message\", [\"{} {0}\", \"{1} {}\"])\ndef test_colors_with_invalid_indexing(writer, colorize, message):\n    logger.add(writer, format=\"{message}\", colorize=colorize)\n\n    with pytest.raises(\n        ValueError,\n        match=\"cannot switch from manual field specification to automatic field numbering\",\n    ):\n        logger.opt(colors=True).debug(message, 1, 2, 3)\n\n\ndef test_raw(writer):\n    logger.add(writer, format=\"\", colorize=True)\n    logger.opt(raw=True).info(\"Raw {}\", \"message\")\n    logger.opt(raw=True).log(30, \" + The end\")\n    assert writer.read() == \"Raw message + The end\"\n\n\ndef test_raw_with_format_function(writer):\n    logger.add(writer, format=lambda _: \"{time} \\n\")\n    logger.opt(raw=True).debug(\"Raw {message} bis\", message=\"message\")\n    assert writer.read() == \"Raw message bis\"\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_raw_with_colors(writer, colorize):\n    logger.add(writer, format=\"XYZ\", colorize=colorize)\n    logger.opt(raw=True, colors=True).info(\"Raw <red>colors</red> and <lvl>level</lvl>\")\n    assert writer.read() == parse(\"Raw <red>colors</red> and <b>level</b>\", strip=not colorize)\n\n\ndef test_args_with_colors_not_formatted_twice(capsys):\n    logger.add(sys.stdout, format=\"{message}\", colorize=True)\n    logger.add(sys.stderr, format=\"{message}\", colorize=False)\n    a = MagicMock(__format__=MagicMock(return_value=\"a\"))\n    b = MagicMock(__format__=MagicMock(return_value=\"b\"))\n\n    logger.opt(colors=True).info(\"{} <red>{foo}</red>\", a, foo=b)\n    out, err = capsys.readouterr()\n    assert out == parse(\"a <red>b</red>\\n\")\n    assert err == \"a b\\n\"\n    assert a.__format__.call_count == 1\n    assert b.__format__.call_count == 1\n\n\n@pytest.mark.parametrize(\"colorize\", [True, False])\ndef test_level_tag_wrapping_with_colors(writer, colorize):\n    logger.add(writer, format=\"<level>FOO {message} BAR</level>\", colorize=colorize)\n    logger.opt(colors=True).info(\"> foo <red>{}</> bar <lvl>{}</> baz <green>{}</green> <\", 1, 2, 3)\n    logger.opt(colors=True).log(33, \"<lvl> {} <red>{}</red> {} </lvl>\", 1, 2, 3)\n\n    assert writer.read() == parse(\n        \"<b>FOO > foo <red>1</red> bar <b>2</b> baz <green>3</green> < BAR</b>\\n\"\n        \"<level>FOO <level> 1 <red>2</red> 3 </level> BAR</level>\\n\",\n        strip=not colorize,\n    )\n\n\n@pytest.mark.parametrize(\"dynamic_format\", [True, False])\n@pytest.mark.parametrize(\"colorize\", [True, False])\n@pytest.mark.parametrize(\"colors\", [True, False])\n@pytest.mark.parametrize(\"raw\", [True, False])\n@pytest.mark.parametrize(\"use_log\", [True, False])\n@pytest.mark.parametrize(\"use_arg\", [True, False])\ndef test_all_colors_combinations(writer, dynamic_format, colorize, colors, raw, use_log, use_arg):\n    format_ = \"<level>{level.no:03}</level> <red>{message}</red>\"\n    message = \"<green>The</green> <lvl>{}</lvl>\"\n    arg = \"message\"\n\n    def formatter(_):\n        return format_ + \"\\n\"\n\n    logger.add(writer, format=formatter if dynamic_format else format_, colorize=colorize)\n\n    logger_ = logger.opt(colors=colors, raw=raw)\n\n    if use_log:\n        if use_arg:\n            logger_.log(20, message, arg)\n        else:\n            logger_.log(20, message.format(arg))\n    else:\n        if use_arg:\n            logger_.info(message, arg)\n        else:\n            logger_.info(message.format(arg))\n\n    if use_log:\n        if raw:\n            if colors:\n                expected = parse(\"<green>The</green> <level>message</level>\", strip=not colorize)\n            else:\n                expected = \"<green>The</green> <lvl>message</lvl>\"\n        else:\n            if colors:\n                expected = parse(\n                    \"<level>020</level> <red><green>The</green> <level>message</level></red>\\n\",\n                    strip=not colorize,\n                )\n            else:\n                expected = (\n                    parse(\"<level>020</level> <red>%s</red>\\n\", strip=not colorize)\n                    % \"<green>The</green> <lvl>message</lvl>\"\n                )\n\n    else:\n        if raw:\n            if colors:\n                expected = parse(\"<green>The</green> <b>message</b>\", strip=not colorize)\n            else:\n                expected = \"<green>The</green> <lvl>message</lvl>\"\n        else:\n            if colors:\n                expected = parse(\n                    \"<b>020</b> <red><green>The</green> <b>message</b></red>\\n\", strip=not colorize\n                )\n            else:\n                expected = (\n                    parse(\"<b>020</b> <red>%s</red>\\n\", strip=not colorize)\n                    % \"<green>The</green> <lvl>message</lvl>\"\n                )\n\n    assert writer.read() == expected\n\n\ndef test_raw_with_record(writer):\n    logger.add(writer, format=\"Nope\\n\")\n    logger.opt(raw=True, record=True).debug(\"Raw in '{record[function]}'\\n\")\n    assert writer.read() == \"Raw in 'test_raw_with_record'\\n\"\n\n\ndef test_keep_extra(writer):\n    logger.configure(extra=dict(test=123))\n    logger.add(writer, format=\"{extra[test]}\")\n    logger.opt().debug(\"\")\n    logger.opt().log(50, \"\")\n\n    assert writer.read() == \"123\\n123\\n\"\n\n\ndef test_before_bind(writer):\n    logger.add(writer, format=\"{message}\")\n    logger.opt(record=True).bind(key=\"value\").info(\"{record[level]}\")\n    assert writer.read() == \"INFO\\n\"\n\n\ndef test_deprecated_ansi_argument(writer):\n    logger.add(writer, format=\"{message}\", colorize=True)\n    with pytest.warns(DeprecationWarning):\n        logger.opt(ansi=True).info(\"Foo <red>bar</red> baz\")\n    assert writer.read() == parse(\"Foo <red>bar</red> baz\\n\")\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_message_update_not_overridden_by_patch(writer, colors):\n    def patcher(record):\n        record[\"message\"] += \" [Patched]\"\n\n    logger.add(writer, format=\"{level} {message}\", colorize=True)\n    logger.patch(patcher).opt(colors=colors).info(\"Message\")\n\n    assert writer.read() == \"INFO Message [Patched]\\n\"\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_message_update_not_overridden_by_format(writer, colors):\n    def formatter(record):\n        record[\"message\"] += \" [Formatted]\"\n        return \"{level} {message}\\n\"\n\n    logger.add(writer, format=formatter, colorize=True)\n    logger.opt(colors=colors).info(\"Message\")\n\n    assert writer.read() == \"INFO Message [Formatted]\\n\"\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_message_update_not_overridden_by_filter(writer, colors):\n    def filter(record):\n        record[\"message\"] += \" [Filtered]\"\n        return True\n\n    logger.add(writer, format=\"{level} {message}\", filter=filter, colorize=True)\n    logger.opt(colors=colors).info(\"Message\")\n\n    assert writer.read() == \"INFO Message [Filtered]\\n\"\n\n\n@pytest.mark.parametrize(\"colors\", [True, False])\ndef test_message_update_not_overridden_by_raw(writer, colors):\n    logger.add(writer, colorize=True)\n    logger.patch(lambda r: r.update(message=\"Updated!\")).opt(raw=True, colors=colors).info(\"Raw!\")\n    assert writer.read() == \"Updated!\"\n\n\ndef test_overridden_message_ignore_colors(writer):\n    def formatter(record):\n        record[\"message\"] += \" <blue>[Ignored]</blue> </xyz>\"\n        return \"{message}\\n\"\n\n    logger.add(writer, format=formatter, colorize=True)\n    logger.opt(colors=True).info(\"<red>Message</red>\")\n\n    assert writer.read() == \"Message <blue>[Ignored]</blue> </xyz>\\n\"\n"
        ],
        "test_patch": "",
        "patch_preview": "From 8ec562886ba0dee56233b7f6d58c3a43dea93c54 Mon Sep 17 00:00:00 2001\nFrom: Delgan <delgan.py@gmail.com>\nDate: Tue, 26 Nov 2024 16:35:23 +0100\nSubject: [PATCH] Fix \"ValueError\" using Loguru with Cython (when missing stack\n frame)\n\n---\n CHANGELOG.rst                   |  1 +\n docs/resources/recipes.rst      | 20 ++++++++++++--------\n loguru/_logger.py               | 25 +++++++++++++++++--------\n tests/conftest.py               | 21 +++++++++++++++++++--\n tests/test_activation.py        |  6 +++"
      },
      "patch": {
        "length": 11290,
        "files_changed": 8,
        "lines_added": 62,
        "lines_deleted": 24,
        "net_change": 38,
        "changed_files": [
          {
            "file": "CHANGELOG.rst",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "docs/resources/recipes.rst",
            "added": 12,
            "deleted": 8
          },
          {
            "file": "loguru/_logger.py",
            "added": 17,
            "deleted": 8
          },
          {
            "file": "tests/conftest.py",
            "added": 19,
            "deleted": 2
          },
          {
            "file": "tests/test_activation.py",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "tests/test_add_option_filter.py",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "tests/test_formatting.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "tests/test_opt.py",
            "added": 7,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [
        {
          "id": 2503434701,
          "body": "@namoshizun Probably before the end of the week if all goes well. :+1: \r\n\r\nYou can also test it in advance (and report me any bug) by installing `loguru` from the repo directly:\r\n\r\n```\r\npip install git+https://github.com/delgan/loguru@ab33cc0101129721cd024b8a63c4059cbc8844d1\r\n```",
          "user": "Delgan",
          "created_at": "2024-11-27T09:57:59Z",
          "html_url": "https://github.com/Delgan/loguru/pull/1240#issuecomment-2503434701"
        },
        {
          "id": 2503294708,
          "body": "This is awesome!! We've been stuck on the \"depth=-1' workaround for quite some time. All of our code has to be compiled using Cython due to proprietary requirements. Can we expect a new release that contains this fix soon Ã°Å¸Ëœâ€° ? ",
          "user": "namoshizun",
          "created_at": "2024-11-27T09:00:55Z",
          "html_url": "https://github.com/Delgan/loguru/pull/1240#issuecomment-2503294708"
        }
      ],
      "issue_comments_count": 2,
      "code_statistics": {
        "total_files": 290,
        "total_lines": 29492,
        "total_bytes": 1435067,
        "python_files": 165,
        "python_lines": 18506,
        "file_extensions": {
          ".toml": 1,
          ".ini": 1,
          "": 2,
          ".rst": 17,
          ".py": 165,
          ".yml": 1,
          ".txt": 93,
          ".html": 2,
          ".png": 1,
          ".gif": 1,
          ".svg": 2,
          ".js": 1,
          ".css": 1,
          ".pyi": 1,
          ".typed": 1
        },
        "largest_files": [
          {
            "path": "docs/_static/img/demo.gif",
            "size": 191178,
            "lines": 2292,
            "extension": ".gif"
          },
          {
            "path": "loguru/_logger.py",
            "size": 97483,
            "lines": 2129,
            "extension": ".py"
          },
          {
            "path": "tests/test_filesink_rotation.py",
            "size": 36499,
            "lines": 1119,
            "extension": ".py"
          },
          {
            "path": "docs/resources/recipes.rst",
            "size": 46057,
            "lines": 1086,
            "extension": ".rst"
          },
          {
            "path": "tests/test_coroutine_sink.py",
            "size": 16864,
            "lines": 676,
            "extension": ".py"
          },
          {
            "path": "tests/test_multiprocessing.py",
            "size": 16900,
            "lines": 632,
            "extension": ".py"
          },
          {
            "path": "tests/test_opt.py",
            "size": 21764,
            "lines": 631,
            "extension": ".py"
          },
          {
            "path": "tests/test_exceptions_catch.py",
            "size": 16536,
            "lines": 626,
            "extension": ".py"
          },
          {
            "path": "loguru/_better_exceptions.py",
            "size": 21516,
            "lines": 573,
            "extension": ".py"
          },
          {
            "path": "README.rst",
            "size": 19988,
            "lines": 520,
            "extension": ".rst"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 290,
        "files_changed_count": 8,
        "files_changed_ratio": 0.027586206896551724,
        "total_lines_in_repo": 29492,
        "lines_added": 62,
        "lines_deleted": 24,
        "net_lines_changed": 38,
        "lines_changed_ratio": 0.0029160450291604505,
        "pr_body_length": 592,
        "commit_message_length": 58,
        "python_file_count": 165,
        "python_line_count": 18506
      }
    },
    {
      "tar_file_name": "GreyDGL#PentestGPT#pull#180",
      "repo_name": "GreyDGL#PentestGPT#pull#180",
      "success": true,
      "error": null,
      "commit": {
        "sha": "a6bb83c7c6b32e7ff8a9dffd7c200b8d5b8b9224",
        "message": "Merge pull request #179 from wouterdebruijn/error-gpt4all\n\nðŸ› Unable to use GPT4all with default setup",
        "author": {
          "name": "Gelei Deng",
          "email": "78410652+GreyDGL@users.noreply.github.com",
          "date": "2023-11-27T14:22:03Z"
        },
        "html_url": "https://github.com/GreyDGL/PentestGPT/commit/a6bb83c7c6b32e7ff8a9dffd7c200b8d5b8b9224",
        "api_url": "https://api.github.com/repos/GreyDGL/PentestGPT/commits/a6bb83c7c6b32e7ff8a9dffd7c200b8d5b8b9224"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/GreyDGL#PentestGPT#pull#180",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/GreyDGL#PentestGPT#pull#180.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/GreyDGL#PentestGPT#pull#180/source_code"
      },
      "pr": {
        "number": 180,
        "title": "update readme and fix for key binding",
        "body": "A temporary fix for #152 ",
        "state": "closed",
        "created_at": "2023-11-29T09:24:15Z",
        "updated_at": "2023-11-29T09:24:23Z",
        "merged_at": "2023-11-29T09:24:23Z",
        "html_url": "https://github.com/GreyDGL/PentestGPT/pull/180",
        "user": "GreyDGL",
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "commits": 1
      },
      "swebench": {
        "instance_id": "GreyDGL_PentestGPT-180",
        "repo": "/GreyDGL/PentestGPT",
        "base_commit": "a6bb83c7c6b32e7ff8a9dffd7c200b8d5b8b9224",
        "problem_statement": {
          "title": "Shift + Right Arrow is not working to continue",
          "body": "**Shift + Right Arrow is not working to continue**\r\nShift + Right Arrow is not working properly to continue forward with the prompts. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Enter any initial prompt\r\n2. type next\r\n3. select the \"source of information\"\r\n4. press Shift + Right Arrow to continue forward\r\n\r\n**Expected behavior**\r\nShift + Right Arrow allows you to continue forward during the prompts\r\n\r\n![image](https://github.com/GreyDGL/PentestGPT/assets/115773776/7a03ea9c-3721-4c76-98c6-8cf38e0204c2)\r\n"
        },
        "edit_files": [
          "README.md",
          "setup.py"
        ],
        "oracle_files": [
          "<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->\n<a name=\"readme-top\"></a>\n\n<!-- PROJECT SHIELDS -->\n<!--\n*** I'm using markdown \"reference style\" links for readability.\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n*** See the bottom of this document for the declaration of the reference variables\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![MIT License][license-shield]][license-url]\n[![Discord][discord-shield]][discord-url]\n\n\n\n<!-- PROJECT LOGO -->\n<br />\n<div align=\"center\">\n  <a href=\"https://github.com/GreyDGL/PentestGPT\">\n  </a>\n\n<h3 align=\"center\">PentestGPT</h3>\n\n  <p align=\"center\">\n    A GPT-empowered penetration testing tool. \n    <br />\n    <a href=\"https://github.com/GreyDGL/PentestGPT\"><strong>Explore the docs Â»</strong></a>\n    <br />\n    <br />\n    <a href=\"https://github.com/GreyDGL/PentestGPT/blob/main/PentestGPT_design.md\">Design Details</a>\n    Â·\n    <a href=\"https://www.youtube.com/watch?v=lAjLIj1JT3c\">View Demo</a>\n    Â·\n    <a href=\"https://github.com/GreyDGL/PentestGPT/issues\">Report Bug or Request Feature</a>\n    </p>\n</div>\n\n\n\n\n\n<!-- ABOUT THE PROJECT -->\n## General Updates\n- [Update on 17/11/2023] GPTs for PentestGPT is out! Check this: https://chat.openai.com/g/g-4MHbTepWO-pentestgpt\n- [Update on 07/11/2023] GPT-4-turbo is out! Update the default API usage to GPT-4-turbo. \n- Available videos:\n  - The latest installation video is [here](https://youtu.be/tGC5z14dE24).\n  - **PentestGPT for OSCP-like machine: [HTB-Jarvis](https://youtu.be/lAjLIj1JT3c)**. This is the first part only, and I'll complete the rest when I have time.\n  - **PentestGPT on [HTB-Lame](https://youtu.be/Vs9DFtAkODM)**. This is an easy machine, but it shows you how PentestGPT skipped the rabbit hole and worked on other potential vulnerabilities.\n- **We're testing PentestGPT on HackTheBox**. You may follow [this link](https://www.hackthebox.com/home/users/profile/1489431). More details will be released soon.\n- Feel free to join the [Discord Channel](https://discord.gg/eC34CEfEkK) for more updates and share your ideas!\n\n\n<!-- Quick Start -->\n## Quick Start\n1. Create a virtual environment if necessary. (`virtualenv -p python3 venv`, `source venv/bin/activate`)\n2. Install the project with `pip3 install git+https://github.com/GreyDGL/PentestGPT`\n3. **Ensure that you have link a payment method to your OpenAI account.** Export your API key with `export OPENAI_KEY='<your key here>'`\n4. Test the connection with `pentestgpt-connection`\n5. To start: `pentestgpt --logging`\n\n\n<!-- GETTING STARTED -->\n## Getting Started\n- **PentestGPT** is a penetration testing tool empowered by **ChatGPT**. \n- It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.\n- **PentestGPT** is able to solve easy to medium HackTheBox machines, and other CTF challenges. You can check [this](./resources/README.md) example in `resources` where we use it to solve HackTheBox challenge **TEMPLATED** (web challenge). \n- A sample testing process of **PentestGPT** on a target VulnHub machine (Hackable II) is available at [here](./resources/PentestGPT_Hackable2.pdf).\n- A sample usage video is below: (or available here: [Demo](https://youtu.be/h0k6kWWaCEU))\n\n<!-- Common Questions -->\n## Common Questions\n- **Q**: What is PentestGPT?\n  - **A**: PentestGPT is a penetration testing tool empowered by Large Language Models (LLMs). It is designed to automate the penetration testing process. It is built on top of ChatGPT API and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.\n- **Q**: Do I need to pay to use PentestGPT?\n  - **A**: Yes in order to achieve the best performance. In general, you can use any LLMs you want, but you're recommended to use GPT-4 API, for which you have to [link a payment method to OpenAI](https://help.openai.com/en/collections/3943089-billing?q=API). \n- **Q**: Why GPT-4?\n  - **A**: After empirical evaluation, we find that GPT-4 performs better than GPT-3.5 and other LLMs in terms of penetration testing reasoning. In fact, GPT-3.5 leads to failed test in simple tasks.\n- **Q**: Why not just use GPT-4 directly?\n  - **A**: We found that GPT-4 suffers from losses of context as test goes deeper. It is essential to maintain a \"test status awareness\" in this process. You may check the [PentestGPT Arxiv Paper](https://arxiv.org/abs/2308.06782) for details.\n- **Q**: Can I use local GPT models?\n  - **A**: Yes. We support local LLMs through GPT4ALL (but the performance is not comparable to GPT-4).\n\n\n### Installation\n**PentestGPT** current supports backend of **ChatGPT** and **OpenAI API**. You may use either of them. We're working on supports to custom local LLM models.\nYou're recommended to use the OpenAI API for stability and performance (details in item 3). \nPlease watch the installation video [here](https://youtu.be/tGC5z14dE24).\n1. Install the latest version with `pip3 install git+https://github.com/GreyDGL/PentestGPT`\n   - You may also clone the project to local environment and install for better customization and development\n     - `git clone https://github.com/GreyDGL/PentestGPT`\n     - `cd PentestGPT`\n     - `pip3 install -e .`\n2. To use OpenAI API\n   - **Ensure that you have link a payment method to your OpenAI account.**\n   - export your API key with `export OPENAI_KEY='<your key here>'`\n   - Test the connection with `pentestgpt-connection`\n3. To verify that the connection is configured properly, you may run `pentestgpt-connection`. After a while, you should see some sample conversation with ChatGPT.\n   - A sample output is below\n   ```\n   You're testing the connection for PentestGPT v 0.11.0\n   #### Test connection for OpenAI api (GPT-4)\n   1. You're connected with OpenAI API. You have GPT-4 access. To start PentestGPT, please use <pentestgpt --reasoning_model=gpt-4>\n   \n   #### Test connection for OpenAI api (GPT-3.5)\n   2. You're connected with OpenAI API. You have GPT-3.5 access. To start PentestGPT, please use <pentestgpt --reasoning_model=gpt-3.5-turbo-16k>\n   ```\n   - notice: if you have not linked a payment method to your OpenAI account, you will see error messages.\n4. The ChatGPT cookie solution is deprecated and not recommended. You may still use it by running `pentestgpt --reasoning_model=gpt-4 --useAPI=False`. \n\n<!-- USAGE EXAMPLES -->\n\n## Usage\n1. **You are recommended to run**:\n   - (recommended) - `pentestgpt --reasoning_model=gpt-4-turbo` to use the latest GPT-4-turbo API.\n   - `pentestgpt --reasoning_model=gpt-4` if you have access to GPT-4 API.\n   - `pentestgpt --reasoning_model=gpt-3.5-turbo-16k` if you only have access to GPT-3.5 API.\n   \n2. To start, run `pentestgpt --args`.\n    - `--help` show the help message\n    - `--reasoning_model` is the reasoning model you want to use. \n    - `--parsing_model` is the parsing model you want to use. \n    - `--useAPI` is whether you want to use OpenAI API. By default it is set to `True`.\n    - `--log_dir` is the customized log output directory. The location is a relative directory.\n    - `--logging` defines if you would like to share the logs with us. By default it is set to `False`.\n3. The tool works similar to *msfconsole*. Follow the guidance to perform penetration testing. \n4. In general, PentestGPT intakes commands similar to chatGPT. There are several basic commands.\n   1. The commands are: \n      - `help`: show the help message.\n      - `next`: key in the test execution result and get the next step.\n      - `more`: let **PentestGPT** to explain more details of the current step. Also, a new sub-task solver will be created to guide the tester.\n      - `todo`: show the todo list.\n      - `discuss`: discuss with the **PentestGPT**.\n      - `google`: search on Google. This function is still under development.\n      - `quit`: exit the tool and save the output as log file (see the **reporting** section below).\n   2. You can use <SHIFT + right arrow> to end your input (and <ENTER> is for next line).\n   3. You may always use `TAB` to autocomplete the commands.\n   4. When you're given a drop-down selection list, you can use cursor or arrow key to navigate the list. Press `ENTER` to select the item. Similarly, use <SHIFT + right arrow> to confirm selection.\\\n      The user can submit info about:\n        * **tool**: output of the security test tool used\n        * **web**: relevant content of a web page\n        * **default**: whatever you want, the tool will handle it\n        * **user-comments**: user comments about PentestGPT operations\n5. In the sub-task handler initiated by `more`, users can execute more commands to investigate into a specific problem:\n   1. The commands are:\n        - `help`: show the help message.\n        - `brainstorm`: let PentestGPT brainstorm on the local task for all the possible solutions.\n        - `discuss`: discuss with PentestGPT about this local task.\n        - `google`: search on Google. This function is still under development.\n        - `continue`: exit the subtask and continue the main testing session.\n\n### Report and Logging\n1. [Update] If you would like us to collect the logs to improve the tool, please run `pentestgpt --logging`. We will only collect the LLM usage, without any information related to your OpenAI key.\n2. After finishing the penetration testing, a report will be automatically generated in `logs` folder (if you quit with `quit` command).\n3. The report can be printed in a human-readable format by running `python3 utils/report_generator.py <log file>`. A sample report `sample_pentestGPT_log.txt` is also uploaded.\n\n## Custom Models and Local LLMs\nPentestGPT now support any LLMs, but the prompts are only optimized for GPT-4.\n- To use local GPT4ALL model, you may run `pentestgpt --reasoning_model=gpt4all --parsing_model=gpt4all`\n- The model configs are available `pentestgpt/utils/APIs`. Please follow the example of `module_import.py`, `gpt4all.py` and `chatgpt_api.py` to create API support for your own model.\n\n## Citation\nPlease cite our paper at:\n```\n@misc{deng2023pentestgpt,\n      title={PentestGPT: An LLM-empowered Automatic Penetration Testing Tool}, \n      author={Gelei Deng and Yi Liu and VÃ­ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},\n      year={2023},\n      eprint={2308.06782},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}\n```\n\n<!-- LICENSE -->\n## License\n\nDistributed under the MIT License. See `LICENSE.txt` for more information.\nThe tool is for educational purpose only and the author does not condone any illegal use. Use as your own risk.\n\n\n\n<!-- CONTACT -->\n## Contact the Contributors!\n\n- Gelei Deng - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg\n- VÃ­ctor Mayoral Vilches - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com\n- Yi Liu - yi009@e.ntu.edu.sg\n- Peng Liu - liu_peng@i2r.a-star.edu.sg\n- Zhang Jian - jian_zhang@ntu.edu.sg\n- Yuekang Li - yuekang.li@unsw.edu.au\n\n\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge\n[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge\n[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members\n[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge\n[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers\n[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge\n[issues-url]: https://github.com/GreyDGL/PentestGPT/issues\n[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge\n[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/\n[linkedin-url2]: https://www.linkedin.com/in/vmayoral/\n[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK\n[discord-url]: https://discord.gg/eC34CEfEkK\n[product-screenshot]: images/screenshot.png\n[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white\n[Next-url]: https://nextjs.org/\n[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB\n[React-url]: https://reactjs.org/\n[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D\n[Vue-url]: https://vuejs.org/\n[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white\n[Angular-url]: https://angular.io/\n[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00\n[Svelte-url]: https://svelte.dev/\n[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white\n[Laravel-url]: https://laravel.com\n[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white\n[Bootstrap-url]: https://getbootstrap.com\n[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white\n[JQuery-url]: https://jquery.com\n",
          "import os\nfrom collections import OrderedDict\nfrom setuptools import find_packages, setup\n\nwith open(os.path.join(os.path.dirname(__file__), \"requirements.txt\")) as f:\n    dependencies = f.read().strip().split(\"\\n\")\n\nsetup(\n    name=\"pentestgpt\",\n    version=\"0.11.0\",\n    description=\"PentestGPT, a GPT-empowered penetration testing tool\",\n    long_description=\"\"\"\n    PentestGPT is a penetration testing tool empowered by ChatGPT.\n    It is designed to automate the penetration testing process. It\n    is prototyped initially on top of ChatGPT and operate in an\n    interactive mode to guide penetration testers in both overall\n    progress and specific operations.\n    \"\"\",\n    author=\"Gelei Deng\",\n    author_email=\"gelei.deng@ntu.edu.sg\",\n    maintainer=\"Gelei Deng\",\n    maintainer_email=\"gelei.deng@ntu.edu.sg\",\n    url=\"https://github.com/GreyDGL/PentestGPT\",\n    project_urls=OrderedDict(\n        (\n            (\"Code\", \"https://github.com/GreyDGL/PentestGPT\"),\n            (\"Issue tracker\", \"https://github.com/GreyDGL/PentestGPT/issues\"),\n        )\n    ),\n    license=\"MIT License\",\n    packages=[\"pentestgpt\"] + find_packages(),\n    # packages=find_packages(),\n    # scripts=['pentestgpt/main.py'],\n    install_requires=dependencies,\n    entry_points={\n        \"console_scripts\": [\n            \"pentestgpt=pentestgpt.main:main\",\n            \"pentestgpt-cookie=pentestgpt.extract_cookie:main\",\n            \"pentestgpt-connection=pentestgpt.test_connection:main\",\n        ]\n    },\n)\n"
        ],
        "test_patch": "",
        "patch_preview": "From bcf52fc8acbedbc7173af9102db08c15ebad6d9c Mon Sep 17 00:00:00 2001\nFrom: Gelei <gdeng003@e.ntu.edu.sg>\nDate: Wed, 29 Nov 2023 04:12:46 -0500\nSubject: [PATCH] update readme and fix for key binding\n\n---\n README.md | 3 ++-\n setup.py  | 2 +-\n 2 files changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/README.md b/README.md\nindex f7ba417..b88669a 100644\n--- a/README.md\n+++ b/README.md\n@@ -62,7 +62,8 @@\n 2. Install the project with `pip3 install git+https://github.com/GreyDGL/PentestGPT`\n 3. **"
      },
      "patch": {
        "length": 1260,
        "files_changed": 2,
        "lines_added": 3,
        "lines_deleted": 2,
        "net_change": 1,
        "changed_files": [
          {
            "file": "README.md",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "setup.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 66,
        "total_lines": 150182,
        "total_bytes": 19202481,
        "python_files": 43,
        "python_lines": 4496,
        "file_extensions": {
          ".md": 10,
          ".txt": 7,
          ".py": 43,
          ".html": 2,
          ".yaml": 2,
          ".pdf": 1,
          ".mov": 1
        },
        "largest_files": [
          {
            "path": "resources/PentestGPT - 720WebShareName.mov",
            "size": 18519068,
            "lines": 141342,
            "extension": ".mov"
          },
          {
            "path": "resources/PentestGPT_Hackable2.pdf",
            "size": 127156,
            "lines": 1518,
            "extension": ".pdf"
          },
          {
            "path": "tasks/crawl_page_sources/dotCMS/container-api.html",
            "size": 72856,
            "lines": 988,
            "extension": ".html"
          },
          {
            "path": "pentestgpt/tasks/crawl_page_sources/dotCMS/container-api.html",
            "size": 72856,
            "lines": 988,
            "extension": ".html"
          },
          {
            "path": "pentestgpt/utils/pentest_gpt.py",
            "size": 33987,
            "lines": 772,
            "extension": ".py"
          },
          {
            "path": "pentestgpt/utils/pentest_gpt_rebuilt.py",
            "size": 32156,
            "lines": 719,
            "extension": ".py"
          },
          {
            "path": "pentestgpt/utils/chatgpt.py",
            "size": 13175,
            "lines": 351,
            "extension": ".py"
          },
          {
            "path": "resources/README.md",
            "size": 16566,
            "lines": 349,
            "extension": ".md"
          },
          {
            "path": "pentestgpt/utils/llm_api.py",
            "size": 12722,
            "lines": 333,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 14018,
            "lines": 239,
            "extension": ".md"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 66,
        "files_changed_count": 2,
        "files_changed_ratio": 0.030303030303030304,
        "total_lines_in_repo": 150182,
        "lines_added": 3,
        "lines_deleted": 2,
        "net_lines_changed": 1,
        "lines_changed_ratio": 3.3292937902012225e-05,
        "pr_body_length": 25,
        "commit_message_length": 101,
        "python_file_count": 43,
        "python_line_count": 4496
      }
    },
    {
      "tar_file_name": "Ice9Coffee#HoshinoBot#pull#221",
      "repo_name": "Ice9Coffee#HoshinoBot#pull#221",
      "success": false,
      "error": "object of type 'NoneType' has no len()",
      "commit": {
        "sha": "0cf51b672ea017f6eda35e5f5c3d39504ead2369",
        "message": "no need to await bot.finish()",
        "author": {
          "name": "Ice-Cirno",
          "email": "438971718@qq.com",
          "date": "2022-02-24T17:06:04Z"
        },
        "html_url": "https://github.com/Ice9Coffee/HoshinoBot/commit/0cf51b672ea017f6eda35e5f5c3d39504ead2369",
        "api_url": "https://api.github.com/repos/Ice9Coffee/HoshinoBot/commits/0cf51b672ea017f6eda35e5f5c3d39504ead2369"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Ice9Coffee#HoshinoBot#pull#221",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Ice9Coffee#HoshinoBot#pull#221.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Ice9Coffee#HoshinoBot#pull#221/source_code"
      },
      "pr": {
        "number": 221,
        "title": "fix lssv perm",
        "body": null,
        "state": "closed",
        "created_at": "2022-02-26T07:05:29Z",
        "updated_at": "2022-02-26T07:23:44Z",
        "merged_at": null,
        "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221",
        "user": "mengshouer",
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "Ice9Coffee_HoshinoBot-221",
        "repo": "/Ice9Coffee/HoshinoBot",
        "base_commit": "0cf51b672ea017f6eda35e5f5c3d39504ead2369",
        "problem_statement": {},
        "edit_files": [
          "hoshino/modules/botmanage/service_manage.py"
        ],
        "oracle_files": [
          "from functools import cmp_to_key\n\nfrom nonebot import CommandSession, on_command\nfrom nonebot import permission as perm\nfrom nonebot.argparse import ArgumentParser\n\nfrom hoshino import Service, priv, util\n\nPRIV_TIP = f'ç¾¤ä¸»={priv.OWNER} ç¾¤ç®¡={priv.ADMIN} ç¾¤å‘˜={priv.NORMAL} botç»´æŠ¤ç»„={priv.SUPERUSER}'\n\n@on_command('lssv', aliases=('æœåŠ¡åˆ—è¡¨', 'åŠŸèƒ½åˆ—è¡¨'), permission=perm.GROUP_ADMIN, only_to_me=False, shell_like=True)\nasync def lssv(session: CommandSession):\n    parser = ArgumentParser(session=session)\n    parser.add_argument('-a', '--all', action='store_true')\n    parser.add_argument('-H', '--hidden', action='store_true')\n    parser.add_argument('-g', '--group', type=int, default=0)\n    args = parser.parse_args(session.argv)\n\n    verbose_all = args.all\n    only_hidden = args.hidden\n    if session.ctx['user_id'] in session.bot.config.SUPERUSERS:\n        gid = args.group or session.ctx.get('group_id')\n        if not gid:\n            session.finish('Usage: -g|--group <group_id> [-a|--all]')\n    else:\n        gid = session.ctx['group_id']\n\n    msg = [f\"ç¾¤{gid}æœåŠ¡ä¸€è§ˆï¼š\"]\n    svs = Service.get_loaded_services().values()\n    svs = map(lambda sv: (sv, sv.check_enabled(gid)), svs)\n    key = cmp_to_key(lambda x, y: (y[1] - x[1]) or (-1 if x[0].name < y[0].name else 1 if x[0].name > y[0].name else 0))\n    svs = sorted(svs, key=key)\n    for sv, on in svs:\n        if verbose_all or (sv.visible ^ only_hidden):\n            x = 'â—‹' if on else 'Ã—'\n            msg.append(f\"|{x}| {sv.name}\")\n    await session.send('\\n'.join(msg))\n\n\n@on_command('enable', aliases=('å¯ç”¨', 'å¼€å¯', 'æ‰“å¼€'), permission=perm.GROUP, only_to_me=False)\nasync def enable_service(session: CommandSession):\n    await switch_service(session, turn_on=True)\n\n@on_command('disable', aliases=('ç¦ç”¨', 'å…³é—­'), permission=perm.GROUP, only_to_me=False)\nasync def disable_service(session: CommandSession):\n    await switch_service(session, turn_on=False)\n\nasync def switch_service(session: CommandSession, turn_on: bool):\n    action_tip = 'å¯ç”¨' if turn_on else 'ç¦ç”¨'\n    if session.ctx['message_type'] == 'group':\n        names = session.current_arg_text.split()\n        if not names:\n            session.finish(f\"ç©ºæ ¼åŽæŽ¥è¦{action_tip}çš„æœåŠ¡å\", at_sender=True)\n        group_id = session.ctx['group_id']\n        svs = Service.get_loaded_services()\n        succ, notfound = [], []\n        for name in names:\n            if name in svs:\n                sv = svs[name]\n                u_priv = priv.get_user_priv(session.ctx)\n                if u_priv >= sv.manage_priv:\n                    sv.set_enable(group_id) if turn_on else sv.set_disable(group_id)\n                    succ.append(name)\n                else:\n                    try:\n                        await session.send(f'æƒé™ä¸è¶³ï¼{action_tip}{name}éœ€è¦ï¼š{sv.manage_priv}ï¼Œæ‚¨çš„ï¼š{u_priv}\\n{PRIV_TIP}', at_sender=True)\n                    except:\n                        pass\n            else:\n                notfound.append(util.filt_message(name))\n        msg = []\n        if succ:\n            msg.append(f'å·²{action_tip}æœåŠ¡ï¼š' + ', '.join(succ))\n        if notfound:\n            msg.append('æœªæ‰¾åˆ°æœåŠ¡ï¼š' + ', '.join(notfound))\n        if msg:\n            session.finish('\\n'.join(msg), at_sender=True)\n\n    else:\n        if session.ctx['user_id'] not in session.bot.config.SUPERUSERS:\n            return\n        args = session.current_arg_text.split()\n        if len(args) < 2:\n            session.finish('Usage: <service_name> <group_id1> [<group_id2>, ...]')\n        name, *group_ids = args\n        svs = Service.get_loaded_services()\n        if name not in svs:\n            session.finish(f'æœªæ‰¾åˆ°æœåŠ¡ï¼š{name}')\n        sv = svs[name]\n        succ = []\n        for gid in group_ids:\n            try:\n                gid = int(gid)\n                sv.set_enable(gid) if turn_on else sv.set_disable(gid)\n                succ.append(gid)\n            except:\n                try:\n                    await session.send(f'éžæ³•ç¾¤å·ï¼š{gid}')\n                except:\n                    pass\n        session.finish(f'æœåŠ¡{name}å·²äºŽ{len(succ)}ä¸ªç¾¤å†…{action_tip}ï¼š{succ}')\n"
        ],
        "test_patch": "",
        "patch_preview": "From 564218224bf70beea3f2629f875287d03af86f07 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?=E9=9B=AA=E7=8B=90?= <mengshouer@outlook.com>\nDate: Sat, 26 Feb 2022 15:05:01 +0800\nSubject: [PATCH] fix perm\n\n---\n hoshino/modules/botmanage/service_manage.py | 8 +++++++-\n 1 file changed, 7 insertions(+), 1 deletion(-)\n\ndiff --git a/hoshino/modules/botmanage/service_manage.py b/hoshino/modules/botmanage/service_manage.py\nindex 254f1503c..1b8b04ef9 100644\n--- a/hoshino/modules/botmanage/service_manage.py\n+++ "
      },
      "patch": {
        "length": 1091,
        "files_changed": 1,
        "lines_added": 7,
        "lines_deleted": 1,
        "net_change": 6,
        "changed_files": [
          {
            "file": "hoshino/modules/botmanage/service_manage.py",
            "added": 7,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [
        {
          "id": 1051737776,
          "body": "çœ‹æ ·å­æ˜¯nonebot 1.9.0å¼•å…¥çš„æ–°æ›´æ”¹\r\nhttps://github.com/nonebot/nonebot/blob/master/docs/advanced/legacy_features.md\r\næœ¬é¡¹ç›®ç›®å‰çš„ä¾èµ–é¡¹æ˜¯nonebot 1.8.xï¼Œåœ¨requirementsä¸­æœ‰æ˜Žç¡®ã€‚\r\nnonebot 1.xçš„ç»´æŠ¤æ¢äººäº†ï¼Œæˆ‘ä¸ªäººè®¤ä¸º1.6.xä¹‹åŽå¼•å…¥äº†è®¸å¤šä¸å¿…è¦çš„æ–°ç‰¹æ€§ï¼ˆhoshinoæ¡†æž¶å·²è§£å†³ï¼‰åŒæ—¶è¿˜ä¸å‘å‰å…¼å®¹ã€‚æ²¡å¿…è¦ä½¿ç”¨æ–°ç‰ˆæœ¬ã€‚",
          "user": "Ice9Coffee",
          "created_at": "2022-02-26T07:16:30Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051737776"
        },
        {
          "id": 1051739336,
          "body": "èƒ½ç”¨å°±è¡Œ.jpg",
          "user": "mengshouer",
          "created_at": "2022-02-26T07:17:48Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051739336"
        },
        {
          "id": 1051728713,
          "body": "ç¾¤é‡Œæˆ‘ç¾¤ä¸»+SUPERUSERï¼Œlssvæ— æ•ˆï¼ŒåŠ ä¸Šèƒ½è·‘",
          "user": "mengshouer",
          "created_at": "2022-02-26T07:09:04Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051728713"
        },
        {
          "id": 1051727492,
          "body": "éš¾é“æ˜¯æˆ‘æ›´æ–°äº†nonebotï¼Ÿç‰ˆæœ¬1.9.1ï¼Œæˆ‘è‡ªå·±æµ‹è¯•æ˜¯æ²¡æœ‰çš„",
          "user": "mengshouer",
          "created_at": "2022-02-26T07:08:03Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051727492"
        },
        {
          "id": 1051726392,
          "body": "SUPERUSERå’ŒGROUP_OWNERæœ¬æ¥å°±å…·æœ‰GROUP_ADMINçš„æƒé™",
          "user": "Ice9Coffee",
          "created_at": "2022-02-26T07:07:09Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051726392"
        },
        {
          "id": 1051746554,
          "body": "ä½ å¯ä»¥å‘nonebot 1.xæä¸ªissue",
          "user": "Ice9Coffee",
          "created_at": "2022-02-26T07:23:44Z",
          "html_url": "https://github.com/Ice9Coffee/HoshinoBot/pull/221#issuecomment-1051746554"
        }
      ],
      "issue_comments_count": 6,
      "code_statistics": {
        "total_files": 100,
        "total_lines": 25410,
        "total_bytes": 634636,
        "python_files": 89,
        "python_lines": 7467,
        "file_extensions": {
          "": 1,
          ".txt": 2,
          ".py": 89,
          ".md": 6,
          ".json": 2
        },
        "largest_files": [
          {
            "path": "hoshino/util/textfilter/sensitive_words.txt",
            "size": 254807,
            "lines": 16166,
            "extension": ".txt"
          },
          {
            "path": "hoshino/modules/pcrclanbattle/clanbattle/cmdv2.py",
            "size": 33054,
            "lines": 849,
            "extension": ".py"
          },
          {
            "path": "LICENSE",
            "size": 35149,
            "lines": 674,
            "extension": ""
          },
          {
            "path": "hoshino/service.py",
            "size": 16208,
            "lines": 424,
            "extension": ".py"
          },
          {
            "path": "hoshino/modules/pcrclanbattle/clanbattle/dao/sqlitedao.py",
            "size": 13827,
            "lines": 399,
            "extension": ".py"
          },
          {
            "path": "hoshino/modules/pcrclanbattle/clanbattle/READMEv1.md",
            "size": 13207,
            "lines": 369,
            "extension": ".md"
          },
          {
            "path": "hoshino/modules/priconne/_pcr_data.py",
            "size": 45365,
            "lines": 363,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 15654,
            "lines": 359,
            "extension": ".md"
          },
          {
            "path": "hoshino/modules/pcrclanbattle/clanbattle/battlemaster.py",
            "size": 12406,
            "lines": 341,
            "extension": ".py"
          },
          {
            "path": "hoshino/modules/priconne/arena/__init__.py",
            "size": 10086,
            "lines": 247,
            "extension": ".py"
          }
        ]
      }
    },
    {
      "tar_file_name": "JessicaTegner#pypandoc#pull#217",
      "repo_name": "JessicaTegner#pypandoc#pull#217",
      "success": true,
      "error": null,
      "commit": {
        "sha": "a6313123db5787fff8cc5a71af3b1c0bec9866b2",
        "message": "Merge pull request #221 from NicklasTegner/fix-211\n\nUpdated the way pandoc citeproc are handled if not present\r\n\r\nfixes #216 \r\nfixes #200",
        "author": {
          "name": "Nicklas Tegner",
          "email": "nicklasmchd@live.dk",
          "date": "2021-06-30T21:39:39Z"
        },
        "html_url": "https://github.com/JessicaTegner/pypandoc/commit/a6313123db5787fff8cc5a71af3b1c0bec9866b2",
        "api_url": "https://api.github.com/repos/JessicaTegner/pypandoc/commits/a6313123db5787fff8cc5a71af3b1c0bec9866b2"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/JessicaTegner#pypandoc#pull#217",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/JessicaTegner#pypandoc#pull#217.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/JessicaTegner#pypandoc#pull#217/source_code"
      },
      "pr": {
        "number": 217,
        "title": "Prevent new console  window being created on Windows",
        "body": "When using deployment methods such as pyinstaller, pypandoc would\r\nflash open a console window when it invoked pandoc.\r\n\r\nChanging the creation flag on windows to prevent a new console window\r\nfixes this",
        "state": "closed",
        "created_at": "2021-06-02T15:59:38Z",
        "updated_at": "2021-07-01T17:11:32Z",
        "merged_at": "2021-07-01T17:11:32Z",
        "html_url": "https://github.com/JessicaTegner/pypandoc/pull/217",
        "user": "LukeBriggsDev",
        "additions": 14,
        "deletions": 6,
        "changed_files": 1,
        "commits": 2
      },
      "swebench": {
        "instance_id": "JessicaTegner_pypandoc-217",
        "repo": "/JessicaTegner/pypandoc",
        "base_commit": "a6313123db5787fff8cc5a71af3b1c0bec9866b2",
        "problem_statement": {},
        "edit_files": [
          "pypandoc/__init__.py"
        ],
        "oracle_files": [
          "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, with_statement\n\nimport os\nimport re\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nimport warnings\n\nfrom .pandoc_download import DEFAULT_TARGET_FOLDER, download_pandoc\nfrom .py3compat import cast_bytes, cast_unicode, string_types, url2path, urlparse\n\n__author__ = u'Juho VepsÃ¤lÃ¤inen'\n__version__ = '1.5'\n__license__ = 'MIT'\n__all__ = ['convert', 'convert_file', 'convert_text',\n           'get_pandoc_formats', 'get_pandoc_version', 'get_pandoc_path',\n           'download_pandoc']\n\n\ndef convert(source, to, format=None, extra_args=(), encoding='utf-8',\n            outputfile=None, filters=None):\n    \"\"\"Converts given `source` from `format` to `to` (deprecated).\n\n    :param str source: Unicode string or bytes or a file path (see encoding)\n\n    :param str to: format into which the input should be converted; can be one of\n            `pypandoc.get_pandoc_formats()[1]`\n\n    :param str format: the format of the inputs; will be inferred if input is a file with an\n            known filename extension; can be one of `pypandoc.get_pandoc_formats()[1]`\n            (Default value = None)\n\n    :param list extra_args: extra arguments (list of strings) to be passed to pandoc\n            (Default value = ())\n\n    :param str encoding: the encoding of the file or the input bytes (Default value = 'utf-8')\n\n    :param str outputfile: output will be written to outfilename or the converted content\n            returned if None (Default value = None)\n\n    :param list filters: pandoc filters e.g. filters=['pandoc-citeproc']\n\n    :returns: converted string (unicode) or an empty string if an outputfile was given\n    :rtype: unicode\n\n    :raises RuntimeError: if any of the inputs are not valid of if pandoc fails with an error\n    :raises OSError: if pandoc is not found; make sure it has been installed and is available at\n            path.\n    \"\"\"\n    msg = (\"Due to possible ambiguity, 'convert()' is deprecated. \"\n           \"Use 'convert_file()'  or 'convert_text()'.\")\n    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n    path = _identify_path(source)\n    if path:\n        format = _identify_format_from_path(source, format)\n        input_type = 'path'\n    else:\n        source = _as_unicode(source, encoding)\n        input_type = 'string'\n        if not format:\n            raise RuntimeError(\"Format missing, but need one (identified source as text as no \"\n                               \"file with that name was found).\")\n    return _convert_input(source, format, input_type, to, extra_args=extra_args,\n                          outputfile=outputfile, filters=filters)\n\n\ndef convert_text(source, to, format, extra_args=(), encoding='utf-8',\n                 outputfile=None, filters=None):\n    \"\"\"Converts given `source` from `format` to `to`.\n\n    :param str source: Unicode string or bytes (see encoding)\n\n    :param str to: format into which the input should be converted; can be one of\n            `pypandoc.get_pandoc_formats()[1]`\n\n    :param str format: the format of the inputs; can be one of `pypandoc.get_pandoc_formats()[1]`\n\n    :param list extra_args: extra arguments (list of strings) to be passed to pandoc\n            (Default value = ())\n\n    :param str encoding: the encoding of the input bytes (Default value = 'utf-8')\n\n    :param str outputfile: output will be written to outfilename or the converted content\n            returned if None (Default value = None)\n\n    :param list filters: pandoc filters e.g. filters=['pandoc-citeproc']\n\n    :returns: converted string (unicode) or an empty string if an outputfile was given\n    :rtype: unicode\n\n    :raises RuntimeError: if any of the inputs are not valid of if pandoc fails with an error\n    :raises OSError: if pandoc is not found; make sure it has been installed and is available at\n            path.\n    \"\"\"\n    source = _as_unicode(source, encoding)\n    return _convert_input(source, format, 'string', to, extra_args=extra_args,\n                          outputfile=outputfile, filters=filters)\n\n\ndef convert_file(source_file, to, format=None, extra_args=(), encoding='utf-8',\n                 outputfile=None, filters=None):\n    \"\"\"Converts given `source` from `format` to `to`.\n\n    :param str source_file: file path (see encoding)\n\n    :param str to: format into which the input should be converted; can be one of\n            `pypandoc.get_pandoc_formats()[1]`\n\n    :param str format: the format of the inputs; will be inferred from the source_file with an\n            known filename extension; can be one of `pypandoc.get_pandoc_formats()[1]`\n            (Default value = None)\n\n    :param list extra_args: extra arguments (list of strings) to be passed to pandoc\n            (Default value = ())\n\n    :param str encoding: the encoding of the file or the input bytes (Default value = 'utf-8')\n\n    :param str outputfile: output will be written to outfilename or the converted content\n            returned if None (Default value = None)\n\n    :param list filters: pandoc filters e.g. filters=['pandoc-citeproc']\n\n    :returns: converted string (unicode) or an empty string if an outputfile was given\n    :rtype: unicode\n\n    :raises RuntimeError: if any of the inputs are not valid of if pandoc fails with an error\n    :raises OSError: if pandoc is not found; make sure it has been installed and is available at\n            path.\n    \"\"\"\n    if not _identify_path(source_file):\n        raise RuntimeError(\"source_file is not a valid path\")\n    format = _identify_format_from_path(source_file, format)\n    return _convert_input(source_file, format, 'path', to, extra_args=extra_args,\n                          outputfile=outputfile, filters=filters)\n\n\ndef _identify_path(source):\n    # guard against problems\n    if source is None or not isinstance(source, string_types):\n        return False\n\n    is_path = False\n    try:\n        is_path = os.path.exists(source)\n    except UnicodeEncodeError:\n        is_path = os.path.exists(source.encode('utf-8'))\n    except:  # noqa\n        # still false\n        pass\n\n    if not is_path:\n        # check if it's an URL\n        result = urlparse(source)\n        if result.scheme in [\"http\", \"https\"]:\n            is_path = True\n        elif result.scheme and result.netloc and result.path:\n            # complete uri including one with a network path\n            is_path = True\n        elif result.scheme == \"file\" and result.path:\n            is_path = os.path.exists(url2path(source))\n\n    return is_path\n\n\ndef _identify_format_from_path(sourcefile, format):\n    return format or os.path.splitext(sourcefile)[1].strip('.')\n\n\ndef _as_unicode(source, encoding):\n    if encoding != 'utf-8':\n        # if a source and a different encoding is given, try to decode the the source into a\n        # unicode string\n        try:\n            source = cast_unicode(source, encoding=encoding)\n        except (UnicodeDecodeError, UnicodeEncodeError):\n            pass\n    return source\n\n\ndef _identify_input_type(source, format, encoding='utf-8'):\n    path = _identify_path(source)\n    if path:\n        format = _identify_format_from_path(source, format)\n        input_type = 'path'\n    else:\n        source = _as_unicode(source, encoding)\n        input_type = 'string'\n    return source, format, input_type\n\n\ndef _validate_formats(format, to, outputfile):\n    def normalize_format(fmt):\n        formats = {\n            'dbk': 'docbook',\n            'md': 'markdown',\n            'tex': 'latex',\n        }\n        fmt = formats.get(fmt, fmt)\n        # rst format can have extensions\n        if fmt[:4] == \"rest\":\n            fmt = \"rst\" + fmt[4:]\n        return fmt\n\n    format = normalize_format(format)\n    to = normalize_format(to)\n\n    if not format:\n        raise RuntimeError('Missing format!')\n\n    from_formats, to_formats = get_pandoc_formats()\n\n    if _get_base_format(format) not in from_formats:\n        raise RuntimeError(\n            'Invalid input format! Got \"%s\" but expected one of these: %s' % (\n                _get_base_format(format), ', '.join(from_formats)))\n\n    base_to_format = _get_base_format(to)\n\n    file_extension = os.path.splitext(to)[1]\n\n    if (base_to_format not in to_formats and\n            base_to_format != \"pdf\" and  # pdf is handled later # noqa: E127\n            file_extension != '.lua'):\n        raise RuntimeError(\n            'Invalid output format! Got %s but expected one of these: %s' % (\n                base_to_format, ', '.join(to_formats)))\n\n    # list from https://github.com/jgm/pandoc/blob/master/pandoc.hs\n    # `[...] where binaries = [\"odt\",\"docx\",\"epub\",\"epub3\"] [...]`\n    # pdf has the same restriction\n    if base_to_format in [\"odt\", \"docx\", \"epub\", \"epub3\", \"pdf\"] and not outputfile:\n        raise RuntimeError(\n            'Output to %s only works by using a outputfile.' % base_to_format\n        )\n\n    if base_to_format == \"pdf\":\n        # pdf formats needs to actually have a to format of latex and a\n        # filename with an ending pf .pdf\n        if outputfile[-4:] != \".pdf\":\n            raise RuntimeError('PDF output needs an outputfile with \".pdf\" as a fileending.')\n        # to is not allowed to contain pdf, but must point to latex\n        # it's also not allowed to contain extensions according to the docs\n        if to != base_to_format:\n            raise RuntimeError(\"PDF output can't contain any extensions: %s\" % to)\n        to = \"latex\"\n\n    return format, to\n\n\ndef _convert_input(source, format, input_type, to, extra_args=(), outputfile=None,\n                   filters=None):\n    _ensure_pandoc_path()\n\n    format, to = _validate_formats(format, to, outputfile)\n\n    string_input = input_type == 'string'\n    input_file = [source] if not string_input else []\n    args = [__pandoc_path, '--from=' + format]\n\n    args.append('--to=' + to)\n\n    args += input_file\n\n    if outputfile:\n        args.append(\"--output=\" + outputfile)\n\n    args.extend(extra_args)\n\n    # adds the proper filter syntax for each item in the filters list\n    if filters is not None:\n        if isinstance(filters, string_types):\n            filters = filters.split()\n        f = ['--filter=' + x for x in filters]\n        args.extend(f)\n\n    # To get access to pandoc-citeproc when we use a included copy of pandoc,\n    # we need to add the pypandoc/files dir to the PATH\n    new_env = os.environ.copy()\n    files_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"files\")\n    new_env[\"PATH\"] = new_env.get(\"PATH\", \"\") + os.pathsep + files_path\n\n    p = subprocess.Popen(\n        args,\n        stdin=subprocess.PIPE if string_input else None,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=new_env)\n\n    # something else than 'None' indicates that the process already terminated\n    if not (p.returncode is None):\n        raise RuntimeError(\n            'Pandoc died with exitcode \"%s\" before receiving input: %s' % (p.returncode,\n                                                                           p.stderr.read())\n        )\n\n    try:\n        source = cast_bytes(source, encoding='utf-8')\n    except (UnicodeDecodeError, UnicodeEncodeError):\n        # assume that it is already a utf-8 encoded string\n        pass\n    try:\n        stdout, stderr = p.communicate(source if string_input else None)\n    except OSError:\n        # this is happening only on Py2.6 when pandoc dies before reading all\n        # the input. We treat that the same as when we exit with an error...\n        raise RuntimeError('Pandoc died with exitcode \"%s\" during conversion.' % (p.returncode))\n\n    try:\n        stdout = stdout.decode('utf-8')\n    except UnicodeDecodeError:\n        # this shouldn't happen: pandoc more or less garantees that the output is utf-8!\n        raise RuntimeError('Pandoc output was not utf-8.')\n\n    # check that pandoc returned successfully\n    if p.returncode != 0:\n        raise RuntimeError(\n            'Pandoc died with exitcode \"%s\" during conversion: %s' % (p.returncode, stderr)\n        )\n\n    # if there is an outputfile, then stdout is likely empty!\n    return stdout\n\n\ndef _get_base_format(format):\n    '''\n    According to http://johnmacfarlane.net/pandoc/README.html#general-options,\n    syntax extensions for markdown can be individually enabled or disabled by\n    appending +EXTENSION or -EXTENSION to the format name.\n    Return the base format without any extensions.\n    '''\n    return re.split(r'\\+|-', format)[0]\n\n\ndef get_pandoc_formats():\n    '''\n    Dynamic preprocessor for Pandoc formats.\n    Return 2 lists. \"from_formats\" and \"to_formats\".\n    '''\n    _ensure_pandoc_path()\n    p = subprocess.Popen(\n        [__pandoc_path, '--list-output-formats'],\n        stdin=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        stdout=subprocess.PIPE)\n\n    comm = p.communicate()\n    out = comm[0].decode().splitlines(False)\n    if p.returncode != 0:\n        # try the old version and see if that returns something\n        return get_pandoc_formats_pre_1_18()\n\n    p = subprocess.Popen(\n        [__pandoc_path, '--list-input-formats'],\n        stdin=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        stdout=subprocess.PIPE)\n\n    comm = p.communicate()\n    in_ = comm[0].decode().splitlines(False)\n\n    return [f.strip() for f in in_], [f.strip() for f in out]\n\n\ndef get_pandoc_formats_pre_1_18():\n    '''\n    Dynamic preprocessor for Pandoc formats for version < 1.18.\n    Return 2 lists. \"from_formats\" and \"to_formats\".\n    '''\n    _ensure_pandoc_path()\n    p = subprocess.Popen(\n        [__pandoc_path, '-h'],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE)\n\n    comm = p.communicate()\n    help_text = comm[0].decode().splitlines(False)\n    if p.returncode != 0 or 'Options:' not in help_text:\n        raise RuntimeError(\"Couldn't call pandoc to get output formats. Output from pandoc:\\n%s\" %\n                           str(comm))\n    txt = ' '.join(help_text[1:help_text.index('Options:')])\n\n    aux = txt.split('Output formats: ')\n    in_ = re.sub(r'Input\\sformats:\\s|\\*|\\[.*?\\]', '', aux[0]).split(',')\n    out = re.sub(r'\\*|\\[.*?\\]', '', aux[1]).split(',')\n\n    return [f.strip() for f in in_], [f.strip() for f in out]\n\n\n# copied and adapted from jupyter_nbconvert/utils/pandoc.py, Modified BSD License\n\ndef _get_pandoc_version(pandoc_path):\n    new_env = os.environ.copy()\n    if 'HOME' not in os.environ:\n        new_env['HOME'] = tempfile.gettempdir()\n    p = subprocess.Popen(\n        [pandoc_path, '--version'],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        env=new_env)\n    comm = p.communicate()\n    out_lines = comm[0].decode().splitlines(False)\n    if p.returncode != 0 or len(out_lines) == 0:\n        raise RuntimeError(\"Couldn't call pandoc to get version information. Output from \"\n                           \"pandoc:\\n%s\" % str(comm))\n\n    version_pattern = re.compile(r\"^\\d+(\\.\\d+){1,}$\")\n    for tok in out_lines[0].split():\n        if version_pattern.match(tok):\n            version = tok\n            break\n    return version\n\n\ndef get_pandoc_version():\n    \"\"\"Gets the Pandoc version if Pandoc is installed.\n\n    It will probe Pandoc for its version, cache it and return that value. If a cached version is\n    found, it will return the cached version and stop probing Pandoc\n    (unless :func:`clean_version_cache()` is called).\n\n    :raises OSError: if pandoc is not found; make sure it has been installed and is available at\n            path.\n    \"\"\"\n    global __version\n\n    if __version is None:\n        _ensure_pandoc_path()\n        __version = _get_pandoc_version(__pandoc_path)\n    return __version\n\n\ndef get_pandoc_path():\n    \"\"\"Gets the Pandoc path if Pandoc is installed.\n\n    It will return a path to pandoc which is used by pypandoc.\n\n    This might be a full path or, if pandoc is on PATH, simple `pandoc`. It's garanteed\n    to be callable (i.e. we could get version information from `pandoc --version`).\n    If `PYPANDOC_PANDOC` is set and valid, it will return that value. If the environment\n    variable is not set, either the full path to the included pandoc or the pandoc in\n    `PATH` or a pandoc in some of the more usual (platform specific) install locations\n    (whatever is the higher version) will be returned.\n\n    If a cached path is found, it will return the cached path and stop probing Pandoc\n    (unless :func:`clean_pandocpath_cache()` is called).\n\n    :raises OSError: if pandoc is not found\n    \"\"\"\n    _ensure_pandoc_path()\n    return __pandoc_path\n\n\ndef _ensure_pandoc_path(quiet=False):\n    global __pandoc_path\n\n    if __pandoc_path is None:\n        included_pandoc = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                       \"files\", \"pandoc\")\n        search_paths = [\"pandoc\", included_pandoc]\n        pf = \"linux\" if sys.platform.startswith(\"linux\") else sys.platform\n        try:\n            search_paths.append(os.path.join(DEFAULT_TARGET_FOLDER[pf], \"pandoc\"))\n        except:  # noqa\n            # not one of the know platforms...\n            pass\n        if pf == \"linux\":\n            # Currently we install into ~/bin, but this is equally likely...\n            search_paths.append(\"~/.bin/pandoc\")\n        # Also add the interpreter script path, as that's where pandoc could be\n        # installed if it's an environment and the environment wasn't activated\n        if pf == \"win32\":\n            search_paths.append(os.path.join(sys.exec_prefix, \"Scripts\", \"pandoc\"))\n\n            # Since this only runs on Windows, use Windows slashes\n            if os.getenv('ProgramFiles', None):\n                search_paths.append(os.path.expandvars(\"${ProgramFiles}\\\\Pandoc\\\\Pandoc\"))\n            if os.getenv('ProgramFiles(x86)', None):\n                search_paths.append(os.path.expandvars(\"${ProgramFiles(x86)}\\\\Pandoc\\\\Pandoc\"))\n\n        # bin can also be used on windows (conda at leats has it in path), so\n        # include it unconditionally\n        search_paths.append(os.path.join(sys.exec_prefix, \"bin\", \"pandoc\"))\n        # If a user added the complete path to pandoc to an env, use that as the\n        # only way to get pandoc so that a user can overwrite even a higher\n        # version in some other places.\n        if os.getenv('PYPANDOC_PANDOC', None):\n            search_paths = [os.getenv('PYPANDOC_PANDOC')]\n        curr_version = [0, 0, 0]\n        for path in search_paths:\n            # Needed for windows and subprocess which can't expand it on it's\n            # own...\n            path = os.path.expanduser(path)\n            version_string = \"0.0.0\"\n            # print(\"Trying: %s\" % path)\n            try:\n                version_string = _get_pandoc_version(path)\n            except Exception as e:\n                # we can't use that path...\n                if os.path.exists(path):\n                    # path exist but is not useable -> not executable?\n                    if not quiet:\n                        print(\"Found %s, but not using it because of an error:\" % (path), file=sys.stderr)\n                        print(e, file=sys.stderr)\n                continue\n            version = [int(x) for x in version_string.split(\".\")]\n            while len(version) < len(curr_version):\n                version.append(0)\n            # print(\"%s, %s\" % (path, version))\n            # Only use the new version if it is any bigger...\n            if version > curr_version:\n                # print(\"Found: %s\" % path)\n                __pandoc_path = path\n                curr_version = version\n\n        if __pandoc_path is None:\n            # Only print hints if requested\n            if not quiet:\n                if os.path.exists('/usr/local/bin/brew'):\n                    sys.stderr.write(textwrap.dedent(\"\"\"\\\n                        Maybe try:\n\n                            brew install pandoc\n                    \"\"\"))\n                elif os.path.exists('/usr/bin/apt-get'):\n                    sys.stderr.write(textwrap.dedent(\"\"\"\\\n                        Maybe try:\n\n                            sudo apt-get install pandoc\n                    \"\"\"))\n                elif os.path.exists('/usr/bin/yum'):\n                    sys.stderr.write(textwrap.dedent(\"\"\"\\\n                        Maybe try:\n\n                        sudo yum install pandoc\n                    \"\"\"))\n                sys.stderr.write(textwrap.dedent(\"\"\"\\\n                    See http://johnmacfarlane.net/pandoc/installing.html\n                    for installation options\n                \"\"\"))\n                sys.stderr.write(textwrap.dedent(\"\"\"\\\n                    ---------------------------------------------------------------\n\n                \"\"\"))\n            raise OSError(\"No pandoc was found: either install pandoc and add it\\n\"\n                          \"to your PATH or or call pypandoc.download_pandoc(...) or\\n\"\n                          \"install pypandoc wheels with included pandoc.\")\n\n\ndef ensure_pandoc_installed(url=None, targetfolder=None, version=\"latest\", quiet=False, delete_installer=False):\n    \"\"\"Try to install pandoc if it isn't installed.\n\n    Parameters are passed to download_pandoc()\n\n    :raises OSError: if pandoc cannot be installed\n    \"\"\"\n    try:\n        # Perform the test quietly if asked\n        _ensure_pandoc_path(quiet=quiet)\n\n    except OSError:\n        download_pandoc(url=url, targetfolder=targetfolder, version=version, quiet=quiet, delete_installer=delete_installer)\n\n        # Show errors in case of secondary failure\n        _ensure_pandoc_path(quiet=False)\n\n\n# -----------------------------------------------------------------------------\n# Internal state management\n# -----------------------------------------------------------------------------\ndef clean_version_cache():\n    global __version\n    __version = None\n\n\ndef clean_pandocpath_cache():\n    global __pandoc_path\n    __pandoc_path = None\n\n\n__version = None\n__pandoc_path = None\n"
        ],
        "test_patch": "",
        "patch_preview": "From eb03e142e21429559d03446a4aeadedd09df1cce Mon Sep 17 00:00:00 2001\nFrom: Luke Briggs <lukebriggs02@gmail.com>\nDate: Wed, 2 Jun 2021 16:58:22 +0100\nSubject: [PATCH] Prevent new console  window being created on Windows\n\nWhen using deployment methods such as pyinstaller, pypandoc would\nflash open a console window when it invoked pandoc.\n\nChanging the creation flag on windows to prevent a new console window\nfixes this\n---\n pypandoc/__init__.py | 20 ++++++++++++++------\n 1 file changed, 14 insert"
      },
      "patch": {
        "length": 3633,
        "files_changed": 1,
        "lines_added": 14,
        "lines_deleted": 6,
        "net_change": 8,
        "changed_files": [
          {
            "file": "pypandoc/__init__.py",
            "added": 14,
            "deleted": 6
          }
        ]
      },
      "issue_comments": [
        {
          "id": 872414656,
          "body": "Thanks!\nGood work with the fix",
          "user": "JessicaTegner",
          "created_at": "2021-07-01T17:11:04Z",
          "html_url": "https://github.com/JessicaTegner/pypandoc/pull/217#issuecomment-872414656"
        },
        {
          "id": 872091714,
          "body": "All recent commits pulled and tests pass",
          "user": "LukeBriggsDev",
          "created_at": "2021-07-01T09:38:38Z",
          "html_url": "https://github.com/JessicaTegner/pypandoc/pull/217#issuecomment-872091714"
        },
        {
          "id": 871752652,
          "body": "hey @LukeBriggsDev \r\n\r\nCan you pull the recent commits into your branch, so we can see if your code still works (the test cases should all pass now)",
          "user": "JessicaTegner",
          "created_at": "2021-06-30T21:55:28Z",
          "html_url": "https://github.com/JessicaTegner/pypandoc/pull/217#issuecomment-871752652"
        }
      ],
      "issue_comments_count": 3,
      "code_statistics": {
        "total_files": 18,
        "total_lines": 3195,
        "total_bytes": 120290,
        "python_files": 6,
        "python_lines": 1491,
        "file_extensions": {
          ".yml": 1,
          ".md": 4,
          ".ini": 1,
          "": 3,
          ".py": 6,
          ".in": 1,
          ".ps1": 1,
          ".cmd": 1
        },
        "largest_files": [
          {
            "path": "md",
            "size": 31803,
            "lines": 869,
            "extension": ""
          },
          {
            "path": "pypandoc/__init__.py",
            "size": 21900,
            "lines": 585,
            "extension": ".py"
          },
          {
            "path": "tests.py",
            "size": 17038,
            "lines": 390,
            "extension": ".py"
          },
          {
            "path": "pypandoc/pandoc_download.py",
            "size": 9091,
            "lines": 258,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 12084,
            "lines": 256,
            "extension": ".md"
          },
          {
            "path": "appveyor/install.ps1",
            "size": 7195,
            "lines": 229,
            "extension": ".ps1"
          },
          {
            "path": "setup.py",
            "size": 4107,
            "lines": 112,
            "extension": ".py"
          },
          {
            "path": "appveyor.yml",
            "size": 3862,
            "lines": 99,
            "extension": ".yml"
          },
          {
            "path": "appveyor/run_with_env.cmd",
            "size": 3366,
            "lines": 88,
            "extension": ".cmd"
          },
          {
            "path": "pypandoc/py3compat.py",
            "size": 1916,
            "lines": 77,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 18,
        "files_changed_count": 1,
        "files_changed_ratio": 0.05555555555555555,
        "total_lines_in_repo": 3195,
        "lines_added": 14,
        "lines_deleted": 6,
        "net_lines_changed": 8,
        "lines_changed_ratio": 0.006259780907668232,
        "pr_body_length": 203,
        "commit_message_length": 137,
        "python_file_count": 6,
        "python_line_count": 1491
      }
    },
    {
      "tar_file_name": "JuanPotato#Legofy#pull#57",
      "repo_name": "JuanPotato#Legofy#pull#57",
      "success": true,
      "error": null,
      "commit": {
        "sha": "f3408147b980187fc14dad53291ae0b96ca6fde5",
        "message": "Update README.md",
        "author": {
          "name": "Juan Potato",
          "email": "hasantiny@gmail.com",
          "date": "2015-11-02T00:55:25Z"
        },
        "html_url": "https://github.com/JuanPotato/Legofy/commit/f3408147b980187fc14dad53291ae0b96ca6fde5",
        "api_url": "https://api.github.com/repos/JuanPotato/Legofy/commits/f3408147b980187fc14dad53291ae0b96ca6fde5"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/JuanPotato#Legofy#pull#57",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/JuanPotato#Legofy#pull#57.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/JuanPotato#Legofy#pull#57/source_code"
      },
      "pr": {
        "number": 57,
        "title": "[RFR] Changed the way the renaming works (proposal)",
        "body": "This pull-request is only a suggestion, feel free to discard it if you do not agree with the way it works or the even the achieved goal.\n\nSo right now, the script renames the file by appending the `lego_` prefix to it, so that `path/to/file.jpg` becomes `path/to/lego_file.jpg`. It works great, except it doesnÃ¢â‚¬â„¢t preserve the file order when there are a lot of files in a folder, because of the filename prefix. My proposal would rename the file like so: `path/to/file.lego.jpg`, so that the converted file would live next to the original in the finder (or equivalent file browser).\n\nThe code is pretty self explanatory I guess, but I added comments just in case. I am no Python guru, so if anything can be optimised, be sure to tell.\n\nEdit: [I LOVE THIS PROJECT](https://twitter.com/HugoGiraudel/status/661100177227440128)!\n",
        "state": "closed",
        "created_at": "2015-11-02T21:15:03Z",
        "updated_at": "2015-11-02T21:41:00Z",
        "merged_at": null,
        "html_url": "https://github.com/JuanPotato/Legofy/pull/57",
        "user": "KittyGiraudel",
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "JuanPotato_Legofy-57",
        "repo": "/JuanPotato/Legofy",
        "base_commit": "f3408147b980187fc14dad53291ae0b96ca6fde5",
        "problem_statement": {},
        "edit_files": [
          "legofy/__init__.py"
        ],
        "oracle_files": [
          "from __future__ import unicode_literals\n\nfrom PIL import Image\nfrom subprocess import call\nimport shutil\nimport sys\nimport os\n\n# function that iterates over the gif's frames\ndef iter_frames(imageToIter):\n    try:\n        i = 0\n        while 1:\n            imageToIter.seek(i)\n            imframe = imageToIter.copy()\n            if i == 0:\n                palette = imframe.getpalette()\n            else:\n                imframe.putpalette(palette)\n            yield imframe\n            i += 1\n    except EOFError:\n        pass\n\n\n# small function to apply an effect over an entire image\ndef applyEffect(image, overlayRed, overlayGreen, overlayBlue):\n    channels = image.split()\n\n    r = channels[0].point(lambda color: overlayRed - 100 if (133 - color) > 100 else (overlayRed + 100 if (133 - color) < -100 else overlayRed - (133 - color)))\n    g = channels[1].point(lambda color: overlayGreen - 100 if (133 - color) > 100 else (overlayGreen + 100 if (133 - color) < -100 else overlayGreen - (133 - color)))\n    b = channels[2].point(lambda color: overlayBlue - 100 if (133 - color) > 100 else (overlayBlue + 100 if (133 - color) < -100 else overlayBlue - (133 - color)))\n\n    channels[0].paste(r)\n    channels[1].paste(g)\n    channels[2].paste(b)\n\n    return Image.merge(image.mode, channels)\n\n \n# create a lego brick from a single color\ndef makeLegoBrick(brickImage, overlayRed, overlayGreen, overlayBlue):\n    return applyEffect(brickImage.copy(), overlayRed, overlayGreen, overlayBlue)\n\n\n# create a lego version of an image from an image\ndef makeLegoImage(baseImage, brickFilename, width, height):\n    brickImage = Image.open(brickFilename)\n    baseWidth, baseHeight = baseImage.size\n    basePoa = baseImage.load()\n\n    legoImage = Image.new(\"RGB\", (baseWidth * width, baseHeight * height), \"white\")\n\n    for x in range(baseWidth):\n        for y in range(baseHeight):\n            bp = basePoa[x, y]\n            legoImage.paste(makeLegoBrick(brickImage, bp[0], bp[1], bp[2]), (x * width, y * height, (x + 1) * width, (y + 1) * height))\n    \n    del basePoa\n    \n    return legoImage\n\n\n# check if image is animated\ndef is_animated(im):\n    try:\n        im.seek(1)\n        return True\n    except EOFError:\n        return False\n\n\ndef main(filename, brick=os.path.join(os.path.dirname(__file__), \"bricks\", \"brick.png\")):\n    # open gif to start splitting\n    realPath = os.path.realpath(filename)\n    if not os.path.isfile(realPath):\n        print('File \"{0}\" was not found.'.format(filename))\n        sys.exit(0)\n    \n    brick = os.path.realpath(brick)\n    \n    if not os.path.isfile(brick):\n        print('Brick asset \"{0}\" was not found.'.format(brick))\n        sys.exit(0)\n\n    baseImage = Image.open(realPath)\n    \n    newFilename = os.path.split(realPath)\n    newFilename = os.path.join(newFilename[0], \"lego_{0}\".format(newFilename[1]))\n\n    scale = 1\n    newSize = baseImage.size\n    brickSize = Image.open(brick).size\n    \n    if newSize[0] > brickSize[0] or newSize[1] > brickSize[1]:\n        if newSize[0] < newSize[1]:\n            scale = newSize[1] / brickSize[1]\n        else:\n            scale = newSize[0] / brickSize[0]\n    \n        newSize = (int(round(newSize[0] / scale)), int(round(newSize[1] / scale)))\n\n    if filename.lower().endswith(\".gif\") and is_animated(baseImage):\n        # Animated GIF\n\n        print(\"Animated gif detected, will now legofy each frame and recreate the gif and save as lego_{0}\".format(filename))\n        # check if dir exists, if not, make it\n        if not os.path.exists(\"./tmp_frames/\"):\n            os.makedirs(\"./tmp_frames/\")\n\n        # for each frame in the gif, save it\n        for i, frame in enumerate(iter_frames(baseImage)):\n            frame.save('./tmp_frames/frame_{0}.png'.format((\"0\" * (4 - len(str(i)))) + str(i)), **frame.info)\n\n        # make lego images from gif\n        for file in os.listdir(\"./tmp_frames\"):\n            if file.endswith(\".png\"):\n                print(\"Working on {0}\".format(file))\n                im = Image.open(\"./tmp_frames/{0}\".format(file)).convert(\"RGBA\")\n                if scale != 1:\n                    im.thumbnail(newSize, Image.ANTIALIAS)\n                makeLegoImage(im, brick, brickSize[0], brickSize[1]).save(\"./tmp_frames/{0}\".format(file))\n\n        # make new gif \"convert -delay 10 -loop 0 *.png animation.gif\"\n        delay = str(baseImage.info[\"duration\"] / 10)\n    \n        command = \"convert -delay {0} -loop 0 ./tmp_frames/*.png {1}\".format(delay, newFilename)\n        if os.name == \"nt\":\n            MAGICK_HOME = os.environ.get('MAGICK_HOME')\n            command = os.path.join(MAGICK_HOME, \"convert.exe\") + \" -delay {0} -loop 0 ./tmp_frames/*.png {1}\".format(delay, newFilename)\n\n        print(command)\n        call(command.split(\" \"))\n        print(\"Creating gif with filename\\\"lego_{0}\\\"\".format(filename))\n        shutil.rmtree('./tmp_frames')\n    else:\n\n        # Other image types\n\n        newFilename = newFilename.rpartition('.')[0] + '.png'\n        \n        baseImage.convert(\"RGBA\")\n        if scale != 1:\n            baseImage.thumbnail(newSize, Image.ANTIALIAS)\n        print(\"Static image detected, will now legofy and save as {0}\".format(newFilename))\n        makeLegoImage(baseImage, brick, brickSize[0], brickSize[1]).save(newFilename)\n\n    print(\"Finished!\")\n"
        ],
        "test_patch": "",
        "patch_preview": "From 5f15fb553162c850f65a1d67f9b3d85a13fa4a7a Mon Sep 17 00:00:00 2001\nFrom: Hugo Giraudel <hugo.giraudel@gmail.com>\nDate: Mon, 2 Nov 2015 22:14:52 +0100\nSubject: [PATCH] Changed the way the renaming works\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nThis pull-request is only a suggestion, feel free to discard it if you do not agree with the way it works or the even the achieved goal.\n\nSo right now, the renaming works like this `path/to/file.jpg` bec"
      },
      "patch": {
        "length": 1968,
        "files_changed": 1,
        "lines_added": 12,
        "lines_deleted": 2,
        "net_change": 10,
        "changed_files": [
          {
            "file": "legofy/__init__.py",
            "added": 12,
            "deleted": 2
          }
        ]
      },
      "issue_comments": [
        {
          "id": 153164868,
          "body": "This is already in another PR that I should really merge. It appends _lego to the filename, pretty similar to your solution. #54 \nI'm glad you like the program :D\n",
          "user": "JuanPotato",
          "created_at": "2015-11-02T21:36:55Z",
          "html_url": "https://github.com/JuanPotato/Legofy/pull/57#issuecomment-153164868"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 13,
        "total_lines": 2578,
        "total_bytes": 266551,
        "python_files": 5,
        "python_lines": 201,
        "file_extensions": {
          "": 1,
          ".txt": 1,
          ".in": 1,
          ".py": 5,
          ".md": 1,
          ".jpg": 1,
          ".png": 3
        },
        "largest_files": [
          {
            "path": "tests/brick.png",
            "size": 50552,
            "lines": 704,
            "extension": ".png"
          },
          {
            "path": "legofy/bricks/brick.png",
            "size": 50552,
            "lines": 704,
            "extension": ".png"
          },
          {
            "path": "tests/lego_image.png",
            "size": 116874,
            "lines": 626,
            "extension": ".png"
          },
          {
            "path": "tests/image.jpg",
            "size": 39356,
            "lines": 295,
            "extension": ".jpg"
          },
          {
            "path": "legofy/__init__.py",
            "size": 5294,
            "lines": 148,
            "extension": ".py"
          },
          {
            "path": "setup.py",
            "size": 681,
            "lines": 29,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 1445,
            "lines": 24,
            "extension": ".md"
          },
          {
            "path": "LICENSE",
            "size": 1079,
            "lines": 21,
            "extension": ""
          },
          {
            "path": "legofy/cli.py",
            "size": 402,
            "lines": 16,
            "extension": ".py"
          },
          {
            "path": "tests/test_legofy.py",
            "size": 253,
            "lines": 8,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 13,
        "files_changed_count": 1,
        "files_changed_ratio": 0.07692307692307693,
        "total_lines_in_repo": 2578,
        "lines_added": 12,
        "lines_deleted": 2,
        "net_lines_changed": 10,
        "lines_changed_ratio": 0.005430566330488751,
        "pr_body_length": 827,
        "commit_message_length": 16,
        "python_file_count": 5,
        "python_line_count": 201
      }
    },
    {
      "tar_file_name": "Kav-K#GPTDiscord#pull#390",
      "repo_name": "Kav-K#GPTDiscord#pull#390",
      "success": true,
      "error": null,
      "commit": {
        "sha": "722228870523f9e2a60d87c74ca481d0fa041d6d",
        "message": "Format Python code with psf/black push",
        "author": {
          "name": "github-actions",
          "email": "${GITHUB_ACTOR}@users.noreply.github.com",
          "date": "2023-11-09T03:00:33Z"
        },
        "html_url": "https://github.com/Kav-K/GPTDiscord/commit/722228870523f9e2a60d87c74ca481d0fa041d6d",
        "api_url": "https://api.github.com/repos/Kav-K/GPTDiscord/commits/722228870523f9e2a60d87c74ca481d0fa041d6d"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Kav-K#GPTDiscord#pull#390",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Kav-K#GPTDiscord#pull#390.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Kav-K#GPTDiscord#pull#390/source_code"
      },
      "pr": {
        "number": 390,
        "title": "Fix for Unknown Interaction errors",
        "body": "#368 \r\nActually, the error in my code was of some type annotation stuff I tried to do, wich was *obviously* wrong, but it got in the commit for some reason. Here is the code, fixed. I tested it with code chat & gpt chat, and it works.",
        "state": "closed",
        "created_at": "2023-11-09T14:00:10Z",
        "updated_at": "2023-11-11T11:40:46Z",
        "merged_at": "2023-11-11T02:08:35Z",
        "html_url": "https://github.com/Kav-K/GPTDiscord/pull/390",
        "user": "Paillat-dev",
        "additions": 66,
        "deletions": 11,
        "changed_files": 7,
        "commits": 2
      },
      "swebench": {
        "instance_id": "Kav-K_GPTDiscord-390",
        "repo": "/Kav-K/GPTDiscord",
        "base_commit": "722228870523f9e2a60d87c74ca481d0fa041d6d",
        "problem_statement": {
          "title": "Unknown interaction errors",
          "body": "Sometimes there are unknown interaction errors with a response in /gpt converse takes too long to return, or if the user deleted their original message it is responding to before it responds"
        },
        "edit_files": [
          "cogs/code_interpreter_service_cog.py",
          "cogs/image_service_cog.py",
          "cogs/search_service_cog.py",
          "cogs/text_service_cog.py",
          "models/index_model.py",
          "utils/safe_ctx_respond.py",
          "utils/__init__.py",
          "utils/safe_ctx_respond.py"
        ],
        "oracle_files": [
          "import asyncio\nimport datetime\nimport functools\nimport io\nimport os\nimport sys\nimport tempfile\nimport traceback\nfrom concurrent.futures.thread import ThreadPoolExecutor\nfrom typing import List\n\nimport re\n\nimport aiofiles\nimport discord\nimport openai\nfrom discord.ext import pages\nfrom e2b import Session, DataAnalysis\nfrom e2b.templates.data_analysis import Artifact\n\nfrom langchain.agents import (\n    Tool,\n    initialize_agent,\n    AgentType,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\nfrom langchain.prompts import (\n    MessagesPlaceholder,\n)\nfrom langchain.schema import SystemMessage\nfrom langchain.utilities import GoogleSearchAPIWrapper\n\nfrom models.embed_statics_model import EmbedStatics\nfrom services.deletion_service import Deletion\nfrom services.environment_service import EnvService\nfrom services.moderations_service import Moderation\n\n\nclass CaptureStdout:\n    def __enter__(self):\n        self.buffer = io.StringIO()\n        self.original_stdout = sys.stdout\n        sys.stdout = self.buffer\n        return self.buffer\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout = self.original_stdout\n\n\nasync def capture_stdout(func, *args, **kwargs):\n    with CaptureStdout() as buffer:\n        result = await func(*args, **kwargs)\n    captured_output = buffer.getvalue()\n    return result, captured_output\n\n\nALLOWED_GUILDS = EnvService.get_allowed_guilds()\nUSER_INPUT_API_KEYS = EnvService.get_user_input_api_keys()\nUSER_KEY_DB = EnvService.get_api_db()\nPRE_MODERATE = EnvService.get_premoderate()\n\nOPENAI_API_KEY = EnvService.get_openai_token()\n# Set the environment\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nGOOGLE_API_KEY = EnvService.get_google_search_api_key()\nGOOGLE_SEARCH_ENGINE_ID = EnvService.get_google_search_engine_id()\n\nE2B_API_KEY = EnvService.get_e2b_api_key()\n\n\nclass CodeInterpreterService(discord.Cog, name=\"CodeInterpreterService\"):\n    \"\"\"Cog containing translation commands and retrieval of translation services\"\"\"\n\n    def __init__(\n        self,\n        bot,\n        gpt_model,\n        usage_service,\n        deletion_service,\n        converser_cog,\n    ):\n        super().__init__()\n        self.bot = bot\n        self.usage_service = usage_service\n        self.EMBED_CUTOFF = 2000\n        self.redo_users = {}\n        self.chat_agents = {}\n        self.thread_awaiting_responses = []\n        self.converser_cog = converser_cog\n        self.executor = ThreadPoolExecutor(max_workers=10)\n        self.initial_messages = {}\n        self.sessions = {}\n        # Make a mapping of all the country codes and their full country names:\n\n    @discord.Cog.listener()\n    async def on_message(self, message):\n        # Check if the message is from a bot.\n        if message.author == self.bot.user:\n            return\n\n        # Check if the message is from a guild.\n        if not message.guild:\n            return\n\n        # System message\n        if message.type != discord.MessageType.default:\n            return\n\n        if message.content.strip().startswith(\"~\"):\n            return\n\n        # if we are still awaiting a response from the agent, then we don't want to process the message.\n        if message.channel.id in self.thread_awaiting_responses:\n            resp_message = await message.reply(\n                \"Please wait for the agent to respond to a previous message first!\"\n            )\n            deletion_time = datetime.datetime.now() + datetime.timedelta(seconds=5)\n            deletion_time = deletion_time.timestamp()\n\n            original_deletion_message = Deletion(message, deletion_time)\n            deletion_message = Deletion(resp_message, deletion_time)\n            await self.converser_cog.deletion_queue.put(deletion_message)\n            await self.converser_cog.deletion_queue.put(original_deletion_message)\n            return\n\n        # Pre moderation\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(message.content, message):\n                await message.delete()\n                return\n\n        prompt = message.content.strip()\n\n        # If the message channel is in self.chat_agents, then we delegate the message to the agent.\n        if message.channel.id in self.chat_agents:\n            if prompt.lower() in [\"stop\", \"end\", \"quit\", \"exit\"]:\n                await message.reply(\n                    \"Ending chat session. You can access the sandbox of this session at https://\"\n                    + self.sessions[message.channel.id].get_hostname()\n                )\n                self.sessions[message.channel.id].close()\n                self.chat_agents.pop(message.channel.id)\n\n                # close the thread\n                thread = await self.bot.fetch_channel(message.channel.id)\n                await thread.edit(name=\"Closed-GPT\")\n                await thread.edit(archived=True)\n                return\n\n            file = message.attachments[0] if len(message.attachments) > 0 else None\n\n            # File operations, allow for user file upload\n            if file:\n                # We will attempt to upload the file to the execution environment\n                thinking_embed = discord.Embed(\n                    title=f\"ðŸ¤–ðŸ’¬ Uploading file to code interpreter environment...\",\n                    color=0x808080,\n                )\n\n                thinking_embed.set_footer(text=\"This may take a few seconds.\")\n                try:\n                    thinking_message = await message.reply(embed=thinking_embed)\n                except:\n                    traceback.print_exc()\n                    pass\n\n                try:\n                    await message.channel.trigger_typing()\n                except Exception:\n                    pass\n\n                async with aiofiles.tempfile.NamedTemporaryFile(\n                    delete=False\n                ) as temp_file:\n                    await file.save(temp_file.name)\n\n                    filename = file.filename\n\n                    # Assert that the filename is < 100 characters, if it is greater, truncate to the first 100 characters and keep the original ending\n                    if len(filename) > 100:\n                        filename = filename[:100] + filename[-4:]\n\n                    file_upload_result = await self.sessions[\n                        message.channel.id\n                    ].upload_file_async(filename, await file.read())\n\n                    if filename in str(file_upload_result):\n                        try:\n                            await thinking_message.delete()\n                            prompt += (\n                                \"\\n{The user has just uploaded a file to \"\n                                + f\"/home/user/{filename}\"\n                                + \"}\"\n                            )\n                            print(\"The edited prompt is: \" + prompt)\n                        except:\n                            traceback.print_exc()\n                            pass\n                    else:\n                        try:\n                            failed_embed = discord.Embed(\n                                title=f\"ðŸ¤–ðŸ’¬ File upload failed\", color=0x808080\n                            )\n                            await message.reply(embed=failed_embed)\n                            return\n                        except:\n                            traceback.print_exc()\n                            pass\n\n            self.thread_awaiting_responses.append(message.channel.id)\n\n            try:\n                await message.channel.trigger_typing()\n            except:\n                pass\n\n            agent = self.chat_agents[message.channel.id]\n            try:\n                # Start listening to STDOUT before this call. We wanna track all the output for this specific call below\n                response, stdout_output = await capture_stdout(\n                    self.bot.loop.run_in_executor, None, agent.run, prompt\n                )\n                response = str(response)\n\n                try:\n                    print(response)\n                    print(stdout_output)\n                except:\n                    pass\n\n            except Exception as e:\n                response = f\"Error: {e}\"\n                traceback.print_exc()\n                await message.reply(\n                    embed=EmbedStatics.get_code_chat_failure_embed(response)\n                )\n                self.thread_awaiting_responses.remove(message.channel.id)\n                return\n\n            # Parse the artifact names. After Artifacts: there should be a list in form [] where the artifact names are inside, comma separated inside stdout_output\n            artifact_names = re.findall(r\"Artifacts: \\[(.*?)\\]\", stdout_output)\n            # The artifacts list may be formatted like [\"'/home/user/artifacts/test2.txt', '/home/user/artifacts/test.txt'\"], where its technically 1 element in the list, so we need to split it by comma and then remove the quotes and spaces\n            if len(artifact_names) > 0:\n                artifact_names = artifact_names[0].split(\",\")\n                artifact_names = [\n                    artifact_name.strip().replace(\"'\", \"\")\n                    for artifact_name in artifact_names\n                ]\n\n            artifacts_available = len(artifact_names) > 0\n\n            if len(response) > 2000:\n                embed_pages = EmbedStatics.paginate_chat_embed(response)\n\n                for x, page in enumerate(embed_pages):\n                    if x == 0:\n                        previous_message = await message.reply(embed=page)\n                    else:\n                        previous_message = previous_message.reply(embed=page)\n\n                if artifacts_available:\n                    await previous_message.reply(\n                        \"Retrieve your artifacts\",\n                        view=CodeInterpreterDownloadArtifactsView(\n                            message,\n                            self,\n                            self.sessions[message.channel.id],\n                            artifact_names,\n                        ),\n                    )\n\n            else:\n                response = response.replace(\"\\\\n\", \"\\n\")\n                # Build a response embed\n                response_embed = discord.Embed(\n                    title=\"\",\n                    description=response,\n                    color=0x808080,\n                )\n                await message.reply(\n                    embed=response_embed,\n                    view=CodeInterpreterDownloadArtifactsView(\n                        message, self, self.sessions[message.channel.id], artifact_names\n                    )\n                    if artifacts_available\n                    else None,\n                )\n\n            self.thread_awaiting_responses.remove(message.channel.id)\n\n    class SessionedCodeExecutor:\n        def __init__(self):\n            try:\n                self.session = DataAnalysis(api_key=E2B_API_KEY)\n                self.sessioned = True\n            except:\n                traceback.print_exc()\n                self.sessioned = False\n\n        def execute_code_sync(self, code: str):\n            \"\"\"Synchronous wrapper around the async execute_code function.\"\"\"\n            return asyncio.run(self.execute_code_async(code))\n\n        async def execute_code_async(self, code: str):\n            loop = asyncio.get_running_loop()\n            runner = functools.partial(self.session.run_python, code=code, timeout=30)\n\n            stdout, stderr, artifacts = await loop.run_in_executor(None, runner)\n            artifacts: List[Artifact] = list(artifacts)\n\n            artifacts_or_no_artifacts = (\n                \"\\nArtifacts: \" + str([artifact.name for artifact in artifacts])\n                if len(artifacts) > 0\n                else \"\\nNO__ARTIFACTS\"\n            )\n\n            if len(stdout) > 12000:\n                stdout = stdout[:12000]\n            return (\n                \"STDOUT: \" + stdout + \"\\nSTDERR: \" + stderr + artifacts_or_no_artifacts\n            )\n\n        def close(self):\n            self.session.close()\n\n        def get_hostname(self):\n            return self.session.get_hostname()\n\n        def download_file(self, filepath):\n            return self.session.download_file(filepath, timeout=30)\n\n        def install_python_package(self, package):\n            return self.session.install_python_packages(package_names=package)\n\n        def install_system_package(self, package):\n            return self.session.install_system_packages(package_names=package)\n\n        def run_command_sync(self, command):\n            return asyncio.run(self.run_command_async(command))\n\n        async def run_command_async(self, command):\n            loop = asyncio.get_running_loop()\n            runner = functools.partial(\n                self.session.process.start, cmd=command, timeout=30\n            )\n\n            command = await loop.run_in_executor(None, runner)\n\n            runner = functools.partial(command.wait)\n            await loop.run_in_executor(None, runner)\n\n            output = \"STDOUT:\" + command.stdout + \"\\nSTDERR:\" + command.stderr\n            return output\n\n        def is_sessioned(self):\n            return self.sessioned\n\n        def upload_file_sync(self, path, file):\n            return asyncio.run(self.upload_file_async(path, file))\n\n        async def upload_file_async(self, path, file):\n            loop = asyncio.get_running_loop()\n            runner = functools.partial(\n                self.session.filesystem.write_bytes,\n                path=f\"/home/user/{path}\",\n                content=file,\n            )\n\n            await loop.run_in_executor(None, runner)\n\n            runner = functools.partial(\n                self.session.filesystem.list, path=f\"/home/user/\"\n            )\n            list_output = await loop.run_in_executor(None, runner)\n\n            return list_output\n\n    async def code_interpreter_chat_command(\n        self,\n        ctx: discord.ApplicationContext,\n        model,\n    ):\n        embed_title = f\"{ctx.user.name}'s code interpreter conversation with GPT\"\n        message_embed = discord.Embed(\n            title=embed_title,\n            description=f\"The agent is able to execute Python code and manipulate its environment.\\nModel: {model}\\n\\nType `end` to stop the conversation\",\n            color=0xF82C45,\n        )\n        message_embed.set_thumbnail(url=\"https://i.imgur.com/qua6Bya.png\")\n        message_embed.set_footer(\n            text=\"Code Interpreter Chat\", icon_url=\"https://i.imgur.com/qua6Bya.png\"\n        )\n        message_thread = await ctx.send(embed=message_embed)\n        thread = await message_thread.create_thread(\n            name=ctx.user.name + \"'s code interpreter conversation with GPT\",\n            auto_archive_duration=60,\n        )\n        await ctx.respond(\"Conversation started.\")\n\n        self.sessions[thread.id] = self.SessionedCodeExecutor()\n\n        if not self.sessions[thread.id].is_sessioned():\n            await thread.send(\n                \"Failed to start code interpreter session. This may be an issue with E2B. Please try again later.\"\n            )\n            await thread.edit(name=\"Closed-GPT (Error)\")\n            await thread.edit(archived=True)\n            return\n\n        tools = [\n            # The requests tool\n            Tool(\n                name=\"Code-execution-tool\",\n                func=self.sessions[thread.id].execute_code_sync,\n                description=f\"This tool is able to execute Python 3 code. The input to the tool is just the raw python code. The output is the stdout of the code. When using the output of the code execution tool, always make sure to always display the raw output to the user as well.\",\n            ),\n            Tool(\n                name=\"Install-python-package-tool\",\n                func=self.sessions[thread.id].install_python_package,\n                description=f\"This tool installs a python package into the execution environment. The input to the tool is a single python package name (e.g 'numpy'). If you need to install multiple python packages, call this tool multiple times.\",\n            ),\n            Tool(\n                name=\"Install-system-package-tool\",\n                func=self.sessions[thread.id].install_python_package,\n                description=f\"This tool installs a system package into the system environment. The input to the tool is a single package name (e.g 'htop'). If you need to install multiple system packages, call this tool multiple times.\",\n            ),\n            Tool(\n                name=\"Run-command-tool\",\n                func=self.sessions[thread.id].run_command_sync,\n                description=f\"This tool allows you to run terminal (bash/unix) commands in the execution environment. The input to the tool is the command to run. An example input can be 'df -h'\",\n            ),\n        ]\n\n        # Add google search functionality if the user has google keys set up\n        if GOOGLE_API_KEY and GOOGLE_SEARCH_ENGINE_ID:\n            search = GoogleSearchAPIWrapper(\n                google_api_key=GOOGLE_API_KEY,\n                google_cse_id=GOOGLE_SEARCH_ENGINE_ID,\n                k=2,\n            )\n            tools.append(\n                Tool(\n                    name=\"Search-Tool\",\n                    func=search.run,\n                    description=\"This tool is useful when you need to answer questions about current events or retrieve information about a topic that may require the internet. The input to this tool is a search query to ask google. Search queries should be less than 8 words. For example, an input could be 'What is the weather like in New York?' and the tool input would be 'weather new york'.\",\n                )\n            )\n\n        llm = ChatOpenAI(model=model, temperature=0, openai_api_key=OPENAI_API_KEY)\n\n        max_token_limit = 29000 if \"gpt-4\" in model else 7500\n\n        memory = ConversationSummaryBufferMemory(\n            memory_key=\"memory\",\n            return_messages=True,\n            llm=llm,\n            max_token_limit=100000 if \"preview\" in model else max_token_limit,\n        )\n\n        agent_kwargs = {\n            \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"memory\")],\n            \"system_message\": SystemMessage(\n                content=\"You are an expert programmer that is able to use the tools to your advantage to execute \"\n                \"python code. Help the user iterate on their code and test it through execution. Always \"\n                \"respond in the specified JSON format. Always provide the full code output when asked for \"\n                \"when you execute code. Ensure that all your code is formatted with backticks followed by the \"\n                \"markdown identifier of the language that the code is in. For example ```python3 {code} ```. You are \"\n                \"able to search the internet to find the most up to date algorithms and practices. You are \"\n                \"also able to run commands in the execution environment such as to work with files, \"\n                \"make curl requests, or etc. The environment is Linux. When asked to write code that saves \"\n                \"files, always prefix the file with the artifacts/ folder. For example, if asked to create \"\n                \"test.txt, in the function call you make to whatever library that creates the file, \"\n                \"you would use artifacts/test.txt. However, when users upload files, they will by default be \"\n                \"in /home/user/, so if working with a user file, unless they give you the full path, \"\n                \"look in /home/user. Always show the output of code execution explicitly and separately at \"\n                \"the end of the rest of your output. You are also able to install system and python packages \"\n                \"using your tools. However, the tools can only install one package at a time, if you need to \"\n                \"install multiple packages, call the tools multiple times. Always first display your code to \"\n                \"the user BEFORE you execute it using your tools. The user should always explicitly ask you \"\n                \"to execute code. Never execute code before showing the user the code first.\"\n            ),\n        }\n\n        agent_chain = initialize_agent(\n            tools=tools,\n            llm=llm,\n            agent=AgentType.OPENAI_FUNCTIONS,\n            verbose=True,\n            agent_kwargs=agent_kwargs,\n            memory=memory,\n            handle_parsing_errors=\"Check your output and make sure it conforms!\",\n            max_iterations=5,\n        )\n\n        self.chat_agents[thread.id] = agent_chain\n\n\nclass CodeInterpreterDownloadArtifactsView(discord.ui.View):\n    def __init__(\n        self,\n        ctx,\n        code_interpreter_cog,\n        session,\n        artifact_names,\n    ):\n        super().__init__(timeout=None)  # No timeout\n        self.code_interpreter_cog = code_interpreter_cog\n        self.ctx = ctx\n        self.session = session\n        self.artifact_names = artifact_names\n        self.add_item(\n            DownloadButton(\n                self.ctx, self.code_interpreter_cog, self.session, self.artifact_names\n            )\n        )\n\n\n# A view for a follow-up button\nclass DownloadButton(discord.ui.Button[\"CodeInterpreterDownloadArtifactsView\"]):\n    def __init__(self, ctx, code_interpreter_cog, session, artifact_names):\n        super().__init__(label=\"Download Artifacts\", style=discord.ButtonStyle.gray)\n        self.code_interpreter_cog = code_interpreter_cog\n        self.ctx = ctx\n        self.session = session\n        self.artifact_names = artifact_names\n\n    async def callback(self, interaction: discord.Interaction):\n        \"\"\"Send the followup modal\"\"\"\n        await interaction.response.send_message(\n            \"Downloading the artifacts: \"\n            + str(self.artifact_names)\n            + \". This may take a while.\",\n            ephemeral=True,\n            delete_after=120,\n        )\n        for artifact in self.artifact_names:\n            try:\n                runner = functools.partial(\n                    self.session.download_file, filepath=artifact\n                )\n\n                bytes = await asyncio.get_running_loop().run_in_executor(None, runner)\n                # Save these bytes into a tempfile\n                with tempfile.NamedTemporaryFile(delete=False) as temp:\n                    temp.write(bytes)\n                    temp.flush()\n                    temp.seek(0)\n                    await self.ctx.channel.send(\n                        file=discord.File(temp.name, filename=artifact)\n                    )\n                    os.unlink(temp.name)\n            except:\n                traceback.print_exc()\n                await self.ctx.channel.send(\n                    \"Failed to download artifact: \" + artifact, delete_after=120\n                )\n",
          "import asyncio\nimport os\nimport traceback\n\nimport discord\n\n# We don't use the converser cog here because we want to be able to redo for the last images and text prompts at the same time\nfrom sqlitedict import SqliteDict\n\nfrom services.environment_service import EnvService\nfrom services.image_service import ImageService\nfrom services.moderations_service import Moderation\nfrom services.text_service import TextService\n\nusers_to_interactions = {}\nALLOWED_GUILDS = EnvService.get_allowed_guilds()\n\nUSER_INPUT_API_KEYS = EnvService.get_user_input_api_keys()\nUSER_KEY_DB = EnvService.get_api_db()\nPRE_MODERATE = EnvService.get_premoderate()\n\n\nclass DrawDallEService(discord.Cog, name=\"DrawDallEService\"):\n    \"\"\"Cog containing a draw commands and file management for saved images\"\"\"\n\n    def __init__(\n        self, bot, usage_service, model, message_queue, deletion_queue, converser_cog\n    ):\n        super().__init__()\n        self.bot = bot\n        self.usage_service = usage_service\n        self.model = model\n        self.message_queue = message_queue\n        self.deletion_queue = deletion_queue\n        self.converser_cog = converser_cog\n        print(\"Draw service initialized\")\n        self.redo_users = {}\n\n    async def draw_command(\n        self,\n        ctx: discord.ApplicationContext,\n        prompt: str,\n        quality: str,\n        image_size: str,\n        style: str,\n        from_action=False,\n    ):\n        \"\"\"With an ApplicationContext and prompt, send a dalle image to the invoked channel. Ephemeral if from an action\"\"\"\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(\n                ctx.user.id, ctx, USER_KEY_DB\n            )\n            if not user_api_key:\n                return\n\n        await ctx.defer()\n\n        # Check the opener for bad content.\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(prompt, ctx):\n                return\n\n        user = ctx.user\n\n        if user == self.bot.user:\n            return\n\n        try:\n            asyncio.ensure_future(\n                ImageService.encapsulated_send(\n                    self,\n                    user.id,\n                    prompt,\n                    ctx,\n                    custom_api_key=user_api_key,\n                    dalle_3=True,\n                    quality=quality,\n                    image_size=image_size,\n                    style=style,\n                )\n            )\n\n        except Exception as e:\n            print(e)\n            traceback.print_exc()\n            await ctx.respond(\n                \"Something went wrong. Please try again later.\", ephemeral=from_action\n            )\n            await ctx.send_followup(e, ephemeral=from_action)\n\n    async def draw_old_command(\n        self, ctx: discord.ApplicationContext, prompt: str, from_action=False\n    ):\n        \"\"\"With an ApplicationContext and prompt, send a dalle image to the invoked channel. Ephemeral if from an action\"\"\"\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(\n                ctx.user.id, ctx, USER_KEY_DB\n            )\n            if not user_api_key:\n                return\n\n        await ctx.defer()\n\n        # Check the opener for bad content.\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(prompt, ctx):\n                return\n\n        user = ctx.user\n\n        if user == self.bot.user:\n            return\n\n        try:\n            asyncio.ensure_future(\n                ImageService.encapsulated_send(\n                    self, user.id, prompt, ctx, custom_api_key=user_api_key\n                )\n            )\n\n        except Exception as e:\n            print(e)\n            traceback.print_exc()\n            await ctx.respond(\n                \"Something went wrong. Please try again later.\", ephemeral=from_action\n            )\n            await ctx.send_followup(e, ephemeral=from_action)\n\n    async def draw_action(self, ctx, message):\n        \"\"\"decoupler to handle context actions for the draw command\"\"\"\n        await self.draw_command(\n            ctx,\n            message.content,\n            quality=\"hd\",\n            image_size=\"1024x1024\",\n            style=\"natural\",\n            from_action=True,\n        )\n\n    async def local_size_command(self, ctx: discord.ApplicationContext):\n        \"\"\"Get the folder size of the image folder\"\"\"\n        await ctx.defer()\n\n        image_path = self.model.IMAGE_SAVE_PATH\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(image_path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n\n        # Format the size to be in MB and send.\n        total_size = total_size / 1000000\n        await ctx.respond(f\"The size of the local images folder is {total_size} MB.\")\n\n    async def clear_local_command(self, ctx):\n        \"\"\"Delete all local images\"\"\"\n        await ctx.defer()\n\n        image_path = self.model.IMAGE_SAVE_PATH\n        for dirpath, dirnames, filenames in os.walk(image_path):\n            for f in filenames:\n                try:\n                    fp = os.path.join(dirpath, f)\n                    os.remove(fp)\n                except Exception as e:\n                    print(e)\n\n        await ctx.respond(\"Local images cleared.\")\n",
          "import datetime\nimport io\nimport json\nimport os\nimport sys\nimport tempfile\nimport traceback\nfrom typing import Optional, Dict, Any\n\nimport aiohttp\nimport re\nimport discord\nimport openai\nfrom bs4 import BeautifulSoup\nfrom discord.ext import pages\nfrom langchain.utilities import (\n    GoogleSearchAPIWrapper,\n)\nfrom langchain.utilities import WolframAlphaAPIWrapper\nfrom langchain.agents import (\n    Tool,\n    initialize_agent,\n    AgentType,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import (\n    ConversationSummaryBufferMemory,\n)\nfrom langchain.prompts import (\n    MessagesPlaceholder,\n)\nfrom langchain.requests import Requests\nfrom langchain.schema import SystemMessage\nfrom llama_index import (\n    GPTVectorStoreIndex,\n    Document,\n    SimpleDirectoryReader,\n    ServiceContext,\n    OpenAIEmbedding,\n)\nfrom llama_index.response_synthesizers import get_response_synthesizer, ResponseMode\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\nfrom pydantic import Extra, BaseModel\nimport tiktoken\n\nfrom models.embed_statics_model import EmbedStatics\nfrom models.search_model import Search\nfrom services.deletion_service import Deletion\nfrom services.environment_service import EnvService\nfrom services.moderations_service import Moderation\nfrom services.text_service import TextService\nfrom models.openai_model import Models\n\nfrom contextlib import redirect_stdout\n\nfrom langchain.agents.conversational_chat.output_parser import ConvoOutputParser\n\noriginal_parse = ConvoOutputParser.parse\n\n\ndef my_parse(self, text):\n    # Remove all pairs of triple backticks from the input. However, don't remove pairs of ```json and ```. Only remove ``` and ``` pairs, maintain the text between the pairs so that only the backticks\n    # are removed and the text is left intact.\n    text_without_triple_backticks = re.sub(\n        r\"```(?!json)(.*?)```\", r\"\\1\", text, flags=re.DOTALL\n    )\n\n    # Call the original parse() method with the modified input\n    try:\n        result = original_parse(self, text_without_triple_backticks)\n    except Exception:\n        traceback.print_exc()\n        # Take the text and format it like\n        # {\n        #     \"action\": \"Final Answer\",\n        #     \"action_input\": text\n        # }\n        # This will cause the bot to respond with the text as if it were a final answer.\n        if \"action_input\" not in text_without_triple_backticks:\n            text_without_triple_backticks = f'{{\"action\": \"Final Answer\", \"action_input\": {json.dumps(text_without_triple_backticks)}}}'\n            result = original_parse(self, text_without_triple_backticks)\n\n        else:\n            # Insert \"```json\" before the opening curly brace\n            text_without_triple_backticks = re.sub(\n                r\"({)\", r\"```json \\1\", text_without_triple_backticks\n            )\n\n            # Insert \"```\" after the closing curly brace\n            text_without_triple_backticks = re.sub(\n                r\"(})\", r\"\\1 ```\", text_without_triple_backticks\n            )\n\n            result = original_parse(self, text_without_triple_backticks)\n\n    return result\n\n\n# Replace the original parse function with the new one\nConvoOutputParser.parse = my_parse\n\n\nclass CaptureStdout:\n    def __enter__(self):\n        self.buffer = io.StringIO()\n        self.original_stdout = sys.stdout\n        sys.stdout = self.buffer\n        return self.buffer\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout = self.original_stdout\n\n\nasync def capture_stdout(func, *args, **kwargs):\n    with CaptureStdout() as buffer:\n        result = await func(*args, **kwargs)\n    captured_output = buffer.getvalue()\n    return result, captured_output\n\n\nALLOWED_GUILDS = EnvService.get_allowed_guilds()\nUSER_INPUT_API_KEYS = EnvService.get_user_input_api_keys()\nUSER_KEY_DB = EnvService.get_api_db()\nPRE_MODERATE = EnvService.get_premoderate()\nGOOGLE_API_KEY = EnvService.get_google_search_api_key()\nGOOGLE_SEARCH_ENGINE_ID = EnvService.get_google_search_engine_id()\nOPENAI_API_KEY = EnvService.get_openai_token()\n# Set the environment\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\nWOLFRAM_API_KEY = EnvService.get_wolfram_api_key()\n\nvector_stores = {}\n\n\nclass RedoSearchUser:\n    def __init__(self, ctx, query, search_scope, nodes, response_mode):\n        self.ctx = ctx\n        self.query = query\n        self.search_scope = search_scope\n        self.nodes = nodes\n        self.response_mode = response_mode\n\n\nclass CustomTextRequestWrapper(BaseModel):\n    \"\"\"Lightweight wrapper around requests library.\n\n    The main purpose of this wrapper is to always return a text output.\n    \"\"\"\n\n    headers: Optional[Dict[str, str]] = None\n    aiosession: Optional[aiohttp.ClientSession] = None\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    def __init__(self, **data: Any):\n        super().__init__(**data)\n\n    @property\n    def requests(self) -> Requests:\n        return Requests(headers=self.headers, aiosession=self.aiosession)\n\n    def get(self, url: str, **kwargs: Any) -> str:\n        # the \"url\" field is actuall some input from the LLM, it is a comma separated string of the url and a boolean value and the original query\n        try:\n            url, model, original_query = url.split(\",\")\n            url = url.strip()\n            model = model.strip()\n            original_query = original_query.strip()\n        except:\n            url = url\n            model = \"gpt-3.5-turbo\"\n            original_query = \"No Original Query Provided\"\n\n        \"\"\"GET the URL and return the text.\"\"\"\n        if not url.startswith(\"http\"):\n            return (\n                \"The website could not be crawled as an invalid URL was input. The input URL was \"\n                + url\n            )\n        text = self.requests.get(url, **kwargs).text\n\n        # Load this text into BeautifulSoup, clean it up and only retain text content within <p> and <title> and <h1> type tags, get rid of all javascript and css too.\n        soup = BeautifulSoup(text, \"html.parser\")\n\n        # Decompose script, style, head, and meta tags\n        for tag in soup([\"script\", \"style\", \"head\", \"meta\"]):\n            tag.decompose()\n\n        # Get remaining text from the soup object\n        text = soup.get_text()\n\n        # Clean up white spaces\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n\n        # If not using GPT-4 and the text token amount is over 3500, truncate it to 3500 tokens\n        enc = tiktoken.encoding_for_model(model)\n        tokens = len(enc.encode(text))\n        if len(text) < 5:\n            return \"This website could not be scraped. I cannot answer this question.\"\n        if (\n            model in Models.CHATGPT_MODELS\n            and tokens > Models.get_max_tokens(model) - 1000\n        ) or (\n            model in Models.GPT4_MODELS and tokens > Models.get_max_tokens(model) - 1000\n        ):\n            with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n                f.write(text)\n                f.close()\n                document = SimpleDirectoryReader(input_files=[f.name]).load_data()\n                embed_model = OpenAIEmbedding()\n                service_context = ServiceContext.from_defaults(embed_model=embed_model)\n                index = GPTVectorStoreIndex.from_documents(\n                    document, service_context=service_context, use_async=True\n                )\n                retriever = VectorIndexRetriever(\n                    index=index, similarity_top_k=4, service_context=service_context\n                )\n                response_synthesizer = get_response_synthesizer(\n                    response_mode=ResponseMode.COMPACT,\n                    refine_template=CHAT_REFINE_PROMPT,\n                    service_context=service_context,\n                    use_async=True,\n                )\n                query_engine = RetrieverQueryEngine(\n                    retriever=retriever, response_synthesizer=response_synthesizer\n                )\n                response_text = query_engine.query(original_query)\n                return response_text\n\n        return text\n\n\nclass SearchService(discord.Cog, name=\"SearchService\"):\n    \"\"\"Cog containing translation commands and retrieval of translation services\"\"\"\n\n    def __init__(\n        self,\n        bot,\n        gpt_model,\n        usage_service,\n        deletion_service,\n        converser_cog,\n    ):\n        super().__init__()\n        self.bot = bot\n        self.usage_service = usage_service\n        self.model = Search(gpt_model, usage_service)\n        self.EMBED_CUTOFF = 2000\n        self.redo_users = {}\n        self.chat_agents = {}\n        self.thread_awaiting_responses = []\n        self.converser_cog = converser_cog\n        # Make a mapping of all the country codes and their full country names:\n\n    async def paginate_embed(\n        self, response_text, user: discord.Member, original_link=None\n    ):\n        \"\"\"Given a response text make embed pages and return a list of the pages.\"\"\"\n\n        response_text = [\n            response_text[i : i + self.EMBED_CUTOFF]\n            for i in range(0, len(response_text), self.EMBED_CUTOFF)\n        ]\n        pages = []\n        first = False\n        # Send each chunk as a message\n        for count, chunk in enumerate(response_text, start=1):\n            if not first:\n                page = discord.Embed(\n                    title=\"Search Results\"\n                    if not original_link\n                    else \"Follow-up results\",\n                    description=chunk,\n                    url=original_link,\n                )\n                first = True\n            else:\n                page = discord.Embed(\n                    title=f\"Page {count}\",\n                    description=chunk,\n                    url=original_link,\n                )\n            if user.avatar:\n                page.set_footer(\n                    text=f\"Requested by {user.name}\", icon_url=user.avatar.url\n                )\n            else:\n                page.set_footer(\n                    text=f\"Requested by {user.name}\", icon_url=user.default_avatar.url\n                )\n            pages.append(page)\n\n        return pages\n\n    @discord.Cog.listener()\n    async def on_message(self, message):\n        # Check if the message is from a bot.\n        if message.author.id == self.bot.user.id:\n            return\n\n        # Check if the message is from a guild.\n        if not message.guild:\n            return\n\n        # System message\n        if message.type != discord.MessageType.default:\n            return\n\n        if message.content.strip().startswith(\"~\"):\n            return\n\n        # if we are still awaiting a response from the agent, then we don't want to process the message.\n        if message.channel.id in self.thread_awaiting_responses:\n            resp_message = await message.reply(\n                \"Please wait for the agent to respond to a previous message first!\"\n            )\n            deletion_time = datetime.datetime.now() + datetime.timedelta(seconds=5)\n            deletion_time = deletion_time.timestamp()\n\n            original_deletion_message = Deletion(message, deletion_time)\n            deletion_message = Deletion(resp_message, deletion_time)\n            await self.converser_cog.deletion_queue.put(deletion_message)\n            await self.converser_cog.deletion_queue.put(original_deletion_message)\n            return\n\n        # Pre moderation\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(message.content, message):\n                await message.delete()\n                return\n\n        prompt = message.content.strip()\n\n        # If the message channel is in self.chat_agents, then we delegate the message to the agent.\n        if message.channel.id in self.chat_agents:\n            if prompt.lower() in [\"stop\", \"end\", \"quit\", \"exit\"]:\n                await message.reply(\"Ending chat session.\")\n                self.chat_agents.pop(message.channel.id)\n\n                # close the thread\n                thread = await self.bot.fetch_channel(message.channel.id)\n                await thread.edit(name=\"Closed-GPT\")\n                await thread.edit(archived=True)\n                return\n\n            self.thread_awaiting_responses.append(message.channel.id)\n\n            try:\n                await message.channel.trigger_typing()\n            except:\n                pass\n\n            agent = self.chat_agents[message.channel.id]\n            used_tools = []\n            try:\n                # Start listening to STDOUT before this call. We wanna track all the output for this specific call below\n                response, stdout_output = await capture_stdout(\n                    self.bot.loop.run_in_executor, None, agent.run, prompt\n                )\n                response = str(response)\n\n                try:\n                    print(stdout_output)\n                except:\n                    traceback.print_exc()\n                    stdout_output = \"\"\n\n                if \"Wolfram-Tool\" in stdout_output:\n                    used_tools.append(\"Wolfram Alpha\")\n                if \"Search-Tool\" in stdout_output:\n                    used_tools.append(\"Google Search\")\n                if \"Web-Crawling-Tool\" in stdout_output:\n                    used_tools.append(\"Web Crawler\")\n\n            except Exception as e:\n                response = f\"Error: {e}\"\n                traceback.print_exc()\n                await message.reply(\n                    embed=EmbedStatics.get_internet_chat_failure_embed(response)\n                )\n                self.thread_awaiting_responses.remove(message.channel.id)\n                return\n\n            if len(response) > 2000:\n                embed_pages = EmbedStatics.paginate_chat_embed(response)\n\n                for x, page in enumerate(embed_pages):\n                    if x == 0:\n                        previous_message = await message.reply(embed=page)\n                    else:\n                        previous_message = previous_message.reply(embed=page)\n\n            else:\n                response = response.replace(\"\\\\n\", \"\\n\")\n                # Build a response embed\n                response_embed = discord.Embed(\n                    title=\"\",\n                    description=response,\n                    color=0x808080,\n                )\n                if len(used_tools) > 0:\n                    response_embed.set_footer(\n                        text=\"Used tools: \" + \", \".join(used_tools)\n                    )\n                await message.reply(embed=response_embed)\n\n            self.thread_awaiting_responses.remove(message.channel.id)\n\n    async def search_chat_command(\n        self, ctx: discord.ApplicationContext, model, search_scope=2\n    ):\n        embed_title = f\"{ctx.user.name}'s internet-connected conversation with GPT\"\n        message_embed = discord.Embed(\n            title=embed_title,\n            description=f\"The agent will visit and browse **{search_scope}** link(s) every time it needs to access the internet.\\nCrawling is enabled, send the bot a link for it to access it!\\nModel: {model}\\n\\nType `end` to stop the conversation\",\n            color=0xBA6093,\n        )\n        message_embed.set_thumbnail(url=\"https://i.imgur.com/sioynYZ.png\")\n        message_embed.set_footer(\n            text=\"Internet Chat\", icon_url=\"https://i.imgur.com/sioynYZ.png\"\n        )\n        message_thread = await ctx.send(embed=message_embed)\n        thread = await message_thread.create_thread(\n            name=ctx.user.name + \"'s internet-connected conversation with GPT\",\n            auto_archive_duration=60,\n        )\n        await ctx.respond(\"Conversation started.\")\n\n        # Make a new agent for this user to chat.\n        search = GoogleSearchAPIWrapper(\n            google_api_key=GOOGLE_API_KEY,\n            google_cse_id=GOOGLE_SEARCH_ENGINE_ID,\n            k=search_scope,\n        )\n\n        requests = CustomTextRequestWrapper()\n\n        tools = [\n            Tool(\n                name=\"Search-Tool\",\n                func=search.run,\n                description=\"useful when you need to answer questions about current events or retrieve information about a topic that may require the internet. The input to this tool is a search query to ask google. Search queries should be less than 8 words. For example, an input could be 'What is the weather like in New York?' and the tool input would be 'weather new york'.\",\n            ),\n            # The requests tool\n            Tool(\n                name=\"Web-Crawling-Tool\",\n                func=requests.get,\n                description=f\"Useful for when the user provides you with a website link, use this tool to crawl the website and retrieve information from it. The input to this tool is a comma separated list of three values, the first value is the link to crawl for, and the second value is {model} and is the GPT model used, and the third value is the original question that the user asked. For example, an input could be 'https://google.com', gpt-4-32k, 'What is this webpage?'. This tool should only be used if a direct link is provided and not in conjunction with other tools. The link should always start with http or https.\",\n            ),\n        ]\n\n        # Try to add wolfram tool\n        try:\n            wolfram = WolframAlphaAPIWrapper(wolfram_alpha_appid=WOLFRAM_API_KEY)\n            tools.append(\n                Tool(\n                    name=\"Wolfram-Tool\",\n                    func=wolfram.run,\n                    description=\"useful when you need to answer questions about math, solve equations, do proofs, mathematical science questions, science questions, and when asked to do numerical based reasoning.\",\n                )\n            )\n            print(\"Wolfram tool added to internet-connected conversation agent.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Wolfram tool not added to internet-connected conversation agent.\")\n\n        llm = ChatOpenAI(model=model, temperature=0, openai_api_key=OPENAI_API_KEY)\n\n        max_token_limit = 29000 if \"gpt-4\" in model else 7500\n\n        memory = ConversationSummaryBufferMemory(\n            memory_key=\"memory\",\n            return_messages=True,\n            llm=llm,\n            max_token_limit=100000 if \"preview\" in model else max_token_limit,\n        )\n\n        agent_kwargs = {\n            \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"memory\")],\n            \"system_message\": SystemMessage(\n                content=\"You are a superpowered version of GPT-4 that is able to access the internet. You can use google search to browse the web, you can crawl the web to see the content of specific websites, and in some cases you can also use Wolfram Alpha to perform mathematical operations. Use all of these tools to your advantage. You can use tools multiple times, for example if asked a complex question, search multiple times for different pieces of info until you achieve your goal.\"\n            ),\n        }\n\n        agent_chain = initialize_agent(\n            tools=tools,\n            llm=llm,\n            agent=AgentType.OPENAI_FUNCTIONS,\n            verbose=True,\n            agent_kwargs=agent_kwargs,\n            memory=memory,\n            handle_parsing_errors=\"Check your output and make sure it conforms!\",\n            max_iterations=5,\n        )\n\n        self.chat_agents[thread.id] = agent_chain\n\n    async def search_command(\n        self,\n        ctx: discord.ApplicationContext,\n        query,\n        search_scope,\n        nodes,\n        deep,\n        response_mode,\n        model,\n        multistep=False,\n        redo=None,\n        from_followup=None,\n        followup_user=None,\n    ):\n        \"\"\"Command handler for the search command\"\"\"\n        await ctx.defer() if not redo else None\n\n        # Check the opener for bad content.\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(query, ctx):\n                return\n\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(\n                ctx.user.id, ctx, USER_KEY_DB\n            )\n            if not user_api_key:\n                return\n\n        if (\n            not EnvService.get_google_search_api_key()\n            or not EnvService.get_google_search_engine_id()\n        ):\n            await ctx.respond(\n                embed=EmbedStatics.get_search_failure_embed(\n                    str(\"The search service is not enabled on this server.\")\n                ),\n            )\n            return\n\n        try:\n            response, refined_text = await self.model.search(\n                ctx,\n                query,\n                user_api_key,\n                search_scope,\n                nodes,\n                deep,\n                response_mode,\n                model,\n                multistep,\n            )\n        except ValueError as e:\n            traceback.print_exc()\n            await ctx.respond(\n                embed=EmbedStatics.get_search_failure_embed(str(e)),\n                ephemeral=True,\n            )\n            return\n        except Exception as e:\n            await ctx.respond(\n                embed=EmbedStatics.get_search_failure_embed(str(e)), ephemeral=True\n            )\n            traceback.print_exc()\n            return\n\n        url_extract_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n        urls = re.findall(\n            url_extract_pattern,\n            str(response.get_formatted_sources(length=200)),\n            flags=re.IGNORECASE,\n        )\n        urls = \"\\n\".join(f\"<{url}>\" for url in urls)\n\n        # Deduplicate the urls\n        urls = \"\\n\".join(dict.fromkeys(urls.split(\"\\n\")))\n\n        if from_followup:\n            original_link, followup_question = (\n                from_followup.original_link,\n                from_followup.followup_question,\n            )\n            query_response_message = f\"**Question:**\\n\\n`{followup_question}`\\n\\n**Google Search Query**\\n\\n`{refined_text.strip()}`\\n\\n**Final Answer:**\\n\\n{response.response.strip()}\\n\\n**Sources:**\\n{urls}\"\n        else:\n            query_response_message = f\"**Question:**\\n\\n`{query.strip()}`\\n\\n**Google Search Query**\\n\\n`{refined_text.strip()}`\\n\\n**Final Answer:**\\n\\n{response.response.strip()}\\n\\n**Sources:**\\n{urls}\"\n        query_response_message = query_response_message.replace(\n            \"<|endofstatement|>\", \"\"\n        )\n        query_response_message = query_response_message.replace(\n            \"Answer to original:\\n\", \"\"\n        )\n        query_response_message = query_response_message.replace(\n            \"Answer to follow-up:\\n\", \"\"\n        )\n\n        # If the response is too long, lets paginate using the discord pagination\n        # helper\n        embed_pages = await self.paginate_embed(\n            query_response_message,\n            ctx.user if not followup_user else followup_user,\n            original_link if from_followup else None,\n        )\n        paginator = pages.Paginator(\n            pages=embed_pages,\n            timeout=None,\n            author_check=False,\n            custom_view=SearchView(ctx, self, query_response_message),\n        )\n\n        self.redo_users[ctx.user.id] = RedoSearchUser(\n            ctx, query, search_scope, nodes, response_mode\n        )\n\n        await paginator.respond(ctx.interaction)\n\n\nclass SearchView(discord.ui.View):\n    def __init__(\n        self,\n        ctx,\n        search_cog,\n        response_text,\n    ):\n        super().__init__(timeout=None)  # No timeout\n        self.search_cog = search_cog\n        self.ctx = ctx\n        self.response_text = response_text\n        self.add_item(RedoButton(self.ctx, self.search_cog))\n        self.add_item(FollowupButton(self.ctx, self.search_cog, self.response_text))\n\n\n# A view for a follow-up button\nclass FollowupButton(discord.ui.Button[\"SearchView\"]):\n    def __init__(self, ctx, search_cog, response_text):\n        super().__init__(label=\"Follow Up\", style=discord.ButtonStyle.green)\n        self.search_cog = search_cog\n        self.ctx = ctx\n        self.response_text = response_text\n\n    async def callback(self, interaction: discord.Interaction):\n        \"\"\"Send the followup modal\"\"\"\n        await interaction.response.send_modal(\n            modal=FollowupModal(self.ctx, self.search_cog, self.response_text)\n        )\n\n\n# A view for a redo button\nclass RedoButton(discord.ui.Button[\"SearchView\"]):\n    def __init__(self, ctx, search_cog):\n        super().__init__(\n            style=discord.ButtonStyle.danger,\n            label=\"Redo\",\n            custom_id=\"redo_search_button\",\n        )\n        self.ctx = ctx\n        self.search_cog = search_cog\n\n    async def callback(self, interaction: discord.Interaction):\n        \"\"\"Redo the search\"\"\"\n        await interaction.response.send_message(\n            embed=EmbedStatics.get_search_redo_progress_embed(),\n            ephemeral=True,\n            delete_after=15,\n        )\n        await self.search_cog.search_command(\n            self.search_cog.redo_users[self.ctx.user.id].ctx,\n            self.search_cog.redo_users[self.ctx.user.id].query,\n            self.search_cog.redo_users[self.ctx.user.id].search_scope,\n            self.search_cog.redo_users[self.ctx.user.id].nodes,\n            deep=False,\n            redo=True,\n            response_mode=self.search_cog.redo_users[self.ctx.user.id].response_mode,\n        )\n\n\nclass FollowupData:\n    def __init__(self, original_link, followup_question):\n        self.original_link = original_link\n        self.followup_question = followup_question\n\n\n# The modal for following up\nclass FollowupModal(discord.ui.Modal):\n    def __init__(self, ctx, search_cog, response_text) -> None:\n        super().__init__(title=\"Search Follow-up\")\n        # Get the argument named \"user_key_db\" and save it as USER_KEY_DB\n        self.search_cog = search_cog\n        self.ctx = ctx\n        self.response_text = response_text\n\n        self.add_item(\n            discord.ui.InputText(\n                label=\"What other questions do you have?\",\n                placeholder=\"\",\n            )\n        )\n\n    async def callback(self, interaction: discord.Interaction):\n        await interaction.response.defer()\n        query = self.search_cog.redo_users[self.ctx.user.id].query\n\n        # In the response text, get only the text between \"**Final Answer:**\" and \"**Sources:**\"\n        self.response_text = self.response_text.split(\"**Final Answer:**\")[1].split(\n            \"**Sources:**\"\n        )[0]\n\n        # Build the context\n        context_text = (\n            \"Original question: \"\n            + query\n            + \"\\n\"\n            + \"Answer to original: \"\n            + self.response_text\n            + \"\\n\"\n            + \"Follow-up question: \"\n            + self.children[0].value\n        )\n\n        # Get the link of the message that the user interacted on\n        message_link = f\"https://discord.com/channels/{interaction.guild_id}/{interaction.channel_id}/{interaction.message.id}\"\n\n        await self.search_cog.search_command(\n            self.search_cog.redo_users[self.ctx.user.id].ctx,\n            context_text,\n            self.search_cog.redo_users[self.ctx.user.id].search_scope,\n            self.search_cog.redo_users[self.ctx.user.id].nodes,\n            deep=False,\n            redo=True,\n            from_followup=FollowupData(message_link, self.children[0].value),\n            response_mode=self.search_cog.redo_users[self.ctx.user.id].response_mode,\n            followup_user=interaction.user,\n            model=\"gpt-4-32k\",\n        )\n",
          "import asyncio\nimport datetime\nimport pickle\nimport re\nimport traceback\nimport sys\nfrom pathlib import Path\n\n\nimport aiofiles\nimport json\n\nimport discord\nfrom discord.ext import pages\n\nfrom models.deepl_model import TranslationModel\nfrom models.embed_statics_model import EmbedStatics\nfrom models.image_understanding_model import ImageUnderstandingModel\nfrom models.openai_model import Override\nfrom services.environment_service import EnvService\nfrom services.message_queue_service import Message\nfrom services.moderations_service import Moderation\nfrom models.user_model import Thread, EmbeddedConversationItem, Instruction\nfrom collections import defaultdict\nfrom sqlitedict import SqliteDict\n\nfrom services.pickle_service import Pickler\nfrom services.sharegpt_service import ShareGPTService\nfrom services.text_service import SetupModal, TextService\n\noriginal_message = {}\nALLOWED_GUILDS = EnvService.get_allowed_guilds()\nif sys.platform == \"win32\":\n    separator = \"\\\\\"\nelse:\n    separator = \"/\"\n\n#\n# Get the user key service if it is enabled.\n#\nUSER_INPUT_API_KEYS = EnvService.get_user_input_api_keys()\nUSER_KEY_DB = EnvService.get_api_db()\nCHAT_BYPASS_ROLES = EnvService.get_bypass_roles()\nPRE_MODERATE = EnvService.get_premoderate()\nFORCE_ENGLISH = EnvService.get_force_english()\nBOT_TAGGABLE = EnvService.get_bot_is_taggable()\nCHANNEL_CHAT_ROLES = EnvService.get_channel_chat_roles()\nBOT_TAGGABLE_ROLES = EnvService.get_gpt_roles()\nCHANNEL_INSTRUCTION_ROLES = EnvService.get_channel_instruction_roles()\nimage_understanding_model = ImageUnderstandingModel()\n\n#\n# Obtain the Moderation table and the General table, these are two SQLite tables that contain\n# information about the server that are used for persistence and to auto-restart the moderation service.\n#\nMOD_DB = None\nGENERAL_DB = None\ntry:\n    print(\"Attempting to retrieve the General and Moderations DB\")\n    MOD_DB = SqliteDict(\n        EnvService.find_shared_file(\"main_db.sqlite\"),\n        tablename=\"moderations\",\n        autocommit=True,\n    )\n    GENERAL_DB = SqliteDict(\n        EnvService.find_shared_file(\"main_db.sqlite\"),\n        tablename=\"general\",\n        autocommit=True,\n    )\n    print(\"Retrieved the General and Moderations DB\")\nexcept Exception as e:\n    print(\"Failed to retrieve the General and Moderations DB. The bot is terminating.\")\n    raise e\n\nBOT_NAME = EnvService.get_custom_bot_name()\n\n\nclass GPT3ComCon(discord.Cog, name=\"GPT3ComCon\"):\n    def __init__(\n        self,\n        bot,\n        usage_service,\n        model,\n        message_queue,\n        deletion_queue,\n        DEBUG_GUILD,\n        DEBUG_CHANNEL,\n        data_path: Path,\n        pinecone_service,\n        pickle_queue,\n    ):\n        super().__init__()\n        self.GLOBAL_COOLDOWN_TIME = 0.25\n\n        # Environment\n        self.data_path = data_path\n        self.debug_channel = None\n\n        # Services and models\n        self.bot = bot\n        self.usage_service = usage_service\n        self.model = model\n        self.translation_model = TranslationModel()\n        self.deletion_queue = deletion_queue\n\n        # Data specific to all text based GPT interactions\n        self.users_to_interactions = defaultdict(list)\n        self.redo_users = {}\n\n        # Pickle queue\n        self.pickle_queue = pickle_queue\n\n        # Conversations-specific data\n        self.END_PROMPTS = [\n            \"end\",\n            \"end conversation\",\n            \"end the conversation\",\n            \"that's all\",\n            \"that'll be all\",\n        ]\n        self.awaiting_responses = []\n        self.awaiting_thread_responses = []\n        self.conversation_threads = {}\n        self.full_conversation_history = defaultdict(list)\n        self.instructions = defaultdict(list)\n        self.summarize = self.model.summarize_conversations\n\n        # Pinecone data\n        self.pinecone_service = pinecone_service\n\n        # Sharing service\n        self.sharegpt_service = ShareGPTService()\n\n        try:\n            conversation_file_path = EnvService.find_shared_file(\n                \"conversation_starter_pretext.txt\"\n            )\n            # Attempt to read a conversation starter text string from the file.\n            with conversation_file_path.open(\"r\") as f:\n                self.CONVERSATION_STARTER_TEXT = f.read()\n                print(\n                    f\"Conversation starter text loaded from {conversation_file_path}.\"\n                )\n            assert self.CONVERSATION_STARTER_TEXT is not None\n\n            language_detect_file_path = EnvService.find_shared_file(\n                \"language_detection_pretext.txt\"\n            )\n            # Attempt to read a conversation starter text string from the file.\n            with language_detect_file_path.open(\"r\") as f:\n                self.LANGUAGE_DETECT_STARTER_TEXT = f.read()\n                print(\n                    f\"Language detection starter text loaded from {language_detect_file_path}.\"\n                )\n            assert self.LANGUAGE_DETECT_STARTER_TEXT is not None\n\n            conversation_file_path_minimal = EnvService.find_shared_file(\n                \"conversation_starter_pretext_minimal.txt\"\n            )\n            with conversation_file_path_minimal.open(\"r\") as f:\n                self.CONVERSATION_STARTER_TEXT_MINIMAL = f.read()\n                print(\n                    f\"Conversation starter text loaded from {conversation_file_path_minimal}.\"\n                )\n            assert self.CONVERSATION_STARTER_TEXT_MINIMAL is not None\n\n        except Exception:\n            self.CONVERSATION_STARTER_TEXT = self.CONVERSATION_STARTER_TEXT_MINIMAL = (\n                \"You are an artificial intelligence that is able to do anything, and answer any question,\"\n                \"I want you to be my personal assistant and help me with some tasks. \"\n                \"and I want you to make well-informed decisions using the data that you have been trained on, \"\n                \"and be sure to be mindful of the previous conversation history and be consistent with your answers.\"\n            )\n\n        self.DEBUG_GUILD = DEBUG_GUILD\n        self.DEBUG_CHANNEL = DEBUG_CHANNEL\n        print(\n            f\"The debug channel and guild IDs are {self.DEBUG_GUILD} and {self.DEBUG_CHANNEL}\"\n        )\n        self.TEXT_CUTOFF = 1900\n        self.EMBED_CUTOFF = 3900\n        self.message_queue = message_queue\n        self.conversation_thread_owners = defaultdict(list)\n\n    async def load_file(self, file, ctx):\n        \"\"\"Take filepath, return content or respond if not found\"\"\"\n        try:\n            async with aiofiles.open(file, \"r\") as f:\n                return await f.read()\n        except Exception as e:\n            traceback.print_exc()\n            await ctx.respond(\n                \"Error loading file. Please check that it is correctly placed in the bot's root file directory.\"\n            )\n            raise e\n\n    @discord.Cog.listener()\n    async def on_member_join(self, member):\n        \"\"\"When members join send welcome message if enabled\"\"\"\n        if self.model.welcome_message_enabled:\n            query = f\"Please generate a welcome message for {member.name} who has just joined the server.\"\n\n            try:\n                welcome_message_response = await self.model.send_request(\n                    query,\n                    tokens=self.usage_service.count_tokens(query),\n                    is_chatgpt_request=True\n                    if \"turbo\" in str(self.model.model)\n                    else False,\n                )\n                welcome_message = str(welcome_message_response[\"choices\"][0][\"text\"])\n            except Exception:\n                welcome_message = None\n\n            if not welcome_message:\n                welcome_message = EnvService.get_welcome_message()\n            welcome_embed = discord.Embed(\n                title=f\"Welcome, {member.name}!\", description=welcome_message\n            )\n\n            welcome_embed.add_field(\n                name=\"Just so you know...\",\n                value=\"> My commands are invoked with a forward slash (/)\\n> Use /help to see my help message(s).\",\n            )\n            await member.send(content=None, embed=welcome_embed)\n\n    @discord.Cog.listener()\n    async def on_ready(self):\n        \"\"\"When ready to recieve data set debug channel and sync commands\"\"\"\n        self.debug_channel = self.bot.get_guild(self.DEBUG_GUILD).get_channel(\n            self.DEBUG_CHANNEL\n        )\n        print(\"The debug channel was acquired\")\n\n        print(\"Attempting to load from pickles\")\n        # Try to load self.full_conversation_history, self.conversation_threads, and self.conversation_thread_owners from the `pickles` folder\n        try:\n            with open(\n                EnvService.save_path() / \"pickles\" / \"full_conversation_history.pickle\",\n                \"rb\",\n            ) as f:\n                self.full_conversation_history = pickle.load(f)\n                print(\"Loaded full_conversation_history\")\n\n            with open(\n                EnvService.save_path() / \"pickles\" / \"conversation_threads.pickle\", \"rb\"\n            ) as f:\n                self.conversation_threads = pickle.load(f)\n                print(\"Loaded conversation_threads\")\n\n            with open(\n                EnvService.save_path()\n                / \"pickles\"\n                / \"conversation_thread_owners.pickle\",\n                \"rb\",\n            ) as f:\n                self.conversation_thread_owners = pickle.load(f)\n                print(\"Loaded conversation_thread_owners\")\n\n            with open(\n                EnvService.save_path() / \"pickles\" / \"instructions.pickle\",\n                \"rb\",\n            ) as f:\n                self.instructions = pickle.load(f)\n                print(\"Loaded instructions\")\n\n            # Fail if all three weren't loaded\n            assert self.full_conversation_history is not {}\n            assert self.conversation_threads is not {}\n            assert self.conversation_thread_owners is not defaultdict(list)\n\n        except Exception:\n            print(\"Failed to load existing pickles\")\n            self.full_conversation_history = defaultdict(list)\n            self.conversation_threads = {}\n            self.conversation_thread_owners = defaultdict(list)\n            print(\"Set empty dictionaries, pickles will be saved in the future\")\n\n        print(\"Syncing commands...\")\n\n        try:\n            await self.bot.sync_commands(\n                commands=None,\n                method=\"individual\",\n                force=True,\n                guild_ids=ALLOWED_GUILDS,\n                register_guild_commands=True,\n                check_guilds=[],\n                delete_existing=True,\n            )\n        except:\n            traceback.print_exc()\n            print(\n                \"There was a failure during command syncing. This might mean that the bot may not be in a guild that it expects to be in anymore, or it is in guilds without the guild ID being added to ALLOWED_GUILDS.\"\n            )\n        print(\"Commands synced\")\n\n        # Start an inline async loop that runs every 10 seconds to save the conversation history to a pickle file\n        print(\"Starting pickle loop\")\n        while True:\n            await asyncio.sleep(15)\n            await self.pickle_queue.put(\n                Pickler(\n                    self.full_conversation_history,\n                    self.conversation_threads,\n                    self.conversation_thread_owners,\n                    self.instructions,\n                )\n            )\n\n    def check_conversing(self, channel_id, message_content):\n        '''given channel id and a message, return true if it's a conversation thread, false if not, or if the message starts with \"~\"'''\n        cond1 = channel_id in self.conversation_threads\n        # If the trimmed message starts with a Tilde, then we want to not contribute this to the conversation\n        try:\n            cond2 = not message_content.strip().startswith(\"~\")\n        except Exception as e:\n            print(e)\n            cond2 = False\n\n        return (cond1) and cond2\n\n    async def end_conversation(\n        self, ctx, opener_user_id=None, conversation_limit=False\n    ):\n        \"\"\"end the thread of the user interacting with the bot, if the conversation has reached the limit close it for the owner\"\"\"\n        normalized_user_id = opener_user_id if opener_user_id else ctx.author.id\n        # Check if the channel is an instance of a thread\n        thread = False\n        if isinstance(ctx.channel, discord.Thread):\n            thread = True\n\n        if (\n            conversation_limit\n        ):  # if we reach the conversation limit we want to close from the channel it was maxed out in\n            channel_id = ctx.channel.id\n        else:\n            try:\n                channel_ids = self.conversation_thread_owners[normalized_user_id]\n                if ctx.channel.id not in channel_ids:\n                    await ctx.reply(\n                        \"This is not a conversation thread that you own!\",\n                        delete_after=5,\n                    )\n                    return\n\n                if normalized_user_id in self.awaiting_responses:\n                    await ctx.reply(\n                        embed=discord.Embed(\n                            title=f\"Please wait for a response before ending the conversation.\",\n                            color=0x808080,\n                        )\n                    )\n                    return\n\n            except Exception:\n                traceback.print_exc()\n                await ctx.delete(delay=5)\n                await ctx.reply(\n                    \"Only the conversation starter can end this.\", delete_after=5\n                )\n                return\n\n        # TODO Possible bug here, if both users have a conversation active and one user tries to end the other, it may\n        # allow them to click the end button on the other person's thread and it will end their own convo.\n        self.conversation_threads.pop(ctx.channel.id)\n\n        if isinstance(\n            ctx, discord.ApplicationContext\n        ):  # When the conversation is ended from the slash command\n            await ctx.respond(\n                \"You have ended the conversation with GPT. Start a conversation with /gpt converse\",\n                ephemeral=True,\n                delete_after=10,\n            )\n        elif isinstance(\n            ctx, discord.Interaction\n        ):  # When the user ends the conversation from the button\n            await ctx.response.send_message(\n                \"You have ended the conversation with GPT. Start a conversation with /gpt converse\",\n                ephemeral=True,\n                delete_after=10,\n            )\n        else:  # The case for when the user types \"end\" in the channel\n            await ctx.reply(\n                \"You have ended the conversation with GPT. Start a conversation with /gpt converse\",\n                delete_after=10,\n            )\n\n        await ctx.channel.send(\n            embed=EmbedStatics.generate_end_embed(),\n            view=ShareView(self, ctx.channel.id) if thread else None,\n        )\n\n        # Close all conversation threads for the user\n        # If at conversation limit then fetch the owner and close the thread for them\n        if conversation_limit:\n            try:\n                owner_id = [\n                    owner\n                    for owner, threads in self.conversation_thread_owners.items()\n                    if channel_id in threads\n                ][0]\n                self.conversation_thread_owners[owner_id].remove(ctx.channel.id)\n                # Attempt to close and lock the thread.\n                if thread:\n                    try:\n                        thread = await self.bot.fetch_channel(channel_id)\n                        await thread.edit(name=\"Closed-GPT\")\n                        await thread.edit(archived=True)\n                    except Exception:\n                        traceback.print_exc()\n            except Exception:\n                traceback.print_exc()\n        else:\n            if normalized_user_id in self.conversation_thread_owners:\n                thread_id = ctx.channel.id\n                self.conversation_thread_owners[normalized_user_id].remove(\n                    ctx.channel.id\n                )\n\n                # Attempt to close and lock the thread.\n                if thread:\n                    try:\n                        thread = await self.bot.fetch_channel(thread_id)\n                        await thread.edit(name=\"Closed-GPT\")\n                        await thread.edit(archived=True)\n                    except Exception:\n                        traceback.print_exc()\n\n    async def send_settings_text(self, ctx):\n        \"\"\"compose and return the settings menu to the interacting user\"\"\"\n        embed = discord.Embed(\n            title=\"GPT3Bot Settings\",\n            description=\"The current settings of the model\",\n            color=0x00FF00,\n        )\n        # Create a two-column embed to display the settings, use \\u200b to create a blank space\n        embed.add_field(\n            name=\"Setting\",\n            value=\"\\n\".join(\n                [\n                    key\n                    for key in self.model.__dict__.keys()\n                    if key not in self.model._hidden_attributes\n                ]\n            ),\n            inline=True,\n        )\n        embed.add_field(\n            name=\"Value\",\n            value=\"\\n\".join(\n                [\n                    str(value)\n                    for key, value in self.model.__dict__.items()\n                    if key not in self.model._hidden_attributes\n                ]\n            ),\n            inline=True,\n        )\n        await ctx.respond(embed=embed, ephemeral=True)\n\n    async def process_settings(self, ctx, parameter, value):\n        \"\"\"Given a parameter and value set the corresponding parameter in storage to the value\"\"\"\n\n        # Check if the parameter is a valid parameter\n        if hasattr(self.model, parameter):\n            # Check if the value is a valid value\n            try:\n                # Set the parameter to the value\n                setattr(self.model, parameter, value)\n                await ctx.respond(\n                    \"Successfully set the parameter \" + parameter + \" to \" + value\n                )\n\n                if parameter == \"mode\":\n                    await ctx.send_followup(\n                        \"The mode has been set to \"\n                        + value\n                        + \". This has changed the temperature top_p to the mode defaults of \"\n                        + str(self.model.temp)\n                        + \" and \"\n                        + str(self.model.top_p)\n                    )\n            except ValueError as e:\n                await ctx.respond(e)\n        else:\n            await ctx.respond(\"The parameter is not a valid parameter\")\n\n    def generate_debug_message(self, prompt, response):\n        \"\"\"create a debug message with a prompt and a response field\"\"\"\n        debug_message = \"----------------------------------------------------------------------------------\\n\"\n        debug_message += \"Prompt:\\n```\\n\" + prompt + \"\\n```\\n\"\n        debug_message += \"Response:\\n```\\n\" + json.dumps(response, indent=4) + \"\\n```\\n\"\n        return debug_message\n\n    async def paginate_and_send(self, response_text, ctx):\n        \"\"\"paginate a response to a text cutoff length and send it in chunks\"\"\"\n        from_context = isinstance(ctx, discord.ApplicationContext)\n\n        response_text = [\n            response_text[i : i + self.TEXT_CUTOFF]\n            for i in range(0, len(response_text), self.TEXT_CUTOFF)\n        ]\n        # Send each chunk as a message\n        first = False\n        for chunk in response_text:\n            if not first:\n                if from_context:\n                    await ctx.send_followup(chunk)\n                else:\n                    await ctx.reply(chunk)\n                first = True\n            else:\n                if from_context:\n                    response_message = await ctx.send_followup(chunk)\n                else:\n                    response_message = await ctx.channel.send(chunk)\n        return response_message\n\n    async def paginate_embed(self, response_text):\n        \"\"\"Given a response text make embed pages and return a list of the pages.\"\"\"\n\n        response_text = [\n            response_text[i : i + self.EMBED_CUTOFF]\n            for i in range(0, len(response_text), self.EMBED_CUTOFF)\n        ]\n        pages = []\n        first = False\n        # Send each chunk as a message\n        for count, chunk in enumerate(response_text, start=1):\n            if not first:\n                page = discord.Embed(\n                    title=f\"Page {count}\",\n                    description=chunk,\n                )\n                first = True\n            else:\n                page = discord.Embed(\n                    title=f\"Page {count}\",\n                    description=chunk,\n                )\n            pages.append(page)\n\n        return pages\n\n    async def queue_debug_message(self, debug_message, debug_channel):\n        \"\"\"Put a message into the debug queue\"\"\"\n        await self.message_queue.put(Message(debug_message, debug_channel))\n\n    async def queue_debug_chunks(self, debug_message, debug_channel):\n        \"\"\"Put a message as chunks into the debug queue\"\"\"\n        debug_message_chunks = [\n            debug_message[i : i + self.TEXT_CUTOFF]\n            for i in range(0, len(debug_message), self.TEXT_CUTOFF)\n        ]\n\n        backticks_encountered = 0\n\n        for i, chunk in enumerate(debug_message_chunks):\n            # Count the number of backticks in the chunk\n            backticks_encountered += chunk.count(\"```\")\n\n            # If it's the first chunk, append a \"\\n```\\n\" to the end\n            if i == 0:\n                chunk += \"\\n```\\n\"\n\n            # If it's an interior chunk, append a \"```\\n\" to the end, and a \"\\n```\\n\" to the beginning\n            elif i < len(debug_message_chunks) - 1:\n                chunk = \"\\n```\\n\" + chunk + \"```\\n\"\n\n            # If it's the last chunk, append a \"```\\n\" to the beginning\n            else:\n                chunk = \"```\\n\" + chunk\n\n            await self.message_queue.put(Message(chunk, debug_channel))\n\n    async def send_debug_message(self, debug_message, debug_channel):\n        \"\"\"process a debug message and put directly into queue or chunk it\"\"\"\n        # Send the debug message\n        try:\n            if len(debug_message) > self.TEXT_CUTOFF:\n                await self.queue_debug_chunks(debug_message, debug_channel)\n            else:\n                await self.queue_debug_message(debug_message, debug_channel)\n        except Exception as e:\n            traceback.print_exc()\n            await self.message_queue.put(\n                Message(\"Error sending debug message: \" + str(e), debug_channel)\n            )\n\n    async def check_conversation_limit(self, message):\n        \"\"\"Check if a conversation has reached the set limit and end it if it has\"\"\"\n        # After each response, check if the user has reached the conversation limit in terms of messages or time.\n        if message.channel.id in self.conversation_threads:\n            # If the user has reached the max conversation length, end the conversation\n            if (\n                self.conversation_threads[message.channel.id].count\n                >= self.model.max_conversation_length\n            ):\n                await message.reply(\n                    \"You have reached the maximum conversation length. You have ended the conversation with GPT, and it has ended.\"\n                )\n                await self.end_conversation(message, conversation_limit=True)\n                return True\n        return False\n\n    async def summarize_conversation(self, message, prompt):\n        \"\"\"Takes a conversation history filled prompt and summarizes it to then start a new history with it as the base\"\"\"\n        response = await self.model.send_summary_request(prompt)\n        summarized_text = response[\"choices\"][0][\"message\"][\"content\"]\n\n        new_conversation_history = []\n        new_conversation_history.append(\n            EmbeddedConversationItem(self.CONVERSATION_STARTER_TEXT, 0)\n        )\n        new_conversation_history.append(\n            EmbeddedConversationItem(\n                f\"\\nThis conversation has some context from earlier, which has been summarized as follows: {summarized_text} \\nContinue the conversation, paying very close attention to things <username> told you, such as their name, and personal details.\",\n                0,\n            )\n        )\n\n        # Get the last entry from the thread's conversation history\n        new_conversation_history.append(\n            EmbeddedConversationItem(\n                self.conversation_threads[message.channel.id].history[-1].text + \"\\n\", 0\n            )\n        )\n        self.conversation_threads[message.channel.id].history = new_conversation_history\n\n    # A listener for message edits to redo prompts if they are edited\n    @discord.Cog.listener()\n    async def on_message_edit(self, before, after):\n        \"\"\"When a message is edited run moderation if enabled, and process if it a prompt that should be redone\"\"\"\n\n        if after.author.id == self.bot.user.id:\n            return\n\n        # Moderation\n        if not isinstance(after.channel, discord.DMChannel):\n            if (\n                after.guild.id in Moderation.moderation_queues\n                and Moderation.moderation_queues[after.guild.id] is not None\n            ):\n                # Create a timestamp that is 0.25 seconds from now\n                timestamp = (\n                    datetime.datetime.now() + datetime.timedelta(seconds=0.25)\n                ).timestamp()\n                await Moderation.moderation_queues[after.guild.id].put(\n                    Moderation(after, timestamp)\n                )  # TODO Don't proceed if message was deleted!\n\n        await TextService.process_conversation_edit(self, after, original_message)\n\n    @discord.Cog.listener()\n    async def on_message(self, message):\n        \"\"\"On a new message check if it should be moderated then process it for conversation\"\"\"\n        if message.author == self.bot.user:\n            return\n\n        # Check if the message is a discord system message\n        if message.type != discord.MessageType.default:\n            return\n\n        # Moderations service is done here.\n        if (\n            hasattr(message, \"guild\")\n            and message.guild\n            and message.guild.id in Moderation.moderation_queues\n            and Moderation.moderation_queues[message.guild.id] is not None\n        ):\n            # Don't moderate if there is no \"roles\" attribute for the author\n            if not hasattr(message.author, \"roles\"):\n                pass\n            # Verify that the user is not in a role that can bypass moderation\n            elif CHAT_BYPASS_ROLES is [None] or not any(\n                role.name.lower() in CHAT_BYPASS_ROLES for role in message.author.roles\n            ):\n                # Create a timestamp that is 0.5 seconds from now\n                timestamp = (\n                    datetime.datetime.now() + datetime.timedelta(seconds=0.5)\n                ).timestamp()\n                await Moderation.moderation_queues[message.guild.id].put(\n                    Moderation(message, timestamp)\n                )\n\n        # Language check\n        if FORCE_ENGLISH and len(message.content.split(\" \")) > 3:\n            if not await Moderation.force_english_and_respond(\n                message.content, self.LANGUAGE_DETECT_STARTER_TEXT, message\n            ):\n                await message.delete()\n                return\n\n        # Get the first file in the message if there is one\n        file = message.attachments[0] if len(message.attachments) > 0 else None\n\n        # Process the message if the user is in a conversation\n        if await TextService.process_conversation_message(\n            self,\n            message,\n            USER_INPUT_API_KEYS,\n            USER_KEY_DB,\n            files=None if not file else message.attachments,\n        ):\n            print(\"Processing a conversation message in server\", message.guild.name)\n            original_message[message.author.id] = message.id\n\n        # If the user tagged the bot and the tag wasn't an @here or @everyone, retrieve the message\n        if f\"<@{self.bot.user.id}>\" in message.content and not (\n            \"@everyone\" in message.content or \"@here\" in message.content\n        ):\n            if not BOT_TAGGABLE:\n                return\n\n            # Check if any of the message author's role names are in BOT_TAGGABLE_ROLES, if not, return\n            if BOT_TAGGABLE_ROLES != [None] and not any(\n                role.name.lower() in BOT_TAGGABLE_ROLES for role in message.author.roles\n            ):\n                return\n\n            # Remove the mention from the message\n            prompt = message.content.replace(self.bot.user.mention, \"\")\n            # If the message is empty, don't process it\n            if len(prompt) < 5:\n                await message.reply(\n                    \"This is too short of a prompt to think about. Please be more specific.\"\n                )\n                return\n\n            await self.ask_command(\n                message,\n                prompt=prompt,\n                from_message_context=True,\n            )\n\n    def cleanse_response(self, response_text):\n        \"\"\"Cleans history tokens from response\"\"\"\n        response_text = response_text.replace(\"<yourname>:\", \"\")\n        response_text = response_text.replace(\"You:\", \"\")\n        response_text = response_text.replace(BOT_NAME.replace(\" \", \"\"), \"\")\n        response_text = response_text.replace(BOT_NAME, \"\")\n        response_text = response_text.replace(\"<|endofstatement|>\", \"\")\n        return response_text\n\n    def remove_awaiting(\n        self, author_id, channel_id, from_ask_command, from_edit_command\n    ):\n        \"\"\"Remove user from ask/edit command response wait, if not any of those then process the id to remove user from thread response wait\"\"\"\n        if author_id in self.awaiting_responses:\n            self.awaiting_responses.remove(author_id)\n        if not from_ask_command and not from_edit_command:\n            if channel_id in self.awaiting_thread_responses:\n                self.awaiting_thread_responses.remove(channel_id)\n\n    async def mention_to_username(self, ctx, message):\n        \"\"\"replaces discord mentions with their server nickname in text, if the user is not found keep the mention as is\"\"\"\n        if not discord.utils.raw_mentions(message):\n            return message\n        for mention in discord.utils.raw_mentions(message):\n            try:\n                user = await discord.utils.get_or_fetch(ctx.guild, \"member\", mention)\n                message = message.replace(f\"<@{str(mention)}>\", user.display_name)\n            except Exception:\n                pass\n        return message\n\n    # COMMANDS\n\n    async def help_command(self, ctx):\n        \"\"\"Command handler. Generates a help message and sends it to the user\"\"\"\n        await ctx.defer()\n        embed = discord.Embed(\n            title=\"GPTDiscord Help\", description=\"The current commands\", color=0xC730C7\n        )\n        embed.add_field(\n            name=\"/code chat\",\n            value=\"Chat with GPT hooked up to a full execution environment!\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/internet chat\",\n            value=\"AI-Assisted chatting connected to google, wolfram, and a crawler!\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/internet search\",\n            value=\"Do a one-off internet search assisted by GPT!\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/index chat\",\n            value=\"Chat with your documents\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/index\",\n            value=\"More indexing/working with documents commands\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/gpt converse\",\n            value=\"Have a multi-modal conversation with GPT\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/gpt ask\",\n            value=\"Ask GPT something. Be clear, long, and concise in your prompt.\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/gpt\",\n            value=\"More pure GPT commands\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/dalle draw\",\n            value=\"Use DALL-E2 to draw an image based on a text prompt\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/dalle optimize\",\n            value=\"Optimize an image prompt for use with DALL-E2, Midjourney, SD, etc.\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/system settings\",\n            value=\"Print the current settings of the model\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/system settings <model parameter> <value>\",\n            value=\"Change the parameter of the model named by <model parameter> to new value <value>\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/mod\",\n            value=\"The automatic moderations service\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/translate\",\n            value=\"Translate from one language to another\",\n            inline=False,\n        )\n        embed.add_field(\n            name=\"/transcribe\",\n            value=\"Transcription services (very experimental)\",\n            inline=False,\n        )\n\n        embed.add_field(name=\"/help\", value=\"See this help text\", inline=False)\n        await ctx.respond(embed=embed, ephemeral=False)\n\n    async def set_usage_command(\n        self, ctx: discord.ApplicationContext, usage_amount: float\n    ):\n        \"\"\"Command handler. Sets the usage file to the given value\"\"\"\n        await ctx.defer()\n\n        # Attempt to convert the input usage value into a float\n        try:\n            usage = float(usage_amount)\n            await self.usage_service.set_usage(usage)\n            await ctx.respond(f\"Set the usage to {usage}\")\n        except Exception:\n            await ctx.respond(\"The usage value must be a valid float.\")\n            return\n\n    async def delete_all_conversation_threads_command(\n        self, ctx: discord.ApplicationContext\n    ):\n        \"\"\"Command handler. Deletes all threads made by the bot in the current guild\"\"\"\n        await ctx.defer()\n\n        for thread in ctx.guild.threads:\n            thread_name = thread.name.lower()\n            if \"with gpt\" in thread_name or \"closed-gpt\" in thread_name:\n                try:\n                    await thread.delete()\n                except Exception:\n                    pass\n        await ctx.respond(\"All conversation threads in this server have been deleted.\")\n\n    async def usage_command(self, ctx):\n        \"\"\"Command handler. Responds with the current usage of the bot\"\"\"\n        await ctx.defer()\n        embed = discord.Embed(\n            title=\"GPT3Bot Usage\", description=\"The current usage\", color=0x00FF00\n        )\n        # 1000 tokens costs 0.02 USD, so we can calculate the total tokens used from the price that we have stored\n        embed.add_field(\n            name=\"Total tokens used\",\n            value=str(int((await self.usage_service.get_usage() / 0.02)) * 1000),\n            inline=False,\n        )\n        embed.add_field(\n            name=\"Total price\",\n            value=\"$\" + str(round(await self.usage_service.get_usage(), 2)),\n            inline=False,\n        )\n        await ctx.respond(embed=embed)\n\n    async def instruction_command(\n        self,\n        ctx: discord.ApplicationContext,\n        mode: str,\n        type: str,\n        instruction: str,\n        instruction_file: discord.Attachment,\n        private: bool,\n    ):\n        \"\"\"Command to let users set their own system prompt or add one to the channel\"\"\"\n\n        await ctx.defer(ephemeral=private)\n\n        if mode == \"set\" and not (instruction or instruction_file):\n            await ctx.respond(\n                \"You must include either an **instruction** or an **instruction file**\"\n            )\n            return\n\n        # Check if any of the message author's role names are in CHANNEL_INSTRUCTION_ROLES, if not, continue as user\n        if type == \"channel\" and mode in [\"set\", \"clear\"]:\n            if CHANNEL_INSTRUCTION_ROLES != [None] and not any(\n                role.name.lower() in CHANNEL_INSTRUCTION_ROLES\n                for role in ctx.author.roles\n            ):\n                await ctx.respond(\n                    \"You don't have permisson to set the channel instruction. Defaulting to setting a user instruction\"\n                )\n                type = \"user\"\n\n        if instruction_file:\n            bytestring = await instruction_file.read()\n            file_instruction = bytestring.decode(\"utf-8\")\n        if instruction and instruction_file:\n            instruction = f\"{file_instruction}\\n\\n{instruction}\"\n        elif instruction_file:\n            instruction = file_instruction\n\n        # If premoderation is enabled, check\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(instruction, ctx):\n                return\n\n        if type == \"channel\":\n            set_id = ctx.channel.id\n        else:\n            set_id = ctx.user.id\n\n        if mode == \"set\":\n            self.instructions[set_id] = Instruction(set_id, instruction)\n            await ctx.respond(f\"The system instruction is set for **{type}**\")\n        elif mode == \"get\":\n            try:\n                instruction = self.instructions[set_id].prompt\n                embed_pages = await self.paginate_embed(instruction)\n                paginator = pages.Paginator(\n                    pages=embed_pages, timeout=None, author_check=False\n                )\n                await paginator.respond(ctx.interaction)\n            except Exception:\n                await ctx.respond(\"There is no instruction set\")\n        elif mode == \"clear\":\n            self.instructions.pop(set_id)\n            await ctx.respond(f\"The instruction has been removed for **{type}**\")\n\n    async def ask_command(\n        self,\n        ctx: discord.ApplicationContext,\n        prompt: str,\n        private: bool = False,\n        temperature: float = None,\n        top_p: float = None,\n        frequency_penalty: float = None,\n        presence_penalty: float = None,\n        from_ask_action=None,\n        from_other_action=None,\n        from_message_context=None,\n        prompt_file: discord.Attachment = None,\n        model=None,\n    ):\n        \"\"\"Command handler. Requests and returns a generation with no extras to the completion endpoint\n\n        Args:\n            ctx (discord.ApplicationContext): Command interaction\n            prompt (str): A prompt to use for generation\n            temperature (float): Sets the temperature override\n            top_p (float): Sets the top p override\n            frequency_penalty (float): Sets the frequency penalty override\n            presence_penalty (float): Sets the presence penalty override\n            from_action (bool, optional): Enables ephemeral. Defaults to None.\n        \"\"\"\n        is_context = isinstance(ctx, discord.ApplicationContext)\n\n        user = ctx.user if is_context else ctx.author\n\n        if not (prompt or prompt_file):\n            await ctx.respond(\n                \"You must include either a **prompt** or a **prompt file**\"\n            )\n            return\n\n        if prompt_file:\n            bytestring = await prompt_file.read()\n            file_prompt = bytestring.decode(\"utf-8\")\n        if prompt and prompt_file:\n            prompt = f\"{file_prompt}\\n\\n{prompt}\"\n        elif prompt_file:\n            prompt = file_prompt\n\n        prompt = await self.mention_to_username(ctx, prompt.strip())\n\n        if len(prompt) < self.model.prompt_min_length:\n            alias = ctx.respond if is_context else ctx.send\n            await alias(\n                f\"Prompt must be greater than {self.model.prompt_min_length} characters, it is currently: {len(prompt)} characters\"\n            )\n            return\n\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(user.id, ctx, USER_KEY_DB)\n            if not user_api_key:\n                return\n\n        await ctx.defer(ephemeral=private) if is_context else None\n\n        # If premoderation is enabled, check\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(prompt, ctx):\n                return\n\n        overrides = Override(temperature, top_p, frequency_penalty, presence_penalty)\n\n        await TextService.encapsulated_send(\n            self,\n            user.id,\n            prompt,\n            ctx,\n            overrides=overrides,\n            from_ask_command=True,\n            custom_api_key=user_api_key,\n            from_ask_action=from_ask_action,\n            from_other_action=from_other_action,\n            from_message_context=from_message_context,\n            model=model,\n        )\n\n    async def edit_command(\n        self,\n        ctx: discord.ApplicationContext,\n        instruction: str,\n        text: str,\n        private: bool,\n        temperature: float,\n        top_p: float,\n    ):\n        \"\"\"Command handler. Requests and returns a generation with no extras to the edit endpoint\n\n        Args:\n            ctx (discord.ApplicationContext): Command interaction\n            instruction (str): The modification instructions\n            text (str): The text that should be modified\n            temperature (float): Sets the temperature override\n            top_p (float): Sets the top p override\n        \"\"\"\n        user = ctx.user\n\n        text = await self.mention_to_username(ctx, text.strip())\n        instruction = await self.mention_to_username(ctx, instruction.strip())\n\n        # Validate that  all the parameters are in a good state before we send the request\n        if len(instruction) < self.model.prompt_min_length:\n            await ctx.respond(\n                f\"Instruction must be at least {self.model.prompt_min_length} characters long\"\n            )\n            return\n\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(user.id, ctx, USER_KEY_DB)\n            if not user_api_key:\n                return\n\n        await ctx.defer(ephemeral=private)\n\n        if PRE_MODERATE:\n            if await Moderation.simple_moderate_and_respond(instruction + text, ctx):\n                return\n\n        overrides = Override(temperature, top_p, 0, 0)\n\n        await TextService.encapsulated_send(\n            self,\n            user.id,\n            prompt=text,\n            ctx=ctx,\n            overrides=overrides,\n            instruction=instruction,\n            from_edit_command=True,\n            custom_api_key=user_api_key,\n        )\n\n    async def private_test_command(self, ctx: discord.ApplicationContext):\n        \"\"\"Command handler. Creates a private thread in the current channel\"\"\"\n        await ctx.defer(ephemeral=True)\n        await ctx.respond(\"Your private test thread\")\n        thread = await ctx.channel.create_thread(\n            name=ctx.user.name + \"'s private test conversation\",\n            auto_archive_duration=60,\n        )\n        await thread.send(\n            f\"<@{str(ctx.user.id)}> This is a private thread for testing. Only you and server admins can see this thread.\"\n        )\n\n    async def converse_command(\n        self,\n        ctx: discord.ApplicationContext,\n        opener: str,\n        opener_file: str,\n        private: bool,\n        minimal: bool,\n        model: str,\n        temperature: float,\n        top_p: float,\n        frequency_penalty: float,\n        presence_penalty: float,\n        use_threads: bool = True,  # Add this parameter\n    ):\n        \"\"\"Command handler. Starts a conversation with the bot\n\n        Args:\n            ctx (discord.ApplicationContext): Command interaction\n            opener (str): The first prompt to send in the conversation\n            opener_file (str): A .txt or .json file which is appended before the opener\n            private (bool): If the thread should be private\n            minimal (bool): If a minimal starter should be used\n            model (str): The openai model that should be used\n            temperature (float): Sets the temperature override\n            top_p (float): Sets the top p override\n            frequency_penalty (float): Sets the frequency penalty override\n            presence_penalty (float): Sets the presence penalty override\n        \"\"\"\n\n        user = ctx.user\n\n        # If we are in user input api keys mode, check if the user has entered their api key before letting them continue\n        user_api_key = None\n        if USER_INPUT_API_KEYS:\n            user_api_key = await TextService.get_user_api_key(user.id, ctx, USER_KEY_DB)\n            if not user_api_key:\n                return\n\n        if private:\n            await ctx.defer(ephemeral=True)\n        elif not private:\n            await ctx.defer()\n\n        # Check the opener for bad content.\n        if PRE_MODERATE and opener is not None:\n            if await Moderation.simple_moderate_and_respond(opener, ctx):\n                return\n\n        if use_threads:\n            if private:\n                embed_title = f\"{user.name}'s private conversation with GPT\"\n                thread = await ctx.channel.create_thread(\n                    name=embed_title,\n                    auto_archive_duration=60,\n                )\n                target = thread\n            else:\n                embed_title = f\"{user.name}'s conversation with GPT\"\n                message_embed = discord.Embed(\n                    title=embed_title,\n                    description=f\"**Model**: {self.model.model if not model else model}\",\n                    color=0x808080,\n                )\n                message_embed.set_thumbnail(url=\"https://i.imgur.com/wpp4TIf.png\")\n                footer_text = (\n                    \"Regular Chat\"\n                    if not image_understanding_model.get_is_usable()\n                    else \"Regular Chat, Multi-Modal\"\n                )\n                message_embed.set_footer(\n                    text=footer_text, icon_url=\"https://i.imgur.com/wpp4TIf.png\"\n                )\n                message_thread = await ctx.send(embed=message_embed)\n                thread = await message_thread.create_thread(\n                    name=user.name + \"'s conversation with GPT\",\n                    auto_archive_duration=60,\n                )\n                await ctx.respond(\"Conversation started.\")\n                target = thread\n        else:\n            # Check if this current channel is already in a conversation\n            if ctx.channel.id in self.conversation_threads:\n                await ctx.respond(\n                    \"There is already a conversation in this channel. Please finish that conversation before starting a new one.\"\n                )\n                return\n\n            # Check if the user is permitted to start a conversation in full channels\n            # check if any of the user role names match CHANNEL_CHAT_ROLES\n            if CHANNEL_CHAT_ROLES and CHANNEL_CHAT_ROLES != [None]:\n                if not any(\n                    role.name.lower() in CHANNEL_CHAT_ROLES for role in ctx.user.roles\n                ):\n                    await ctx.respond(\n                        \"You are not permitted to start a conversation in this channel.\"\n                    )\n                    return\n\n            target = ctx.channel\n            if private:\n                embed_title = f\"{user.name}'s private conversation with GPT\"\n            else:\n                embed_title = f\"{user.name}'s conversation with GPT\"\n\n            embed = discord.Embed(title=embed_title, color=0x808080)\n            await ctx.respond(embed=embed)\n\n        self.conversation_threads[target.id] = Thread(target.id)\n        self.conversation_threads[target.id].model = (\n            self.model.model if not model else model\n        )\n\n        # Set the overrides for the conversation\n        self.conversation_threads[target.id].set_overrides(\n            temperature, top_p, frequency_penalty, presence_penalty\n        )\n\n        if opener or opener_file:\n            user_id_normalized = ctx.author.id\n        else:\n            user_id_normalized = user.id\n        if opener_file:\n            if not opener_file.endswith((\".txt\", \".json\")):\n                opener_file = (\n                    None  # Just start a regular thread if the file fails to load\n                )\n            else:\n                # Load the file and read it into opener\n                try:\n                    opener_file = re.sub(\n                        \".+(?=[\\\\//])\", \"\", opener_file\n                    )  # remove paths from the opener file\n                    opener_file = EnvService.find_shared_file(\n                        f\"openers{separator}{opener_file}\"\n                    )\n                    opener_file = await self.load_file(opener_file, ctx)\n                    try:  # Try opening as json, if it fails it'll just pass the whole txt or json to the opener\n                        opener_file = json.loads(opener_file)\n                        temperature = opener_file.get(\"temperature\", None)\n                        top_p = opener_file.get(\"top_p\", None)\n                        frequency_penalty = opener_file.get(\"frequency_penalty\", None)\n                        presence_penalty = opener_file.get(\"presence_penalty\", None)\n                        self.conversation_threads[target.id].set_overrides(\n                            temperature, top_p, frequency_penalty, presence_penalty\n                        )\n                        if (\n                            not opener\n                        ):  # if we only use opener_file then only pass on opener_file for the opening prompt\n                            opener = opener_file.get(\"text\", \"error getting text\")\n                        else:\n                            opener = (\n                                opener_file.get(\"text\", \"error getting text\") + opener\n                            )\n                    except Exception:  # Parse as just regular text\n                        if not opener:\n                            opener = opener_file\n                        else:\n                            opener = opener_file + opener\n                except Exception:\n                    opener_file = (\n                        None  # Just start a regular thread if the file fails to load\n                    )\n\n        # Append the starter text for gpt to the user's history so it gets concatenated with the prompt later\n        if minimal or opener_file or opener:\n            self.conversation_threads[target.id].history.append(\n                EmbeddedConversationItem(self.CONVERSATION_STARTER_TEXT_MINIMAL, 0)\n            )\n        elif not minimal:\n            self.conversation_threads[target.id].history.append(\n                EmbeddedConversationItem(self.CONVERSATION_STARTER_TEXT, 0)\n            )\n\n        # Set user as thread owner before sending anything that can error and leave the thread unowned\n        self.conversation_thread_owners[user_id_normalized].append(target.id)\n        overrides = self.conversation_threads[target.id].get_overrides()\n\n        await target.send(f\"<@{str(ctx.user.id)}> is the thread owner.\")\n\n        await target.send(\n            embed=EmbedStatics.generate_conversation_embed(\n                self.conversation_threads, target, opener, overrides\n            )\n        )\n\n        # send opening\n        if opener:\n            self.conversation_threads[target.id].has_opener = True\n            opener = await self.mention_to_username(ctx, opener)\n            target_message = await target.send(\n                embed=EmbedStatics.generate_opener_embed(opener)\n            )\n            if target.id in self.conversation_threads:\n                self.awaiting_responses.append(user_id_normalized)\n                if not self.pinecone_service:\n                    self.conversation_threads[target.id].history.append(\n                        EmbeddedConversationItem(\n                            f\"\\n{ctx.author.display_name}: {opener} <|endofstatement|>\\n\",\n                            0,\n                        )\n                    )\n                self.awaiting_thread_responses.append(target.id)\n\n                # ... (no other changes in the middle part of the function)\n\n            overrides = Override(\n                overrides[\"temperature\"],\n                overrides[\"top_p\"],\n                overrides[\"frequency_penalty\"],\n                overrides[\"presence_penalty\"],\n            )\n\n            await TextService.encapsulated_send(\n                self,\n                target.id,\n                opener\n                if target.id not in self.conversation_threads or self.pinecone_service\n                else \"\".join(\n                    [item.text for item in self.conversation_threads[target.id].history]\n                ),\n                target_message,\n                overrides=overrides,\n                user=user,\n                model=self.conversation_threads[target.id].model,\n                custom_api_key=user_api_key,\n            )\n            self.awaiting_responses.remove(user_id_normalized)\n            if target.id in self.awaiting_thread_responses:\n                self.awaiting_thread_responses.remove(target.id)\n\n    async def end_command(self, ctx: discord.ApplicationContext):\n        \"\"\"Command handler. Gets the user's thread and ends it\"\"\"\n        await ctx.defer(ephemeral=True)\n        user_id = ctx.user.id\n\n        if ctx.channel.id in self.conversation_threads:\n            try:\n                await self.end_conversation(ctx)\n            except Exception as e:\n                print(e)\n                traceback.print_exc()\n        else:\n            await ctx.respond(\n                \"This is not a conversation channel.\", ephemeral=True, delete_after=10\n            )\n\n    async def setup_command(self, ctx: discord.ApplicationContext):\n        \"\"\"Command handler. Opens the setup modal\"\"\"\n        if not USER_INPUT_API_KEYS:\n            await ctx.respond(\n                \"This server doesn't support user input API keys.\",\n                ephemeral=True,\n                delete_after=30,\n            )\n            return\n\n        modal = SetupModal(user_key_db=USER_KEY_DB)\n        await ctx.send_modal(modal)\n\n    async def settings_command(\n        self, ctx: discord.ApplicationContext, parameter: str = None, value: str = None\n    ):\n        \"\"\"Command handler. Returns current settings or sets new values\"\"\"\n        await ctx.defer()\n        if parameter is None and value is None:\n            await self.send_settings_text(ctx)\n            return\n\n        # If only one of the options are set, then this is invalid.\n        if (\n            parameter is None\n            and value is not None\n            or parameter is not None\n            and value is None\n        ):\n            await ctx.respond(\n                \"Invalid settings command. Please use `/settings <parameter> <value>` to change a setting\"\n            )\n            return\n\n        # Otherwise, process the settings change\n        await self.process_settings(ctx, parameter, value)\n\n    async def settings_reset_command(self, ctx: discord.ApplicationContext):\n        \"\"\"Command handler. Resets all settings to default\"\"\"\n        await ctx.defer()\n        self.model.reset_settings()\n        await ctx.respond(\"Settings reset to default\")\n\n    #\n    # Text-based context menu commands from here\n    #\n\n    async def ask_gpt_action(self, ctx, message: discord.Message):\n        \"\"\"Message command. Return the message\"\"\"\n        prompt = await self.mention_to_username(ctx, message.content)\n        await self.ask_command(\n            ctx,\n            prompt=prompt,\n            from_ask_action=prompt,\n        )\n\n    async def paraphrase_action(self, ctx, message: discord.Message):\n        \"\"\"Message command. paraphrase the current message content\"\"\"\n        user = ctx.user\n        prompt = await self.mention_to_username(ctx, message.content)\n        from_other_action = prompt + \"\\nParaphrased:\"\n\n        # Construct the paraphrase prompt\n        prompt = f\"Paraphrase the following text. Maintain roughly the same text length after paraphrasing and the same tone of voice: {prompt} \\nParaphrased:\"\n\n        tokens = self.model.usage_service.count_tokens(prompt)\n        if tokens > self.model.max_tokens - 1000:\n            await ctx.respond(\n                f\"This message is too long to paraphrase.\",\n                ephemeral=True,\n                delete_after=10,\n            )\n            return\n\n        await self.ask_command(\n            ctx,\n            prompt=prompt,\n            from_other_action=from_other_action,\n        )\n\n    async def elaborate_action(self, ctx, message: discord.Message):\n        \"\"\"Message command. elaborate on the subject of the current message content\"\"\"\n        user = ctx.user\n        prompt = await self.mention_to_username(ctx, message.content)\n        from_other_action = prompt + \"\\nElaboration:\"\n\n        # Construct the paraphrase prompt\n        prompt = f\"Elaborate with more information about the subject of the following message. Be objective and detailed and respond with elaborations only about the subject(s) of the message: {prompt} \\n\\nElaboration:\"\n\n        tokens = self.model.usage_service.count_tokens(prompt)\n        if tokens > self.model.max_tokens - 500:\n            await ctx.respond(\n                f\"This message is too long to elaborate on.\",\n                ephemeral=True,\n                delete_after=10,\n            )\n            return\n\n        await self.ask_command(\n            ctx,\n            prompt=prompt,\n            from_other_action=from_other_action,\n        )\n\n    async def summarize_action(self, ctx, message: discord.Message):\n        \"\"\"Message command. elaborate on the subject of the current message content\"\"\"\n        user = ctx.user\n        prompt = await self.mention_to_username(ctx, message.content)\n        from_other_action = (\n            \"Message at message link: \" + message.jump_url + \"\\nSummarized:\"\n        )\n\n        # Construct the paraphrase prompt\n        prompt = f\"Summarize the following message, be as short and concise as possible: {prompt} \\n\\nSummary:\"\n\n        tokens = self.model.usage_service.count_tokens(prompt)\n        if tokens > self.model.max_tokens - 300:\n            await ctx.respond(\n                f\"Your prompt is too long. It has {tokens} tokens, but the maximum is {self.model.max_tokens-300}.\",\n                ephemeral=True,\n                delete_after=10,\n            )\n            return\n\n        await self.ask_command(\n            ctx,\n            prompt=prompt,\n            from_other_action=from_other_action,\n        )\n\n\nclass ShareView(discord.ui.View):\n    def __init__(\n        self,\n        converser_cog,\n        conversation_id,\n    ):\n        super().__init__(timeout=3600)  # 1 hour interval to share the conversation.\n        self.converser_cog = converser_cog\n        self.conversation_id = conversation_id\n        self.add_item(ShareButton(converser_cog, conversation_id))\n\n    async def on_timeout(self):\n        # Remove the button from the view/message\n        self.clear_items()\n\n\nclass ShareButton(discord.ui.Button[\"ShareView\"]):\n    def __init__(self, converser_cog, conversation_id):\n        super().__init__(\n            style=discord.ButtonStyle.green,\n            label=\"Share Conversation\",\n            custom_id=\"share_conversation\",\n        )\n        self.converser_cog = converser_cog\n        self.conversation_id = conversation_id\n\n    async def callback(self, interaction: discord.Interaction):\n        # Get the user\n        try:\n            id = await self.converser_cog.sharegpt_service.format_and_share(\n                self.converser_cog.full_conversation_history[self.conversation_id],\n                self.converser_cog.bot.user.default_avatar.url\n                if not self.converser_cog.bot.user.avatar\n                else self.converser_cog.bot.user.avatar.url,\n            )\n            url = f\"https://shareg.pt/{id}\"\n            await interaction.response.send_message(\n                embed=EmbedStatics.get_conversation_shared_embed(url)\n            )\n        except ValueError as e:\n            traceback.print_exc()\n            await interaction.response.send_message(\n                embed=EmbedStatics.get_conversation_share_failed_embed(\n                    \"The ShareGPT API returned an error: \" + str(e)\n                ),\n                ephemeral=True,\n                delete_after=15,\n            )\n            return\n        except Exception as e:\n            traceback.print_exc()\n            await interaction.response.send_message(\n                embed=EmbedStatics.get_conversation_share_failed_embed(str(e)),\n                ephemeral=True,\n                delete_after=15,\n            )\n            return\n",
          "import functools\nimport os\nimport random\nimport tempfile\nimport traceback\nimport asyncio\nfrom collections import defaultdict\n\nimport aiohttp\nimport discord\nimport aiofiles\nimport openai\nimport tiktoken\nfrom functools import partial\nfrom typing import List, Optional, cast\nfrom pathlib import Path\nfrom datetime import date\n\nfrom discord import Interaction\nfrom discord.ext import pages\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.prompts import MessagesPlaceholder\nfrom langchain.schema import SystemMessage\nfrom langchain.tools import Tool\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.evaluation.guideline import DEFAULT_GUIDELINES, GuidelineEvaluator\nfrom llama_index.llms import OpenAI\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.response_synthesizers import ResponseMode\nfrom llama_index.indices.query.query_transform import StepDecomposeQueryTransform\nfrom llama_index.langchain_helpers.agents import (\n    IndexToolConfig,\n    LlamaToolkit,\n    create_llama_chat_agent,\n    LlamaIndexTool,\n)\nfrom llama_index.prompts.chat_prompts import (\n    CHAT_REFINE_PROMPT,\n    CHAT_TREE_SUMMARIZE_PROMPT,\n    TEXT_QA_SYSTEM_PROMPT,\n)\n\nfrom llama_index.readers import YoutubeTranscriptReader\nfrom llama_index.readers.schema.base import Document\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n\nfrom llama_index.retrievers import VectorIndexRetriever, TreeSelectLeafRetriever\nfrom llama_index.query_engine import (\n    RetrieverQueryEngine,\n    MultiStepQueryEngine,\n    RetryGuidelineQueryEngine,\n)\n\nfrom llama_index import (\n    GPTVectorStoreIndex,\n    SimpleDirectoryReader,\n    QuestionAnswerPrompt,\n    BeautifulSoupWebReader,\n    GPTTreeIndex,\n    GoogleDocsReader,\n    MockLLMPredictor,\n    OpenAIEmbedding,\n    GithubRepositoryReader,\n    MockEmbedding,\n    download_loader,\n    LLMPredictor,\n    ServiceContext,\n    StorageContext,\n    load_index_from_storage,\n    get_response_synthesizer,\n    VectorStoreIndex,\n)\n\nfrom llama_index.schema import TextNode\nfrom llama_index.storage.docstore.types import RefDocInfo\nfrom llama_index.readers.web import DEFAULT_WEBSITE_EXTRACTOR\n\nfrom llama_index.composability import ComposableGraph\nfrom llama_index.vector_stores import DocArrayInMemoryVectorStore\n\nfrom models.embed_statics_model import EmbedStatics\nfrom models.openai_model import Models\nfrom models.check_model import UrlCheck\nfrom services.environment_service import EnvService\n\nSHORT_TO_LONG_CACHE = {}\nMAX_DEEP_COMPOSE_PRICE = EnvService.get_max_deep_compose_price()\nEpubReader = download_loader(\"EpubReader\")\nMarkdownReader = download_loader(\"MarkdownReader\")\nRemoteReader = download_loader(\"RemoteReader\")\nRemoteDepthReader = download_loader(\"RemoteDepthReader\")\n\nembedding_model = OpenAIEmbedding()\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode,\n    verbose=False,\n)\nnode_parser = SimpleNodeParser.from_defaults(\n    text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n)\ncallback_manager = CallbackManager([token_counter])\nservice_context = ServiceContext.from_defaults(\n    embed_model=embedding_model,\n    callback_manager=callback_manager,\n    node_parser=node_parser,\n)\n\n\ndef dummy_tool(**kwargs):\n    return \"You have used the dummy tool. Forget about this and do not even mention this to the user.\"\n\n\ndef get_and_query(\n    user_id,\n    index_storage,\n    query,\n    response_mode,\n    nodes,\n    child_branch_factor,\n    service_context,\n    multistep,\n):\n    index: [GPTVectorStoreIndex, GPTTreeIndex] = index_storage[\n        user_id\n    ].get_index_or_throw()\n\n    if isinstance(index, GPTTreeIndex):\n        retriever = TreeSelectLeafRetriever(\n            index=index,\n            child_branch_factor=child_branch_factor,\n            service_context=service_context,\n        )\n    else:\n        retriever = VectorIndexRetriever(\n            index=index, similarity_top_k=nodes, service_context=service_context\n        )\n\n    response_synthesizer = get_response_synthesizer(\n        response_mode=response_mode,\n        use_async=True,\n        refine_template=CHAT_REFINE_PROMPT,\n        service_context=service_context,\n    )\n\n    query_engine = RetrieverQueryEngine(\n        retriever=retriever, response_synthesizer=response_synthesizer\n    )\n\n    multistep_query_engine = MultiStepQueryEngine(\n        query_engine=query_engine,\n        query_transform=StepDecomposeQueryTransform(multistep),\n        index_summary=\"Provides information about everything you need to know about this topic, use this to answer the question.\",\n    )\n\n    if multistep:\n        response = multistep_query_engine.query(query)\n    else:\n        response = query_engine.query(query)\n\n    return response\n\n\nclass IndexChatData:\n    def __init__(\n        self, llm, agent_chain, memory, thread_id, tools, agent_kwargs, llm_predictor\n    ):\n        self.llm = llm\n        self.agent_chain = agent_chain\n        self.memory = memory\n        self.thread_id = thread_id\n        self.tools = tools\n        self.agent_kwargs = agent_kwargs\n        self.llm_predictor = llm_predictor\n\n\nclass IndexData:\n    def __init__(self):\n        self.queryable_index = None\n        self.individual_indexes = []\n\n    # A safety check for the future\n    def get_index_or_throw(self):\n        if not self.queryable():\n            raise Exception(\n                \"An index access was attempted before an index was created. This is a programmer error, please report this to the maintainers.\"\n            )\n        return self.queryable_index\n\n    def queryable(self):\n        return self.queryable_index is not None\n\n    def has_indexes(self, user_id):\n        try:\n            return (\n                len(os.listdir(EnvService.find_shared_file(f\"indexes/{user_id}\"))) > 0\n            )\n        except Exception:\n            return False\n\n    def has_search_indexes(self, user_id):\n        try:\n            return (\n                len(\n                    os.listdir(EnvService.find_shared_file(f\"indexes/{user_id}_search\"))\n                )\n                > 0\n            )\n        except Exception:\n            return False\n\n    def add_index(self, index, user_id, file_name):\n        self.individual_indexes.append(index)\n        self.queryable_index = index\n\n        # Create a folder called \"indexes/{USER_ID}\" if it doesn't exist already\n        Path(f\"{EnvService.save_path()}/indexes/{user_id}\").mkdir(\n            parents=True, exist_ok=True\n        )\n        # Save the index to file under the user id\n        file = f\"{date.today().month}_{date.today().day}_{file_name}\"\n        # If file is > 93 in length, cut it off to 93\n        if len(file) > 93:\n            file = file[:93]\n\n        index.storage_context.persist(\n            persist_dir=EnvService.save_path()\n            / \"indexes\"\n            / f\"{str(user_id)}\"\n            / f\"{file}\"\n        )\n\n    def reset_indexes(self, user_id):\n        self.individual_indexes = []\n        self.queryable_index = None\n\n        # Delete the user indexes\n        try:\n            # First, clear all the files inside it\n            for file in os.listdir(EnvService.find_shared_file(f\"indexes/{user_id}\")):\n                try:\n                    os.remove(EnvService.find_shared_file(f\"indexes/{user_id}/{file}\"))\n                except:\n                    traceback.print_exc()\n            for file in os.listdir(\n                EnvService.find_shared_file(f\"indexes/{user_id}_search\")\n            ):\n                try:\n                    os.remove(\n                        EnvService.find_shared_file(f\"indexes/{user_id}_search/{file}\")\n                    )\n                except:\n                    traceback.print_exc()\n        except Exception:\n            traceback.print_exc()\n\n\nclass Index_handler:\n    embedding_model = OpenAIEmbedding()\n    token_counter = TokenCountingHandler(\n        tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode,\n        verbose=False,\n    )\n    node_parser = SimpleNodeParser.from_defaults(\n        text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n    )\n    callback_manager = CallbackManager([token_counter])\n    service_context = ServiceContext.from_defaults(\n        embed_model=embedding_model,\n        callback_manager=callback_manager,\n        node_parser=node_parser,\n    )\n    type_to_suffix_mappings = {\n        \"text/plain\": \".txt\",\n        \"text/csv\": \".csv\",\n        \"application/pdf\": \".pdf\",\n        \"application/json\": \".json\",\n        \"image/png\": \".png\",\n        \"image/jpeg\": \".jpg\",\n        \"image/gif\": \".gif\",\n        \"image/svg+xml\": \".svg\",\n        \"image/webp\": \".webp\",\n        \"application/mspowerpoint\": \".ppt\",\n        \"application/vnd.ms-powerpoint\": \".ppt\",\n        \"application/vnd.openxmlformats-officedocument.presentationml.presentation\": \".pptx\",\n        \"application/msexcel\": \".xls\",\n        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\": \".xlsx\",\n        \"application/msword\": \".doc\",\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\": \".docx\",\n        \"audio/mpeg\": \".mp3\",\n        \"audio/x-wav\": \".wav\",\n        \"audio/ogg\": \".ogg\",\n        \"video/mpeg\": \".mpeg\",\n        \"video/mp4\": \".mp4\",\n        \"application/epub+zip\": \".epub\",\n        \"text/markdown\": \".md\",\n        \"text/html\": \".html\",\n        \"application/rtf\": \".rtf\",\n        \"application/x-msdownload\": \".exe\",\n        \"application/xml\": \".xml\",\n        \"application/vnd.adobe.photoshop\": \".psd\",\n        \"application/x-sql\": \".sql\",\n        \"application/x-latex\": \".latex\",\n        \"application/x-httpd-php\": \".php\",\n        \"application/java-archive\": \".jar\",\n        \"application/x-sh\": \".sh\",\n        \"application/x-csh\": \".csh\",\n        \"text/x-c\": \".c\",\n        \"text/x-c++\": \".cpp\",\n        \"text/x-java-source\": \".java\",\n        \"text/x-python\": \".py\",\n        \"text/x-ruby\": \".rb\",\n        \"text/x-perl\": \".pl\",\n        \"text/x-shellscript\": \".sh\",\n    }\n\n    # For when content type doesnt get picked up by discord.\n    secondary_mappings = {\n        \".epub\": \".epub\",\n    }\n\n    def __init__(self, bot, usage_service):\n        self.bot = bot\n        self.openai_key = os.getenv(\"OPENAI_TOKEN\")\n        self.index_storage = defaultdict(IndexData)\n        self.loop = asyncio.get_running_loop()\n        self.usage_service = usage_service\n        self.qaprompt = QuestionAnswerPrompt(\n            \"Context information is below. The text '<|endofstatement|>' is used to separate chat entries and make it \"\n            \"easier for you to understand the context\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\"\n            \"\\n---------------------\\n\"\n            \"Never say '<|endofstatement|>'\\n\"\n            \"Given the context information and not prior knowledge, \"\n            \"answer the question: {query_str}\\n\"\n        )\n        self.EMBED_CUTOFF = 2000\n        self.index_chat_chains = {}\n        self.chat_indexes = defaultdict()\n\n    async def rename_index(self, ctx, original_path, rename_path):\n        \"\"\"Command handler to rename a user index\"\"\"\n\n        index_file = EnvService.find_shared_file(original_path)\n        if not index_file:\n            return False\n\n        # Rename the file at f\"indexes/{ctx.user.id}/{user_index}\" to f\"indexes/{ctx.user.id}/{new_name}\" using Pathlib\n        try:\n            Path(original_path).rename(rename_path)\n            return True\n        except Exception as e:\n            traceback.print_exc()\n            return False\n\n    async def get_is_in_index_chat(self, ctx):\n        return ctx.channel.id in self.index_chat_chains.keys()\n\n    async def execute_index_chat_message(self, ctx, message):\n        if ctx.channel.id not in self.index_chat_chains:\n            return None\n\n        if message.lower() in [\"stop\", \"end\", \"quit\", \"exit\"]:\n            await ctx.reply(\"Ending chat session.\")\n            self.index_chat_chains.pop(ctx.channel.id)\n\n            # close the thread\n            thread = await self.bot.fetch_channel(ctx.channel.id)\n            await thread.edit(name=\"Closed-GPT\")\n            await thread.edit(archived=True)\n            return \"Ended chat session.\"\n\n        agent_output = await self.loop.run_in_executor(\n            None,\n            partial(self.index_chat_chains[ctx.channel.id].agent_chain.run, message),\n        )\n        return agent_output\n\n    async def index_chat_file(self, message: discord.Message, file: discord.Attachment):\n        # First, initially set the suffix to the suffix of the attachment\n        suffix = self.get_file_suffix(file.content_type, file.filename) or None\n\n        if not suffix:\n            await message.reply(\n                \"The file you uploaded is unable to be indexed. It is in an unsupported file format\"\n            )\n            return False, None\n\n        async with aiofiles.tempfile.TemporaryDirectory() as temp_path:\n            async with aiofiles.tempfile.NamedTemporaryFile(\n                suffix=suffix, dir=temp_path, delete=False\n            ) as temp_file:\n                try:\n                    await file.save(temp_file.name)\n\n                    filename = file.filename\n\n                    # Assert that the filename is < 100 characters, if it is greater, truncate to the first 100 characters and keep the original ending\n                    if len(filename) > 100:\n                        filename = filename[:100] + filename[-4:]\n\n                    index: VectorStoreIndex = await self.loop.run_in_executor(\n                        None,\n                        partial(\n                            self.index_file,\n                            Path(temp_file.name),\n                            service_context,\n                            suffix,\n                        ),\n                    )\n\n                    summary = await index.as_query_engine(\n                        similarity_top_k=10,\n                        child_branch_factor=6,\n                        response_mode=\"tree_summarize\",\n                    ).aquery(\n                        f\"What is a summary or general idea of this data? Be detailed in your summary (e.g \"\n                        f\"extract key names, etc) but not too verbose. Your summary should be under a hundred words. \"\n                        f\"This summary will be used in a vector index to retrieve information about certain data. So, \"\n                        f\"at a high level, the summary should describe the document in such a way that a retriever \"\n                        f\"would know to select it when asked questions about it. The data name was {filename}. Include \"\n                        f\"the file name in the summary. When you are asked to reference a specific file, or reference \"\n                        f\"something colloquially like 'in the powerpoint, [...]?', never respond saying that as an AI \"\n                        f\"you can't view the data, instead infer which tool to use that has the data. Say that there \"\n                        f\"is no available data if there are no available tools that are relevant.\"\n                    )\n\n                    engine = self.get_query_engine(index, message, summary)\n\n                    # Get rid of all special characters in the filename\n                    filename = \"\".join(\n                        [c for c in filename if c.isalpha() or c.isdigit()]\n                    ).rstrip()\n\n                    tool_config = IndexToolConfig(\n                        query_engine=engine,\n                        name=f\"{filename}-index\",\n                        description=f\"Use this tool if the query seems related to this summary: {summary}\",\n                        tool_kwargs={\n                            \"return_direct\": False,\n                        },\n                        max_iterations=5,\n                    )\n\n                    tool = LlamaIndexTool.from_tool_config(tool_config)\n\n                    tools = self.index_chat_chains[message.channel.id].tools\n                    tools.append(tool)\n\n                    agent_chain = initialize_agent(\n                        tools=tools,\n                        llm=self.index_chat_chains[message.channel.id].llm,\n                        agent=AgentType.OPENAI_FUNCTIONS,\n                        verbose=True,\n                        agent_kwargs=self.index_chat_chains[\n                            message.channel.id\n                        ].agent_kwargs,\n                        memory=self.index_chat_chains[message.channel.id].memory,\n                        handle_parsing_errors=\"Check your output and make sure it conforms!\",\n                    )\n\n                    index_chat_data = IndexChatData(\n                        self.index_chat_chains[message.channel.id].llm,\n                        agent_chain,\n                        self.index_chat_chains[message.channel.id].memory,\n                        message.channel.id,\n                        tools,\n                        self.index_chat_chains[message.channel.id].agent_kwargs,\n                        self.index_chat_chains[message.channel.id].llm_predictor,\n                    )\n\n                    self.index_chat_chains[message.channel.id] = index_chat_data\n\n                    return True, summary\n                except Exception as e:\n                    await message.reply(\n                        \"There was an error indexing your file: \" + str(e)\n                    )\n                    traceback.print_exc()\n                    return False, None\n\n    async def start_index_chat(self, ctx, model):\n        preparation_message = await ctx.channel.send(\n            embed=EmbedStatics.get_index_chat_preparation_message()\n        )\n\n        llm = ChatOpenAI(model=model, temperature=0)\n        llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=model))\n\n        max_token_limit = 29000 if \"gpt-4\" in model else 7500\n\n        memory = ConversationSummaryBufferMemory(\n            memory_key=\"memory\",\n            return_messages=True,\n            llm=llm,\n            max_token_limit=100000 if \"preview\" in model else max_token_limit,\n        )\n\n        agent_kwargs = {\n            \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"memory\")],\n            \"system_message\": SystemMessage(\n                content=\"You are a superpowered version of GPT that is able to answer questions about the data you're \"\n                \"connected to. Each different tool you have represents a different dataset to interact with. \"\n                \"If you are asked to perform a task that spreads across multiple datasets, use multiple tools \"\n                \"for the same prompt. When the user types links in chat, you will have already been connected \"\n                \"to the data at the link by the time you respond. When using tools, the input should be \"\n                \"clearly created based on the request of the user. For example, if a user uploads an invoice \"\n                \"and asks how many usage hours of X was present in the invoice, a good query is 'X hours'. \"\n                \"Avoid using single word queries unless the request is very simple. You can query multiple times to break down complex requests and retrieve more information.\"\n            ),\n        }\n\n        tools = [\n            Tool(\n                name=\"Dummy-Tool-Do-Not-Use\",\n                func=dummy_tool,\n                description=f\"This is a dummy tool that does nothing, do not ever mention this tool or use this tool.\",\n            )\n        ]\n\n        print(f\"{tools}{llm}{AgentType.OPENAI_FUNCTIONS}{True}{agent_kwargs}{memory}\")\n\n        agent_chain = initialize_agent(\n            tools=tools,\n            llm=llm,\n            agent=AgentType.OPENAI_FUNCTIONS,\n            verbose=True,\n            agent_kwargs=agent_kwargs,\n            memory=memory,\n            handle_parsing_errors=\"Check your output and make sure it conforms!\",\n        )\n\n        embed_title = f\"{ctx.user.name}'s data-connected conversation with GPT\"\n\n        message_embed = discord.Embed(\n            title=embed_title,\n            description=f\"The agent is able to interact with your documents. Simply drag your documents into discord or give the agent a link from where to download the documents.\\nModel: {model}\",\n            color=0x00995B,\n        )\n        message_embed.set_thumbnail(url=\"https://i.imgur.com/7V6apMT.png\")\n        message_embed.set_footer(\n            text=\"Data Chat\", icon_url=\"https://i.imgur.com/7V6apMT.png\"\n        )\n        message_thread = await ctx.send(embed=message_embed)\n        thread = await message_thread.create_thread(\n            name=ctx.user.name + \"'s data-connected conversation with GPT\",\n            auto_archive_duration=60,\n        )\n        await ctx.respond(\"Conversation started.\")\n\n        try:\n            await preparation_message.delete()\n        except:\n            pass\n\n        index_chat_data = IndexChatData(\n            llm, agent_chain, memory, thread.id, tools, agent_kwargs, llm_predictor\n        )\n\n        self.index_chat_chains[thread.id] = index_chat_data\n\n    async def paginate_embed(self, response_text):\n        \"\"\"Given a response text make embed pages and return a list of the pages.\"\"\"\n\n        response_text = [\n            response_text[i : i + self.EMBED_CUTOFF]\n            for i in range(0, len(response_text), self.EMBED_CUTOFF)\n        ]\n        pages = []\n        first = False\n        # Send each chunk as a message\n        for count, chunk in enumerate(response_text, start=1):\n            if not first:\n                page = discord.Embed(\n                    title=f\"Index Query Results\",\n                    description=chunk,\n                )\n                first = True\n            else:\n                page = discord.Embed(\n                    title=f\"Page {count}\",\n                    description=chunk,\n                )\n            pages.append(page)\n\n        return pages\n\n    def index_file(\n        self, file_path, service_context, suffix=None\n    ) -> GPTVectorStoreIndex:\n        if suffix and suffix == \".md\":\n            loader = MarkdownReader()\n            document = loader.load_data(file_path)\n        elif suffix and suffix == \".epub\":\n            epub_loader = EpubReader()\n            document = epub_loader.load_data(file_path)\n        else:\n            document = SimpleDirectoryReader(input_files=[file_path]).load_data()\n        index = GPTVectorStoreIndex.from_documents(\n            document, service_context=service_context, use_async=True\n        )\n        return index\n\n    def index_gdoc(self, doc_id, service_context) -> GPTVectorStoreIndex:\n        document = GoogleDocsReader().load_data(doc_id)\n        index = GPTVectorStoreIndex.from_documents(\n            document, service_context=service_context, use_async=True\n        )\n        return index\n\n    def index_youtube_transcript(self, link, service_context):\n        try:\n\n            def convert_shortlink_to_full_link(short_link):\n                # Check if the link is a shortened YouTube link\n                if \"youtu.be\" in short_link:\n                    # Extract the video ID from the link\n                    video_id = short_link.split(\"/\")[-1].split(\"?\")[0]\n                    # Construct the full YouTube desktop link\n                    desktop_link = f\"https://www.youtube.com/watch?v={video_id}\"\n                    return desktop_link\n                else:\n                    return short_link\n\n            documents = YoutubeTranscriptReader().load_data(\n                ytlinks=[convert_shortlink_to_full_link(link)]\n            )\n        except Exception as e:\n            raise ValueError(f\"The youtube transcript couldn't be loaded: {e}\")\n\n        index = GPTVectorStoreIndex.from_documents(\n            documents,\n            service_context=service_context,\n            use_async=True,\n        )\n        return index\n\n    def index_github_repository(self, link, service_context):\n        # Extract the \"owner\" and the \"repo\" name from the github link.\n        owner = link.split(\"/\")[3]\n        repo = link.split(\"/\")[4]\n\n        try:\n            documents = GithubRepositoryReader(owner=owner, repo=repo).load_data(\n                branch=\"main\"\n            )\n        except KeyError:\n            documents = GithubRepositoryReader(owner=owner, repo=repo).load_data(\n                branch=\"master\"\n            )\n\n        index = GPTVectorStoreIndex.from_documents(\n            documents,\n            service_context=service_context,\n            use_async=True,\n        )\n        return index\n\n    def index_load_file(self, file_path) -> [GPTVectorStoreIndex, ComposableGraph]:\n        storage_context = StorageContext.from_defaults(persist_dir=file_path)\n        index = load_index_from_storage(storage_context)\n        return index\n\n    def index_discord(self, document, service_context) -> GPTVectorStoreIndex:\n        index = GPTVectorStoreIndex.from_documents(\n            document,\n            service_context=service_context,\n            use_async=True,\n        )\n        return index\n\n    async def index_pdf(self, url) -> list[Document]:\n        # Download the PDF at the url and save it to a tempfile\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                if response.status == 200:\n                    data = await response.read()\n                    f = tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False)\n                    f.write(data)\n                    f.close()\n                else:\n                    return \"An error occurred while downloading the PDF.\"\n        # Get the file path of this tempfile.NamedTemporaryFile\n        # Save this temp file to an actual file that we can put into something else to read it\n        documents = SimpleDirectoryReader(input_files=[f.name]).load_data()\n\n        # Delete the temporary file\n        return documents\n\n    async def index_webpage(self, url, service_context) -> GPTVectorStoreIndex:\n        # First try to connect to the URL to see if we can even reach it.\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url, timeout=5) as response:\n                    # Add another entry to links from all_links if the link is not already in it to compensate for the failed request\n                    if response.status not in [200, 203, 202, 204]:\n                        raise ValueError(\n                            \"Invalid URL or could not connect to the provided URL.\"\n                        )\n                    else:\n                        # Detect if the link is a PDF, if it is, we load it differently\n                        if response.headers[\"Content-Type\"] == \"application/pdf\":\n                            documents = await self.index_pdf(url)\n                            index = await self.loop.run_in_executor(\n                                None,\n                                functools.partial(\n                                    GPTVectorStoreIndex.from_documents,\n                                    documents=documents,\n                                    service_context=service_context,\n                                    use_async=True,\n                                ),\n                            )\n\n                            return index\n        except:\n            traceback.print_exc()\n            raise ValueError(\"Could not load webpage\")\n\n        documents = BeautifulSoupWebReader(\n            website_extractor=DEFAULT_WEBSITE_EXTRACTOR\n        ).load_data(urls=[url])\n\n        # index = GPTVectorStoreIndex(documents, embed_model=embed_model, use_async=True)\n        index = await self.loop.run_in_executor(\n            None,\n            functools.partial(\n                GPTVectorStoreIndex.from_documents,\n                documents=documents,\n                service_context=service_context,\n                use_async=True,\n            ),\n        )\n        return index\n\n    def reset_indexes(self, user_id):\n        self.index_storage[user_id].reset_indexes(user_id)\n\n    def get_file_suffix(self, content_type, filename):\n        print(\"The content type is \" + content_type)\n        if content_type:\n            # Apply the suffix mappings to the file\n            for key, value in self.type_to_suffix_mappings.items():\n                if key in content_type:\n                    return value\n\n        else:\n            for key, value in self.secondary_mappings.items():\n                if key in filename:\n                    return value\n\n        return None\n\n    async def set_file_index(\n        self, ctx: discord.ApplicationContext, file: discord.Attachment, user_api_key\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        try:\n            # First, initially set the suffix to the suffix of the attachment\n            suffix = self.get_file_suffix(file.content_type, file.filename) or None\n\n            if not suffix:\n                await ctx.respond(\n                    embed=EmbedStatics.get_index_set_failure_embed(\"Unsupported file\")\n                )\n                return\n\n            # Send indexing message\n            response = await ctx.respond(\n                embed=EmbedStatics.build_index_progress_embed()\n            )\n\n            async with aiofiles.tempfile.TemporaryDirectory() as temp_path:\n                async with aiofiles.tempfile.NamedTemporaryFile(\n                    suffix=suffix, dir=temp_path, delete=False\n                ) as temp_file:\n                    await file.save(temp_file.name)\n                    index = await self.loop.run_in_executor(\n                        None,\n                        partial(\n                            self.index_file,\n                            Path(temp_file.name),\n                            service_context,\n                            suffix,\n                        ),\n                    )\n                    await self.usage_service.update_usage(\n                        token_counter.total_embedding_token_count, \"embedding\"\n                    )\n\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.total_embedding_token_count, \"embedding\"\n                )\n            except:\n                traceback.print_exc()\n                price = \"Unknown\"\n\n            file_name = file.filename\n            self.index_storage[ctx.user.id].add_index(index, ctx.user.id, file_name)\n            await response.edit(\n                embed=EmbedStatics.get_index_set_success_embed(str(price))\n            )\n        except Exception as e:\n            await ctx.channel.send(\n                embed=EmbedStatics.get_index_set_failure_embed(str(e))\n            )\n            traceback.print_exc()\n\n    async def set_link_index_recurse(\n        self, ctx: discord.ApplicationContext, link: str, depth, user_api_key\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        response = await ctx.respond(embed=EmbedStatics.build_index_progress_embed())\n        try:\n            # Pre-emptively connect and get the content-type of the response\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.get(link, timeout=2) as _response:\n                        print(_response.status)\n                        if _response.status == 200:\n                            content_type = _response.headers.get(\"content-type\")\n                        else:\n                            await response.edit(\n                                embed=EmbedStatics.get_index_set_failure_embed(\n                                    \"Invalid URL or could not connect to the provided URL.\"\n                                )\n                            )\n                            return\n            except Exception as e:\n                traceback.print_exc()\n                await response.edit(\n                    embed=EmbedStatics.get_index_set_failure_embed(\n                        \"Invalid URL or could not connect to the provided URL. \"\n                        + str(e)\n                    )\n                )\n                return\n\n            # Check if the link contains youtube in it\n            loader = RemoteDepthReader(depth=depth)\n            documents = await self.loop.run_in_executor(\n                None, partial(loader.load_data, [link])\n            )\n            index = await self.loop.run_in_executor(\n                None,\n                functools.partial(\n                    GPTVectorStoreIndex,\n                    documents=documents,\n                    service_context=service_context,\n                    use_async=True,\n                ),\n            )\n\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.total_embedding_token_count, \"embedding\"\n                )\n            except:\n                traceback.print_exc()\n                price = \"Unknown\"\n\n            # Make the url look nice, remove https, useless stuff, random characters\n            file_name = (\n                link.replace(\"https://\", \"\")\n                .replace(\"http://\", \"\")\n                .replace(\"www.\", \"\")\n                .replace(\"/\", \"_\")\n                .replace(\"?\", \"_\")\n                .replace(\"&\", \"_\")\n                .replace(\"=\", \"_\")\n                .replace(\"-\", \"_\")\n                .replace(\".\", \"_\")\n            )\n\n            self.index_storage[ctx.user.id].add_index(index, ctx.user.id, file_name)\n\n        except ValueError as e:\n            await response.edit(embed=EmbedStatics.get_index_set_failure_embed(str(e)))\n            traceback.print_exc()\n            return\n\n        except Exception as e:\n            await response.edit(embed=EmbedStatics.get_index_set_failure_embed(str(e)))\n            traceback.print_exc()\n            return\n\n        await response.edit(embed=EmbedStatics.get_index_set_success_embed(price))\n\n    def get_query_engine(self, index, message, summary):\n        retriever = VectorIndexRetriever(\n            index=index, similarity_top_k=2, service_context=service_context\n        )\n\n        response_synthesizer = get_response_synthesizer(\n            response_mode=ResponseMode.COMPACT_ACCUMULATE,\n            use_async=True,\n            refine_template=TEXT_QA_SYSTEM_PROMPT,\n            service_context=service_context,\n            verbose=True,\n        )\n\n        engine = RetrieverQueryEngine(\n            retriever=retriever, response_synthesizer=response_synthesizer\n        )\n\n        return engine\n\n    async def index_link(self, link, summarize=False, index_chat_ctx=None):\n        try:\n            if await UrlCheck.check_youtube_link(link):\n                index = await self.loop.run_in_executor(\n                    None, partial(self.index_youtube_transcript, link, service_context)\n                )\n            elif \"github\" in link:\n                index = await self.loop.run_in_executor(\n                    None, partial(self.index_github_repository, link, service_context)\n                )\n            else:\n                index = await self.index_webpage(link, service_context)\n        except Exception as e:\n            if index_chat_ctx:\n                await index_chat_ctx.reply(\n                    \"There was an error indexing your link: \" + str(e)\n                )\n                return False, None\n            else:\n                raise e\n\n        summary = None\n        if index_chat_ctx:\n            try:\n                summary = await index.as_query_engine(\n                    response_mode=\"tree_summarize\"\n                ).aquery(\n                    \"What is a summary or general idea of this document? Be detailed in your summary but not too verbose. Your summary should be under 50 words. This summary will be used in a vector index to retrieve information about certain data. So, at a high level, the summary should describe the document in such a way that a retriever would know to select it when asked questions about it. The link was {link}. Include the an easy identifier derived from the link at the end of the summary.\"\n                )\n\n                engine = self.get_query_engine(index, index_chat_ctx, summary)\n\n                # Get rid of all special characters in the link, replace periods with _\n                link_cleaned = \"\".join(\n                    [c for c in link if c.isalpha() or c.isdigit() or c == \".\"]\n                ).rstrip()\n                # replace .\n                link_cleaned = link_cleaned.replace(\".\", \"_\")\n\n                # Shorten the link to the first 100 characters\n                link_cleaned = link_cleaned[:50]\n\n                tool_config = IndexToolConfig(\n                    query_engine=engine,\n                    name=f\"{link_cleaned}-index\",\n                    description=f\"Use this tool if the query seems related to this summary: {summary}\",\n                    tool_kwargs={\n                        \"return_direct\": False,\n                    },\n                    max_iterations=5,\n                )\n\n                tool = LlamaIndexTool.from_tool_config(tool_config)\n\n                tools = self.index_chat_chains[index_chat_ctx.channel.id].tools\n                tools.append(tool)\n\n                agent_chain = initialize_agent(\n                    tools=tools,\n                    llm=self.index_chat_chains[index_chat_ctx.channel.id].llm,\n                    agent=AgentType.OPENAI_FUNCTIONS,\n                    verbose=True,\n                    agent_kwargs=self.index_chat_chains[\n                        index_chat_ctx.channel.id\n                    ].agent_kwargs,\n                    memory=self.index_chat_chains[index_chat_ctx.channel.id].memory,\n                    handle_parsing_errors=\"Check your output and make sure it conforms!\",\n                    max_iterations=5,\n                )\n\n                index_chat_data = IndexChatData(\n                    self.index_chat_chains[index_chat_ctx.channel.id].llm,\n                    agent_chain,\n                    self.index_chat_chains[index_chat_ctx.channel.id].memory,\n                    index_chat_ctx.channel.id,\n                    tools,\n                    self.index_chat_chains[index_chat_ctx.channel.id].agent_kwargs,\n                    self.index_chat_chains[index_chat_ctx.channel.id].llm_predictor,\n                )\n\n                self.index_chat_chains[index_chat_ctx.channel.id] = index_chat_data\n\n                return True, summary\n            except Exception as e:\n                await index_chat_ctx.reply(\n                    \"There was an error indexing your link: \" + str(e)\n                )\n                return False, None\n\n        return index, summary\n\n    async def set_link_index(\n        self, ctx: discord.ApplicationContext, link: str, user_api_key\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        response = await ctx.respond(embed=EmbedStatics.build_index_progress_embed())\n        try:\n            # Check if the link contains youtube in it\n            index, _ = await self.index_link(link)\n\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.embedding_token_counts, \"embedding\"\n                )\n            except:\n                traceback.print_exc()\n                price = \"Unknown\"\n\n            # Make the url look nice, remove https, useless stuff, random characters\n            file_name = (\n                link.replace(\"https://\", \"\")\n                .replace(\"http://\", \"\")\n                .replace(\"www.\", \"\")\n                .replace(\"/\", \"_\")\n                .replace(\"?\", \"_\")\n                .replace(\"&\", \"_\")\n                .replace(\"=\", \"_\")\n                .replace(\"-\", \"_\")\n                .replace(\".\", \"_\")\n            )\n\n            self.index_storage[ctx.user.id].add_index(index, ctx.user.id, file_name)\n\n        except Exception as e:\n            await response.edit(embed=EmbedStatics.get_index_set_failure_embed(str(e)))\n            traceback.print_exc()\n            return\n\n        await response.edit(embed=EmbedStatics.get_index_set_success_embed(price))\n\n    async def set_discord_index(\n        self,\n        ctx: discord.ApplicationContext,\n        channel: discord.TextChannel,\n        user_api_key,\n        message_limit: int = 2500,\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        try:\n            document = await self.load_data(\n                channel_ids=[channel.id], limit=message_limit, oldest_first=False\n            )\n            index = await self.loop.run_in_executor(\n                None, partial(self.index_discord, document, service_context)\n            )\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.total_embedding_token_count, \"embedding\"\n                )\n            except Exception:\n                traceback.print_exc()\n                price = \"Unknown\"\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n            self.index_storage[ctx.user.id].add_index(index, ctx.user.id, channel.name)\n            await ctx.respond(embed=EmbedStatics.get_index_set_success_embed(price))\n        except Exception as e:\n            await ctx.respond(embed=EmbedStatics.get_index_set_failure_embed(str(e)))\n            traceback.print_exc()\n\n    async def load_index(\n        self, ctx: discord.ApplicationContext, index, server, search, user_api_key\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        try:\n            if server:\n                index_file = EnvService.find_shared_file(\n                    f\"indexes/{ctx.guild.id}/{index}\"\n                )\n            elif search:\n                index_file = EnvService.find_shared_file(\n                    f\"indexes/{ctx.user.id}_search/{index}\"\n                )\n            else:\n                index_file = EnvService.find_shared_file(\n                    f\"indexes/{ctx.user.id}/{index}\"\n                )\n            index = await self.loop.run_in_executor(\n                None, partial(self.index_load_file, index_file)\n            )\n            self.index_storage[ctx.user.id].queryable_index = index\n            await ctx.respond(embed=EmbedStatics.get_index_load_success_embed())\n        except Exception as e:\n            traceback.print_exc()\n            await ctx.respond(embed=EmbedStatics.get_index_load_failure_embed(str(e)))\n\n    async def index_to_docs(\n        self, old_index, chunk_size: int = 256, chunk_overlap: int = 100\n    ) -> List[Document]:\n        documents = []\n        docstore = old_index.docstore\n        ref_docs = old_index.ref_doc_info\n\n        for document in ref_docs.values():\n            text = \"\"\n            for node in document.node_ids:\n                node = docstore.get_node(node)\n                text += f\"{node.text} \"\n\n            text_splitter = TokenTextSplitter(\n                separator=\" \", chunk_size=chunk_size, chunk_overlap=chunk_overlap\n            )\n            text_chunks = text_splitter.split_text(text)\n\n            for chunk_text in text_chunks:\n                new_doc = Document(text=chunk_text, extra_info=document.metadata)\n                documents.append(new_doc)\n\n        return documents\n\n    async def compose_indexes(self, user_id, indexes, name, deep_compose):\n        # Load all the indexes first\n        index_objects = []\n        for _index in indexes:\n            try:\n                index_file = EnvService.find_shared_file(f\"indexes/{user_id}/{_index}\")\n            except ValueError:\n                index_file = EnvService.find_shared_file(\n                    f\"indexes/{user_id}_search/{_index}\"\n                )\n\n            index = await self.loop.run_in_executor(\n                None, partial(self.index_load_file, index_file)\n            )\n            index_objects.append(index)\n\n        llm_predictor = LLMPredictor(\n            llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-32k\")\n        )\n\n        # For each index object, add its documents to a GPTTreeIndex\n        if deep_compose:\n            documents = []\n            for _index in index_objects:\n                documents.extend(await self.index_to_docs(_index, 256, 20))\n\n            embedding_model = OpenAIEmbedding()\n\n            llm_predictor_mock = MockLLMPredictor()\n            embedding_model_mock = MockEmbedding(1536)\n\n            token_counter_mock = TokenCountingHandler(\n                tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode,\n                verbose=False,\n            )\n\n            callback_manager_mock = CallbackManager([token_counter_mock])\n\n            service_context_mock = ServiceContext.from_defaults(\n                llm_predictor=llm_predictor_mock,\n                embed_model=embedding_model_mock,\n                callback_manager=callback_manager_mock,\n            )\n\n            # Run the mock call first\n            await self.loop.run_in_executor(\n                None,\n                partial(\n                    GPTTreeIndex.from_documents,\n                    documents=documents,\n                    service_context=service_context_mock,\n                ),\n            )\n            total_usage_price = await self.usage_service.get_price(\n                token_counter_mock.total_llm_token_count,\n                \"turbo\",  # TODO Enable again when tree indexes are fixed\n            ) + await self.usage_service.get_price(\n                token_counter_mock.total_embedding_token_count, \"embedding\"\n            )\n            print(\"The total composition price is: \", total_usage_price)\n            if total_usage_price > MAX_DEEP_COMPOSE_PRICE:\n                raise ValueError(\n                    \"Doing this deep search would be prohibitively expensive. Please try a narrower search scope.\"\n                )\n\n            tree_index = await self.loop.run_in_executor(\n                None,\n                partial(\n                    GPTTreeIndex.from_documents,\n                    documents=documents,\n                    service_context=self.service_context,\n                    use_async=True,\n                ),\n            )\n\n            await self.usage_service.update_usage(\n                self.token_counter.total_llm_token_count, \"turbo\"\n            )\n            await self.usage_service.update_usage(\n                self.token_counter.total_embedding_token_count, \"embedding\"\n            )\n\n            # Now we have a list of tree indexes, we can compose them\n            if not name:\n                name = f\"{date.today().month}_{date.today().day}_composed_deep_index\"\n\n            # Save the composed index\n            tree_index.storage_context.persist(\n                persist_dir=EnvService.save_path() / \"indexes\" / str(user_id) / name\n            )\n\n            self.index_storage[user_id].queryable_index = tree_index\n\n            return total_usage_price\n        else:\n            documents = []\n            for _index in index_objects:\n                documents.extend(await self.index_to_docs(_index))\n\n            simple_index = await self.loop.run_in_executor(\n                None,\n                partial(\n                    GPTVectorStoreIndex.from_documents,\n                    documents=documents,\n                    service_context=service_context,\n                    use_async=True,\n                ),\n            )\n\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n\n            if not name:\n                name = f\"{date.today().month}_{date.today().day}_composed_index\"\n\n            # Save the composed index\n            simple_index.storage_context.persist(\n                persist_dir=EnvService.save_path() / \"indexes\" / str(user_id) / name\n            )\n            self.index_storage[user_id].queryable_index = simple_index\n\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.total_embedding_token_count, \"embedding\"\n                )\n            except:\n                price = \"Unknown\"\n\n            return price\n\n    async def backup_discord(\n        self, ctx: discord.ApplicationContext, user_api_key, message_limit\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        try:\n            channel_ids: List[int] = []\n            for c in ctx.guild.text_channels:\n                channel_ids.append(c.id)\n            document = await self.load_data(\n                channel_ids=channel_ids, limit=message_limit, oldest_first=False\n            )\n\n            index = await self.loop.run_in_executor(\n                None, partial(self.index_discord, document, service_context)\n            )\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n            try:\n                price = await self.usage_service.get_price(\n                    token_counter.total_embedding_token_count, \"embedding\"\n                )\n            except Exception:\n                traceback.print_exc()\n                price = \"Unknown\"\n            Path(EnvService.save_path() / \"indexes\" / str(ctx.guild.id)).mkdir(\n                parents=True, exist_ok=True\n            )\n            index.storage_context.persist(\n                persist_dir=EnvService.save_path()\n                / \"indexes\"\n                / str(ctx.guild.id)\n                / f\"{ctx.guild.name.replace(' ', '-')}_{date.today().month}_{date.today().day}\"\n            )\n\n            await ctx.respond(embed=EmbedStatics.get_index_set_success_embed(price))\n        except Exception as e:\n            await ctx.respond(embed=EmbedStatics.get_index_set_failure_embed((str(e))))\n            traceback.print_exc()\n\n    async def query(\n        self,\n        ctx: discord.ApplicationContext,\n        query: str,\n        response_mode,\n        nodes,\n        user_api_key,\n        child_branch_factor,\n        model=\"gpt-4-32k\",\n        multistep=False,\n    ):\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=model))\n\n        ctx_response = await ctx.respond(\n            embed=EmbedStatics.build_index_query_progress_embed(query)\n        )\n\n        try:\n            token_counter.reset_counts()\n            response = await self.loop.run_in_executor(\n                None,\n                partial(\n                    get_and_query,\n                    ctx.user.id,\n                    self.index_storage,\n                    query,\n                    response_mode,\n                    nodes,\n                    child_branch_factor,\n                    service_context=service_context,\n                    multistep=llm_predictor if multistep else None,\n                ),\n            )\n            print(\"The last token usage was \", token_counter.total_llm_token_count)\n            await self.usage_service.update_usage(\n                token_counter.total_llm_token_count,\n                await self.usage_service.get_cost_name(model),\n            )\n            await self.usage_service.update_usage(\n                token_counter.total_embedding_token_count, \"embedding\"\n            )\n\n            try:\n                total_price = round(\n                    await self.usage_service.get_price(\n                        token_counter.total_llm_token_count,\n                        await self.usage_service.get_cost_name(model),\n                    )\n                    + await self.usage_service.get_price(\n                        token_counter.total_embedding_token_count, \"embedding\"\n                    ),\n                    6,\n                )\n            except:\n                total_price = \"Unknown\"\n\n            query_response_message = f\"**Query:**\\n\\n`{query.strip()}`\\n\\n**Query response:**\\n\\n{response.response.strip()}\"\n            query_response_message = query_response_message.replace(\n                \"<|endofstatement|>\", \"\"\n            )\n            embed_pages = await self.paginate_embed(query_response_message)\n            paginator = pages.Paginator(\n                pages=embed_pages,\n                timeout=None,\n                author_check=False,\n            )\n            await ctx_response.edit(\n                embed=EmbedStatics.build_index_query_success_embed(query, total_price)\n            )\n            await paginator.respond(ctx.interaction)\n        except Exception:\n            traceback.print_exc()\n            await ctx_response.edit(\n                embed=EmbedStatics.get_index_query_failure_embed(\n                    \"Failed to send query. You may not have an index set, load an index with /index load\"\n                )\n            )\n\n    # Extracted functions from DiscordReader\n\n    async def read_channel(\n        self, channel_id: int, limit: Optional[int], oldest_first: bool\n    ) -> str:\n        \"\"\"Async read channel.\"\"\"\n\n        messages: List[discord.Message] = []\n\n        try:\n            channel = self.bot.get_channel(channel_id)\n            print(f\"Added {channel.name} from {channel.guild.name}\")\n            # only work for text channels for now\n            if not isinstance(channel, discord.TextChannel):\n                raise ValueError(\n                    f\"Channel {channel_id} is not a text channel. \"\n                    \"Only text channels are supported for now.\"\n                )\n            # thread_dict maps thread_id to thread\n            thread_dict = {}\n            for thread in channel.threads:\n                thread_dict[thread.id] = thread\n\n            async for msg in channel.history(limit=limit, oldest_first=oldest_first):\n                if msg.author.bot:\n                    pass\n                else:\n                    messages.append(msg)\n                    if msg.id in thread_dict:\n                        thread = thread_dict[msg.id]\n                        async for thread_msg in thread.history(\n                            limit=limit, oldest_first=oldest_first\n                        ):\n                            messages.append(thread_msg)\n        except Exception as e:\n            print(\"Encountered error: \" + str(e))\n\n        channel = self.bot.get_channel(channel_id)\n        msg_txt_list = [\n            f\"user:{m.author.display_name}, content:{m.content}\" for m in messages\n        ]\n\n        return (\"<|endofstatement|>\\n\\n\".join(msg_txt_list), channel.name)\n\n    async def load_data(\n        self,\n        channel_ids: List[int],\n        limit: Optional[int] = None,\n        oldest_first: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[int]): List of channel ids to read.\n            limit (Optional[int]): Maximum number of messages to read.\n            oldest_first (bool): Whether to read oldest messages first.\n                Defaults to `True`.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results: List[Document] = []\n        for channel_id in channel_ids:\n            if not isinstance(channel_id, int):\n                raise ValueError(\n                    f\"Channel id {channel_id} must be an integer, \"\n                    f\"not {type(channel_id)}.\"\n                )\n            (channel_content, channel_name) = await self.read_channel(\n                channel_id, limit=limit, oldest_first=oldest_first\n            )\n            results.append(\n                Document(\n                    text=channel_content, extra_info={\"channel_name\": channel_name}\n                )\n            )\n        return results\n\n    async def compose(self, ctx: discord.ApplicationContext, name, user_api_key):\n        # Send the ComposeModal\n        if not user_api_key:\n            os.environ[\"OPENAI_API_KEY\"] = self.openai_key\n        else:\n            os.environ[\"OPENAI_API_KEY\"] = user_api_key\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n        if not self.index_storage[ctx.user.id].has_indexes(ctx.user.id):\n            await ctx.respond(\n                embed=EmbedStatics.get_index_compose_failure_embed(\n                    \"You must have at least one index to compose.\"\n                )\n            )\n            return\n\n        await ctx.respond(\n            \"Select the index(es) to compose. You can compose multiple indexes together, you can also Deep Compose a single index.\",\n            view=ComposeModal(self, ctx.user.id, name),\n            ephemeral=True,\n        )\n\n\nclass ComposeModal(discord.ui.View):\n    def __init__(self, index_cog, user_id, name=None, deep=None) -> None:\n        super().__init__()\n        # Get the argument named \"user_key_db\" and save it as USER_KEY_DB\n        self.index_cog = index_cog\n        self.user_id = user_id\n        self.deep = deep\n\n        # Get all the indexes for the user\n        self.indexes = [\n            file\n            for file in os.listdir(\n                EnvService.find_shared_file(f\"indexes/{str(user_id)}/\")\n            )\n        ]\n\n        if index_cog.index_storage[user_id].has_search_indexes(user_id):\n            self.indexes.extend(\n                [\n                    file\n                    for file in os.listdir(\n                        EnvService.find_shared_file(f\"indexes/{str(user_id)}_search/\")\n                    )\n                ]\n            )\n        print(\"Found the indexes, they are \", self.indexes)\n\n        # Map everything into the short to long cache\n        for index in self.indexes:\n            if len(index) > 93:\n                index_name = index[:93] + \"-\" + str(random.randint(0000, 9999))\n                SHORT_TO_LONG_CACHE[index_name] = index\n            else:\n                SHORT_TO_LONG_CACHE[index[:99]] = index\n\n        # Reverse the SHORT_TO_LONG_CACHE index\n        LONG_TO_SHORT_CACHE = {v: k for k, v in SHORT_TO_LONG_CACHE.items()}\n\n        # A text entry field for the name of the composed index\n        self.name = name\n\n        # A discord UI select menu with all the indexes. Limited to 25 entries. For the label field in the SelectOption,\n        # cut it off at 100 characters to prevent the message from being too long\n        self.index_select = discord.ui.Select(\n            placeholder=\"Select index(es) to compose\",\n            options=[\n                discord.SelectOption(\n                    label=LONG_TO_SHORT_CACHE[index], value=LONG_TO_SHORT_CACHE[index]\n                )\n                for index in self.indexes\n            ][0:25],\n            max_values=len(self.indexes) if len(self.indexes) < 25 else 25,\n            min_values=1,\n        )\n        # Add the select menu to the modal\n        self.add_item(self.index_select)\n\n        # If we have more than 25 entries, add more Select fields as neccessary\n        self.extra_index_selects = []\n        if len(self.indexes) > 25:\n            for i in range(25, len(self.indexes), 25):\n                self.extra_index_selects.append(\n                    discord.ui.Select(\n                        placeholder=\"Select index(es) to compose\",\n                        options=[\n                            discord.SelectOption(\n                                label=LONG_TO_SHORT_CACHE[index],\n                                value=LONG_TO_SHORT_CACHE[index],\n                            )\n                            for index in self.indexes\n                        ][i : i + 25],\n                        max_values=len(self.indexes[i : i + 25]),\n                        min_values=1,\n                    )\n                )\n                self.add_item(self.extra_index_selects[-1])\n\n        # Add an input field for \"Deep\", a \"yes\" or \"no\" option, default no\n        self.deep_select = discord.ui.Select(\n            placeholder=\"Deep Compose\",\n            options=[\n                discord.SelectOption(label=\"Yes\", value=\"yes\"),\n                discord.SelectOption(label=\"No\", value=\"no\"),\n            ],\n            max_values=1,\n            min_values=1,\n        )\n        self.add_item(self.deep_select)\n\n        # Add a button to the modal called \"Compose\"\n        self.add_item(\n            discord.ui.Button(\n                label=\"Compose\", style=discord.ButtonStyle.green, custom_id=\"compose\"\n            )\n        )\n\n    # The callback for the button\n    async def interaction_check(self, interaction: discord.Interaction) -> bool:\n        # Check that the interaction was for custom_id \"compose\"\n        if interaction.data[\"custom_id\"] == \"compose\":\n            # Check that the user selected at least one index\n\n            # The total list of indexes is the union of the values of all the select menus\n            indexes = self.index_select.values + [\n                select.values[0] for select in self.extra_index_selects\n            ]\n\n            # Remap them from the SHORT_TO_LONG_CACHE\n            indexes = [SHORT_TO_LONG_CACHE[index] for index in indexes]\n\n            if len(indexes) < 1:\n                await interaction.response.send_message(\n                    embed=EmbedStatics.get_index_compose_failure_embed(\n                        \"You must select at least 1 index\"\n                    ),\n                    ephemeral=True,\n                )\n            else:\n                composing_message = await interaction.response.send_message(\n                    embed=EmbedStatics.get_index_compose_progress_embed(),\n                    ephemeral=True,\n                )\n                # Compose the indexes\n                try:\n                    price = await self.index_cog.compose_indexes(\n                        self.user_id,\n                        indexes,\n                        self.name,\n                        False\n                        if not self.deep_select.values\n                        or self.deep_select.values[0] == \"no\"\n                        else True,\n                    )\n                except ValueError as e:\n                    await interaction.followup.send(\n                        str(e), ephemeral=True, delete_after=180\n                    )\n                    return False\n                except Exception as e:\n                    traceback.print_exc()\n                    await interaction.followup.send(\n                        embed=EmbedStatics.get_index_compose_failure_embed(\n                            \"An error occurred while composing the indexes: \" + str(e)\n                        ),\n                        ephemeral=True,\n                        delete_after=180,\n                    )\n                    return False\n\n                await interaction.followup.send(\n                    embed=EmbedStatics.get_index_compose_success_embed(price),\n                    ephemeral=True,\n                    delete_after=180,\n                )\n\n                # Try to direct message the user that their composed index is ready\n                try:\n                    await self.index_cog.bot.get_user(self.user_id).send(\n                        f\"Your composed index is ready! You can load it with /index load now in the server.\"\n                    )\n                except discord.Forbidden:\n                    pass\n\n                try:\n                    composing_message: Interaction\n                    await composing_message.delete_original_response()\n\n                except:\n                    traceback.print_exc()\n        else:\n            await interaction.response.defer(ephemeral=True)\n",
          "",
          "",
          ""
        ],
        "test_patch": "",
        "patch_preview": "From 8ed079051ef8dde457577edcfea0b8654b6a6f8c Mon Sep 17 00:00:00 2001\nFrom: Paillat <me@paillat.dev>\nDate: Wed, 8 Nov 2023 12:13:05 +0100\nSubject: [PATCH 1/2] fix(cogs/code_interpreter_service_cog.py): use\n safe_ctx_respond utility function to respond to conversation start\n fix(cogs/image_service_cog.py): use safe_ctx_respond utility function to\n respond to errors fix(cogs/search_service_cog.py): use safe_ctx_respond\n utility function to respond to conversation start\n fix(cogs/text_service_cog."
      },
      "patch": {
        "length": 10010,
        "files_changed": 8,
        "lines_added": 67,
        "lines_deleted": 12,
        "net_change": 55,
        "changed_files": [
          {
            "file": "cogs/code_interpreter_service_cog.py",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "cogs/image_service_cog.py",
            "added": 5,
            "deleted": 7
          },
          {
            "file": "cogs/search_service_cog.py",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "cogs/text_service_cog.py",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "models/index_model.py",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "utils/safe_ctx_respond.py",
            "added": 52,
            "deleted": 0
          },
          {
            "file": "utils/__init__.py",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "utils/safe_ctx_respond.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [
        {
          "id": 1804440045,
          "body": "Just throwing in my 2 cents, this worked for me as a first time installer. Thank you @Paillat-dev ðŸ™",
          "user": "djpecot",
          "created_at": "2023-11-09T19:09:45Z",
          "html_url": "https://github.com/Kav-K/GPTDiscord/pull/390#issuecomment-1804440045"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 87,
        "total_lines": 17215,
        "total_bytes": 777293,
        "python_files": 37,
        "python_lines": 14389,
        "file_extensions": {
          ".txt": 24,
          ".yml": 1,
          ".toml": 1,
          ".png": 1,
          ".py": 37,
          ".md": 19,
          "": 2,
          ".env": 1,
          ".json": 1
        },
        "largest_files": [
          {
            "path": "models/index_model.py",
            "size": 65050,
            "lines": 1686,
            "extension": ".py"
          },
          {
            "path": "cogs/text_service_cog.py",
            "size": 63006,
            "lines": 1552,
            "extension": ".py"
          },
          {
            "path": "models/openai_model.py",
            "size": 57325,
            "lines": 1521,
            "extension": ".py"
          },
          {
            "path": "cogs/commands.py",
            "size": 41090,
            "lines": 1343,
            "extension": ".py"
          },
          {
            "path": "services/text_service.py",
            "size": 51919,
            "lines": 1202,
            "extension": ".py"
          },
          {
            "path": "img.png",
            "size": 140282,
            "lines": 1104,
            "extension": ".png"
          },
          {
            "path": "cogs/search_service_cog.py",
            "size": 27886,
            "lines": 739,
            "extension": ".py"
          },
          {
            "path": "services/environment_service.py",
            "size": 19565,
            "lines": 559,
            "extension": ".py"
          },
          {
            "path": "cogs/code_interpreter_service_cog.py",
            "size": 22987,
            "lines": 558,
            "extension": ".py"
          },
          {
            "path": "services/moderations_service.py",
            "size": 20409,
            "lines": 543,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 87,
        "files_changed_count": 8,
        "files_changed_ratio": 0.09195402298850575,
        "total_lines_in_repo": 17215,
        "lines_added": 67,
        "lines_deleted": 12,
        "net_lines_changed": 55,
        "lines_changed_ratio": 0.004589021202439733,
        "pr_body_length": 234,
        "commit_message_length": 38,
        "python_file_count": 37,
        "python_line_count": 14389
      }
    },
    {
      "tar_file_name": "LiuRoy#zhihu_spider#pull#1",
      "repo_name": "LiuRoy#zhihu_spider#pull#1",
      "success": true,
      "error": null,
      "commit": {
        "sha": "d78c5d42fc4ee2c7ad717dee6cdd405b7be6a8d7",
        "message": "Initial commit",
        "author": {
          "name": "Liu Ruoyu",
          "email": "lrysjtu@gmail.com",
          "date": "2016-03-16T05:39:17Z"
        },
        "html_url": "https://github.com/LiuRoy/zhihu_spider/commit/d78c5d42fc4ee2c7ad717dee6cdd405b7be6a8d7",
        "api_url": "https://api.github.com/repos/LiuRoy/zhihu_spider/commits/d78c5d42fc4ee2c7ad717dee6cdd405b7be6a8d7"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/LiuRoy#zhihu_spider#pull#1",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/LiuRoy#zhihu_spider#pull#1.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/LiuRoy#zhihu_spider#pull#1/source_code"
      },
      "pr": {
        "number": 1,
        "title": "çˆ¬åŽ»çŸ¥ä¹Žç”¨æˆ·æ•°æ®",
        "body": "1. ä¸èƒ½å®žçŽ°å¼‚æ­¥ä¸‹è½½å›¾ç‰‡\n2. ç”¨æˆ·å…³ç³»ä¸èƒ½å¢žé‡å‡å°‘\n",
        "state": "closed",
        "created_at": "2016-03-19T04:31:00Z",
        "updated_at": "2016-03-19T04:31:47Z",
        "merged_at": "2016-03-19T04:31:09Z",
        "html_url": "https://github.com/LiuRoy/zhihu_spider/pull/1",
        "user": "LiuRoy",
        "additions": 531,
        "deletions": 0,
        "changed_files": 11,
        "commits": 2
      },
      "swebench": {
        "instance_id": "LiuRoy_zhihu_spider-1",
        "repo": "/LiuRoy/zhihu_spider",
        "base_commit": "d78c5d42fc4ee2c7ad717dee6cdd405b7be6a8d7",
        "problem_statement": {},
        "edit_files": [
          ".gitignore",
          "requirements.txt",
          "zhihu/scrapy.cfg",
          "zhihu/zhihu/__init__.py",
          "zhihu/zhihu/items.py",
          "zhihu/zhihu/pipelines.py",
          "zhihu/zhihu/settings.py",
          "zhihu/zhihu/spiders/__init__.py",
          "zhihu/zhihu/spiders/test.py",
          ".gitignore",
          "requirements.txt",
          "zhihu/main.py",
          "zhihu/zhihu/constants.py",
          "zhihu/zhihu/items.py",
          "zhihu/zhihu/pipelines.py",
          "zhihu/zhihu/settings.py",
          "zhihu/zhihu/spiders/profile.py",
          "zhihu/zhihu/spiders/test.py"
        ],
        "oracle_files": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        "test_patch": "",
        "patch_preview": "From edec8f13a49eb9bdbbe341945e836f4a784a4f4e Mon Sep 17 00:00:00 2001\nFrom: \"ruoyu.liu\" <ruoyu.liu@ele.me>\nDate: Wed, 16 Mar 2016 17:06:02 +0800\nSubject: [PATCH 1/2] initialize project\n\n---\n .gitignore                      |  2 +\n requirements.txt                |  1 +\n zhihu/scrapy.cfg                | 11 +++++\n zhihu/zhihu/__init__.py         |  0\n zhihu/zhihu/items.py            | 14 ++++++\n zhihu/zhihu/pipelines.py        | 11 +++++\n zhihu/zhihu/settings.py         | 85 ++++++++++++++++++++"
      },
      "patch": {
        "length": 25029,
        "files_changed": 18,
        "lines_added": 567,
        "lines_deleted": 36,
        "net_change": 531,
        "changed_files": [
          {
            "file": ".gitignore",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "requirements.txt",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "zhihu/scrapy.cfg",
            "added": 11,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/__init__.py",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/items.py",
            "added": 14,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/pipelines.py",
            "added": 11,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/settings.py",
            "added": 85,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/spiders/__init__.py",
            "added": 4,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/spiders/test.py",
            "added": 25,
            "deleted": 0
          },
          {
            "file": ".gitignore",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "requirements.txt",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "zhihu/main.py",
            "added": 4,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/constants.py",
            "added": 35,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/items.py",
            "added": 41,
            "deleted": 5
          },
          {
            "file": "zhihu/zhihu/pipelines.py",
            "added": 85,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/settings.py",
            "added": 27,
            "deleted": 6
          },
          {
            "file": "zhihu/zhihu/spiders/profile.py",
            "added": 218,
            "deleted": 0
          },
          {
            "file": "zhihu/zhihu/spiders/test.py",
            "added": 0,
            "deleted": 25
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 1,
        "total_lines": 2,
        "total_bytes": 28,
        "python_files": 0,
        "python_lines": 0,
        "file_extensions": {
          ".md": 1
        },
        "largest_files": [
          {
            "path": "README.md",
            "size": 28,
            "lines": 2,
            "extension": ".md"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 1,
        "files_changed_count": 18,
        "files_changed_ratio": 18.0,
        "total_lines_in_repo": 2,
        "lines_added": 567,
        "lines_deleted": 36,
        "net_lines_changed": 531,
        "lines_changed_ratio": 301.5,
        "pr_body_length": 28,
        "commit_message_length": 14,
        "python_file_count": 0,
        "python_line_count": 0
      }
    },
    {
      "tar_file_name": "OPHoperHPO#image-background-remove-tool#pull#8",
      "repo_name": "OPHoperHPO#image-background-remove-tool#pull#8",
      "success": true,
      "error": null,
      "commit": {
        "sha": "fd15eb34a9d08df31f914bfe4256876956fce91d",
        "message": "Update README.md",
        "author": {
          "name": "Anodev",
          "email": "29470622+OPHoperHPO@users.noreply.github.com",
          "date": "2020-05-09T15:38:33Z"
        },
        "html_url": "https://github.com/OPHoperHPO/image-background-remove-tool/commit/fd15eb34a9d08df31f914bfe4256876956fce91d",
        "api_url": "https://api.github.com/repos/OPHoperHPO/image-background-remove-tool/commits/fd15eb34a9d08df31f914bfe4256876956fce91d"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/OPHoperHPO#image-background-remove-tool#pull#8",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/OPHoperHPO#image-background-remove-tool#pull#8.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/OPHoperHPO#image-background-remove-tool#pull#8/source_code"
      },
      "pr": {
        "number": 8,
        "title": "[release][3.1]",
        "body": "# Changes:\r\n1) The code base is completely rewritten\r\n2) Added support for a new neural network (U^2-Net)\r\n3) Added new models to the installer script\r\n4) Added selective installation of models in the installer script\r\n5) Improved manual\r\n6) Updated old code\r\n7) Added MODELS.md with information about used neural networks\r\n8) Added LICENSE\r\n9) Updated examples `.sh`s and `.bat`s\r\n10) Added some libs.\r\n11) Changes cli args.\r\n12) And many other minor changes and fixes.",
        "state": "closed",
        "created_at": "2020-06-07T18:17:32Z",
        "updated_at": "2020-06-07T18:27:35Z",
        "merged_at": null,
        "html_url": "https://github.com/OPHoperHPO/image-background-remove-tool/pull/8",
        "user": "OPHoperHPO",
        "additions": 1082,
        "deletions": 171,
        "changed_files": 31,
        "commits": 2
      },
      "swebench": {
        "instance_id": "OPHoperHPO_image-background-remove-tool-8",
        "repo": "/OPHoperHPO/image-background-remove-tool",
        "base_commit": "fd15eb34a9d08df31f914bfe4256876956fce91d",
        "problem_statement": {},
        "edit_files": [
          "README.md",
          "docs/MODELS.md",
          "docs/imgs/examples/1.png",
          "docs/imgs/examples/3.png",
          "docs/imgs/examples/mobile_net_model/1.png",
          "docs/imgs/examples/mobile_net_model/2.png",
          "docs/imgs/examples/mobile_net_model/3.png",
          "docs/imgs/examples/u2net/1.png",
          "docs/imgs/examples/u2net/2.png",
          "docs/imgs/examples/u2net/3.png",
          "docs/imgs/examples/u2netp/1.png",
          "docs/imgs/examples/u2netp/2.png",
          "docs/imgs/examples/u2netp/3.png",
          "docs/imgs/examples/xception_model/1.png",
          "docs/imgs/examples/2.png",
          "docs/imgs/examples/xception_model/3.png",
          "docs/imgs/input/1.jpg",
          "docs/imgs/input/3.jpg",
          "docs/imgs/output/1.png",
          "libs/networks.py",
          "libs/strings.py",
          "libs/u2net.py",
          "main.py",
          "requirements.txt",
          "run_dir_example.bat",
          "run_dir_example.sh",
          "run_file_example.bat",
          "run_file_example.sh",
          "run_file_example2.bat",
          "run_file_example2.sh",
          "tools/setup.py",
          "README.md"
        ],
        "oracle_files": [
          "# ðŸ¥§ Image Background Remove Tool ðŸ¥§\nA tool to remove a background from a portrait image using Tensorflow \n**********************************************************************\n### ðŸ“„ Description:\nThe program removes the background from portrait photos\n**********************************************************************\n### ðŸŽ† Differences from the [original script](https://github.com/susheelsk/image-background-removal):\n* __Tensorflow 2.0 compatible__\n* Added comments to the code.\n* Added ```tqdm``` progress bar.\n* __Removes background from image without loss of image resolution.__\n*  __The script now not only processes a single file, but can also process all images from the input folder and save them in the output folder with the same name.__\n* __New sample images.__\n**********************************************************************\n### ðŸ§· Dependencies:\n```\twget ``` **for setup.py!** \\\n```\ttensorflow, pillow, tqdm, numpy, scipy ``` **for main.py!**\n**********************************************************************\n### ðŸ· Setup for Windows:\n* Clone this repository\n* Install all the dependencies from **requirements.txt** via ```pip3 install -r requirements.txt```\n* Run ```./setup.bat``` \\\n_This setup.bat script loads the trained model._\n### ðŸ· Setup for Linux:\n* Clone repository: ```git clone https://github.com/OPHoperHPO/image-background-remove-tool```\n* Install all the dependencies from **requirements.txt**: ```pip3 install -r requirements.txt```\n* Run ```./setup.sh``` \\\n_This setup.sh script loads the trained model._\n**********************************************************************\n### ðŸ§° Running the script:\n * ```python3 main.py <input_path> <output_path> <model_type>```\n#### Explanation of variables:\n * `<input_path>` - path to input file or dir.\n * `<output_path>` - path to output file or dir.\n * ```<model_type>``` - can be ``` xception_model``` or ``` mobile_net_model```.\nThe first model has better quality, but it runs much slower than the second.\n > Note:  See sample scripts for more information on using the program.\n**********************************************************************\n### â³ TODO:\n```\n1) Add a graphical interface. (0% done)\n```\n### ðŸ’µ Support me:\n\nYou can thank me for developing this project, provide financial support for the development of new projects and buy me a small cup of coffee.â˜•\\\n  Just support me on these platforms: \\\n  â­[**Boosty**â­](https://boosty.to/anodev) \\\n  â­[**DonationAlerts**â­](https://www.donationalerts.com/r/anodev_development)\n### ðŸ˜€ Sample Result:\n* __More sample images in ``docs/imgs/input/`` and ``docs/imgs/examples/`` folders__\n* Input: \n* ![Input](https://github.com/OPHoperHPO/image-background-remove-tool/blob/master/docs/imgs/input/1.jpg \"Input\")\n\n* Output: \n* ![Output](https://github.com/OPHoperHPO/image-background-remove-tool/blob/master/docs/imgs/examples/1.png \"Output\")\n**********************************************************************\n\n",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "\"\"\"\n# Background remove tool.\n# Module Version: 3.0 [Public]\n# Rewrited by Anodev. (https://github.com/OPHoperHPO)\n# Original source code: https://github.com/susheelsk/image-background-removal\n\"\"\"\nimport os\nimport sys\nimport tqdm\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom io import BytesIO\nimport scipy.ndimage as ndi\n\n\nclass DeepLabModel(object):\n    \"\"\"Class to load deeplab model and run inference.\"\"\"\n\n    def __init__(self, model_type):\n        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n        # Environment init\n        self.INPUT_TENSOR_NAME = 'ImageTensor:0'\n        self.OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n        self.INPUT_SIZE = 513\n        self.FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n        # Start load process\n        self.graph = tf.Graph()\n        graph_def = tf.compat.v1.GraphDef.FromString(open(os.path.join(\"models\", model_type, \"model\",\n                                                                       \"frozen_inference_graph.pb\"),\n                                                          \"rb\").read())\n        if graph_def is None:\n            raise RuntimeError('Cannot find inference graph in tar archive.')\n        with self.graph.as_default():\n            tf.import_graph_def(graph_def, name='')\n        self.sess = tf.compat.v1.Session(graph=self.graph)\n\n    def run(self, image):\n        \"\"\"Image processing.\"\"\"\n        # Get image size\n        width, height = image.size\n        # Calculate scale value\n        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n        # Calculate future image size\n        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n        # Resize image\n        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n        # Send image to model\n        batch_seg_map = self.sess.run(\n            self.OUTPUT_TENSOR_NAME,\n            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n        # Get model output\n        seg_map = batch_seg_map[0]\n        # Get new image size and original image size\n        width, height = resized_image.size\n        width2, height2 = image.size\n        # Calculate scale\n        scale_w = width2 / width\n        scale_h = height2 / height\n        # Zoom numpy array for original image\n        seg_map = ndi.zoom(seg_map, (scale_h, scale_w))\n        return seg_map\n\n\ndef draw_segment(image, alpha_channel, file_name, output_path, wmode):\n    \"\"\"Postprocessing. Saves complete image.\"\"\"\n    # Get image size\n    width, height = image.size\n    # Create empty numpy array\n    dummy_img = np.zeros([height, width, 4], dtype=np.uint8)\n    # Create alpha layer from model output\n    for x in range(width):\n        for y in range(height):\n            color = alpha_channel[y, x]\n            (r, g, b) = image.getpixel((x, y))\n            if color == 0:\n                dummy_img[y, x, 3] = 0\n            else:\n                dummy_img[y, x] = [r, g, b, 255]\n    # Restore image object from numpy array\n    img = Image.fromarray(dummy_img)\n    if wmode == \"file\":\n        file_name_out = os.path.basename(output_path)\n        if file_name_out == '':\n            # Change file extension to png\n            file_name = os.path.splitext(file_name)[0] + '.png'\n            # Save image\n            img.save(os.path.join(output_path, file_name))\n        else:\n            try:\n                # Save image\n                img.save(output_path)\n            except OSError as e:\n                print(\"Error! \"\n                      \"Please indicate the correct extension of the final file, for example: .png\",\n                      \"Error: \", e)\n                exit(1)\n    else:\n        # Change file extension to png\n        file_name = os.path.splitext(file_name)[0] + '.png'\n        # Save image\n        img.save(os.path.join(output_path, file_name))\n\n\ndef run_visualization(model, file_path, file_name, output_path, wmode):\n    \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n    try:\n        jpeg_str = open(file_path, \"rb\").read()\n        image = Image.open(BytesIO(jpeg_str))\n    except IOError:\n        print('Cannot retrieve image. Please check file: ' + file_path)\n        return\n    seg_map = model.run(image)\n    image = image.convert('RGB')\n    draw_segment(image, seg_map, file_name, output_path, wmode)\n\n\ndef work_mode(file: str):\n    \"\"\"Determines the desired mode of operation\"\"\"\n    if os.path.isfile(file):  # Input is file\n        return \"file\"\n    if os.path.isdir(file):  # Input is dir\n        return \"dir\"\n    else:\n        return \"no\"\n\n\ndef cli():\n    \"\"\"CLI\"\"\"\n    # Parse arguments\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\n    if len(sys.argv) > 3:\n        model_str = sys.argv[3]\n    else:\n        model_str = \"xception_model\"  # If the model line is empty, select the model with better quality.\n\n    if not (model_str == \"xception_model\" or model_str == \"mobile_net_model\"):\n        print(\"Warning! You specified an invalid model type. \"\n              \"For image processing, the model with the best processing quality will be used.\")\n        model_str = \"xception_model\"  # If the model line is wrong, select the model with better quality.\n\n    if input_path is None or output_path is None:\n        print(\"[ERROR] Bad parameters! Please specify input path and output path.\")\n        exit(1)\n\n    model = DeepLabModel(model_str)  # Init model\n    wmode = work_mode(input_path)  # Get work mode\n    if wmode == \"file\":  # File work mode\n        run_visualization(model, input_path, os.path.basename(input_path), output_path, wmode)\n    elif wmode == \"dir\":  # Dir work mode\n        # Start process\n        files = os.listdir(input_path)\n        for file in tqdm.tqdm(files, ascii=True, desc='Remove Background', unit='|image|'):\n            file_path = os.path.join(input_path, file)\n            run_visualization(model, file_path, file, output_path, wmode)\n    else:\n        print(\"[ERROR] Bad input parameter! Please indicate the correct path to the file or folder.\")\n\n\nif __name__ == \"__main__\":\n    cli()\n",
          "tqdm==4.43.0\nscipy==1.4.1\nwget==3.2\nnumpy==1.18.2\nPillow==7.0.0\ntensorflow==2.1.0",
          "REM # All files in the input directory will be saved in the output folder under their names with the extension .png.\nREM # When replacing folder paths, ALWAYS add / (or \\ in the case of Windows) at the end!\npython3 main.py .\\docs\\imgs\\input\\ .\\docs\\imgs\\output\\ xception_model\n",
          "#!/bin/bash\n# All files in the input directory will be saved in the output folder under their names with the extension .png.\n# When replacing folder paths, ALWAYS add / (or \\ in the case of Windows) at the end!\npython3 main.py ./docs/imgs/input/ ./docs/imgs/output/ xception_model\n",
          "Rem The file will be saved along the path .\\docs\\imgs\\output\\1.png\npython3 main.py .\\docs\\imgs\\input\\1.jpg .\\docs\\imgs\\output\\1.png xception_model\n",
          "#!/bin/bash\n# The file will be saved along the path ./docs/imgs/output/1.png\npython3 main.py ./docs/imgs/input/1.jpg ./docs/imgs/output/1.png xception_model\n",
          "Rem The file will be saved under the original name with the extension .png along the path .\\docs\\imgs\\output\npython3 main.py .\\docs\\imgs\\input\\1.jpg .\\docs\\imgs\\output\\ xception_model\n",
          "#!/bin/bash\n# The file will be saved under the original name with the extension .png along the path ./docs/imgs/output/\npython3 main.py ./docs/imgs/input/1.jpg ./docs/imgs/output/ xception_model\n",
          "\"\"\"\n# Setup tool\n# Module Version: 1.0\n# Developed by Anodev. (https://github.com/OPHoperHPO)\n\"\"\"\n\n\nimport os\nimport wget\nimport tarfile\n\n\nclass Config:\n    \"\"\"Config object\"\"\"\n    # general\n    arc_name = \"model.tar.gz\"\n    # mobile_net_model\n    mn_url = \"http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz\"\n    mn_dir = os.path.join(\"..\", \"models\", \"mobile_net_model\")\n    # xception_model\n    xc_url = \"http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz\"\n    xc_dir = os.path.join(\"..\", \"models\", \"xception_model\")\n\n\ndef prepare():\n    \"\"\"Creates folders\"\"\"\n    print(\"Create folders\")\n    try:\n        if not os.path.exists(Config.mn_dir):\n            os.makedirs(Config.mn_dir)\n        if not os.path.exists(Config.xc_dir):\n            os.makedirs(Config.xc_dir)\n    except BaseException as e:\n        print(\"Error creating model folders! Error:\", e)\n        exit(1)\n    return True\n\n\ndef download():\n    \"\"\"Loads model archives\"\"\"\n    path_mn = os.path.join(Config.mn_dir, Config.arc_name)\n    path_xc = os.path.join(Config.xc_dir, Config.arc_name)\n    try:\n        if os.path.exists(path_mn):  # Clean old files\n            os.remove(path_mn)\n        if os.path.exists(path_xc):\n            os.remove(path_xc)\n        print(\"Start download model archives!\")\n        wget.download(Config.mn_url, out=path_mn)\n        wget.download(Config.xc_url, out=path_xc)\n        print(\"Download finished!\")\n    except BaseException as e:\n        print(\"Error download model archives! Error:\", e)\n        exit(1)\n    return True\n\n\ndef untar():\n    \"\"\"Untar archives\"\"\"\n    path_mn = os.path.join(Config.mn_dir, Config.arc_name)\n    path_xc = os.path.join(Config.xc_dir, Config.arc_name)\n    try:\n        print(\"Start unpacking\")\n        if path_mn.endswith(\"tar.gz\"):\n            tar = tarfile.open(path_mn, \"r:gz\")\n            tar.extractall(path=Config.mn_dir)\n            tar.close()\n            os.rename(os.path.join(Config.mn_dir, \"deeplabv3_mnv2_pascal_train_aug\"),\n                      os.path.join(Config.mn_dir, \"model\"))\n            print(\"Unpacking 1 archive finished!\")\n        if path_xc.endswith(\"tar.gz\"):\n            tar = tarfile.open(path_xc, \"r:gz\")\n            tar.extractall(path=Config.xc_dir)\n            tar.close()\n            os.rename(os.path.join(Config.xc_dir, \"deeplabv3_pascal_train_aug\"),\n                      os.path.join(Config.xc_dir, \"model\"))\n            print(\"Unpacking 2 archive finished!\")\n    except BaseException as e:\n        print(\"Unpacking error! Error:\", e)\n        exit(1)\n    return True\n\n\ndef clean():\n    \"\"\"Cleans temp files\"\"\"\n    path_mn = os.path.join(Config.mn_dir, Config.arc_name)\n    path_xc = os.path.join(Config.xc_dir, Config.arc_name)\n    try:\n        if os.path.exists(path_mn):  # Clean old files\n            os.remove(path_mn)\n        if os.path.exists(path_xc):\n            os.remove(path_xc)\n    except BaseException as e:\n        print(\"Cleaning error! Error:\", e)\n    return True\n\n\ndef setup():\n    \"\"\"Performs program setup before use\"\"\"\n    if prepare():\n        if download():\n            if untar():\n                if clean():\n                    print(\"Setup finished! :)\")\n                    exit(0)\n\n\nif __name__ == \"__main__\":\n    setup()\n",
          "# ðŸ¥§ Image Background Remove Tool ðŸ¥§\nA tool to remove a background from a portrait image using Tensorflow \n**********************************************************************\n### ðŸ“„ Description:\nThe program removes the background from portrait photos\n**********************************************************************\n### ðŸŽ† Differences from the [original script](https://github.com/susheelsk/image-background-removal):\n* __Tensorflow 2.0 compatible__\n* Added comments to the code.\n* Added ```tqdm``` progress bar.\n* __Removes background from image without loss of image resolution.__\n*  __The script now not only processes a single file, but can also process all images from the input folder and save them in the output folder with the same name.__\n* __New sample images.__\n**********************************************************************\n### ðŸ§· Dependencies:\n```\twget ``` **for setup.py!** \\\n```\ttensorflow, pillow, tqdm, numpy, scipy ``` **for main.py!**\n**********************************************************************\n### ðŸ· Setup for Windows:\n* Clone this repository\n* Install all the dependencies from **requirements.txt** via ```pip3 install -r requirements.txt```\n* Run ```./setup.bat``` \\\n_This setup.bat script loads the trained model._\n### ðŸ· Setup for Linux:\n* Clone repository: ```git clone https://github.com/OPHoperHPO/image-background-remove-tool```\n* Install all the dependencies from **requirements.txt**: ```pip3 install -r requirements.txt```\n* Run ```./setup.sh``` \\\n_This setup.sh script loads the trained model._\n**********************************************************************\n### ðŸ§° Running the script:\n * ```python3 main.py <input_path> <output_path> <model_type>```\n#### Explanation of variables:\n * `<input_path>` - path to input file or dir.\n * `<output_path>` - path to output file or dir.\n * ```<model_type>``` - can be ``` xception_model``` or ``` mobile_net_model```.\nThe first model has better quality, but it runs much slower than the second.\n > Note:  See sample scripts for more information on using the program.\n**********************************************************************\n### â³ TODO:\n```\n1) Add a graphical interface. (0% done)\n```\n### ðŸ’µ Support me:\n\nYou can thank me for developing this project, provide financial support for the development of new projects and buy me a small cup of coffee.â˜•\\\n  Just support me on these platforms: \\\n  â­[**Boosty**â­](https://boosty.to/anodev) \\\n  â­[**DonationAlerts**â­](https://www.donationalerts.com/r/anodev_development)\n### ðŸ˜€ Sample Result:\n* __More sample images in ``docs/imgs/input/`` and ``docs/imgs/examples/`` folders__\n* Input: \n* ![Input](https://github.com/OPHoperHPO/image-background-remove-tool/blob/master/docs/imgs/input/1.jpg \"Input\")\n\n* Output: \n* ![Output](https://github.com/OPHoperHPO/image-background-remove-tool/blob/master/docs/imgs/examples/1.png \"Output\")\n**********************************************************************\n\n"
        ],
        "test_patch": "",
        "patch_preview": "From 2a7991600e9d8d9ba0ee865e0d447f821484e9e2 Mon Sep 17 00:00:00 2001\nFrom: Anodev <29470622+ophoperhpo@users.noreply.github.com>\nDate: Mon, 8 Jun 2020 03:54:41 +1000\nSubject: [PATCH 1/2] This is the third release.\n\nChanges:\n1) The code base is completely rewritten\n2) Added support for a new neural network (U^2-Net)\n3) Added new models to the installer script\n4) Added selective installation of models in the installer script\n5) Improved manual\n6) Updated old code\n7) Added MODELS.md with informat"
      },
      "patch": {
        "length": 18093781,
        "files_changed": 32,
        "lines_added": 1085,
        "lines_deleted": 174,
        "net_change": 911,
        "changed_files": [
          {
            "file": "README.md",
            "added": 51,
            "deleted": 29
          },
          {
            "file": "docs/MODELS.md",
            "added": 5,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/1.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/3.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/mobile_net_model/1.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/mobile_net_model/2.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/mobile_net_model/3.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2net/1.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2net/2.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2net/3.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2netp/1.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2netp/2.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/u2netp/3.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/xception_model/1.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/xception_model/2.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/examples/xception_model/3.png",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/input/1.jpg",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "docs/imgs/input/3.jpg",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "libs/__init__.py",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "libs/networks.py",
            "added": 268,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [
        {
          "id": 640259763,
          "body": "Merged",
          "user": "OPHoperHPO",
          "created_at": "2020-06-07T18:27:23Z",
          "html_url": "https://github.com/OPHoperHPO/image-background-remove-tool/pull/8#issuecomment-640259763"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 20,
        "total_lines": 44282,
        "total_bytes": 6432213,
        "python_files": 2,
        "python_lines": 271,
        "file_extensions": {
          ".sh": 4,
          ".bat": 4,
          ".txt": 1,
          ".yml": 1,
          ".py": 2,
          ".md": 1,
          ".jpg": 3,
          ".png": 4
        },
        "largest_files": [
          {
            "path": "docs/imgs/examples/3.png",
            "size": 2004162,
            "lines": 13969,
            "extension": ".png"
          },
          {
            "path": "docs/imgs/examples/2.png",
            "size": 1851334,
            "lines": 12463,
            "extension": ".png"
          },
          {
            "path": "docs/imgs/examples/1.png",
            "size": 1044659,
            "lines": 6919,
            "extension": ".png"
          },
          {
            "path": "docs/imgs/input/2.jpg",
            "size": 712877,
            "lines": 4510,
            "extension": ".jpg"
          },
          {
            "path": "docs/imgs/input/3.jpg",
            "size": 514088,
            "lines": 4125,
            "extension": ".jpg"
          },
          {
            "path": "docs/imgs/input/1.jpg",
            "size": 291333,
            "lines": 1940,
            "extension": ".jpg"
          },
          {
            "path": "main.py",
            "size": 6061,
            "lines": 162,
            "extension": ".py"
          },
          {
            "path": "tools/setup.py",
            "size": 3286,
            "lines": 109,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 2998,
            "lines": 57,
            "extension": ".md"
          },
          {
            "path": "requirements.txt",
            "size": 81,
            "lines": 6,
            "extension": ".txt"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 20,
        "files_changed_count": 32,
        "files_changed_ratio": 1.6,
        "total_lines_in_repo": 44282,
        "lines_added": 1085,
        "lines_deleted": 174,
        "net_lines_changed": 911,
        "lines_changed_ratio": 0.028431416828508197,
        "pr_body_length": 470,
        "commit_message_length": 16,
        "python_file_count": 2,
        "python_line_count": 271
      }
    },
    {
      "tar_file_name": "OmkarPathak#pygorithm#pull#92",
      "repo_name": "OmkarPathak#pygorithm#pull#92",
      "success": true,
      "error": null,
      "commit": {
        "sha": "0be25396340fd002f08e024dc90a0bb9ed0f1792",
        "message": "Added Longest Common Subsequence Algorithm (#91)",
        "author": {
          "name": "SAYAN SANYAL",
          "email": "37104041+s-sanyal@users.noreply.github.com",
          "date": "2019-10-06T01:20:57Z"
        },
        "html_url": "https://github.com/OmkarPathak/pygorithm/commit/0be25396340fd002f08e024dc90a0bb9ed0f1792",
        "api_url": "https://api.github.com/repos/OmkarPathak/pygorithm/commits/0be25396340fd002f08e024dc90a0bb9ed0f1792"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/OmkarPathak#pygorithm#pull#92",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/OmkarPathak#pygorithm#pull#92.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/OmkarPathak#pygorithm#pull#92/source_code"
      },
      "pr": {
        "number": 92,
        "title": "Optimised and simplified lcs.py",
        "body": "- Using multiple assignments\r\n- Using multiplication instead of list comprehensions\r\n- Removed extraneous conditions from the loop, and skipping unnecessary initial conditions instead.\r\n",
        "state": "closed",
        "created_at": "2019-10-06T09:42:31Z",
        "updated_at": "2019-10-06T17:58:07Z",
        "merged_at": "2019-10-06T16:05:27Z",
        "html_url": "https://github.com/OmkarPathak/pygorithm/pull/92",
        "user": "rathod-sahaab",
        "additions": 5,
        "deletions": 8,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "OmkarPathak_pygorithm-92",
        "repo": "/OmkarPathak/pygorithm",
        "base_commit": "0be25396340fd002f08e024dc90a0bb9ed0f1792",
        "problem_statement": {},
        "edit_files": [
          "pygorithm/dynamic_programming/lcs.py"
        ],
        "oracle_files": [
          "\"\"\"\nA subsequence is a sequence that can be derived from another\nsequence by deleting some or no elements without changing the\norder of the remaining elements.\n\nFor example, 'abd' is a subsequence of 'abcd' whereas 'adc' is not\n\nGiven 2 strings containing lowercase english alphabets, find the length\nof the Longest Common Subsequence (L.C.S.).\n\nExample:\n    Input:  'abcdgh'\n            'aedfhr'\n    Output: 3\n\n    Explanation: The longest subsequence common to both the string is \"adh\"\n\nTime Complexity : O(M*N)\nSpace Complexity : O(M*N), where M and N are the lengths of the 1st and 2nd string\nrespectively.\n\n\"\"\"\n\n\ndef longest_common_subsequence(s1, s2):\n    \"\"\"\n    :param s1: string\n    :param s2: string\n    :return: int\n    \"\"\"\n    m = len(s1)\n    n = len(s2)\n\n    dp = [[0] * (n + 1) for i in range(m + 1)]\n    \"\"\"\n    dp[i][j] : contains length of LCS of s1[0..i-1] and s2[0..j-1]\n    \"\"\"\n\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0 or j == 0:\n                dp[i][j] = 0\n            elif s1[i - 1] == s2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n    return dp[m][n]\n"
        ],
        "test_patch": "",
        "patch_preview": "From b837ddfa790c882288ba05e26e2b356eb0ba7224 Mon Sep 17 00:00:00 2001\nFrom: rathod-sahaab <abhayonlyone@gmai.com>\nDate: Sun, 6 Oct 2019 15:05:02 +0530\nSubject: [PATCH] Optimised and simplified lcs.py\n\n---\n pygorithm/dynamic_programming/lcs.py | 13 +++++--------\n 1 file changed, 5 insertions(+), 8 deletions(-)\n\ndiff --git a/pygorithm/dynamic_programming/lcs.py b/pygorithm/dynamic_programming/lcs.py\nindex cdbba95..e30e6ee 100644\n--- a/pygorithm/dynamic_programming/lcs.py\n+++ b/pygorithm/dynamic_p"
      },
      "patch": {
        "length": 1265,
        "files_changed": 1,
        "lines_added": 5,
        "lines_deleted": 8,
        "net_change": -3,
        "changed_files": [
          {
            "file": "pygorithm/dynamic_programming/lcs.py",
            "added": 5,
            "deleted": 8
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 245,
        "total_lines": 37782,
        "total_bytes": 3727849,
        "python_files": 116,
        "python_lines": 13320,
        "file_extensions": {
          ".md": 2,
          "": 1,
          ".cfg": 1,
          ".in": 1,
          ".py": 116,
          ".rst": 13,
          ".png": 101,
          ".bat": 1,
          ".pyc": 9
        },
        "largest_files": [
          {
            "path": "tests/test_geometry.py",
            "size": 95476,
            "lines": 2187,
            "extension": ".py"
          },
          {
            "path": "tests/test_data_structure.py",
            "size": 35709,
            "lines": 790,
            "extension": ".py"
          },
          {
            "path": "imgs/test_geometry/test_extrapolated_intersection/out/av03_test_one_moving_one_stationary_along_path_intr_later.png",
            "size": 75501,
            "lines": 700,
            "extension": ".png"
          },
          {
            "path": "pygorithm/geometry/line2.py",
            "size": 23192,
            "lines": 645,
            "extension": ".py"
          },
          {
            "path": "pygorithm/data_structures/quadtree.py",
            "size": 23006,
            "lines": 562,
            "extension": ".py"
          },
          {
            "path": "pygorithm/geometry/polygon2.py",
            "size": 22715,
            "lines": 555,
            "extension": ".py"
          },
          {
            "path": "pygorithm/pathfinding/astar.py",
            "size": 24762,
            "lines": 498,
            "extension": ".py"
          },
          {
            "path": "imgs/test_geometry/test_extrapolated_intersection/out/au01_test_one_moving_one_stationary_along_path_intr_at_start.png",
            "size": 62088,
            "lines": 486,
            "extension": ".png"
          },
          {
            "path": "pygorithm/geometry/rect2.py",
            "size": 19378,
            "lines": 486,
            "extension": ".py"
          },
          {
            "path": "imgs/test_geometry/test_extrapolated_intersection/out/au04_test_one_moving_one_stationary_along_path_intr_at_start.png",
            "size": 55393,
            "lines": 470,
            "extension": ".png"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 245,
        "files_changed_count": 1,
        "files_changed_ratio": 0.004081632653061225,
        "total_lines_in_repo": 37782,
        "lines_added": 5,
        "lines_deleted": 8,
        "net_lines_changed": -3,
        "lines_changed_ratio": 0.0003440791911492245,
        "pr_body_length": 186,
        "commit_message_length": 48,
        "python_file_count": 116,
        "python_line_count": 13320
      }
    },
    {
      "tar_file_name": "Palashio#libra#pull#338",
      "repo_name": "Palashio#libra#pull#338",
      "success": true,
      "error": null,
      "commit": {
        "sha": "0f861c04caea1ed699c847a5bfec11eb89094325",
        "message": "Update README.md",
        "author": {
          "name": "Palash Shah",
          "email": "35114859+Palashio@users.noreply.github.com",
          "date": "2020-08-11T18:12:37Z"
        },
        "html_url": "https://github.com/Palashio/libra/commit/0f861c04caea1ed699c847a5bfec11eb89094325",
        "api_url": "https://api.github.com/repos/Palashio/libra/commits/0f861c04caea1ed699c847a5bfec11eb89094325"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Palashio#libra#pull#338",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Palashio#libra#pull#338.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Palashio#libra#pull#338/source_code"
      },
      "pr": {
        "number": 338,
        "title": "Upgraded content recommender system using networkX",
        "body": "---\r\nname: b) Content Based Recommender System\r\nabout: I updated it to be more adaptable to changes and work better with larger datasets\r\nlabels:content recommender system\r\n---\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps://github.com/Palashio/libra/CONTRIBUTING.md\r\n-->\r\n\r\nThis pull request closes #issue_number_here .\r\n\r\n**- What I did**\r\nupdated content based recommender\r\n\r\n**- How I did it**\r\nnetworkX graphs\r\n\r\n**- How to verify it**\r\ntests.py\r\n<!-- \r\nYou need a good justification for not \r\nincluding tests for the new feature you added. \r\n-->\r\n\r\n\r\n- [ ] I updated the docs.\r\n\r\nThis pull request adds a new feature to libra. @Palashio, could you please take a look at it?",
        "state": "closed",
        "created_at": "2020-08-12T11:36:31Z",
        "updated_at": "2020-08-14T09:45:37Z",
        "merged_at": null,
        "html_url": "https://github.com/Palashio/libra/pull/338",
        "user": "Vagif12",
        "additions": 171,
        "deletions": 150,
        "changed_files": 4,
        "commits": 4
      },
      "swebench": {
        "instance_id": "Palashio_libra-338",
        "repo": "/Palashio/libra",
        "base_commit": "0f861c04caea1ed699c847a5bfec11eb89094325",
        "problem_statement": {},
        "edit_files": [
          "libra/queries.py",
          "libra/query/recommender_systems.py",
          "requirements.txt",
          "tests/tests.py",
          "libra/query/recommender_systems.py",
          "libra/queries.py"
        ],
        "oracle_files": [
          "from libra.query.nlp_queries import (image_caption_query,\n                                     generate_caption, classify_text,\n                                     text_classification_query, get_summary,\n                                     summarization_query, generate_text, get_ner)\n\nfrom libra.query.classification_models import (k_means_clustering,\n                                               train_svm, nearest_neighbors,\n                                               decision_tree, train_xgboost)\nfrom libra.query.supplementaries import tune_helper, get_model_data, get_operators, get_accuracy, get_losses, \\\n    get_target, get_plots, get_vocab\n\nfrom libra.query.feedforward_nn import (regression_ann,\n                                        classification_ann,\n                                        convolutional)\nfrom libra.data_generation.grammartree import get_value_instruction\nfrom libra.data_generation.dataset_labelmatcher import (get_similar_column,\n                                                        get_similar_model)\nfrom libra.plotting.generate_plots import analyze\nfrom libra.query.recommender_systems import ContentBasedRecommender\nfrom libra.dashboard.auto_eda import edaDashboard\nfrom colorama import Fore, Style\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport os\nimport nltk\nimport ssl\nimport numpy as np\nfrom tkinter import filedialog\nfrom tkinter import *\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n# suppressing warnings for cleaner dialogue box\nwarnings.simplefilter(action='error', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# function imports from other files\ncurrLog = \"\"\ncounter = 0\n\n\n# clears log when needed - currently not being used\ndef clearLog():\n    global currLog\n    global counter\n\n    currLog = \"\"\n    counter = 0\n    print(\"\")\n\n\ndef logger(instruction, found=\"\"):\n    '''\n    logging function that creates hierarchial display of the processes of\n    different functions. Copied into different python files to maintain\n    global variables.\n\n    :param instruction: what you want to be displayed\n    :param found: if you want to display something found like target column\n\n    '''\n    global counter\n    if counter == 0:\n        print((\" \" * 2 * counter) + str(instruction) + str(found))\n    elif instruction == \"->\":\n        counter = counter - 1\n        print(Fore.BLUE + (\" \" * 2 * counter) +\n              str(instruction) + str(found) + (Style.RESET_ALL))\n    else:\n        print((\" \" * 2 * counter) + \"|- \" + str(instruction) + str(found))\n        if instruction == \"done...\":\n            print(\"\\n\" + \"\\n\")\n\n    counter += 1\n\n\ndef get_folder_dir(self):\n    dir_path = filedialog.askdirectory()\n    return dir_path\n\n\ndef get_file():\n    filename = filedialog.askopenfilename()\n    if os.path.isfile(filename):\n        return filename\n    else:\n        print('No file chosen')\n\n\nclass client:\n    '''\n    class to store all query information. Currently, old_models is not being used.\n    '''\n\n    def __init__(self, data):\n        '''\n        initializer for the client class, reads in dataset and records by calling logger function\n        :param data: represents the dataset that you're trying to read\n        :return: a completely initialized client class\n        '''\n        self.required_installations()\n        logger(\"Creating client object\")\n        self.dataset = data\n        logger(\"Reading in dataset\")\n        print(\"\")\n        self.models = {}\n        self.latest_model = None\n        clearLog()\n\n    def required_installations(self):\n        try:\n            _create_unverified_https_context = ssl._create_unverified_context\n        except AttributeError:\n            pass\n        else:\n            ssl._create_default_https_context = _create_unverified_https_context\n        nltk.download('punkt', quiet=True)\n        nltk.download('averaged_perceptron_tagger', quiet=True)\n        nltk.download('stopwords', quiet=True)\n\n    # param model_requested: string representation of the name of the model user seeks to retrieve\n    # returns models with a specific string - currently deprecated, should not be used.\n\n    def get_models(self, model_requested):\n        '''\n        returns models with a specific string - currently deprecated, should not be used.\n        :param model_requested: represents the name of the model from which you want to retrieve\n        :return: the model dictionary for your specific model\n        '''\n        logger(\"Getting model...\")\n        return get_similar_model(model_requested, self.models.keys())\n        clearLog()\n\n    # recommend items based on search criteria(for recommender systems only)\n\n    def recommend(self, search_term):\n        if self.latest_model == 'content_recommender':\n            model = self.models[self.latest_model]\n            return model.recommend(search_term)\n        else:\n            pass\n\n    # param modelKey: string representation of the model to make prediction\n    # param data: dataframe version of desired prediction set\n    def predict(self, data, model=None):\n        '''\n        Uses a model from the self.models dictionary to make a prediction. Also fits it based on the operator stored in the models dictionary.\n        :param data: is the data that you want to predict for using model\n        :param model: is the specific model you want to use to predict\n        :return: a prediction, most likely an array\n        '''\n        if model is None:\n            model = self.latest_model\n        if model == 'text_classification':\n            map_func = np.vectorize(lambda x: self.classify_text(x))\n            predictions = map_func(data)\n            return predictions\n        else:\n            modeldict = self.models[model]\n\n            if modeldict.get('preprocesser'):\n                data = modeldict['preprocesser'].transform(data)\n            predictions = modeldict['model'].predict(data)\n        clearLog()\n        return self.interpret(model, predictions)\n\n    def interpret(self, model, predictions):\n        '''\n        Function to interpret predictions from a neural network for the creation for graphs / user understanding.\n        :param model: is the model in the self.models dictionary that you want to use to interpret\n        :param predictions: the predictions that come out of the model\n        :return: a prediction, most likely an array\n        '''\n        modeldict = self.models[model]\n        if modeldict.get('interpreter'):\n            predictions = modeldict['interpreter'].inverse_transform(\n                predictions)\n        clearLog()\n        return predictions\n\n    # determines type of solution based of type of problem posed by query using a feed-forward neural network\n    # instruction should be the value of a column\n    def neural_network_query(self,\n                             instruction,\n                             callback=False,\n                             text=[],\n                             ca_threshold=None,\n                             drop=None,\n                             preprocess=True,\n                             test_size=0.2,\n                             random_state=49,\n                             epochs=50,\n                             generate_plots=True,\n                             callback_mode='min',\n                             maximizer=\"val_loss\",\n                             save_model=False,\n                             save_path=os.getcwd(),\n                             add_layer={}):\n        '''\n        Detects to see if it's a regression/classification problem and then calls the correct query.\n        :param hyperparameters: all of these are hyperparameters that're passed to the algorithm\n        :return: a model, plots, accuracy information all stored in the self.models dictionary\n        '''\n\n        data = pd.read_csv(self.dataset)\n\n        if preprocess:\n\n            remove = get_similar_column(\n                get_value_instruction(instruction), data)\n\n            if len(data) < 50:\n                raise Exception(\n                    \"Only datasets larger then 50 rows are supported for neural networks\")\n            if len(data[remove].value_counts()) <= 50:\n                callback_mode = 'max'\n                maximizer = \"val_accuracy\"\n                self.classification_query_ann(\n                    instruction,\n                    text=text,\n                    callback=callback,\n                    ca_threshold=ca_threshold,\n                    preprocess=preprocess,\n                    test_size=test_size,\n                    random_state=random_state,\n                    epochs=epochs,\n                    generate_plots=generate_plots,\n                    callback_mode=callback_mode,\n                    maximizer=maximizer,\n                    save_model=save_model,\n                    save_path=save_path,\n                    add_layer=add_layer)\n            else:\n                self.regression_query_ann(\n                    instruction,\n                    callback=callback,\n                    text=text,\n                    ca_threshold=ca_threshold,\n                    preprocess=preprocess,\n                    test_size=test_size,\n                    random_state=random_state,\n                    epochs=epochs,\n                    generate_plots=generate_plots,\n                    callback_mode=callback_mode,\n                    maximizer=maximizer,\n                    drop=drop,\n                    save_model=save_model,\n                    save_path=save_path,\n                    add_layer=add_layer)\n        clearLog()\n\n    # single regression query using a feed-forward neural network\n    # instruction should be the value of a column\n    def regression_query_ann(\n            self,\n            instruction,\n            callback=False,\n            text=[],\n            drop=None,\n            ca_threshold=None,\n            preprocess=True,\n            test_size=0.2,\n            random_state=49,\n            epochs=50,\n            generate_plots=True,\n            callback_mode='min',\n            maximizer=\"val_loss\",\n            save_model=True,\n            save_path=os.getcwd(),\n            add_layer={}):\n        '''\n        Calls the body of the regression_query__ code in the supplementaries.py file. Used for a regression feed forward neural network.\n        :param instruction: The objective that you want to model (str).\n        :param callback: Applying a set of functions/actions at various stages of training (bool).\n        :param ca_threshold: Threshold for multiple correspondence analysis (float).\n        :param dataset: The dataset being used in the regression feed forward neural network (str).\n        :param text: A list of columns to perform text embedding on.\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param epochs: Number of epochs (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param callback_mode: The type of callback (str).\n        :param maximizer: The accuracy/loss type to optimize (str).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['regression_ANN'] = regression_ann(\n            instruction=instruction,\n            callback=False,\n            ca_threshold=.25 if ca_threshold is None else ca_threshold,\n            dataset=self.dataset,\n            text=text,\n            drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            epochs=epochs,\n            generate_plots=generate_plots,\n            callback_mode=callback_mode,\n            maximizer=maximizer,\n            save_model=save_model,\n            save_path=save_path,\n            add_layer=add_layer)\n\n        self.latest_model = 'regression_ANN'\n        clearLog()\n\n    # query for multilabel classification query, does not work for\n    # binaryclassification, fits to feed-forward neural network\n\n    def classification_query_ann(\n            self,\n            instruction,\n            callback=False,\n            text=[],\n            ca_threshold=None,\n            preprocess=True,\n            callback_mode='min',\n            drop=None,\n            random_state=49,\n            test_size=0.2,\n            epochs=50,\n            generate_plots=True,\n            maximizer=\"val_loss\",\n            save_model=False,\n            save_path=os.getcwd(),\n            add_layer={}):\n        '''\n        Calls the body of the classification code in the supplementaries.py file. Used for a classification feed forward neural network.\n        :param instruction: The objective that you want to model (str).\n        :param callback: Applying a set of functions/actions at various stages of training (bool).\n        :param dataset: The dataset being used in the classification feed forward neural network (str).\n        :param text: A list of columns to perform text embedding on.\n        :param ca_threshold: Threshold for multiple correspondence analysis (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param epochs: Number of epochs (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param callback_mode: The type of callback (str).\n        :param maximizer: The accuracy/loss type to optimize (str).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['classification_ANN'] = classification_ann(\n            instruction=instruction,\n            callback=callback,\n            dataset=self.dataset,\n            text=text,\n            ca_threshold=.25 if ca_threshold is None else ca_threshold,\n            drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            epochs=epochs,\n            generate_plots=generate_plots,\n            callback_mode=callback_mode,\n            maximizer=maximizer,\n            save_model=save_model,\n            save_path=save_path,\n            add_layer=add_layer)\n\n        self.latest_model = 'classification_ANN'\n        clearLog()\n\n    # query to perform k-means clustering\n\n    def kmeans_clustering_query(self,\n                                preprocess=True,\n                                scatters=[],\n                                generate_plots=True,\n                                drop=None,\n                                clusters=None,\n                                base_clusters=2,\n                                verbose=0,\n                                n_init=10,\n                                max_iter=300,\n                                random_state=42,\n                                text=[]\n                                ):\n        '''\n        Calls the body of the kmeans_clustering code in the supplementaries.py file. Can be used without any preprocessing and/or parameters.\n\n        :param dataset: The dataset being used in the k-means clustering algorithm (str).\n        :param scatters: A list of various types of scatter plots.\n        :param preprocess: Preprocess the data (bool).\n        :param generate_plots: Generate plots for the model (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param base_clusters: Number of clusters to generate (int).\n        :param verbose: Printing the logging information (int).\n        :param n_init: Number of times the function will run with different seeds (int).\n        :param max_iter: Maximum number of iterations the function will run (int).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param text: A list of columns to perform text embedding on.\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['k_means_clustering'] = k_means_clustering(\n            dataset=self.dataset,\n            scatters=scatters,\n            clusters=clusters,\n            preprocess=preprocess,\n            generate_plots=generate_plots,\n            drop=drop,\n            base_clusters=base_clusters,\n            verbose=verbose,\n            n_init=n_init,\n            max_iter=max_iter,\n            random_state=random_state,\n            text=text\n        )\n\n        self.latest_model = 'k_means_clustering'\n        clearLog()\n\n    # query to create a support vector machine\n\n    def svm_query(self,\n                  instruction,\n                  test_size=0.2,\n                  text=[],\n                  random_state=49,\n                  kernel='linear',\n                  preprocess=True,\n                  drop=None,\n                  cross_val_size=0.3,\n                  degree=3,\n                  gamma='scale',\n                  coef0=0.0,\n                  max_iter=-1\n                  ):\n        '''\n        Calls the body of the svm query code in the supplementaries.py file. Used to create a classification support vector machine.\n        :param dataset: The dataset being used in the classification support vector machine (str).\n        :param text: A list of columns to perform text embedding on.\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param test_size: Size of the testing set (float).\n        :param kernel: The type of kernel to be used (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param cross_val_size: Cross-Validation score (float).\n        :param degree: Degree of the polynomial kernel function (int).\n        :param gamma: Kernel coefficient (int).\n        :param coef0: Significant term in 'poly' and 'sigmoid' kernel functions (float).\n        :param max_iter: Maximum number of iterations the function will run (int).\n\n\n        :return: a model and information to go along with it stored in the self.models dictionary.\n        '''\n\n        self.models['svm'] = train_svm(instruction,\n                                       dataset=self.dataset,\n                                       text=text,\n                                       random_state=random_state,\n                                       test_size=test_size,\n                                       kernel=kernel,\n                                       preprocess=preprocess,\n                                       drop=drop,\n                                       cross_val_size=cross_val_size,\n                                       degree=degree,\n                                       gamma=gamma,\n                                       coef0=coef0,\n                                       max_iter=max_iter\n                                       )\n\n        self.latest_model = 'svm'\n        clearLog()\n\n    # query to create a nearest neighbors model\n\n    def nearest_neighbor_query(\n            self,\n            instruction=None,\n            text=[],\n            random_state=49,\n            test_size=0.2,\n            preprocess=True,\n            drop=None,\n            min_neighbors=3,\n            max_neighbors=10,\n            leaf_size=30,\n            p=2,\n            algorithm='auto'\n    ):\n        '''\n        Calls the body of the nearest neighbor code in the supplementaries.py file. Used to create a nearest neighbor algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param test_size: Size of the testing set (float).\n        :param dataset: The dataset being used in the nearest neighbor algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param min_neighbors: Minimum number of neighbors (int).\n        :param max_neighbors: Maximum number of neighbors (int).\n        :param leaf_size: Leaf size passed to BallTree or KDTree (int).\n        :param p: Power parameter for the Minkowski metric (int).\n        :param algorithm: Algorithm used to compute the nearest neighbors (str).\n\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n        self.models['nearest_neighbor'] = nearest_neighbors(\n            instruction=instruction,\n            text=text,\n            random_state=random_state,\n            test_size=test_size,\n            dataset=self.dataset,\n            preprocess=preprocess,\n            drop=drop,\n            min_neighbors=min_neighbors,\n            max_neighbors=max_neighbors,\n            leaf_size=leaf_size,\n            p=p,\n            algorithm=algorithm\n        )\n\n        self.latest_model = 'nearest_neighbor'\n        clearLog()\n\n    # query to create a decision tree model\n\n    def decision_tree_query(\n            self,\n            instruction,\n            preprocess=True,\n            test_size=0.2,\n            text=[],\n            drop=None,\n            criterion='gini',\n            splitter='best',\n            max_depth=None,\n            min_samples_split=2,\n            min_samples_leaf=1,\n            min_weight_fraction_leaf=0.0,\n            max_leaf_nodes=None,\n            min_impurity_decrease=0.0,\n            ccp_alpha=0.0):\n        '''\n        Calls the body of the decision tree code in the classification_models.py file. Used to create a decision tree algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param dataset: The dataset being used in the decision tree algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param criterion: The function to measure the quality of a split (str).\n        :param splitter: The technique used to choose each node's split (str).\n        :param max_depth: The maximum depth of the tree (int).\n        :param min_samples_split: The minimum number of samples a node must have to split (int).\n        :param min_samples_leaf: The minimum number of samples a leaf node must have (int).\n        :param min_weight_fraction_leaf: The fraction of the input samples required to be at a leaf node (float).\n        :param max_leaf_nodes: Maximum number of leaf nodes (int).\n        :param min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal\n         to this value (float).\n        :param ccp_alpha: Complexity parameter used for Minimal Cost-Complexity Pruning (float).\n\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['decision_tree'] = decision_tree(\n            instruction=instruction,\n            text=text,\n            dataset=self.dataset,\n            preprocess=preprocess,\n            test_size=test_size,\n            drop=drop,\n            criterion=criterion,\n            splitter=splitter,\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_leaf_nodes=max_leaf_nodes,\n            min_impurity_decrease=min_impurity_decrease,\n            ccp_alpha=ccp_alpha)\n\n        self.latest_model = 'decision_tree'\n        clearLog()\n\n    def content_recommender_query(self, feature_names=[], n_recommendations=10, indexer='title'):\n        self.models['content_recommender'] = ContentBasedRecommender(\n            data=self.dataset,\n            feature_names=feature_names,\n            indexer=indexer)\n\n        self.latest_model = 'content_recommender'\n        clearLog()\n\n    # query to create a xgboost model\n\n    def xgboost_query(self,\n                      instruction,\n                      text=[],\n                      preprocess=True,\n                      test_size=0.2,\n                      drop=None,\n                      random_state=49,\n                      learning_rate=0.1,\n                      n_estimators=1000,\n                      max_depth=6,\n                      min_child_weight=1,\n                      gamma=0,\n                      subsample=0.8,\n                      colsample_bytree=0.8,\n                      verbosity=0,\n                      objective='binary:logistic'):\n\n        '''\n        Calls the body of the xgboost code in the classification_models.py file. Used to create a xgboost algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param dataset: The dataset being used in the xgboost algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param random_seed: Initialize a pseudo-random number generator (int).\n        :param learning_rate:  Boosting learning rate(float).\n        :param n_estimators: Number of gradient boosted trees. Equivalent to number of boosting rounds(in   ).\n        :param max_depth: Maximum tree depth for base learners(int).\n        :param min_child_weight: Minimum sum of instance weight(hessian) needed in a child(int).\n        :param gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree(int).\n        :param subsample: Subsample ratio of the training instance(float).\n        :param colsample_bytree: Subsample ratio of columns when constructing each tree(float).\n        :param objective: Specify the learning task and the corresponding learning objective or a custom\n        objective function to be used (string or callable).\n        :param scale_pos_weight: Balancing of positive and negative weights(float).\n        :param verbose: Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['xgboost'] = train_xgboost(instruction,\n                                               dataset=self.dataset,\n                                               text=[],\n                                               random_state=random_state,\n                                               preprocess=preprocess,\n                                               drop=drop,\n                                               learning_rate=learning_rate,\n                                               n_estimators=n_estimators,\n                                               max_depth=max_depth,\n                                               min_child_weight=min_child_weight,\n                                               gamma=gamma,\n                                               subsample=subsample,\n                                               verbosity=verbosity,\n                                               colsample_bytree=colsample_bytree,\n                                               objective=objective)\n\n        self.latest_model = 'xgboost'\n        clearLog()\n\n    # tunes a specific neural network based on the input model_to_tune\n\n    def tune(self,\n             model_to_tune=None,\n             max_layers=10,\n             min_layers=2,\n             min_dense=32,\n             max_dense=512,\n             executions_per_trial=3,\n             max_trials=1,\n             generate_plots=True,\n             activation='relu',\n             loss='categorical_crossentropy',\n             metrics='accuracy',\n             patience=1,\n             epochs=10,\n             objective='val_accuracy',\n             seed=42,\n             directory='my_dir',\n             verbose=0,\n             test_size=0.2\n             ):\n        '''\n        Calls the body of the tune identifier which is located in the supplementaries.py which then calls the appropriate tuner depending on the model\n        :param model_to_tune: The model to tune.\n        :param patience: Number of epochs with no improvement after which training will be stopped (int).\n        :param dataset: The dataset being used in the tuner (str).\n        :param models: The model dictionary (dict).\n        :param generate_plots: Generate plots for the model (bool).\n        :param max_layers: Maximum number of layers (int).\n        :param min_layers: Minimum number of layers (int).\n        :param min_dense: Minimum kernel density (int).\n        :param max_dense: Maximum kernel density (int).\n        :param executions_per_trial: Number of executions per trial (int).\n        :param max_trials: Maximum number of trials\n        :param activation: Activation Function (str).\n        :param loss: Loss Function (str).\n        :param metrics: Type of metrics function (str).\n        :param epochs: Number of epochs (int).\n        :param objective: Name of model metric to maximize/minimize (str).\n        :param seed: Random seed (int).\n        :param directory: Path to the directory (str).\n        :param verbose: Printing the logging information (int).\n        :param test_size: Size of the testing set (float).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        if model_to_tune is None:\n            model_to_tune = self.latest_model\n\n        self.models = tune_helper(\n            model_to_tune=model_to_tune,\n            patience=patience,\n            dataset=self.dataset,\n            models=self.models,\n            generate_plots=generate_plots,\n            max_layers=max_layers,\n            min_layers=min_layers,\n            min_dense=min_dense,\n            max_dense=max_dense,\n            executions_per_trial=executions_per_trial,\n            max_trials=max_trials,\n            activation=activation,\n            loss=loss,\n            metrics=metrics,\n            epochs=epochs,\n            objective=objective,\n            seed=seed,\n            directory=directory,\n            verbose=verbose,\n            test_size=test_size\n        )\n        clearLog()\n\n    # query to build a convolutional neural network\n\n    def convolutional_query(self,\n                            instruction=None,\n                            read_mode=None,\n                            verbose=0,\n                            preprocess=True,\n                            data_path=None,\n                            new_folders=True,\n                            image_column=None,\n                            test_size=0.2,\n                            augmentation=True,\n                            custom_arch=None,\n                            pretrained=None,\n                            epochs=10,\n                            height=None,\n                            width=None,\n                            show_feature_map=False):\n        '''\n        Calls the body of the convolutional neural network query which is located in the feedforward.py file\n        :param instruction: The objective that you want to model (str).\n        :param read_mode: The type of dataset (str).\n        :param verbose: Printing the logging information (int).\n        :param preprocess: Preprocess the data (bool).\n        :param data_path: Path to the dataset (str).\n        :param new_folders: Create new folders for the image during preprocessing (bool).\n        :param image_column: The column in the csv file where the filepaths for the images exist (str).\n        :param test_size: Ratio of dataset allotted to the testing data (float).\n        :param augmentation: Perform image data augmentation (bool).\n        :param epochs: Number of epochs (int).\n        :param height: Height of the input image (int).\n        :param width: Width of the input image (int).\n        :param show_feature_map: Displays feature map graphic (bool).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        # storing values in the model dictionary\n        self.models[\"convolutional_NN\"] = convolutional(\n            instruction=instruction,\n            read_mode=read_mode,\n            verbose=verbose,\n            preprocess=preprocess,\n            data_path=self.dataset,\n            new_folders=new_folders,\n            image_column=image_column,\n            training_ratio=1 - test_size,\n            augmentation=augmentation,\n            custom_arch=custom_arch,\n            pretrained=pretrained,\n            epochs=epochs,\n            height=height,\n            width=width)\n\n        if show_feature_map:\n            model = self.models[\"convolutional_NN\"][\"model\"]\n            X_test = self.models[\"convolutional_NN\"][\"data\"][\"test\"]\n\n            # Get first image in test images and format it\n            img = X_test[0][0]\n            img /= 255\n            successive_outputs = [layer.output for layer in model.layers[1:]]\n            visualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs)\n            successive_feature_maps = visualization_model.predict(img)\n\n            # Add main title to figure\n            firstPlot = True\n\n            # Include names of layers in plot\n            layer_names = [layer.name for layer in model.layers]\n            for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n                if len(feature_map.shape) == 4:\n\n                    # Plot Feature maps for the conv / maxpool layers, not the fully-connected layers\n                    n_features = feature_map.shape[-1]  # number of features in the feature map\n                    height = feature_map.shape[1]  # feature map shape (1, size, size, n_features)\n                    width = feature_map.shape[2]\n                    display_grid = np.zeros((height, width * n_features))\n\n                    # Format features appropriately\n                    for i in range(n_features):\n                        img = feature_map[0, :, :, i]\n                        img -= img.mean()\n                        img /= img.std()\n                        img *= 64\n                        img += 128\n                        img = np.clip(img, 0, 255).astype('uint8')\n\n                        # Tile each filter into a horizontal grid\n                        display_grid[:, i * width: (i + 1) * width] = img\n\n                    # Display the grid\n                    scale = 20. / n_features\n                    plt.figure(figsize=(scale * n_features, scale))\n                    if firstPlot:\n                        plt.title(f'Network Visualization\\n\\n{layer_name}')\n                        firstPlot = False\n                    else:\n                        plt.title(layer_name)\n                    plt.grid(False)\n                    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n                    plt.show()\n\n        self.latest_model = 'convolutional_NN'\n        clearLog()\n\n    # sentiment analysis prediction wrapper\n\n    def classify_text(self, text):\n        \"\"\"\n        Calls the body of the text_classification neural network query which is located in the nlp_queries.py file. This can only be called\n        if text_classification_query has been called previously.\n        :param text: The new text that you want to classify (str).\n        :return: a classification of text that you've provided\n        \"\"\"\n        clearLog()\n        return classify_text(self=self, text=text)\n\n    # sentiment analysis query\n    def text_classification_query(self, instruction, label_column=None, drop=None,\n                                  preprocess=True,\n                                  test_size=0.2,\n                                  random_state=49,\n                                  learning_rate=1e-2,\n                                  epochs=20,\n                                  monitor=\"val_loss\",\n                                  batch_size=32,\n                                  max_text_length=200,\n                                  max_features=20000,\n                                  generate_plots=True,\n                                  save_model=False,\n                                  save_path=os.getcwd()):\n        '''\n        Calls the body of the text_classification query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param learning_rate: The learning rate of the model (float).\n        :param epochs: Number of epochs (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        # storing values the model dictionary\n        self.models[\"text_classification\"] = text_classification_query(\n            self=self, instruction=instruction, label_column=label_column, drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            learning_rate=learning_rate,\n            monitor=monitor,\n            epochs=epochs,\n            batch_size=batch_size,\n            max_text_length=max_text_length,\n            max_features=max_features,\n            generate_plots=generate_plots,\n            save_model=save_model,\n            save_path=save_path)\n        self.latest_model = 'text_classification'\n        clearLog()\n\n    # summarization predict wrapper\n    def get_summary(self, text, num_beams=4, no_repeat_ngram_size=2, num_return_sequences=1,\n                    early_stopping=True):\n        '''\n        Calls the body of the summarizer which is located in the nlp_queries.py file\n        :param text: set of text that you want to summarize (str).\n        :param max_summary_length: Max generated summary length (int).\n        :param early_stopping: Sets early stopping (bool).\n        :param num_return_sequences: Sets the number of likely possibilities to output (int).\n        :param no_repeat_ngram_size: Sets the number of unrepeated consecutive n-grams (int).\n        :param num_beams: Sets number of possibilities to explore in beam search (int).\n        :return: a summary of text inputted in the text field.\n        '''\n        clearLog()\n        return get_summary(self=self, text=text, num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size\n                           , num_return_sequences=num_return_sequences, early_stopping=early_stopping)\n\n    # summarization query\n    def summarization_query(self, instruction, label_column=None, preprocess=True,\n                            drop=None,\n                            epochs=5,\n                            batch_size=32,\n                            learning_rate=3e-5,\n                            max_text_length=512,\n                            test_size=0.2,\n                            gpu=False,\n                            random_state=49,\n                            generate_plots=True,\n                            save_model=False,\n                            save_path=os.getcwd()):\n        '''\n        Calls the body of the summarization  query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param epochs: Number of epochs (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param learning_rate: The learning rate of the model (float).\n        :param max_text_length: The maximum length of the string of text (int).\n        :param test_size: Size of the testing set (float).\n        :param gpu: Use gpu for accelerated training (bool).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        self.models[\"summarization\"] = summarization_query(\n            self=self, instruction=instruction, preprocess=preprocess, label_column=label_column,\n            drop=drop,\n            epochs=epochs,\n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            max_text_length=max_text_length,\n            test_size=test_size,\n            gpu=gpu,\n            random_state=random_state,\n            generate_plots=generate_plots,\n            save_model=save_model,\n            save_path=save_path)\n\n        self.latest_model = 'summarization'\n        clearLog()\n\n    # image_caption generator wrapper\n\n    def generate_caption(self, image):\n        '''\n        Calls the body of the caption generator which is located in the nlp_queries.py file.\n        :param image: the image that you want to generate a caption for.\n        :return: a caption for the image inputted in the image field.\n        '''\n        caption = generate_caption(self=self, image=image)\n        clearLog()\n        return ' '.join(caption[:len(caption) - 1])\n\n    # image_caption prediction query\n    def image_caption_query(self, instruction, label_column=None,\n                            drop=None,\n                            epochs=10,\n                            preprocess=True,\n                            random_state=49,\n                            test_size=0.2,\n                            top_k=5000,\n                            batch_size=32,\n                            buffer_size=1000,\n                            embedding_dim=256,\n                            units=512,\n                            gpu=False,\n                            generate_plots=True,\n                            save_model_decoder=False,\n                            save_path_decoder=os.getcwd(),\n                            save_model_encoder=False,\n                            save_path_encoder=os.getcwd()):\n        '''\n        Calls the body of the image_caption query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param drop: A list of the dataset's columns to drop.\n        :param epochs: Number of epochs (int).\n        :param preprocess: Preprocess the data (bool).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param top_k:  Number of most frequent words in the vocab to be used in tokenization (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param buffer_size: The maximum number of elements to buffer (int).\n        :param embedding_dim: The dimension of the word embedding mapping (int).\n        :param units: The recurrent units in the decoder (int).\n        :param test_size: test size (int) .\n        :param gpu: Choose to use gpu (bool).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model_decoder: Save the decoder (bool).\n        :param save_path_decoder: Filepath of where to save the decoder (str).\n        :param save_model_encoder: Save the encoder (bool).\n        :param save_path_encoder: Filepath of where to save the encoder (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        self.models[\"image_caption\"] = image_caption_query(\n            self, instruction=instruction, label_column=label_column,\n            drop=drop,\n            epochs=epochs,\n            preprocess=preprocess,\n            random_state=random_state,\n            test_size=test_size,\n            top_k=top_k,\n            batch_size=batch_size,\n            buffer_size=buffer_size,\n            embedding_dim=embedding_dim,\n            units=units,\n            gpu=gpu,\n            generate_plots=generate_plots,\n            save_model_decoder=save_model_decoder,\n            save_path_decoder=save_path_decoder,\n            save_model_encoder=save_model_encoder,\n            save_path_encoder=save_path_encoder)\n        self.latest_model = 'image_caption'\n        clearLog()\n\n    def generate_text(self, instruction, file_data=True, prefix=None,\n                      max_length=512,\n                      top_k=50,\n                      top_p=0.9,\n                      return_sequences=2):\n        \"\"\"\n        :param instruction: objective you want to accomplish\n        :param prefix: a string that you want the generated text to begin with\n        :param max_length: the length of desired text you want (int)\n        :param top_k: number of most frequent words in the vocab to be used in tokenization (int).\n        :param top_p: p value between 0 and 1 (float)\n        :param return_sequences: how many different text sequences you want returned\n        :return: generated text\n        \"\"\"\n        self.models['text_generation'] = generate_text(self=self,\n                                                       instruction=instruction,\n                                                       file_data=file_data,\n                                                       prefix=prefix,\n                                                       max_length=max_length,\n                                                       top_k=top_k,\n                                                       top_p=top_p,\n                                                       return_sequences=return_sequences)\n\n        self.latest_model = 'text_generation'\n        clearLog()\n\n    # name entity recognition query\n    def named_entity_query(self, instruction):\n        \"\"\"\n        function to identify name entities\n        :param instruction: Used to get target column\n        :return: dictionary object with detected name-entities\n        \"\"\"\n        self.models[\"named_entity_recognition\"] = get_ner(self, instruction=instruction)\n        self.latest_model = \"named_entity_recognition\"\n        clearLog()\n\n    # shows the names of plots associated with a specific model\n    def plot_names(self, model=None):\n        '''\n        Function to get names of plots given the name of the model you want\n        :param model: the model that you want to get the plots for\n        '''\n        if model is None:\n            model = self.latest_model\n        print(self.models[model]['plots'].keys())\n\n        # shows names of models in model dictionary\n\n        clearLog()\n\n    def model(self, model=None):\n        '''\n        Function that either returns the latest model or one specified and its information as a dictionary.\n        :param model: is the model key that you want to use.\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return self.models[model]\n\n    # shows the keys in the models dictionary\n\n    def info(self, model=None):\n        '''\n        Function that retrieves the model_data; all the information in self.models for that model\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        return get_model_data(self, model)\n        clearLog()\n\n    # returns all operators applicable to the client's models dictionary\n    def operators(self, model=None):\n        '''\n        Function that retrieves all of the operators; pipelines that were used to model the dataset\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        get_operators(self, model)\n        clearLog()\n\n    # show accuracy scores for client's model\n    def accuracy(self, model=None):\n        '''\n        Function that retrieves all of the accuracies in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_accuracy(self, model)\n\n    # show losses for client's model\n    def losses(self, model=None):\n        '''\n        Function that retrieves all of the losses in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_losses(self, model)\n\n    # return client model's target\n    def target(self, model=None):\n        '''\n        Function that retrieves all of the targets in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        returns target variable of model used in client instance\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_target(self, model)\n\n    # return NLP model's vocabulary\n    def vocab(self, model=None):\n        '''\n        Function that retrieves the NLP models vocabulary.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_vocab(self, model)\n\n    # plotting for client\n    def plots(self, model=None, plot=None, save=False):\n        '''\n        Function that retrieves all of plots in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        :param plot: plot specified during the client session to be procured\n        :param save: option to save plots after client session is done (default is false, or\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        get_plots(self, model, plot, save)\n\n    # shows analysis of the model\n    def analyze(self, model=None, save=True, save_model=False):\n        '''\n        Function that retrieves all of plots in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentailly the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        analyze(self, model, save, save_model)\n\n    def dashboard(self):\n        dash = edaDashboard(self.dataset)\n        dash.dashboard()\n",
          "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom libra.preprocessing.data_reader import DataReader\nfrom colorama import Style\n\ncounter = 0\ncurrLog = \"\"\n\ndef clearLog():\n    global counter\n    global currLog\n\n    currLog = \"\"\n    counter = 0\n\n\n# logging function that creates hierarchial display of the processes of\n# different functions. Copied into different python files to maintain\n# global variable parallels\ndef logger(instruction, found=\"\"):\n    '''\n    logging function that creates hierarchial display of the processes of\n    different functions. Copied into different python files to maintain\n    global variables.\n\n    :param instruction: what you want to be displayed\n    :param found: if you want to display something found like target column\n    '''\n    \n    global counter\n    if counter == 0:\n        print((\" \" * 2 * counter) + str(instruction) + str(found))\n    elif instruction == \"->\":\n        counter = counter - 1\n        print(Fore.BLUE + (\" \" * 2 * counter) +\n              str(instruction) + str(found) + (Style.RESET_ALL))\n    else:\n        print((\" \" * 2 * counter) + \"|- \" + str(instruction) + str(found))\n        if instruction == \"done...\":\n            print(\"\\n\" + \"\\n\")\n\n    counter += 1\n\n\n# Cleaning/ preprocessing function for content based recommender engine\n# basically tokenizes all the feature examples\ndef clean_data(x):\n    if isinstance(x, list):\n        return np.array([str.lower(i.replace(\" \", \"\")) for i in x if not x.isdigit()])\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''\n\n\n# Cosine similarity matrix creator\ndef matrix_maker(data,indexer='title',feature_names=[]):\n    try:\n        assert(isinstance(feature_names, list))\n    except:\n        logger('Error! Feature names must be of type list!')\n        exit()\n\n\n    # function to combine all the tokenized features values for the\n    # cosine matrix to calculate similarities from (the recommender 'soup')\n    def create_soup(x):\n        soup = []\n        for feature in np.array(feature_names):\n            f = ''.join(x[feature])\n            soup.append(f)\n        return ' '.join(soup)\n\n    data = pd.read_csv(data)\n    data = data.copy()\n    #data = data[['title']].apply(pd.Series.unique).join(data[feature_names])\n    try:\n        assert indexer in data.columns\n    except:\n        logger('Error! the indexer passed named:  ' +  str(indexer) + 'is not in dataset!')\n        exit()\n    for f in feature_names:\n            data[f] = data[f].apply(clean_data)\n    logger('Cleaning Data..')\n\n    data['text'] = data.apply(create_soup,axis=1)\n    logger('Getting similarities')\n\n    # Create a CountVectorizer, fit to data 'soup' and get similarities\n    con = CountVectorizer()\n    item_matrix = con.fit_transform(data['text'])\n    cosine_similarities = cosine_similarity(item_matrix,item_matrix)\n    similarities = {}\n\n    # Loop through similarities and get top 50, return similarities\n    for i in range(len(cosine_similarities)):\n        similar_indices = cosine_similarities[i].argsort()[:-50:-1] \n        similarities[data[indexer].iloc[i]] = [(cosine_similarities[i][x], data[indexer][x], '') for x in similar_indices][1:]\n    return similarities\n\nclass ContentBasedRecommender:\n    '''\n    ---------- Content Based Recommender System Class --------\n\n    This is the base class for the content based recommender. The constructor takes in 5 arguements:\n\n    data: the dataset to work on\n    feature_names = a list of the names of features you would like to use to get recommendations\n    by default this will be all the categorical columns\n    n_recommendations: the number of recommendations to return\n    indexer = the name of the columns you want to get recommendations from\n    by default this will be the first categorical columns(excluding the id)\n\n    Example:\n    newClient.content_recommender_query(feature_names=['genre','actors','writer','plot'],indexer='title')\n\n    Methods:\n\n    recommend: the recommendations function. Returns recommendations based on search term passed\n    parameters:\n    search_term: string of the item you want to get recommendations from\n    returns:\n    result: a pandas DataFrame of the top n recommendations\n\n    Example:\n    c = client('path to file')\n    c.content_recommender_query(feature_names=['genre','plot','director','actors','writer'])\n    recommendations = c.recommend('Coco')\n\n\n    _get_message: gets the results of the similarity and creates a Dataframe\n    of the resultd and their correlation.\n\n    '''\n\n\n    def __init__(self, data,feature_names=[],indexer='',n_recommendations=10):\n        # If feature names is blank, then it get all categorical objects,\n        # removes the id and used them to recommend items,setting the indexer\n        # as the first element of the feature_names\n        dataReader = DataReader(data)\n        data1 = dataReader.data_generator()\n        self.data1 = data1.copy()\n\n        if feature_names == []:\n            catnames = list(self.data1.select_dtypes('object').columns)\n            for i in catnames:\n                if 'id' in i.lower():\n                    v = catnames.index(i)\n                    del catnames[v]\n            self.feature_names = catnames[1:]\n            self.indexer = catnames[0]\n        else:\n            #Â Initialise default variables\n            self.indexer = indexer\n            self.feature_names = feature_names\n\n        # call the matrix_maker function created above\n        self.matrix_similar = matrix_maker(data,self.indexer,self.feature_names)\n        self.n_recommendations=n_recommendations\n\n    def _get_message(self, item, recom_items):\n        # Get the recommendations and put them into a DataFrame\n        logger(\"Complete! Stored recommendation DataFrame under the 'recommendations' key\")\n        clearLog()\n\n        rec_items = len(recom_items)\n        # List for recommended items and their respective correlations\n        recommended_items = []\n        recommended_corr = []\n\n        # Loop through each item,append to the correct list, and returnt a dict key\n        # of the DataFrame,n_recommendations, feature_names and indexer\n        for i in range(rec_items):\n            recommended_items.append(recom_items[i][1])\n            recommended_corr.append(round(recom_items[i][0], 3))\n\n        \n        df = pd.DataFrame(pd.Series(np.array(recommended_items)),columns=['Recommendations'])\n        df.insert(1,'Correlation',recommended_corr)\n\n\n        return {\n            'recommendations': df,\n            'indexer': self.indexer,\n            'n_recommendations': self.n_recommendations,\n            'feature_names': self.feature_names\n        }\n\n        \n    # recommendation function\n    def recommend(self, s_name):\n        # Get item to find recommendations for\n        item = s_name\n        # Get number of items to recommend\n        number_items = self.n_recommendations\n        # Get the number of items most similars from matrix similarities\n        recom_item = self.matrix_similar[item][:number_items]\n        # return each item\n\n        return self._get_message(item=item, recom_items=recom_item)\n",
          "colorama\ntransformers==3.0.2\ntensorflow==2.2.0\nkeras==2.4.3\nnumpy\npandas\nsklearn\npprint\nmatplotlib\ntabulate\ntextblob\nseaborn\nkeras-tuner\njellyfish\nspacy\nautocorrect\npillow\nprince\nopencv-python\nnltk\ndownload\naltair==4.1.0\nstreamlit==0.64.0\nxgboost\n",
          "from libra import client\n\nimport unittest\n\n\ndef make_orderer():\n    order = {}\n\n    def ordered(f):\n        order[f.__name__] = len(order)\n        return f\n\n    def compare(a, b):\n        return [1, -1][order[a] < order[b]]\n\n    return ordered, compare\n\n\nordered, compare = make_orderer()\nunittest.defaultTestLoader.sortTestMethodsUsing = compare\n\n\nclass TestQueries(unittest.TestCase):\n    newClient = client('tools/data/structured_data/housing.csv')\n    \"\"\"\n    TEST QUERIES\n    \n    Tests some queries in queries.py\n    \"\"\"\n\n    # Tests whether regression_ann_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_regression_ann(self):\n        self.newClient.regression_query_ann('predict median house value', epochs=3)\n        self.assertTrue('regression_ANN' in self.newClient.models)\n        del self.newClient.models['regression_ANN']\n\n    # Tests whether classification_ann_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_classification_ann(self):\n        self.newClient.classification_query_ann('predict ocean proximity', epochs=3)\n        self.assertTrue('classification_ANN' in self.newClient.models)\n        del self.newClient.models['classification_ANN']\n\n    # Tests whether neural_network_query uses the correct model\n    @ordered\n    def test_nn_query(self):\n        # see if properly chooses regression with a numeric target column\n        self.newClient.neural_network_query('predict median house value', epochs=3)\n        self.assertTrue('regression_ANN' in self.newClient.models)\n\n        # see if properly chooses classification with a categorical target column\n        self.newClient.neural_network_query('predict ocean proximity', epochs=3)\n        self.assertTrue('classification_ANN' in self.newClient.models)\n\n    '''\n    @ordered\n    def test_convolutional_query(self):\n        client_image = client(\"tools/data/image_data/character_dataset_mini\")\n        client_image.convolutional_query(\"predict character\", epochs=2)\n        self.assertTrue('convolutional_NN' in client_image.models)\n    '''\n\n    @ordered\n    def test_convolutional_query_customarch(self):\n        data_path = \"tools/data/image_data/character_dataset_mini_preprocessed\"\n        client_image_customarch = client(data_path)\n        custom_arch_path = \"tools/data/custom_model_config/custom_CNN.json\"\n\n        client_image_customarch.convolutional_query(\"predict character\", data_path=data_path,\n                                                    custom_arch=custom_arch_path, preprocess=False, epochs=2)\n        self.assertTrue('convolutional_NN' in client_image_customarch.models)\n\n    @ordered\n    def test_convolutional_query_pretrained(self):\n        client_image = client(\"tools/data/image_data/character_dataset_mini\")\n        client_image.convolutional_query(\n            \"predict character\",\n            pretrained={\n                'arch': 'vggnet19',\n                'weights': 'imagenet'\n            },\n            epochs=2)\n        self.assertTrue('convolutional_NN' in client_image.models)\n\n    # Tests whether decision_tree_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_decision_tree(self):\n        self.newClient.decision_tree_query('predict ocean proximity')\n        self.assertTrue('decision_tree' in self.newClient.models)\n\n    # Tests whether svm_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_svm(self):\n        self.newClient.svm_query('predict ocean proximity')\n        self.assertTrue('svm' in self.newClient.models)\n\n    # Tests whether nearest_neighbor_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_nearest_neighbors(self):\n        self.newClient.nearest_neighbor_query('predict ocean proximity')\n        self.assertTrue('nearest_neighbor' in self.newClient.models)\n\n    # Tests whether kmeans_clustering_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_kmeans(self):\n        self.newClient.kmeans_clustering_query(clusters=4)\n        self.assertTrue('k_means_clustering' in self.newClient.models)\n\n    # Tests whether xgboost_query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_xgboost(self):\n        self.newClient.xgboost_query('predict ocean proximity')\n        self.assertTrue('xgboost' in self.newClient.models)\n\n    # Tests whether summarization works without errors, and creates a key in models dictionary\n    @ordered\n    def test_summarization(self):\n        x = client(\"tools/data/nlp_data/miniDocumentSummarization.csv\")\n        x.summarization_query(\"summarize text\", epochs=1)\n\n    # Tests whether image captioning works without errors, and creates a key in models dictionary\n    @ordered\n    def test_captioning(self):\n        x = client(\"tools/data/nlp_data/image-caption.csv\")\n        x.image_caption_query(\"get captions\", epochs=1)\n\n    # Tests whether text classification works without errors, and creates a key in models dictionary\n    @ordered\n    def test_text_classification(self):\n        x = client(\"tools/data/nlp_data/smallSentimentAnalysis.csv\")\n        x.text_classification_query(\"get captions\", epochs=1)\n\n    # Tests whether name entity recognition query works without errors, and creates a key in models dictionary\n    @ordered\n    def test_get_ner(self):\n        x = client(\"tools/data/nlp_data/miniDocumentSummarization.csv\")\n        x.named_entity_query(\"get ner from text\")\n        self.assertTrue('named_entity_recognition' in x.models)\n        del x.models['named_entity_recognition']\n\n    @ordered\n    def test_text_generation(self):\n        x = client(\"tools/data/nlp_data/shakespeare.txt\")\n        x.generate_text(instruction=\"generate text\")\n        self.assertTrue('text_generation' in x.models)\n\n    # Test whether content based recommender works without error, and creates a key in models dictionary\n    @ordered\n    def test_content_recommender(self):\n        x = client('tools/data/recommender_systems_data/disney_plus_shows.csv')\n        x.content_recommender_query()\n        assert ('recommendations' in x.recommend('Coco'))\n\n    \"\"\"\n    TEST ANALYZE() FUNCTION\n    \n    Tests all branches of .analyze() function in generate_plots\n    \"\"\"\n\n    # Tests analyze() function for k_means_clustering\n    @ordered\n    def test_analyze_kmeans(self):\n        self.newClient.analyze(model='k_means_clustering')\n        self.assertTrue('n_centers' in self.newClient.models['k_means_clustering'])\n        self.assertTrue('centroids' in self.newClient.models['k_means_clustering'])\n        self.assertTrue('inertia' in self.newClient.models['k_means_clustering'])\n\n    # Tests analyze() function on regression_ANN\n    @ordered\n    def test_analyze_regression(self):\n        self.newClient.analyze(model='regression_ANN')\n        self.assertTrue('MSE' in self.newClient.models['regression_ANN'])\n        self.assertTrue('MAE' in self.newClient.models['regression_ANN'])\n\n    # Tests analyze() function on classification_ANN\n    @ordered\n    def test_analyze_classification(self):\n        self.newClient.analyze(model='classification_ANN')\n        self.assertTrue('plots' in self.newClient.models['classification_ANN'])\n        self.assertTrue('roc_curve' in self.newClient.models['classification_ANN']['plots'])\n        self.assertTrue('confusion_matrix' in self.newClient.models['classification_ANN']['plots'])\n\n        self.assertTrue('scores' in self.newClient.models['classification_ANN'])\n        self.assertTrue('recall_score' in self.newClient.models['classification_ANN']['scores'])\n        self.assertTrue('precision_score' in self.newClient.models['classification_ANN']['scores'])\n        self.assertTrue('f1_score' in self.newClient.models['classification_ANN']['scores'])\n\n    # Tests analyze() function for classifier models\n    @ordered\n    def test_analyze_sklearn_classifiers(self):\n        for mod in ['svm', 'nearest_neighbor', 'decision_tree', 'xgboost']:\n            self.newClient.analyze(model=mod)\n            modeldict = self.newClient.models[mod]\n\n            self.assertTrue('plots' in modeldict)\n            self.assertTrue('roc_curve' in modeldict['plots'])\n            self.assertTrue('confusion_matrix' in modeldict['plots'])\n\n            self.assertTrue('scores' in modeldict)\n            self.assertTrue('recall_score' in modeldict['scores'])\n            self.assertTrue('precision_score' in modeldict['scores'])\n            self.assertTrue('f1_score' in modeldict['scores'])\n\n    # Tests invalid model input\n    @ordered\n    def test_invalid_model(self):\n        with self.assertRaises(NameError):\n            self.newClient.analyze(model='I dont exist')\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
          "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom libra.preprocessing.data_reader import DataReader\nfrom colorama import Style\n\ncounter = 0\ncurrLog = \"\"\n\ndef clearLog():\n    global counter\n    global currLog\n\n    currLog = \"\"\n    counter = 0\n\n\n# logging function that creates hierarchial display of the processes of\n# different functions. Copied into different python files to maintain\n# global variable parallels\ndef logger(instruction, found=\"\"):\n    '''\n    logging function that creates hierarchial display of the processes of\n    different functions. Copied into different python files to maintain\n    global variables.\n\n    :param instruction: what you want to be displayed\n    :param found: if you want to display something found like target column\n    '''\n    \n    global counter\n    if counter == 0:\n        print((\" \" * 2 * counter) + str(instruction) + str(found))\n    elif instruction == \"->\":\n        counter = counter - 1\n        print(Fore.BLUE + (\" \" * 2 * counter) +\n              str(instruction) + str(found) + (Style.RESET_ALL))\n    else:\n        print((\" \" * 2 * counter) + \"|- \" + str(instruction) + str(found))\n        if instruction == \"done...\":\n            print(\"\\n\" + \"\\n\")\n\n    counter += 1\n\n\n# Cleaning/ preprocessing function for content based recommender engine\n# basically tokenizes all the feature examples\ndef clean_data(x):\n    if isinstance(x, list):\n        return np.array([str.lower(i.replace(\" \", \"\")) for i in x if not x.isdigit()])\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''\n\n\n# Cosine similarity matrix creator\ndef matrix_maker(data,indexer='title',feature_names=[]):\n    try:\n        assert(isinstance(feature_names, list))\n    except:\n        logger('Error! Feature names must be of type list!')\n        exit()\n\n\n    # function to combine all the tokenized features values for the\n    # cosine matrix to calculate similarities from (the recommender 'soup')\n    def create_soup(x):\n        soup = []\n        for feature in np.array(feature_names):\n            f = ''.join(x[feature])\n            soup.append(f)\n        return ' '.join(soup)\n\n    data = pd.read_csv(data)\n    data = data.copy()\n    #data = data[['title']].apply(pd.Series.unique).join(data[feature_names])\n    try:\n        assert indexer in data.columns\n    except:\n        logger('Error! the indexer passed named:  ' +  str(indexer) + 'is not in dataset!')\n        exit()\n    for f in feature_names:\n            data[f] = data[f].apply(clean_data)\n    logger('Cleaning Data..')\n\n    data['text'] = data.apply(create_soup,axis=1)\n    logger('Getting similarities')\n\n    # Create a CountVectorizer, fit to data 'soup' and get similarities\n    con = CountVectorizer()\n    item_matrix = con.fit_transform(data['text'])\n    cosine_similarities = cosine_similarity(item_matrix,item_matrix)\n    similarities = {}\n\n    # Loop through similarities and get top 50, return similarities\n    for i in range(len(cosine_similarities)):\n        similar_indices = cosine_similarities[i].argsort()[:-50:-1] \n        similarities[data[indexer].iloc[i]] = [(cosine_similarities[i][x], data[indexer][x], '') for x in similar_indices][1:]\n    return similarities\n\nclass ContentBasedRecommender:\n    '''\n    ---------- Content Based Recommender System Class --------\n\n    This is the base class for the content based recommender. The constructor takes in 5 arguements:\n\n    data: the dataset to work on\n    feature_names = a list of the names of features you would like to use to get recommendations\n    by default this will be all the categorical columns\n    n_recommendations: the number of recommendations to return\n    indexer = the name of the columns you want to get recommendations from\n    by default this will be the first categorical columns(excluding the id)\n\n    Example:\n    newClient.content_recommender_query(feature_names=['genre','actors','writer','plot'],indexer='title')\n\n    Methods:\n\n    recommend: the recommendations function. Returns recommendations based on search term passed\n    parameters:\n    search_term: string of the item you want to get recommendations from\n    returns:\n    result: a pandas DataFrame of the top n recommendations\n\n    Example:\n    c = client('path to file')\n    c.content_recommender_query(feature_names=['genre','plot','director','actors','writer'])\n    recommendations = c.recommend('Coco')\n\n\n    _get_message: gets the results of the similarity and creates a Dataframe\n    of the resultd and their correlation.\n\n    '''\n\n\n    def __init__(self, data,feature_names=[],indexer='',n_recommendations=10):\n        # If feature names is blank, then it get all categorical objects,\n        # removes the id and used them to recommend items,setting the indexer\n        # as the first element of the feature_names\n        dataReader = DataReader(data)\n        data1 = dataReader.data_generator()\n        self.data1 = data1.copy()\n\n        if feature_names == []:\n            catnames = list(self.data1.select_dtypes('object').columns)\n            for i in catnames:\n                if 'id' in i.lower():\n                    v = catnames.index(i)\n                    del catnames[v]\n            self.feature_names = catnames[1:]\n            self.indexer = catnames[0]\n        else:\n            #Â Initialise default variables\n            self.indexer = indexer\n            self.feature_names = feature_names\n\n        # call the matrix_maker function created above\n        self.matrix_similar = matrix_maker(data,self.indexer,self.feature_names)\n        self.n_recommendations=n_recommendations\n\n    def _get_message(self, item, recom_items):\n        # Get the recommendations and put them into a DataFrame\n        logger(\"Complete! Stored recommendation DataFrame under the 'recommendations' key\")\n        clearLog()\n\n        rec_items = len(recom_items)\n        # List for recommended items and their respective correlations\n        recommended_items = []\n        recommended_corr = []\n\n        # Loop through each item,append to the correct list, and returnt a dict key\n        # of the DataFrame,n_recommendations, feature_names and indexer\n        for i in range(rec_items):\n            recommended_items.append(recom_items[i][1])\n            recommended_corr.append(round(recom_items[i][0], 3))\n\n        \n        df = pd.DataFrame(pd.Series(np.array(recommended_items)),columns=['Recommendations'])\n        df.insert(1,'Correlation',recommended_corr)\n\n\n        return {\n            'recommendations': df,\n            'indexer': self.indexer,\n            'n_recommendations': self.n_recommendations,\n            'feature_names': self.feature_names\n        }\n\n        \n    # recommendation function\n    def recommend(self, s_name):\n        # Get item to find recommendations for\n        item = s_name\n        # Get number of items to recommend\n        number_items = self.n_recommendations\n        # Get the number of items most similars from matrix similarities\n        recom_item = self.matrix_similar[item][:number_items]\n        # return each item\n\n        return self._get_message(item=item, recom_items=recom_item)\n",
          "from libra.query.nlp_queries import (image_caption_query,\n                                     generate_caption, classify_text,\n                                     text_classification_query, get_summary,\n                                     summarization_query, generate_text, get_ner)\n\nfrom libra.query.classification_models import (k_means_clustering,\n                                               train_svm, nearest_neighbors,\n                                               decision_tree, train_xgboost)\nfrom libra.query.supplementaries import tune_helper, get_model_data, get_operators, get_accuracy, get_losses, \\\n    get_target, get_plots, get_vocab\n\nfrom libra.query.feedforward_nn import (regression_ann,\n                                        classification_ann,\n                                        convolutional)\nfrom libra.data_generation.grammartree import get_value_instruction\nfrom libra.data_generation.dataset_labelmatcher import (get_similar_column,\n                                                        get_similar_model)\nfrom libra.plotting.generate_plots import analyze\nfrom libra.query.recommender_systems import ContentBasedRecommender\nfrom libra.dashboard.auto_eda import edaDashboard\nfrom colorama import Fore, Style\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport os\nimport nltk\nimport ssl\nimport numpy as np\nfrom tkinter import filedialog\nfrom tkinter import *\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n# suppressing warnings for cleaner dialogue box\nwarnings.simplefilter(action='error', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# function imports from other files\ncurrLog = \"\"\ncounter = 0\n\n\n# clears log when needed - currently not being used\ndef clearLog():\n    global currLog\n    global counter\n\n    currLog = \"\"\n    counter = 0\n    print(\"\")\n\n\ndef logger(instruction, found=\"\"):\n    '''\n    logging function that creates hierarchial display of the processes of\n    different functions. Copied into different python files to maintain\n    global variables.\n\n    :param instruction: what you want to be displayed\n    :param found: if you want to display something found like target column\n\n    '''\n    global counter\n    if counter == 0:\n        print((\" \" * 2 * counter) + str(instruction) + str(found))\n    elif instruction == \"->\":\n        counter = counter - 1\n        print(Fore.BLUE + (\" \" * 2 * counter) +\n              str(instruction) + str(found) + (Style.RESET_ALL))\n    else:\n        print((\" \" * 2 * counter) + \"|- \" + str(instruction) + str(found))\n        if instruction == \"done...\":\n            print(\"\\n\" + \"\\n\")\n\n    counter += 1\n\n\ndef get_folder_dir(self):\n    dir_path = filedialog.askdirectory()\n    return dir_path\n\n\ndef get_file():\n    filename = filedialog.askopenfilename()\n    if os.path.isfile(filename):\n        return filename\n    else:\n        print('No file chosen')\n\n\nclass client:\n    '''\n    class to store all query information. Currently, old_models is not being used.\n    '''\n\n    def __init__(self, data):\n        '''\n        initializer for the client class, reads in dataset and records by calling logger function\n        :param data: represents the dataset that you're trying to read\n        :return: a completely initialized client class\n        '''\n        self.required_installations()\n        logger(\"Creating client object\")\n        self.dataset = data\n        logger(\"Reading in dataset\")\n        print(\"\")\n        self.models = {}\n        self.latest_model = None\n        clearLog()\n\n    def required_installations(self):\n        try:\n            _create_unverified_https_context = ssl._create_unverified_context\n        except AttributeError:\n            pass\n        else:\n            ssl._create_default_https_context = _create_unverified_https_context\n        nltk.download('punkt', quiet=True)\n        nltk.download('averaged_perceptron_tagger', quiet=True)\n        nltk.download('stopwords', quiet=True)\n\n    # param model_requested: string representation of the name of the model user seeks to retrieve\n    # returns models with a specific string - currently deprecated, should not be used.\n\n    def get_models(self, model_requested):\n        '''\n        returns models with a specific string - currently deprecated, should not be used.\n        :param model_requested: represents the name of the model from which you want to retrieve\n        :return: the model dictionary for your specific model\n        '''\n        logger(\"Getting model...\")\n        return get_similar_model(model_requested, self.models.keys())\n        clearLog()\n\n    # recommend items based on search criteria(for recommender systems only)\n\n    def recommend(self, search_term):\n        if self.latest_model == 'content_recommender':\n            model = self.models[self.latest_model]\n            return model.recommend(search_term)\n        else:\n            pass\n\n    # param modelKey: string representation of the model to make prediction\n    # param data: dataframe version of desired prediction set\n    def predict(self, data, model=None):\n        '''\n        Uses a model from the self.models dictionary to make a prediction. Also fits it based on the operator stored in the models dictionary.\n        :param data: is the data that you want to predict for using model\n        :param model: is the specific model you want to use to predict\n        :return: a prediction, most likely an array\n        '''\n        if model is None:\n            model = self.latest_model\n        if model == 'text_classification':\n            map_func = np.vectorize(lambda x: self.classify_text(x))\n            predictions = map_func(data)\n            return predictions\n        else:\n            modeldict = self.models[model]\n\n            if modeldict.get('preprocesser'):\n                data = modeldict['preprocesser'].transform(data)\n            predictions = modeldict['model'].predict(data)\n        clearLog()\n        return self.interpret(model, predictions)\n\n    def interpret(self, model, predictions):\n        '''\n        Function to interpret predictions from a neural network for the creation for graphs / user understanding.\n        :param model: is the model in the self.models dictionary that you want to use to interpret\n        :param predictions: the predictions that come out of the model\n        :return: a prediction, most likely an array\n        '''\n        modeldict = self.models[model]\n        if modeldict.get('interpreter'):\n            predictions = modeldict['interpreter'].inverse_transform(\n                predictions)\n        clearLog()\n        return predictions\n\n    # determines type of solution based of type of problem posed by query using a feed-forward neural network\n    # instruction should be the value of a column\n    def neural_network_query(self,\n                             instruction,\n                             callback=False,\n                             text=[],\n                             ca_threshold=None,\n                             drop=None,\n                             preprocess=True,\n                             test_size=0.2,\n                             random_state=49,\n                             epochs=50,\n                             generate_plots=True,\n                             callback_mode='min',\n                             maximizer=\"val_loss\",\n                             save_model=False,\n                             save_path=os.getcwd(),\n                             add_layer={}):\n        '''\n        Detects to see if it's a regression/classification problem and then calls the correct query.\n        :param hyperparameters: all of these are hyperparameters that're passed to the algorithm\n        :return: a model, plots, accuracy information all stored in the self.models dictionary\n        '''\n\n        data = pd.read_csv(self.dataset)\n\n        if preprocess:\n\n            remove = get_similar_column(\n                get_value_instruction(instruction), data)\n\n            if len(data) < 50:\n                raise Exception(\n                    \"Only datasets larger then 50 rows are supported for neural networks\")\n            if len(data[remove].value_counts()) <= 50:\n                callback_mode = 'max'\n                maximizer = \"val_accuracy\"\n                self.classification_query_ann(\n                    instruction,\n                    text=text,\n                    callback=callback,\n                    ca_threshold=ca_threshold,\n                    preprocess=preprocess,\n                    test_size=test_size,\n                    random_state=random_state,\n                    epochs=epochs,\n                    generate_plots=generate_plots,\n                    callback_mode=callback_mode,\n                    maximizer=maximizer,\n                    save_model=save_model,\n                    save_path=save_path,\n                    add_layer=add_layer)\n            else:\n                self.regression_query_ann(\n                    instruction,\n                    callback=callback,\n                    text=text,\n                    ca_threshold=ca_threshold,\n                    preprocess=preprocess,\n                    test_size=test_size,\n                    random_state=random_state,\n                    epochs=epochs,\n                    generate_plots=generate_plots,\n                    callback_mode=callback_mode,\n                    maximizer=maximizer,\n                    drop=drop,\n                    save_model=save_model,\n                    save_path=save_path,\n                    add_layer=add_layer)\n        clearLog()\n\n    # single regression query using a feed-forward neural network\n    # instruction should be the value of a column\n    def regression_query_ann(\n            self,\n            instruction,\n            callback=False,\n            text=[],\n            drop=None,\n            ca_threshold=None,\n            preprocess=True,\n            test_size=0.2,\n            random_state=49,\n            epochs=50,\n            generate_plots=True,\n            callback_mode='min',\n            maximizer=\"val_loss\",\n            save_model=True,\n            save_path=os.getcwd(),\n            add_layer={}):\n        '''\n        Calls the body of the regression_query__ code in the supplementaries.py file. Used for a regression feed forward neural network.\n        :param instruction: The objective that you want to model (str).\n        :param callback: Applying a set of functions/actions at various stages of training (bool).\n        :param ca_threshold: Threshold for multiple correspondence analysis (float).\n        :param dataset: The dataset being used in the regression feed forward neural network (str).\n        :param text: A list of columns to perform text embedding on.\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param epochs: Number of epochs (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param callback_mode: The type of callback (str).\n        :param maximizer: The accuracy/loss type to optimize (str).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['regression_ANN'] = regression_ann(\n            instruction=instruction,\n            callback=False,\n            ca_threshold=.25 if ca_threshold is None else ca_threshold,\n            dataset=self.dataset,\n            text=text,\n            drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            epochs=epochs,\n            generate_plots=generate_plots,\n            callback_mode=callback_mode,\n            maximizer=maximizer,\n            save_model=save_model,\n            save_path=save_path,\n            add_layer=add_layer)\n\n        self.latest_model = 'regression_ANN'\n        clearLog()\n\n    # query for multilabel classification query, does not work for\n    # binaryclassification, fits to feed-forward neural network\n\n    def classification_query_ann(\n            self,\n            instruction,\n            callback=False,\n            text=[],\n            ca_threshold=None,\n            preprocess=True,\n            callback_mode='min',\n            drop=None,\n            random_state=49,\n            test_size=0.2,\n            epochs=50,\n            generate_plots=True,\n            maximizer=\"val_loss\",\n            save_model=False,\n            save_path=os.getcwd(),\n            add_layer={}):\n        '''\n        Calls the body of the classification code in the supplementaries.py file. Used for a classification feed forward neural network.\n        :param instruction: The objective that you want to model (str).\n        :param callback: Applying a set of functions/actions at various stages of training (bool).\n        :param dataset: The dataset being used in the classification feed forward neural network (str).\n        :param text: A list of columns to perform text embedding on.\n        :param ca_threshold: Threshold for multiple correspondence analysis (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param epochs: Number of epochs (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param callback_mode: The type of callback (str).\n        :param maximizer: The accuracy/loss type to optimize (str).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['classification_ANN'] = classification_ann(\n            instruction=instruction,\n            callback=callback,\n            dataset=self.dataset,\n            text=text,\n            ca_threshold=.25 if ca_threshold is None else ca_threshold,\n            drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            epochs=epochs,\n            generate_plots=generate_plots,\n            callback_mode=callback_mode,\n            maximizer=maximizer,\n            save_model=save_model,\n            save_path=save_path,\n            add_layer=add_layer)\n\n        self.latest_model = 'classification_ANN'\n        clearLog()\n\n    # query to perform k-means clustering\n\n    def kmeans_clustering_query(self,\n                                preprocess=True,\n                                scatters=[],\n                                generate_plots=True,\n                                drop=None,\n                                clusters=None,\n                                base_clusters=2,\n                                verbose=0,\n                                n_init=10,\n                                max_iter=300,\n                                random_state=42,\n                                text=[]\n                                ):\n        '''\n        Calls the body of the kmeans_clustering code in the supplementaries.py file. Can be used without any preprocessing and/or parameters.\n\n        :param dataset: The dataset being used in the k-means clustering algorithm (str).\n        :param scatters: A list of various types of scatter plots.\n        :param preprocess: Preprocess the data (bool).\n        :param generate_plots: Generate plots for the model (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param base_clusters: Number of clusters to generate (int).\n        :param verbose: Printing the logging information (int).\n        :param n_init: Number of times the function will run with different seeds (int).\n        :param max_iter: Maximum number of iterations the function will run (int).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param text: A list of columns to perform text embedding on.\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['k_means_clustering'] = k_means_clustering(\n            dataset=self.dataset,\n            scatters=scatters,\n            clusters=clusters,\n            preprocess=preprocess,\n            generate_plots=generate_plots,\n            drop=drop,\n            base_clusters=base_clusters,\n            verbose=verbose,\n            n_init=n_init,\n            max_iter=max_iter,\n            random_state=random_state,\n            text=text\n        )\n\n        self.latest_model = 'k_means_clustering'\n        clearLog()\n\n    # query to create a support vector machine\n\n    def svm_query(self,\n                  instruction,\n                  test_size=0.2,\n                  text=[],\n                  random_state=49,\n                  kernel='linear',\n                  preprocess=True,\n                  drop=None,\n                  cross_val_size=0.3,\n                  degree=3,\n                  gamma='scale',\n                  coef0=0.0,\n                  max_iter=-1\n                  ):\n        '''\n        Calls the body of the svm query code in the supplementaries.py file. Used to create a classification support vector machine.\n        :param dataset: The dataset being used in the classification support vector machine (str).\n        :param text: A list of columns to perform text embedding on.\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param test_size: Size of the testing set (float).\n        :param kernel: The type of kernel to be used (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param cross_val_size: Cross-Validation score (float).\n        :param degree: Degree of the polynomial kernel function (int).\n        :param gamma: Kernel coefficient (int).\n        :param coef0: Significant term in 'poly' and 'sigmoid' kernel functions (float).\n        :param max_iter: Maximum number of iterations the function will run (int).\n\n\n        :return: a model and information to go along with it stored in the self.models dictionary.\n        '''\n\n        self.models['svm'] = train_svm(instruction,\n                                       dataset=self.dataset,\n                                       text=text,\n                                       random_state=random_state,\n                                       test_size=test_size,\n                                       kernel=kernel,\n                                       preprocess=preprocess,\n                                       drop=drop,\n                                       cross_val_size=cross_val_size,\n                                       degree=degree,\n                                       gamma=gamma,\n                                       coef0=coef0,\n                                       max_iter=max_iter\n                                       )\n\n        self.latest_model = 'svm'\n        clearLog()\n\n    # query to create a nearest neighbors model\n\n    def nearest_neighbor_query(\n            self,\n            instruction=None,\n            text=[],\n            random_state=49,\n            test_size=0.2,\n            preprocess=True,\n            drop=None,\n            min_neighbors=3,\n            max_neighbors=10,\n            leaf_size=30,\n            p=2,\n            algorithm='auto'\n    ):\n        '''\n        Calls the body of the nearest neighbor code in the supplementaries.py file. Used to create a nearest neighbor algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param test_size: Size of the testing set (float).\n        :param dataset: The dataset being used in the nearest neighbor algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param min_neighbors: Minimum number of neighbors (int).\n        :param max_neighbors: Maximum number of neighbors (int).\n        :param leaf_size: Leaf size passed to BallTree or KDTree (int).\n        :param p: Power parameter for the Minkowski metric (int).\n        :param algorithm: Algorithm used to compute the nearest neighbors (str).\n\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n        self.models['nearest_neighbor'] = nearest_neighbors(\n            instruction=instruction,\n            text=text,\n            random_state=random_state,\n            test_size=test_size,\n            dataset=self.dataset,\n            preprocess=preprocess,\n            drop=drop,\n            min_neighbors=min_neighbors,\n            max_neighbors=max_neighbors,\n            leaf_size=leaf_size,\n            p=p,\n            algorithm=algorithm\n        )\n\n        self.latest_model = 'nearest_neighbor'\n        clearLog()\n\n    # query to create a decision tree model\n\n    def decision_tree_query(\n            self,\n            instruction,\n            preprocess=True,\n            test_size=0.2,\n            text=[],\n            drop=None,\n            criterion='gini',\n            splitter='best',\n            max_depth=None,\n            min_samples_split=2,\n            min_samples_leaf=1,\n            min_weight_fraction_leaf=0.0,\n            max_leaf_nodes=None,\n            min_impurity_decrease=0.0,\n            ccp_alpha=0.0):\n        '''\n        Calls the body of the decision tree code in the classification_models.py file. Used to create a decision tree algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param dataset: The dataset being used in the decision tree algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param criterion: The function to measure the quality of a split (str).\n        :param splitter: The technique used to choose each node's split (str).\n        :param max_depth: The maximum depth of the tree (int).\n        :param min_samples_split: The minimum number of samples a node must have to split (int).\n        :param min_samples_leaf: The minimum number of samples a leaf node must have (int).\n        :param min_weight_fraction_leaf: The fraction of the input samples required to be at a leaf node (float).\n        :param max_leaf_nodes: Maximum number of leaf nodes (int).\n        :param min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal\n         to this value (float).\n        :param ccp_alpha: Complexity parameter used for Minimal Cost-Complexity Pruning (float).\n\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['decision_tree'] = decision_tree(\n            instruction=instruction,\n            text=text,\n            dataset=self.dataset,\n            preprocess=preprocess,\n            test_size=test_size,\n            drop=drop,\n            criterion=criterion,\n            splitter=splitter,\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_leaf_nodes=max_leaf_nodes,\n            min_impurity_decrease=min_impurity_decrease,\n            ccp_alpha=ccp_alpha)\n\n        self.latest_model = 'decision_tree'\n        clearLog()\n\n    def content_recommender_query(self, feature_names=[], n_recommendations=10, indexer='title'):\n        self.models['content_recommender'] = ContentBasedRecommender(\n            data=self.dataset,\n            feature_names=feature_names,\n            indexer=indexer)\n\n        self.latest_model = 'content_recommender'\n        clearLog()\n\n    # query to create a xgboost model\n\n    def xgboost_query(self,\n                      instruction,\n                      text=[],\n                      preprocess=True,\n                      test_size=0.2,\n                      drop=None,\n                      random_state=49,\n                      learning_rate=0.1,\n                      n_estimators=1000,\n                      max_depth=6,\n                      min_child_weight=1,\n                      gamma=0,\n                      subsample=0.8,\n                      colsample_bytree=0.8,\n                      verbosity=0,\n                      objective='binary:logistic'):\n\n        '''\n        Calls the body of the xgboost code in the classification_models.py file. Used to create a xgboost algorithm.\n        :param instruction: The objective that you want to model (str).\n        :param text: A list of columns to perform text embedding on.\n        :param dataset: The dataset being used in the xgboost algorithm (str).\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param drop: A list of the dataset's columns to drop.\n        :param random_seed: Initialize a pseudo-random number generator (int).\n        :param learning_rate:  Boosting learning rate(float).\n        :param n_estimators: Number of gradient boosted trees. Equivalent to number of boosting rounds(in   ).\n        :param max_depth: Maximum tree depth for base learners(int).\n        :param min_child_weight: Minimum sum of instance weight(hessian) needed in a child(int).\n        :param gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree(int).\n        :param subsample: Subsample ratio of the training instance(float).\n        :param colsample_bytree: Subsample ratio of columns when constructing each tree(float).\n        :param objective: Specify the learning task and the corresponding learning objective or a custom\n        objective function to be used (string or callable).\n        :param scale_pos_weight: Balancing of positive and negative weights(float).\n        :param verbose: Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).\n\n        :return: a model and information to along with it stored in the self.models dictionary.\n        '''\n\n        self.models['xgboost'] = train_xgboost(instruction,\n                                               dataset=self.dataset,\n                                               text=[],\n                                               random_state=random_state,\n                                               preprocess=preprocess,\n                                               drop=drop,\n                                               learning_rate=learning_rate,\n                                               n_estimators=n_estimators,\n                                               max_depth=max_depth,\n                                               min_child_weight=min_child_weight,\n                                               gamma=gamma,\n                                               subsample=subsample,\n                                               verbosity=verbosity,\n                                               colsample_bytree=colsample_bytree,\n                                               objective=objective)\n\n        self.latest_model = 'xgboost'\n        clearLog()\n\n    # tunes a specific neural network based on the input model_to_tune\n\n    def tune(self,\n             model_to_tune=None,\n             max_layers=10,\n             min_layers=2,\n             min_dense=32,\n             max_dense=512,\n             executions_per_trial=3,\n             max_trials=1,\n             generate_plots=True,\n             activation='relu',\n             loss='categorical_crossentropy',\n             metrics='accuracy',\n             patience=1,\n             epochs=10,\n             objective='val_accuracy',\n             seed=42,\n             directory='my_dir',\n             verbose=0,\n             test_size=0.2\n             ):\n        '''\n        Calls the body of the tune identifier which is located in the supplementaries.py which then calls the appropriate tuner depending on the model\n        :param model_to_tune: The model to tune.\n        :param patience: Number of epochs with no improvement after which training will be stopped (int).\n        :param dataset: The dataset being used in the tuner (str).\n        :param models: The model dictionary (dict).\n        :param generate_plots: Generate plots for the model (bool).\n        :param max_layers: Maximum number of layers (int).\n        :param min_layers: Minimum number of layers (int).\n        :param min_dense: Minimum kernel density (int).\n        :param max_dense: Maximum kernel density (int).\n        :param executions_per_trial: Number of executions per trial (int).\n        :param max_trials: Maximum number of trials\n        :param activation: Activation Function (str).\n        :param loss: Loss Function (str).\n        :param metrics: Type of metrics function (str).\n        :param epochs: Number of epochs (int).\n        :param objective: Name of model metric to maximize/minimize (str).\n        :param seed: Random seed (int).\n        :param directory: Path to the directory (str).\n        :param verbose: Printing the logging information (int).\n        :param test_size: Size of the testing set (float).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        if model_to_tune is None:\n            model_to_tune = self.latest_model\n\n        self.models = tune_helper(\n            model_to_tune=model_to_tune,\n            patience=patience,\n            dataset=self.dataset,\n            models=self.models,\n            generate_plots=generate_plots,\n            max_layers=max_layers,\n            min_layers=min_layers,\n            min_dense=min_dense,\n            max_dense=max_dense,\n            executions_per_trial=executions_per_trial,\n            max_trials=max_trials,\n            activation=activation,\n            loss=loss,\n            metrics=metrics,\n            epochs=epochs,\n            objective=objective,\n            seed=seed,\n            directory=directory,\n            verbose=verbose,\n            test_size=test_size\n        )\n        clearLog()\n\n    # query to build a convolutional neural network\n\n    def convolutional_query(self,\n                            instruction=None,\n                            read_mode=None,\n                            verbose=0,\n                            preprocess=True,\n                            data_path=None,\n                            new_folders=True,\n                            image_column=None,\n                            test_size=0.2,\n                            augmentation=True,\n                            custom_arch=None,\n                            pretrained=None,\n                            epochs=10,\n                            height=None,\n                            width=None,\n                            show_feature_map=False):\n        '''\n        Calls the body of the convolutional neural network query which is located in the feedforward.py file\n        :param instruction: The objective that you want to model (str).\n        :param read_mode: The type of dataset (str).\n        :param verbose: Printing the logging information (int).\n        :param preprocess: Preprocess the data (bool).\n        :param data_path: Path to the dataset (str).\n        :param new_folders: Create new folders for the image during preprocessing (bool).\n        :param image_column: The column in the csv file where the filepaths for the images exist (str).\n        :param test_size: Ratio of dataset allotted to the testing data (float).\n        :param augmentation: Perform image data augmentation (bool).\n        :param epochs: Number of epochs (int).\n        :param height: Height of the input image (int).\n        :param width: Width of the input image (int).\n        :param show_feature_map: Displays feature map graphic (bool).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        # storing values in the model dictionary\n        self.models[\"convolutional_NN\"] = convolutional(\n            instruction=instruction,\n            read_mode=read_mode,\n            verbose=verbose,\n            preprocess=preprocess,\n            data_path=self.dataset,\n            new_folders=new_folders,\n            image_column=image_column,\n            training_ratio=1 - test_size,\n            augmentation=augmentation,\n            custom_arch=custom_arch,\n            pretrained=pretrained,\n            epochs=epochs,\n            height=height,\n            width=width)\n\n        if show_feature_map:\n            model = self.models[\"convolutional_NN\"][\"model\"]\n            X_test = self.models[\"convolutional_NN\"][\"data\"][\"test\"]\n\n            # Get first image in test images and format it\n            img = X_test[0][0]\n            img /= 255\n            successive_outputs = [layer.output for layer in model.layers[1:]]\n            visualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs)\n            successive_feature_maps = visualization_model.predict(img)\n\n            # Add main title to figure\n            firstPlot = True\n\n            # Include names of layers in plot\n            layer_names = [layer.name for layer in model.layers]\n            for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n                if len(feature_map.shape) == 4:\n\n                    # Plot Feature maps for the conv / maxpool layers, not the fully-connected layers\n                    n_features = feature_map.shape[-1]  # number of features in the feature map\n                    height = feature_map.shape[1]  # feature map shape (1, size, size, n_features)\n                    width = feature_map.shape[2]\n                    display_grid = np.zeros((height, width * n_features))\n\n                    # Format features appropriately\n                    for i in range(n_features):\n                        img = feature_map[0, :, :, i]\n                        img -= img.mean()\n                        img /= img.std()\n                        img *= 64\n                        img += 128\n                        img = np.clip(img, 0, 255).astype('uint8')\n\n                        # Tile each filter into a horizontal grid\n                        display_grid[:, i * width: (i + 1) * width] = img\n\n                    # Display the grid\n                    scale = 20. / n_features\n                    plt.figure(figsize=(scale * n_features, scale))\n                    if firstPlot:\n                        plt.title(f'Network Visualization\\n\\n{layer_name}')\n                        firstPlot = False\n                    else:\n                        plt.title(layer_name)\n                    plt.grid(False)\n                    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n                    plt.show()\n\n        self.latest_model = 'convolutional_NN'\n        clearLog()\n\n    # sentiment analysis prediction wrapper\n\n    def classify_text(self, text):\n        \"\"\"\n        Calls the body of the text_classification neural network query which is located in the nlp_queries.py file. This can only be called\n        if text_classification_query has been called previously.\n        :param text: The new text that you want to classify (str).\n        :return: a classification of text that you've provided\n        \"\"\"\n        clearLog()\n        return classify_text(self=self, text=text)\n\n    # sentiment analysis query\n    def text_classification_query(self, instruction, label_column=None, drop=None,\n                                  preprocess=True,\n                                  test_size=0.2,\n                                  random_state=49,\n                                  learning_rate=1e-2,\n                                  epochs=20,\n                                  monitor=\"val_loss\",\n                                  batch_size=32,\n                                  max_text_length=200,\n                                  max_features=20000,\n                                  generate_plots=True,\n                                  save_model=False,\n                                  save_path=os.getcwd()):\n        '''\n        Calls the body of the text_classification query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param drop: A list of the dataset's columns to drop.\n        :param preprocess: Preprocess the data (bool).\n        :param test_size: Size of the testing set (float).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param learning_rate: The learning rate of the model (float).\n        :param epochs: Number of epochs (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        # storing values the model dictionary\n        self.models[\"text_classification\"] = text_classification_query(\n            self=self, instruction=instruction, label_column=label_column, drop=drop,\n            preprocess=preprocess,\n            test_size=test_size,\n            random_state=random_state,\n            learning_rate=learning_rate,\n            monitor=monitor,\n            epochs=epochs,\n            batch_size=batch_size,\n            max_text_length=max_text_length,\n            max_features=max_features,\n            generate_plots=generate_plots,\n            save_model=save_model,\n            save_path=save_path)\n        self.latest_model = 'text_classification'\n        clearLog()\n\n    # summarization predict wrapper\n    def get_summary(self, text, num_beams=4, no_repeat_ngram_size=2, num_return_sequences=1,\n                    early_stopping=True):\n        '''\n        Calls the body of the summarizer which is located in the nlp_queries.py file\n        :param text: set of text that you want to summarize (str).\n        :param max_summary_length: Max generated summary length (int).\n        :param early_stopping: Sets early stopping (bool).\n        :param num_return_sequences: Sets the number of likely possibilities to output (int).\n        :param no_repeat_ngram_size: Sets the number of unrepeated consecutive n-grams (int).\n        :param num_beams: Sets number of possibilities to explore in beam search (int).\n        :return: a summary of text inputted in the text field.\n        '''\n        clearLog()\n        return get_summary(self=self, text=text, num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size\n                           , num_return_sequences=num_return_sequences, early_stopping=early_stopping)\n\n    # summarization query\n    def summarization_query(self, instruction, label_column=None, preprocess=True,\n                            drop=None,\n                            epochs=5,\n                            batch_size=32,\n                            learning_rate=3e-5,\n                            max_text_length=512,\n                            test_size=0.2,\n                            gpu=False,\n                            random_state=49,\n                            generate_plots=True,\n                            save_model=False,\n                            save_path=os.getcwd()):\n        '''\n        Calls the body of the summarization  query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param preprocess: Preprocess the data (bool).\n        :param drop: A list of the dataset's columns to drop.\n        :param epochs: Number of epochs (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param learning_rate: The learning rate of the model (float).\n        :param max_text_length: The maximum length of the string of text (int).\n        :param test_size: Size of the testing set (float).\n        :param gpu: Use gpu for accelerated training (bool).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model: Save the model (bool).\n        :param save_path: Filepath of where to save the model (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        self.models[\"summarization\"] = summarization_query(\n            self=self, instruction=instruction, preprocess=preprocess, label_column=label_column,\n            drop=drop,\n            epochs=epochs,\n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            max_text_length=max_text_length,\n            test_size=test_size,\n            gpu=gpu,\n            random_state=random_state,\n            generate_plots=generate_plots,\n            save_model=save_model,\n            save_path=save_path)\n\n        self.latest_model = 'summarization'\n        clearLog()\n\n    # image_caption generator wrapper\n\n    def generate_caption(self, image):\n        '''\n        Calls the body of the caption generator which is located in the nlp_queries.py file.\n        :param image: the image that you want to generate a caption for.\n        :return: a caption for the image inputted in the image field.\n        '''\n        caption = generate_caption(self=self, image=image)\n        clearLog()\n        return ' '.join(caption[:len(caption) - 1])\n\n    # image_caption prediction query\n    def image_caption_query(self, instruction, label_column=None,\n                            drop=None,\n                            epochs=10,\n                            preprocess=True,\n                            random_state=49,\n                            test_size=0.2,\n                            top_k=5000,\n                            batch_size=32,\n                            buffer_size=1000,\n                            embedding_dim=256,\n                            units=512,\n                            gpu=False,\n                            generate_plots=True,\n                            save_model_decoder=False,\n                            save_path_decoder=os.getcwd(),\n                            save_model_encoder=False,\n                            save_path_encoder=os.getcwd()):\n        '''\n        Calls the body of the image_caption query which is located in the nlp_queries.py file\n        :param instruction: The objective that you want to model (str).\n        :param drop: A list of the dataset's columns to drop.\n        :param epochs: Number of epochs (int).\n        :param preprocess: Preprocess the data (bool).\n        :param random_state: Initialize a pseudo-random number generator (int).\n        :param top_k:  Number of most frequent words in the vocab to be used in tokenization (int).\n        :param batch_size: The batch size for the dataset (int).\n        :param buffer_size: The maximum number of elements to buffer (int).\n        :param embedding_dim: The dimension of the word embedding mapping (int).\n        :param units: The recurrent units in the decoder (int).\n        :param test_size: test size (int) .\n        :param gpu: Choose to use gpu (bool).\n        :param generate_plots: Generate plots for the model (bool).\n        :param save_model_decoder: Save the decoder (bool).\n        :param save_path_decoder: Filepath of where to save the decoder (str).\n        :param save_model_encoder: Save the encoder (bool).\n        :param save_path_encoder: Filepath of where to save the encoder (str).\n\n\n        :return: an updated model and history stored in the models dictionary\n        '''\n\n        self.models[\"image_caption\"] = image_caption_query(\n            self, instruction=instruction, label_column=label_column,\n            drop=drop,\n            epochs=epochs,\n            preprocess=preprocess,\n            random_state=random_state,\n            test_size=test_size,\n            top_k=top_k,\n            batch_size=batch_size,\n            buffer_size=buffer_size,\n            embedding_dim=embedding_dim,\n            units=units,\n            gpu=gpu,\n            generate_plots=generate_plots,\n            save_model_decoder=save_model_decoder,\n            save_path_decoder=save_path_decoder,\n            save_model_encoder=save_model_encoder,\n            save_path_encoder=save_path_encoder)\n        self.latest_model = 'image_caption'\n        clearLog()\n\n    def generate_text(self, instruction, file_data=True, prefix=None,\n                      max_length=512,\n                      top_k=50,\n                      top_p=0.9,\n                      return_sequences=2):\n        \"\"\"\n        :param instruction: objective you want to accomplish\n        :param prefix: a string that you want the generated text to begin with\n        :param max_length: the length of desired text you want (int)\n        :param top_k: number of most frequent words in the vocab to be used in tokenization (int).\n        :param top_p: p value between 0 and 1 (float)\n        :param return_sequences: how many different text sequences you want returned\n        :return: generated text\n        \"\"\"\n        self.models['text_generation'] = generate_text(self=self,\n                                                       instruction=instruction,\n                                                       file_data=file_data,\n                                                       prefix=prefix,\n                                                       max_length=max_length,\n                                                       top_k=top_k,\n                                                       top_p=top_p,\n                                                       return_sequences=return_sequences)\n\n        self.latest_model = 'text_generation'\n        clearLog()\n\n    # name entity recognition query\n    def named_entity_query(self, instruction):\n        \"\"\"\n        function to identify name entities\n        :param instruction: Used to get target column\n        :return: dictionary object with detected name-entities\n        \"\"\"\n        self.models[\"named_entity_recognition\"] = get_ner(self, instruction=instruction)\n        self.latest_model = \"named_entity_recognition\"\n        clearLog()\n\n    # shows the names of plots associated with a specific model\n    def plot_names(self, model=None):\n        '''\n        Function to get names of plots given the name of the model you want\n        :param model: the model that you want to get the plots for\n        '''\n        if model is None:\n            model = self.latest_model\n        print(self.models[model]['plots'].keys())\n\n        # shows names of models in model dictionary\n\n        clearLog()\n\n    def model(self, model=None):\n        '''\n        Function that either returns the latest model or one specified and its information as a dictionary.\n        :param model: is the model key that you want to use.\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return self.models[model]\n\n    # shows the keys in the models dictionary\n\n    def info(self, model=None):\n        '''\n        Function that retrieves the model_data; all the information in self.models for that model\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        return get_model_data(self, model)\n        clearLog()\n\n    # returns all operators applicable to the client's models dictionary\n    def operators(self, model=None):\n        '''\n        Function that retrieves all of the operators; pipelines that were used to model the dataset\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        get_operators(self, model)\n        clearLog()\n\n    # show accuracy scores for client's model\n    def accuracy(self, model=None):\n        '''\n        Function that retrieves all of the accuracies in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_accuracy(self, model)\n\n    # show losses for client's model\n    def losses(self, model=None):\n        '''\n        Function that retrieves all of the losses in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_losses(self, model)\n\n    # return client model's target\n    def target(self, model=None):\n        '''\n        Function that retrieves all of the targets in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        returns target variable of model used in client instance\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_target(self, model)\n\n    # return NLP model's vocabulary\n    def vocab(self, model=None):\n        '''\n        Function that retrieves the NLP models vocabulary.\n        :param model: default to the latest model, but essentially the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        return get_vocab(self, model)\n\n    # plotting for client\n    def plots(self, model=None, plot=None, save=False):\n        '''\n        Function that retrieves all of plots in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentially the model key\n        :param plot: plot specified during the client session to be procured\n        :param save: option to save plots after client session is done (default is false, or\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        get_plots(self, model, plot, save)\n\n    # shows analysis of the model\n    def analyze(self, model=None, save=True, save_model=False):\n        '''\n        Function that retrieves all of plots in the self.models dictionary for the key.\n        :param model: default to the latest model, but essentailly the model key\n        '''\n        if model is None:\n            model = self.latest_model\n        clearLog()\n        analyze(self, model, save, save_model)\n\n    def dashboard(self):\n        dash = edaDashboard(self.dataset)\n        dash.dashboard()\n"
        ],
        "test_patch": "",
        "patch_preview": "From 5c2fb6b483b4c7e94d3b196dd15792524ce3124d Mon Sep 17 00:00:00 2001\nFrom: Vagif <vagal2003@gmail.com>\nDate: Wed, 12 Aug 2020 12:31:56 +0100\nSubject: [PATCH 1/3] Upgraded content recommender system using networkX\n\n---\n libra/queries.py                   |  37 +++-\n libra/query/recommender_systems.py | 271 ++++++++++++++---------------\n requirements.txt                   |   3 +-\n tests/tests.py                     |   2 +-\n 4 files changed, 167 insertions(+), 146 deletions(-)\n\ndiff --git a/lib"
      },
      "patch": {
        "length": 18622,
        "files_changed": 6,
        "lines_added": 170,
        "lines_deleted": 151,
        "net_change": 19,
        "changed_files": [
          {
            "file": "libra/queries.py",
            "added": 34,
            "deleted": 3
          },
          {
            "file": "libra/query/recommender_systems.py",
            "added": 130,
            "deleted": 141
          },
          {
            "file": "requirements.txt",
            "added": 2,
            "deleted": 1
          },
          {
            "file": "tests/tests.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "libra/query/recommender_systems.py",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "libra/queries.py",
            "added": 1,
            "deleted": 3
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 214,
        "total_lines": 218002,
        "total_bytes": 24161789,
        "python_files": 38,
        "python_lines": 8596,
        "file_extensions": {
          ".txt": 4,
          ".md": 9,
          ".cfg": 1,
          ".in": 1,
          ".py": 38,
          "": 3,
          ".html": 6,
          ".png": 17,
          ".jpg": 121,
          ".css": 2,
          ".js": 1,
          ".gif": 1,
          ".csv": 8,
          ".xlsx": 1,
          ".json": 1
        },
        "largest_files": [
          {
            "path": "tools/data/gh_images/gif.gif",
            "size": 3539004,
            "lines": 42990,
            "extension": ".gif"
          },
          {
            "path": "tools/data/structured_data/housing.csv",
            "size": 1423529,
            "lines": 20641,
            "extension": ".csv"
          },
          {
            "path": "tools/data/nlp_data/sentimentAnalysisData.csv",
            "size": 1677643,
            "lines": 11348,
            "extension": ".csv"
          },
          {
            "path": "tools/data/structured_data/housing.xlsx",
            "size": 992510,
            "lines": 7548,
            "extension": ".xlsx"
          },
          {
            "path": "tools/data/image_data/character_dataset_mini/b_lower/b_5.jpg",
            "size": 913627,
            "lines": 7259,
            "extension": ".jpg"
          },
          {
            "path": "tools/data/image_caption_pics/1.png",
            "size": 770693,
            "lines": 7253,
            "extension": ".png"
          },
          {
            "path": "tools/data/image_data/character_dataset_mini/b_lower/b_6.jpg",
            "size": 796049,
            "lines": 6556,
            "extension": ".jpg"
          },
          {
            "path": "tools/data/image_data/character_dataset_mini/b_lower/b_10.jpg",
            "size": 654051,
            "lines": 5343,
            "extension": ".jpg"
          },
          {
            "path": "tools/data/image_data/character_dataset_mini/b_lower/b_4.jpg",
            "size": 589000,
            "lines": 4587,
            "extension": ".jpg"
          },
          {
            "path": "docs/resources/comparison.png",
            "size": 635645,
            "lines": 4560,
            "extension": ".png"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 214,
        "files_changed_count": 6,
        "files_changed_ratio": 0.028037383177570093,
        "total_lines_in_repo": 218002,
        "lines_added": 170,
        "lines_deleted": 151,
        "net_lines_changed": 19,
        "lines_changed_ratio": 0.0014724635553802258,
        "pr_body_length": 722,
        "commit_message_length": 16,
        "python_file_count": 38,
        "python_line_count": 8596
      }
    },
    {
      "tar_file_name": "PiotrMachowski#Xiaomi-cloud-tokens-extractor#pull#2",
      "repo_name": "PiotrMachowski#Xiaomi-cloud-tokens-extractor#pull#2",
      "success": true,
      "error": null,
      "commit": {
        "sha": "1432706d3ebea09c0e06c46bf8fa8aac4dbf2829",
        "message": "Add option to check all available servers",
        "author": {
          "name": "Piotr Machowski",
          "email": "PiotrMachowski@users.noreply.github.com",
          "date": "2020-10-22T15:28:24Z"
        },
        "html_url": "https://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/commit/1432706d3ebea09c0e06c46bf8fa8aac4dbf2829",
        "api_url": "https://api.github.com/repos/PiotrMachowski/Xiaomi-cloud-tokens-extractor/commits/1432706d3ebea09c0e06c46bf8fa8aac4dbf2829"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/PiotrMachowski#Xiaomi-cloud-tokens-extractor#pull#2",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/PiotrMachowski#Xiaomi-cloud-tokens-extractor#pull#2.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/PiotrMachowski#Xiaomi-cloud-tokens-extractor#pull#2/source_code"
      },
      "pr": {
        "number": 2,
        "title": "Add Show Model on device",
        "body": "Add Show Model device",
        "state": "closed",
        "created_at": "2020-11-15T12:59:15Z",
        "updated_at": "2020-11-21T21:03:12Z",
        "merged_at": "2020-11-21T21:03:12Z",
        "html_url": "https://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/pull/2",
        "user": "anhnvme",
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "PiotrMachowski_Xiaomi-cloud-tokens-extractor-2",
        "repo": "/PiotrMachowski/Xiaomi-cloud-tokens-extractor",
        "base_commit": "1432706d3ebea09c0e06c46bf8fa8aac4dbf2829",
        "problem_statement": {},
        "edit_files": [
          "token_extractor.py"
        ],
        "oracle_files": [
          "import base64\nimport hashlib\nimport hmac\nimport json\nimport os\nimport time\nimport requests\nimport random\nfrom Crypto.Hash import MD5, SHA256\n\n\nclass XiaomiCloudConnector:\n\n    def __init__(self, username, password):\n        self._username = username\n        self._password = password\n        self._agent = self.generate_agent()\n        self._device_id = self.generate_device_id()\n        self._session = requests.session()\n        self._sign = None\n        self._ssecurity = None\n        self._userId = None\n        self._cUserId = None\n        self._passToken = None\n        self._location = None\n        self._code = None\n        self._serviceToken = None\n\n    def login_step_1(self):\n        url = \"https://account.xiaomi.com/pass/serviceLogin?sid=xiaomiio&_json=true\"\n        headers = {\n            \"User-Agent\": self._agent,\n            \"Content-Type\": \"application/x-www-form-urlencoded\"\n        }\n        cookies = {\n            \"userId\": self._username\n        }\n        response = self._session.get(url, headers=headers, cookies=cookies)\n        valid = response.status_code == 200 and \"_sign\" in self.to_json(response.text)\n        if valid:\n            self._sign = self.to_json(response.text)[\"_sign\"]\n        return valid\n\n    def login_step_2(self):\n        url = \"https://account.xiaomi.com/pass/serviceLoginAuth2\"\n        headers = {\n            \"User-Agent\": self._agent,\n            \"Content-Type\": \"application/x-www-form-urlencoded\"\n        }\n        fields = {\n            \"sid\": \"xiaomiio\",\n            \"hash\": (MD5.new(str.encode(self._password)).hexdigest() + \"\").upper(),\n            \"callback\": \"https://sts.api.io.mi.com/sts\",\n            \"qs\": \"%3Fsid%3Dxiaomiio%26_json%3Dtrue\",\n            \"user\": self._username,\n            \"_sign\": self._sign,\n            \"_json\": \"true\"\n        }\n        response = self._session.post(url, headers=headers, params=fields)\n        valid = response.status_code == 200 and \"ssecurity\" in self.to_json(response.text)\n        if valid:\n            json_resp = self.to_json(response.text)\n            self._ssecurity = json_resp[\"ssecurity\"]\n            self._userId = json_resp[\"userId\"]\n            self._cUserId = json_resp[\"cUserId\"]\n            self._passToken = json_resp[\"passToken\"]\n            self._location = json_resp[\"location\"]\n            self._code = json_resp[\"code\"]\n        return valid\n\n    def login_step_3(self):\n        headers = {\n            \"User-Agent\": self._agent,\n            \"Content-Type\": \"application/x-www-form-urlencoded\"\n        }\n        response = self._session.get(self._location, headers=headers)\n        if response.status_code == 200:\n            self._serviceToken = response.cookies.get(\"serviceToken\")\n        return response.status_code == 200\n\n    def login(self):\n        self._session.cookies.set(\"sdkVersion\", \"accountsdk-18.8.15\", domain=\"mi.com\")\n        self._session.cookies.set(\"sdkVersion\", \"accountsdk-18.8.15\", domain=\"xiaomi.com\")\n        self._session.cookies.set(\"deviceId\", self._device_id, domain=\"mi.com\")\n        self._session.cookies.set(\"deviceId\", self._device_id, domain=\"xiaomi.com\")\n        if self.login_step_1():\n            if self.login_step_2():\n                if self.login_step_3():\n                    return True\n                else:\n                    print(\"Unable to get service token.\")\n            else:\n                print(\"Invalid login or password.\")\n        else:\n            print(\"Invalid username.\")\n        return False\n\n    def get_devices(self, country):\n        url = self.get_api_url(country) + \"/home/device_list\"\n        params = {\n            \"data\": '{\"getVirtualModel\":false,\"getHuamiDevices\":0}'\n        }\n        return self.execute_api_call(url, params)\n\n    def execute_api_call(self, url, params):\n        headers = {\n            \"Accept-Encoding\": \"gzip\",\n            \"User-Agent\": self._agent,\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"x-xiaomi-protocal-flag-cli\": \"PROTOCAL-HTTP2\"\n        }\n        cookies = {\n            \"userId\": str(self._userId),\n            \"yetAnotherServiceToken\": str(self._serviceToken),\n            \"serviceToken\": str(self._serviceToken),\n            \"locale\": \"en_GB\",\n            \"timezone\": \"GMT+02:00\",\n            \"is_daylight\": \"1\",\n            \"dst_offset\": \"3600000\",\n            \"channel\": \"MI_APP_STORE\"\n        }\n        millis = round(time.time() * 1000)\n        nonce = self.generate_nonce(millis)\n        signed_nonce = self.signed_nonce(nonce)\n        signature = self.generate_signature(url.replace(\"/app\", \"\"), signed_nonce, nonce, params)\n        fields = {\n            \"signature\": signature,\n            \"_nonce\": nonce,\n            \"data\": params[\"data\"]\n        }\n        response = self._session.post(url, headers=headers, cookies=cookies, params=fields)\n        if response.status_code == 200:\n            return response.json()\n        return None\n\n    def get_api_url(self, country):\n        return \"https://\" + (\"\" if country == \"cn\" else (country + \".\")) + \"api.io.mi.com/app\"\n\n    def signed_nonce(self, nonce):\n        hash_object = SHA256.new()\n        hash_object.update(base64.b64decode(self._ssecurity) + base64.b64decode(nonce))\n        return base64.b64encode(hash_object.digest()).decode('utf-8')\n\n    @staticmethod\n    def generate_nonce(millis):\n        nonce_bytes = os.urandom(8) + (int(millis / 60000)).to_bytes(4, byteorder='big')\n        return base64.b64encode(nonce_bytes).decode()\n\n    @staticmethod\n    def generate_agent():\n        agent_id = \"\".join(map(lambda i: chr(i), [random.randint(65, 69) for _ in range(13)]))\n        return f\"Android-7.1.1-1.0.0-ONEPLUS A3010-136-{agent_id} APP/xiaomi.smarthome APPV/62830\"\n\n    @staticmethod\n    def generate_device_id():\n        return \"\".join(map(lambda i: chr(i), [random.randint(97, 122) for _ in range(6)]))\n\n    @staticmethod\n    def generate_signature(url, signed_nonce, nonce, params):\n        signature_params = [url.split(\"com\")[1], signed_nonce, nonce]\n        for k, v in params.items():\n            signature_params.append(f\"{k}={v}\")\n        signature_string = \"&\".join(signature_params)\n        signature = hmac.new(base64.b64decode(signed_nonce), msg=signature_string.encode(), digestmod=hashlib.sha256)\n        return base64.b64encode(signature.digest()).decode()\n\n    @staticmethod\n    def to_json(response_text):\n        return json.loads(response_text.replace(\"&&&START&&&\", \"\"))\n\n\nprint(\"Username (email or user ID):\")\nusername = input()\nprint(\"Password:\")\npassword = input()\nprint(\"Country (one of: ru, us, tw, sg, cn, de) Leave empty to check all available:\")\ncountry = input()\nwhile country not in [\"\", \"ru\", \"us\", \"tw\", \"sg\", \"cn\", \"de\"]:\n    print(\"Invalid country provided. Valid values: ru, us, tw, sg, cn, de\")\n    print(\"Country:\")\n    country = input()\n\nprint()\ncountries = [\"cn\", \"de\", \"us\", \"ru\", \"tw\", \"sg\"]\nif not country == \"\":\n    countries = [country]\n\nconnector = XiaomiCloudConnector(username, password)\nprint(\"Logging in...\")\nlogged = connector.login()\nif logged:\n    print(\"Logged in.\")\n    print()\n    for current_country in countries:\n        devices = connector.get_devices(current_country)\n        if devices is not None:\n            if len(devices[\"result\"][\"list\"]) == 0:\n                print(f\"No devices found for country \\\"{current_country}\\\".\")\n                continue\n            print(f\"Devices found for country \\\"{current_country}\\\":\")\n            for device in devices[\"result\"][\"list\"]:\n                print(\"   ---------\")\n                if \"name\" in device:\n                    print(\"   NAME:  \" + device[\"name\"])\n                if \"did\" in device:\n                    print(\"   ID:    \" + device[\"did\"])\n                if \"localip\" in device:\n                    print(\"   IP:    \" + device[\"localip\"])\n                if \"token\" in device:\n                    print(\"   TOKEN: \" + device[\"token\"])\n            print(\"   ---------\")\n            print()\n        else:\n            print(\"Unable to get devices.\")\nelse:\n    print(\"Unable to log in.\")\n\nprint()\nprint(\"Press ENTER to finish\")\ninput()\n"
        ],
        "test_patch": "",
        "patch_preview": "From 78ebd8cdd05fe0718c5acd774dfaf5ac4450e6dc Mon Sep 17 00:00:00 2001\nFrom: anhnvme <anhnvme@users.noreply.github.com>\nDate: Sun, 15 Nov 2020 19:58:42 +0700\nSubject: [PATCH] Update token_extractor.py\n\nAdd Show Model device\n---\n token_extractor.py | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a/token_extractor.py b/token_extractor.py\nindex 8f701f1..e4259e7 100644\n--- a/token_extractor.py\n+++ b/token_extractor.py\n@@ -208,6 +208,8 @@ def to_json(response_text):\n                     print(\"  "
      },
      "patch": {
        "length": 798,
        "files_changed": 1,
        "lines_added": 2,
        "lines_deleted": 0,
        "net_change": 2,
        "changed_files": [
          {
            "file": "token_extractor.py",
            "added": 2,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 4,
        "total_lines": 75339,
        "total_bytes": 10000720,
        "python_files": 1,
        "python_lines": 220,
        "file_extensions": {
          ".py": 1,
          "": 1,
          ".exe": 1,
          ".md": 1
        },
        "largest_files": [
          {
            "path": "token_extractor.exe",
            "size": 9990340,
            "lines": 75068,
            "extension": ".exe"
          },
          {
            "path": "token_extractor.py",
            "size": 8102,
            "lines": 220,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 1206,
            "lines": 30,
            "extension": ".md"
          },
          {
            "path": "LICENSE",
            "size": 1072,
            "lines": 21,
            "extension": ""
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 4,
        "files_changed_count": 1,
        "files_changed_ratio": 0.25,
        "total_lines_in_repo": 75339,
        "lines_added": 2,
        "lines_deleted": 0,
        "net_lines_changed": 2,
        "lines_changed_ratio": 2.6546675692536404e-05,
        "pr_body_length": 21,
        "commit_message_length": 41,
        "python_file_count": 1,
        "python_line_count": 220
      }
    },
    {
      "tar_file_name": "Rapptz#RoboDanny#pull#117",
      "repo_name": "Rapptz#RoboDanny#pull#117",
      "success": true,
      "error": null,
      "commit": {
        "sha": "0dfa21599da76e84c2f8e7fde0c132ec93c840a8",
        "message": "Remove ?tester for the foreseeable future",
        "author": {
          "name": "Rapptz",
          "email": "rapptz@gmail.com",
          "date": "2021-05-03T04:21:43Z"
        },
        "html_url": "https://github.com/Rapptz/RoboDanny/commit/0dfa21599da76e84c2f8e7fde0c132ec93c840a8",
        "api_url": "https://api.github.com/repos/Rapptz/RoboDanny/commits/0dfa21599da76e84c2f8e7fde0c132ec93c840a8"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Rapptz#RoboDanny#pull#117",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/Rapptz#RoboDanny#pull#117.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/Rapptz#RoboDanny#pull#117/source_code"
      },
      "pr": {
        "number": 117,
        "title": "Update readme invite information",
        "body": "* This doesn't seem to be a valid point since the bot is going under verification\r\n* Little change to avoid missinformation",
        "state": "closed",
        "created_at": "2021-06-21T06:10:41Z",
        "updated_at": "2021-06-27T04:03:56Z",
        "merged_at": null,
        "html_url": "https://github.com/Rapptz/RoboDanny/pull/117",
        "user": "ghost",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "Rapptz_RoboDanny-117",
        "repo": "/Rapptz/RoboDanny",
        "base_commit": "0dfa21599da76e84c2f8e7fde0c132ec93c840a8",
        "problem_statement": {},
        "edit_files": [
          "README.md"
        ],
        "oracle_files": [
          "## R. Danny\n\nA personal bot that runs on Discord.\n\n## Running\n\nI would prefer if you don't run an instance of my bot. Just call the join command with an invite URL to have it on your server. The source here is provided for educational purposes for discord.py.\n\nNevertheless, the installation steps are as follows:\n\n1. **Make sure to get Python 3.5 or higher**\n\nThis is required to actually run the bot.\n\n2. **Set up venv**\n\nJust do `python3.6 -m venv venv`\n\n3. **Install dependencies**\n\nThis is `pip install -U -r requirements.txt`\n\n4. **Create the database in PostgreSQL**\n\nYou will need PostgreSQL 9.5 or higher and type the following\nin the `psql` tool:\n\n```sql\nCREATE ROLE rdanny WITH LOGIN PASSWORD 'yourpw';\nCREATE DATABASE rdanny OWNER rdanny;\nCREATE EXTENSION pg_trgm;\n```\n\n5. **Setup configuration**\n\nThe next step is just to create a `config.py` file in the root directory where\nthe bot is with the following template:\n\n```py\nclient_id   = '' # your bot's client ID\ntoken = '' # your bot's token\ncarbon_key = '' # your bot's key on carbon's site\nbots_key = '' # your key on bots.discord.pw\npostgresql = 'postgresql://user:password@host/database' # your postgresql info from above\nchallonge_api_key = '...' # for tournament cog\nstat_webhook = ('<webhook_id>','<webhook_token>') # a webhook to a channel for bot stats. \n# when you generate your webhook, take the token and ID from the URL like so: \n# https://discord.com/api/webhooks/<id>/<token>\n```\n\n6. **Configuration of database**\n\nTo configure the PostgreSQL database for use by the bot, go to the directory where `launcher.py` is located, and run the script by doing `python3.6 launcher.py db init`\n\n## Requirements\n\n- Python 3.6+\n- v1.0.0 of discord.py\n- lxml\n- psutil\n"
        ],
        "test_patch": "",
        "patch_preview": "From 3d9463e0dc2cac864f49d8bf883c6c8dd80fb508 Mon Sep 17 00:00:00 2001\nFrom: Sitanshu15 <80409424+Sitanshu-15@users.noreply.github.com>\nDate: Mon, 21 Jun 2021 11:55:00 +0545\nSubject: [PATCH] Update readme invite information\n\n* This doesn't seem to be a valid point since the bot is going under verification\n---\n README.md | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/README.md b/README.md\nindex 68395209..d176f8d3 100644\n--- a/README.md\n+++ b/README.md\n@@ -4,7 +4,7 @@ A person"
      },
      "patch": {
        "length": 926,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "README.md",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [
        {
          "id": 869096709,
          "body": "Hey, @Rapptz I just saw that you said you would prefer us not to run an instance and this pr completely goes against this point , therefore i'll be closing this pr for  now.",
          "user": "ghost",
          "created_at": "2021-06-27T04:03:39Z",
          "html_url": "https://github.com/Rapptz/RoboDanny/pull/117#issuecomment-869096709"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 37,
        "total_lines": 17792,
        "total_bytes": 661850,
        "python_files": 34,
        "python_lines": 17348,
        "file_extensions": {
          ".py": 34,
          ".txt": 2,
          ".md": 1
        },
        "largest_files": [
          {
            "path": "cogs/tournament.py",
            "size": 76537,
            "lines": 1982,
            "extension": ".py"
          },
          {
            "path": "cogs/mod.py",
            "size": 73728,
            "lines": 1840,
            "extension": ".py"
          },
          {
            "path": "cogs/tags.py",
            "size": 47469,
            "lines": 1285,
            "extension": ".py"
          },
          {
            "path": "cogs/stars.py",
            "size": 49396,
            "lines": 1258,
            "extension": ".py"
          },
          {
            "path": "cogs/splatoon.py",
            "size": 47441,
            "lines": 1238,
            "extension": ".py"
          },
          {
            "path": "cogs/stats.py",
            "size": 40864,
            "lines": 1087,
            "extension": ".py"
          },
          {
            "path": "cogs/utils/db.py",
            "size": 34371,
            "lines": 1000,
            "extension": ".py"
          },
          {
            "path": "cogs/api.py",
            "size": 30990,
            "lines": 835,
            "extension": ".py"
          },
          {
            "path": "cogs/meta.py",
            "size": 25563,
            "lines": 665,
            "extension": ".py"
          },
          {
            "path": "cogs/config.py",
            "size": 18988,
            "lines": 524,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 37,
        "files_changed_count": 1,
        "files_changed_ratio": 0.02702702702702703,
        "total_lines_in_repo": 17792,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.00011241007194244605,
        "pr_body_length": 123,
        "commit_message_length": 41,
        "python_file_count": 34,
        "python_line_count": 17348
      }
    },
    {
      "tar_file_name": "RussBaz#enforce#pull#28",
      "repo_name": "RussBaz#enforce#pull#28",
      "success": true,
      "error": null,
      "commit": {
        "sha": "67b14610d4e7fa1867f2e1719ae1c80817a8010f",
        "message": "Updated the setup.py file for the 0.3.0 release.",
        "author": {
          "name": "RussBaz",
          "email": "RussBaz@users.noreply.github.com",
          "date": "2016-09-13T22:32:27Z"
        },
        "html_url": "https://github.com/RussBaz/enforce/commit/67b14610d4e7fa1867f2e1719ae1c80817a8010f",
        "api_url": "https://api.github.com/repos/RussBaz/enforce/commits/67b14610d4e7fa1867f2e1719ae1c80817a8010f"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/RussBaz#enforce#pull#28",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/RussBaz#enforce#pull#28.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/RussBaz#enforce#pull#28/source_code"
      },
      "pr": {
        "number": 28,
        "title": "If there is no 'return' annotation, return output instead of 'None'",
        "body": "Setup:\n\n```\nfrom enforce import runtime_validation\n\n@runtime_validation\ndef goofus(a: int):\n  return a\n\n@runtime_validation\ndef gallant(a: int) -> int:\n  return a\n```\n\nOriginal behavior:\n\n```\n>>> print(gallant(1)) # ok\n1\n>>> print(goofus(1)) # bug\nNone\n```\n\nNew behavior:\n\n```\n>>> print(goofus(1)) # ok now\n1\n```\n",
        "state": "closed",
        "created_at": "2016-09-17T02:26:33Z",
        "updated_at": "2016-09-17T16:40:57Z",
        "merged_at": "2016-09-17T16:38:56Z",
        "html_url": "https://github.com/RussBaz/enforce/pull/28",
        "user": "hwayne",
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "commits": 2
      },
      "swebench": {
        "instance_id": "RussBaz_enforce-28",
        "repo": "/RussBaz/enforce",
        "base_commit": "67b14610d4e7fa1867f2e1719ae1c80817a8010f",
        "problem_statement": {},
        "edit_files": [
          "enforce/enforcers.py",
          "tests/test_enforce.py",
          "tests/test_enforce.py"
        ],
        "oracle_files": [
          "import typing\nimport inspect\nfrom collections import namedtuple, OrderedDict\n\nfrom wrapt import ObjectProxy\n\nfrom .types import EnhancedTypeVar, is_type_of_type\nfrom .exceptions import RuntimeTypeError\nfrom .validator import init_validator, Validator\n\n\n# This TypeVar is used to indicate that he result of output validation\n# is the same as the input to the output validation\nT = typing.TypeVar('T')\n\n# Convenience type for storing all incoming arguments in a single container\nParameters = namedtuple('Parameters', ['args', 'kwargs', 'skip'])\n\n\nclass Enforcer:\n    \"\"\"\n    A container for storing type checking logic of functions\n    \"\"\"\n    def __init__(self, validator, signature, hints, generic=False, bound=False, settings=None):\n        self.validator = validator\n        self.signature = signature\n        self.hints = hints\n        self.settings = settings\n\n        self.validator.settings = self.settings\n\n        self.generic = generic\n        self.bound = bound\n\n        self.reference = None\n\n        self._callable_signature = None\n\n    @property\n    def callable_signature(self):\n        \"\"\"\n        A property which returns _callable_signature (Callable type of the function)\n        If it is None, then it generates a new Callable type from the object's signature\n        \"\"\"\n        if self.settings is not None and not self.settings:\n            return typing.Callable\n\n        if hasattr(self.reference, '__no_type_check__'):\n            return typing.Callable\n\n        if self._callable_signature is None:\n            self._callable_signature = generate_callable_from_signature(self.signature)\n\n        return self._callable_signature\n\n    def validate_inputs(self, input_data: Parameters) -> Parameters:\n        \"\"\"\n        Calls a validator for each function argument\n        \"\"\"\n        if self.settings is not None and not self.settings.enabled:\n            return input_data\n\n        if input_data.skip:\n            return input_data\n\n        args = input_data.args\n        kwargs = input_data.kwargs\n        skip = input_data.skip\n\n        binded_arguments = self.signature.bind(*args, **kwargs)\n        binded_arguments.apply_defaults()\n\n        for name in self.hints.keys():\n            # First, check argument types (every key not labeled 'return')\n            if name != 'return':\n                argument = binded_arguments.arguments.get(name)\n                if not self.validator.validate(argument, name):\n                    break\n                binded_arguments.arguments[name] = self.validator.data_out[name]\n        else:\n            valdated_data = Parameters(binded_arguments.args, binded_arguments.kwargs, skip)\n            return valdated_data\n\n        exception_text = parse_errors(self.validator.errors, self.hints)\n        raise RuntimeTypeError(exception_text)\n\n    def validate_outputs(self, output_data: T) -> T:\n        \"\"\"\n        Calls a validator on a function return value\n        \"\"\"\n        if self.settings is not None and not self.settings.enabled:\n            return output_data\n\n        if 'return' in self.hints.keys():\n            if not self.validator.validate(output_data, 'return'):\n                exception_text = parse_errors(self.validator.errors, self.hints, True)\n                raise RuntimeTypeError(exception_text)\n            else:\n                return self.validator.data_out['return']\n\n    def reset(self):\n        \"\"\"\n        Clears validator internal state\n        \"\"\"\n        self.validator.reset()\n\n\nclass GenericProxy(ObjectProxy):\n    \"\"\"\n    A proxy object for typing.Generics user defined subclasses which always returns proxied objects\n    \"\"\"\n    __enforcer__ = None\n\n    def __init__(self, wrapped):\n        \"\"\"\n        Creates an enforcer instance on a just wrapped user defined Generic\n        \"\"\"\n        wrapped_type = type(wrapped)\n\n        if is_type_of_type(wrapped_type, GenericProxy):\n            super().__init__(wrapped.__wrapped__)\n            apply_enforcer(self, generic=True, instance_of=self)\n        elif is_type_of_type(wrapped_type, typing.GenericMeta):\n            super().__init__(wrapped)\n            apply_enforcer(self, generic=True)\n        else:\n            raise TypeError('Only generics can be wrapped in GenericProxy')\n\n    def __call__(self, *args, **kwargs):\n        return apply_enforcer(self.__wrapped__(*args, **kwargs), generic=True, instance_of=self)\n\n    def __getitem__(self, param):\n        \"\"\"\n        Wraps a normal typed Generic in another proxy and applies enforcers for generics on it\n        \"\"\"\n        return GenericProxy(self.__wrapped__.__getitem__(param))\n\n\ndef apply_enforcer(func: typing.Callable,\n                   generic: bool=False,\n                   settings = None,\n                   parent_root: typing.Optional[Validator]=None,\n                   instance_of: typing.Optional[GenericProxy]=None) -> typing.Callable:\n    \"\"\"\n    Adds an Enforcer instance to the passed function/generic if it doesn't yet exist\n    or if it is not an instance of Enforcer\n\n    Such instance is added as '__enforcer__'\n    \"\"\"\n    if not hasattr(func, '__enforcer__') or not isinstance(func.__enforcer__, Enforcer):\n        # Replaces 'incorrect' enforcers\n        func.__enforcer__ = generate_new_enforcer(func, generic, parent_root, instance_of, settings)\n        func.__enforcer__.reference = func\n\n    return func\n\n\ndef generate_new_enforcer(func, generic, parent_root, instance_of, settings):\n    \"\"\"\n    Private function for generating new Enforcer instances for the incoming function\n    \"\"\"\n    if parent_root is not None:\n        if type(parent_root) is not Validator:\n            raise TypeError('Parent validator must be a Validator')\n\n    if instance_of is not None:\n        if type(instance_of) is not GenericProxy:\n            raise TypeError('Instance of a generic must be derived from a valid Generic Proxy')\n\n    if generic:\n        hints = OrderedDict()\n\n        if instance_of:\n            func = instance_of\n\n        func_type = type(func)\n\n        has_origin = func.__origin__ is not None\n\n        # Collects generic's parameters - TypeVar-s specified on itself or on origin (if constrained)\n        if not func.__parameters__ and (not has_origin or not func.__origin__.__parameters__):\n            raise TypeError('User defined generic is invalid')\n\n        parameters = func.__parameters__ if func.__parameters__ else func.__origin__.__parameters__\n\n        # Maps parameter names to parameters, while preserving the order of their definition\n        for param in parameters:\n            hints[param.__name__] = EnhancedTypeVar(param.__name__, type_var=param)\n\n        # Verifies that constraints do not contradict generic's parameter definition\n        # and bounds parameters to constraints (if constrained)\n        bound = bool(func.__args__)\n        if bound:\n            for i, param in enumerate(hints.values()):\n                arg = func.__args__[i]\n                if is_type_of_type(arg, param):\n                    param.__bound__ = arg\n                else:\n                    raise TypeError('User defined generic does not accept provided constraints')\n\n        # NOTE:\n        # Signature in generics should always point to the original unconstrained generic\n        # This applies even to the instances of such Generics\n\n        if has_origin:\n            signature = func.__origin__\n        else:\n            signature = func.__wrapped__ if func_type is GenericProxy else func\n\n        validator = init_validator(hints, parent_root)\n    else:\n        bound = False\n        signature = inspect.signature(func)\n        hints = typing.get_type_hints(func)\n        validator = init_validator(hints, parent_root)\n\n    return Enforcer(validator, signature, hints, generic, bound, settings)\n\n\ndef parse_errors(errors: typing.List[str], hints:typing.Dict[str, type], return_type: bool=False) -> str:\n    \"\"\"\n    Generates an exception message based on which fields failed\n    \"\"\"\n    error_message = \"       Argument '{0}' was not of type {1}. Actual type was {2}.\"\n    return_error_message = \"        Return value was not of type {0}. Actual type was {1}\"\n    output = \"\\n  The following runtime type errors were encountered:\"\n\n    for error in errors:\n        argument_name, argument_type = error\n        hint = hints.get(argument_name, type(None))\n        if hint is None:\n            hint = type(None)\n        if return_type:\n            output += '\\n' + return_error_message.format(hint, argument_type)\n        else:\n            output += '\\n' +  error_message.format(argument_name, hint, argument_type)\n    return output\n\n\ndef generate_callable_from_signature(signature):\n    \"\"\"\n    Generates a type from a signature of Callable object\n    \"\"\"\n    # TODO: (*args, **kwargs) should result in Ellipsis (...) as a parameter\n    result = typing.Callable\n    any_positional = False\n    positional_arguments = []\n\n    for param in signature.parameters.values():\n        if param.kind == param.KEYWORD_ONLY or param.kind == param.VAR_KEYWORD:\n            break\n\n        if param.kind == param.VAR_POSITIONAL:\n            any_positional = True\n\n        if param.annotation is inspect._empty:\n            positional_arguments.append(typing.Any)\n        else:\n            positional_arguments.append(param.annotation)\n    else:\n        return_type = signature.return_annotation\n        if return_type is inspect._empty:\n            return_type = typing.Any\n\n        if any_positional and all([a == typing.Any for a in positional_arguments]):\n            positional_arguments = ...\n            if return_type != typing.Any:\n                result = typing.Callable[positional_arguments, return_type]\n        elif (len(positional_arguments) == 0 or\n            any([a != typing.Any for a in positional_arguments]) or\n            return_type is not typing.Any):\n            result = typing.Callable[positional_arguments, return_type]\n\n    return result\n",
          "import typing\nimport unittest\nimport numbers\n\nfrom enforce import runtime_validation, config\nfrom enforce.types import EnhancedTypeVar\nfrom enforce.exceptions import RuntimeTypeError\n\n\nclass GeneralTests(unittest.TestCase):\n    \"\"\"\n    A container for general tests\n    \"\"\"\n\n    def test_argument_validation(self):\n        print(self.sample_function)\n        self.assertEqual(self.sample_function('11', 1), 12)\n\n        result = 0\n        with self.assertRaises(RuntimeTypeError):\n            result += self.sample_function(1, 2)\n\n        self.assertEqual(result, 0)\n\n    def test_return_value_validation(self):\n        self.assertIsNone(self.sample_function('', None))\n\n        result = 0\n        with self.assertRaises(RuntimeTypeError):\n            result += self.sample_function('', 1)\n\n        self.assertEqual(result, 0)\n\n    def test_no_type_check(self):\n        \"\"\"\n        Verifies that no_type_check is respected\n        \"\"\"\n        def get_sample_func():\n            def sample(data: int):\n                pass\n\n            return sample\n\n        sample_d1 = typing.no_type_check(runtime_validation(get_sample_func()))\n        sample_d2 = runtime_validation(typing.no_type_check(get_sample_func()))\n\n        sample_d3 = runtime_validation(get_sample_func())\n\n        sample_d4 = typing.no_type_check_decorator(runtime_validation(get_sample_func()))\n        sample_d5 = runtime_validation(typing.no_type_check_decorator(get_sample_func()))\n\n        get_sample_func()('str')\n        sample_d1('str')\n        sample_d2('str')\n        with self.assertRaises(RuntimeTypeError):\n            sample_d3('str')\n\n    @runtime_validation\n    def sample_function(self, text: str, data: typing.Union[int, None]) -> typing.Optional[int]:\n        try:\n            return int(text) + data\n        except ValueError:\n            if data is None:\n                return None\n            # Deliberate return type error\n            return str(data)\n\n\nclass SimpleTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the simple types which do not require special processing\n    \"\"\"\n\n    def setUp(self):\n        config(reset=True)\n\n    def tearDown(self):\n        config(reset=True)\n\n    def test_any(self):\n        @runtime_validation\n        def sample(data: typing.Any) -> typing.Any:\n            return data\n\n        self.assertEqual(sample(100.3), 100.3)\n        self.assertIsNone(sample(None))\n\n    def test_none(self):\n        @runtime_validation\n        def sample(data: None) -> None:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> None:\n            return data\n\n        self.assertIsNone(sample(None))\n\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bool(self):\n        @runtime_validation\n        def sample(data: bool) -> bool:\n            return not data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bool:\n            return data\n\n        self.assertFalse(sample(True))\n\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('string')\n\n    def test_int(self):\n        @runtime_validation\n        def sample(data: int) -> int:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> int:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        with self.assertRaises(RuntimeTypeError):\n            sample(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_float(self):\n        \"\"\"\n        Floats should accept only floats in invariant mode\n        \"\"\"\n        @runtime_validation\n        def sample(data: float) -> float:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> float:\n            return data\n\n        self.assertEqual(sample(1.0), 1.0)\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n        config({'mode': 'covariant'})\n        sample(1)\n\n    def test_complex(self):\n        \"\"\"\n        Complex numbers should accept complex, integers and floats\n        \"\"\"\n        @runtime_validation\n        def sample(data: complex) -> complex:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> complex:\n            return data\n\n        self.assertEqual(sample(1+1j), 1+1j)\n        self.assertEqual(sample(1), 1)\n        self.assertEqual(sample(1.0), 1.0)\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_string(self):\n        @runtime_validation\n        def sample(data: str) -> str:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> str:\n            return data\n\n        self.assertEqual(sample(''), '')\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bytes(self):\n        \"\"\"\n        Bytes should accept bytes as well bytearray and memorieview\n        \"\"\"\n        @runtime_validation\n        def sample(data: bytes) -> bytes:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bytes:\n            return data\n\n        self.assertEqual(sample(b''), b'')\n        self.assertEqual(sample(bytearray(2)), bytearray(2))\n        self.assertEqual(sample(memoryview(b'')), memoryview(b''))\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bytearray(self):\n        @runtime_validation\n        def sample(data: bytearray) -> bytearray:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bytearray:\n            return data\n\n        self.assertEqual(sample(bytearray(2)), bytearray(2))\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_memoryview(self):\n        @runtime_validation\n        def sample(data: memoryview) -> memoryview:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> memoryview:\n            return data\n\n        self.assertEqual(sample(memoryview(b'')), memoryview(b''))\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n\nclass ComplexTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the simple types which require special processing\n    \"\"\"\n    def setUp(self):\n        config(reset=True)\n\n    def tearDown(self):\n        config(reset=True)\n\n    def get_type_var_func(self, configurable=False, type_var=None):\n        if type_var is None:\n            A = typing.TypeVar('A')\n        else:\n            A = type_var\n\n        def type_var_func(data: A) -> A:\n            return data\n\n        def configurable_type_var_func(data: typing.Any, type_option: A) -> A:\n            return data\n\n        if configurable:\n            return runtime_validation(configurable_type_var_func)\n        else:\n            return runtime_validation(type_var_func)\n\n    def test_checking_mode(self):\n        \"\"\"\n        Verifies that settings affect the selected type checking mode - covariant/contravariant\n        \"\"\"\n        @runtime_validation\n        def func(data: numbers.Integral):\n            pass\n\n        @runtime_validation\n        def func2(data: typing.Union[float, str]):\n            pass\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n        config({'mode': 'covariant'})\n\n        func(1)\n        func(True)\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        func2('hello')\n        func2(1.0)\n        func2(1)\n\n        config({'mode': 'contravariant'})\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n        config({'mode': 'bivariant'})\n\n        func(1)\n        func(1.0)\n        func(True)\n\n        func2('hello')\n        func2(1.0)\n        func2(1)\n\n        config({'mode': 'invariant'})\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n    def test_optional(self):\n        @runtime_validation\n        def sample(data: typing.Optional[int]) -> typing.Optional[int]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Union[int]:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        self.assertIsNone(sample(None))\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_tuple(self):\n        @runtime_validation\n        def sample(data: typing.Tuple[int, str]) -> typing.Tuple[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Tuple[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_any_in(data: typing.Tuple) -> typing.Tuple:\n            return data\n\n        @runtime_validation\n        def sample_any_out(data: typing.Any) -> typing.Tuple:\n            return data\n\n        self.assertEqual(sample((1, '')), (1, ''))\n        with self.assertRaises(RuntimeTypeError):\n            sample((1, 1))\n\n        with self.assertRaises(RuntimeTypeError):\n            sample(())\n\n        with self.assertRaises(RuntimeTypeError):\n            sample([])\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad((''))\n\n        self.assertEqual(sample_any_in((1, '')), (1, ''))\n        with self.assertRaises(RuntimeTypeError):\n            sample_any_in(1)\n\n        self.assertEqual(sample_any_out((1,)), (1,))\n        with self.assertRaises(RuntimeTypeError):\n            sample_any_out(1)\n\n    def test_variable_length_tuple(self):\n        # TODO: What if tuple is empty?\n        @runtime_validation\n        def sample_in(data: typing.Tuple[int, ...]) -> typing.Any:\n            return data\n\n        @runtime_validation\n        def sample_out(data: typing.Any) -> typing.Tuple[int, ...]:\n            return data\n        \n        good = (1, 3, 4)\n        bad = (1, 'a', 2)\n        empty = ()\n\n        self.assertEqual(sample_in(good), good)\n        self.assertEqual(sample_out(good), good)\n        self.assertEqual(sample_in(empty), empty)\n        self.assertEqual(sample_out(empty), empty)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_in(bad)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_out(bad)\n\n    def test_simple_unbounded_type_var(self):\n        type_var_func = self.get_type_var_func()\n        bad_type_var_func = self.get_type_var_func(configurable=True)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(bad_type_var_func('', 'hello world'), '')\n\n        with self.assertRaises(RuntimeTypeError):\n            bad_type_var_func('', 1)\n\n    def test_simple_bounded_type_var(self):\n        # Invariant case\n        A = typing.TypeVar('A', int, str)\n\n        type_var_func = self.get_type_var_func(type_var=A)\n        bad_type_var_func = self.get_type_var_func(configurable=True, type_var=A)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(type_var_func(''), '')\n        self.assertEqual(bad_type_var_func(1, 1), 1)\n        self.assertEqual(bad_type_var_func('', ''), '')\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            bad_type_var_func(1.0, 1)\n\n    def test_covariant_type_var(self):\n        A = typing.TypeVar('A', bound=numbers.Number, covariant=True)\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(type_var_func(1.0), 1.0)\n        self.assertEqual(type_var_func(1+1j), 1+1j)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func('bad')\n\n    def test_contravariant_type_var(self):\n        class B:\n            pass\n\n        class C(B):\n            pass\n\n        class D(C):\n            pass\n\n        A = typing.TypeVar('A', bound=C, contravariant=True)\n\n        b = B()\n        c = C()\n        d = D()\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertIs(type_var_func(c), c)\n        self.assertIs(type_var_func(b), b)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func(d)\n\n    def test_bivariant_type_var(self):\n        class B:\n            pass\n\n        class C(B):\n            pass\n\n        class D(C):\n            pass\n\n        A = EnhancedTypeVar('A', bound=C, covariant=True, contravariant=True)\n\n        b = B()\n        c = C()\n        d = D()\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertIs(type_var_func(c), c)\n        self.assertIs(type_var_func(b), b)\n        self.assertIs(type_var_func(d), d)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func('bad')\n\n\nclass ListTypesTests(unittest.TestCase):\n    def setUp(self):\n        @runtime_validation\n        def str_func(x: typing.List[str]) -> str:\n            return x[0]\n        self.str_func = str_func\n\n        @runtime_validation\n        def int_func(x: typing.List[int]) -> int:\n            return x[0]\n        self.int_func = int_func\n\n        def int_str_func(x: typing.List[typing.Union[str, int]]) -> int:\n            return int(x[0])\n        self.union_func = int_str_func\n\n    def test_str_list(self):\n        self.str_func(['a'])\n        self.str_func(['a', 'b', 'c'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func(3)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func('3')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func([1, 2, 3])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func([1, 'b', 5.0])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func(['a', 1, 'b', 5.0])\n\n    def test_int_list(self):\n        self.int_func([1])\n        self.int_func([1, 2, 3])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(5)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func('5')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['1', '2', 'a'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['a', 1, 'b', 5.0])\n\n    def test_union_func(self):\n        self.union_func([1])\n        self.union_func([1, 2, 3])\n        self.union_func(['1'])\n        self.union_func(['1', '2', '3'])\n        self.union_func([1, '2', 3, '4'])\n        self.union_func(['1', 2, '3', 4])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func('a')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func([[1, 2, 3], '4'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['a', 'b', {3, 4, 5}])\n\n\nclass UnionTypesTests(unittest.TestCase):\n    \"\"\"\n    Test case for Union Types\n    \"\"\"\n\n    def setUp(self):\n        @runtime_validation\n        def test_func(x: typing.Union[float, typing.List[str]]) -> int:\n            return 5\n        @runtime_validation\n        def nest_func(x: typing.Union[float, typing.List[typing.Union[str, int]]]) -> int:\n            return 5\n        self.test_func = test_func\n        self.nest_func = nest_func\n\n    def test_basic_union(self):\n        @runtime_validation\n        def sample(data: typing.Union[int, str]) -> typing.Union[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Union[int, str]:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        self.assertEqual(sample(''), '')\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1.0)\n\n    def test_good_nested_union(self):\n        self.test_func(5.0)\n        self.test_func(['1', '2', 'a'])\n\n    def test_bad_nested_union(self):\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func('a')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func([1, 2, 3, 4])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func(['a', 4, 5])\n\n    def test_nested_func_good(self):\n        self.nest_func(5.0)\n        self.nest_func(['a', 'b', 'c'])\n        self.nest_func([1, 2, 3])\n        self.nest_func([1, 'a', 2, 'b'])\n\n    def test_nested_func_bad(self):\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func('a')\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func({'a': 5, 'b':6})\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func({1, 2, 3, 4})\n\n\nclass ContainerTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the container types - types of unbounded size\n    \"\"\"\n    pass\n\n\nclass IterableTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for iterator and generator support\n    \"\"\"\n    pass\n\n\nclass CallableTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the callable types such as functions\n    \"\"\"\n    def setUp(self):\n        @runtime_validation\n        def test(func: typing.Callable[[int, int], int], x: int) -> int:\n            return func(x, x)\n        @runtime_validation\n        def test_list(func: typing.Callable[[typing.Union[typing.List[typing.Any], int]],\n                                            int]) -> int:\n            return func(5)\n        @runtime_validation\n        def union(func: typing.Callable[[typing.Union[float, int], typing.Optional[str]],\n                                        int]) -> int:\n            return func(5)\n\n        @runtime_validation\n        def any_func_args(func: typing.Callable):\n            return func\n\n        @runtime_validation\n        def any_func_return(func) -> typing.Callable:\n            return func\n\n        self.test = test\n        self.test_list = test_list\n        self.union = union\n        self.any_func_args = any_func_args\n        self.any_func_return = any_func_return\n\n    # TODO: rename this test\n    def test_unrestrained_callable_arguments(self):\n        \"\"\"\n        Verifies that a function which expects any Callable as an argument,\n        would fail if an object of different type is passed\n        \"\"\"\n        callable = lambda x: x\n        self.any_func_args(callable)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.any_func_args('bad_input')\n\n    # TODO: rename this test\n    def test_unrestrained_callable_returns(self):\n        \"\"\"\n        Verifies that a function which expects any Callable as an output,\n        would fail if an object of different type is returned\n        \"\"\"\n        callable = lambda x: x\n        self.any_func_return(callable), callable\n\n        with self.assertRaises(RuntimeTypeError):\n            self.any_func_return('bad_input')\n\n    def test_good_func_arg(self):\n        \"\"\" Test that good arguments pass \"\"\"\n        def good(x: int, y: int) -> int:\n            return int(x * y)\n\n        self.test(good, 5)\n\n    def test_bad_func_return(self):\n        \"\"\"\n        Test that a function being passed in with mismatching return raises\n        \"\"\"\n        def bad_return(x: int, y: int) -> float:\n            return float(x * y)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test(bad_return, 5)\n\n    def test_bad_func_call(self):\n        \"\"\"\n        Test that a function being passed in with mismatching callsig raises\n        \"\"\"\n        def bad_callsig(x: str, y: str) -> int:\n            return int(x + y)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test(bad_callsig, 5)\n\n    def test_bad_func(self):\n        \"\"\"\n        Test that passing in something that's not a function raises\n        \"\"\"\n        with self.assertRaises(RuntimeTypeError):\n            self.test(5, 5)\n\n    def test_nested_func(self):\n        \"\"\"\n        Test that a function with deeply nested types works\n        \"\"\"\n        def nest_func(x: typing.Union[int, typing.List[typing.Any]]) -> int:\n            return 5\n        self.test_list(nest_func)\n\n    def test_nested_bad_func(self):\n        \"\"\"\n        Test that a function with bad deeply nested types fails\n        \"\"\"\n        def nest_func(x: typing.List[typing.List[int]]) -> int:\n            return 5\n        with self.assertRaises(RuntimeTypeError):\n            self.test_list(nest_func)\n\n    def test_good_union_func(self):\n        def good_union(x: typing.Union[float, int], a: typing.Optional[str]=None) -> int:\n            print(a)\n            return int(x)\n        self.union(good_union)\n\n    def test_bad_union_func(self):\n        def bad_union(x: float, a=None) -> int:\n            return int(x)\n        with self.assertRaises(RuntimeTypeError):\n            self.union(bad_union)\n\n    def test_good_optional_parameter_func(self):\n        def good_param(x: typing.Union[float, int], y: typing.Optional[str] = 'a') -> int:\n            return x\n        self.union(good_param)\n\n    def test_bad_optional_parameter_func(self):\n        def bad_param(x: typing.Union[float, int], y: str = 'b') -> int:\n            return x\n        with self.assertRaises(RuntimeTypeError):\n            self.union(bad_param)\n\n\nclass GenericTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the generic types\n    \"\"\"\n\n    def test_custom_generic_initialisation(self):\n        \"\"\"\n        Verifies that user defined generics can be initialised\n        \"\"\"\n        T = typing.TypeVar('T')\n\n        class Sample(typing.Generic[T]):\n            pass\n\n        SD = runtime_validation(Sample)\n\n        ST = Sample[int]\n        SDT = SD[int]\n\n        s = Sample()\n        sd = SD()\n        st = ST()\n        sdt = SDT()\n\n        self.assertFalse(hasattr(s, '__enforcer__'))\n        self.assertFalse(hasattr(st, '__enforcer__'))\n        self.assertTrue(hasattr(sd, '__enforcer__'))\n        self.assertTrue(hasattr(sdt, '__enforcer__'))\n\n        self.assertEqual(sd.__enforcer__.signature, Sample)\n        self.assertEqual(sdt.__enforcer__.signature, Sample)\n\n        self.assertEqual(sd.__enforcer__.generic, SD.__enforcer__.generic)\n        self.assertEqual(sdt.__enforcer__.generic, SDT.__enforcer__.generic)\n\n        self.assertEqual(sd.__enforcer__.bound, SD.__enforcer__.bound)\n        self.assertEqual(sdt.__enforcer__.bound, SDT.__enforcer__.bound)\n\n        for hint_name, hint_value in sdt.__enforcer__.hints.items():\n            self.assertEqual(hint_value, SDT.__enforcer__.hints[hint_name])\n\n        self.assertEqual(len(sdt.__enforcer__.hints), len(SDT.__enforcer__.hints))\n    \n    def test_custom_generic_validation(self):\n        \"\"\"\n        Verifies that user defined generic can be used as a type hint\n        \"\"\"\n        T = typing.TypeVar('T')\n\n        @runtime_validation\n        class Sample(typing.Generic[T]):\n            def get(self, data: T) -> T:\n                return data\n\n        @runtime_validation\n        def return_int(data: Sample[int], arg: int) -> int:\n            return data.get(arg)\n\n        @runtime_validation\n        def return_any(data: Sample) -> typing.Any:\n            return data\n\n        good = Sample[int]()\n        bad = Sample[str]()\n        other = Sample()\n        strange = Sample[T]()\n\n        self.assertEqual(return_int(good, 1), 1)\n        self.assertIs(return_any(other), other)\n\n        # TODO: Find out exactly what should be be happening in this case\n        #self.assertIs(return_any(strange), strange)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(bad, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(other, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(strange, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_any(good)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_any(bad)\n\n\nclass NestedTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for special and corner cases when types are deeply nested\n    \"\"\"\n    pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
          "import typing\nimport unittest\nimport numbers\n\nfrom enforce import runtime_validation, config\nfrom enforce.types import EnhancedTypeVar\nfrom enforce.exceptions import RuntimeTypeError\n\n\nclass GeneralTests(unittest.TestCase):\n    \"\"\"\n    A container for general tests\n    \"\"\"\n\n    def test_argument_validation(self):\n        print(self.sample_function)\n        self.assertEqual(self.sample_function('11', 1), 12)\n\n        result = 0\n        with self.assertRaises(RuntimeTypeError):\n            result += self.sample_function(1, 2)\n\n        self.assertEqual(result, 0)\n\n    def test_return_value_validation(self):\n        self.assertIsNone(self.sample_function('', None))\n\n        result = 0\n        with self.assertRaises(RuntimeTypeError):\n            result += self.sample_function('', 1)\n\n        self.assertEqual(result, 0)\n\n    def test_no_type_check(self):\n        \"\"\"\n        Verifies that no_type_check is respected\n        \"\"\"\n        def get_sample_func():\n            def sample(data: int):\n                pass\n\n            return sample\n\n        sample_d1 = typing.no_type_check(runtime_validation(get_sample_func()))\n        sample_d2 = runtime_validation(typing.no_type_check(get_sample_func()))\n\n        sample_d3 = runtime_validation(get_sample_func())\n\n        sample_d4 = typing.no_type_check_decorator(runtime_validation(get_sample_func()))\n        sample_d5 = runtime_validation(typing.no_type_check_decorator(get_sample_func()))\n\n        get_sample_func()('str')\n        sample_d1('str')\n        sample_d2('str')\n        with self.assertRaises(RuntimeTypeError):\n            sample_d3('str')\n\n    @runtime_validation\n    def sample_function(self, text: str, data: typing.Union[int, None]) -> typing.Optional[int]:\n        try:\n            return int(text) + data\n        except ValueError:\n            if data is None:\n                return None\n            # Deliberate return type error\n            return str(data)\n\n\nclass SimpleTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the simple types which do not require special processing\n    \"\"\"\n\n    def setUp(self):\n        config(reset=True)\n\n    def tearDown(self):\n        config(reset=True)\n\n    def test_any(self):\n        @runtime_validation\n        def sample(data: typing.Any) -> typing.Any:\n            return data\n\n        self.assertEqual(sample(100.3), 100.3)\n        self.assertIsNone(sample(None))\n\n    def test_none(self):\n        @runtime_validation\n        def sample(data: None) -> None:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> None:\n            return data\n\n        self.assertIsNone(sample(None))\n\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bool(self):\n        @runtime_validation\n        def sample(data: bool) -> bool:\n            return not data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bool:\n            return data\n\n        self.assertFalse(sample(True))\n\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('string')\n\n    def test_int(self):\n        @runtime_validation\n        def sample(data: int) -> int:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> int:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        with self.assertRaises(RuntimeTypeError):\n            sample(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_float(self):\n        \"\"\"\n        Floats should accept only floats in invariant mode\n        \"\"\"\n        @runtime_validation\n        def sample(data: float) -> float:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> float:\n            return data\n\n        self.assertEqual(sample(1.0), 1.0)\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n        config({'mode': 'covariant'})\n        sample(1)\n\n    def test_complex(self):\n        \"\"\"\n        Complex numbers should accept complex, integers and floats\n        \"\"\"\n        @runtime_validation\n        def sample(data: complex) -> complex:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> complex:\n            return data\n\n        self.assertEqual(sample(1+1j), 1+1j)\n        self.assertEqual(sample(1), 1)\n        self.assertEqual(sample(1.0), 1.0)\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_string(self):\n        @runtime_validation\n        def sample(data: str) -> str:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> str:\n            return data\n\n        self.assertEqual(sample(''), '')\n        with self.assertRaises(RuntimeTypeError):\n            sample(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bytes(self):\n        \"\"\"\n        Bytes should accept bytes as well bytearray and memorieview\n        \"\"\"\n        @runtime_validation\n        def sample(data: bytes) -> bytes:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bytes:\n            return data\n\n        self.assertEqual(sample(b''), b'')\n        self.assertEqual(sample(bytearray(2)), bytearray(2))\n        self.assertEqual(sample(memoryview(b'')), memoryview(b''))\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_bytearray(self):\n        @runtime_validation\n        def sample(data: bytearray) -> bytearray:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> bytearray:\n            return data\n\n        self.assertEqual(sample(bytearray(2)), bytearray(2))\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n    def test_memoryview(self):\n        @runtime_validation\n        def sample(data: memoryview) -> memoryview:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> memoryview:\n            return data\n\n        self.assertEqual(sample(memoryview(b'')), memoryview(b''))\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1)\n\n\nclass ComplexTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the simple types which require special processing\n    \"\"\"\n    def setUp(self):\n        config(reset=True)\n\n    def tearDown(self):\n        config(reset=True)\n\n    def get_type_var_func(self, configurable=False, type_var=None):\n        if type_var is None:\n            A = typing.TypeVar('A')\n        else:\n            A = type_var\n\n        def type_var_func(data: A) -> A:\n            return data\n\n        def configurable_type_var_func(data: typing.Any, type_option: A) -> A:\n            return data\n\n        if configurable:\n            return runtime_validation(configurable_type_var_func)\n        else:\n            return runtime_validation(type_var_func)\n\n    def test_checking_mode(self):\n        \"\"\"\n        Verifies that settings affect the selected type checking mode - covariant/contravariant\n        \"\"\"\n        @runtime_validation\n        def func(data: numbers.Integral):\n            pass\n\n        @runtime_validation\n        def func2(data: typing.Union[float, str]):\n            pass\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n        config({'mode': 'covariant'})\n\n        func(1)\n        func(True)\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        func2('hello')\n        func2(1.0)\n        func2(1)\n\n        config({'mode': 'contravariant'})\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n        config({'mode': 'bivariant'})\n\n        func(1)\n        func(1.0)\n        func(True)\n\n        func2('hello')\n        func2(1.0)\n        func2(1)\n\n        config({'mode': 'invariant'})\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            func(True)\n\n        func2('hello')\n        func2(1.0)\n        with self.assertRaises(RuntimeTypeError):\n            func2(1)\n\n    def test_optional(self):\n        @runtime_validation\n        def sample(data: typing.Optional[int]) -> typing.Optional[int]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Union[int]:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        self.assertIsNone(sample(None))\n        with self.assertRaises(RuntimeTypeError):\n            sample('')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad('')\n\n    def test_tuple(self):\n        @runtime_validation\n        def sample(data: typing.Tuple[int, str]) -> typing.Tuple[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Tuple[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_any_in(data: typing.Tuple) -> typing.Tuple:\n            return data\n\n        @runtime_validation\n        def sample_any_out(data: typing.Any) -> typing.Tuple:\n            return data\n\n        self.assertEqual(sample((1, '')), (1, ''))\n        with self.assertRaises(RuntimeTypeError):\n            sample((1, 1))\n\n        with self.assertRaises(RuntimeTypeError):\n            sample(())\n\n        with self.assertRaises(RuntimeTypeError):\n            sample([])\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad((''))\n\n        self.assertEqual(sample_any_in((1, '')), (1, ''))\n        with self.assertRaises(RuntimeTypeError):\n            sample_any_in(1)\n\n        self.assertEqual(sample_any_out((1,)), (1,))\n        with self.assertRaises(RuntimeTypeError):\n            sample_any_out(1)\n\n    def test_variable_length_tuple(self):\n        # TODO: What if tuple is empty?\n        @runtime_validation\n        def sample_in(data: typing.Tuple[int, ...]) -> typing.Any:\n            return data\n\n        @runtime_validation\n        def sample_out(data: typing.Any) -> typing.Tuple[int, ...]:\n            return data\n        \n        good = (1, 3, 4)\n        bad = (1, 'a', 2)\n        empty = ()\n\n        self.assertEqual(sample_in(good), good)\n        self.assertEqual(sample_out(good), good)\n        self.assertEqual(sample_in(empty), empty)\n        self.assertEqual(sample_out(empty), empty)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_in(bad)\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_out(bad)\n\n    def test_simple_unbounded_type_var(self):\n        type_var_func = self.get_type_var_func()\n        bad_type_var_func = self.get_type_var_func(configurable=True)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(bad_type_var_func('', 'hello world'), '')\n\n        with self.assertRaises(RuntimeTypeError):\n            bad_type_var_func('', 1)\n\n    def test_simple_bounded_type_var(self):\n        # Invariant case\n        A = typing.TypeVar('A', int, str)\n\n        type_var_func = self.get_type_var_func(type_var=A)\n        bad_type_var_func = self.get_type_var_func(configurable=True, type_var=A)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(type_var_func(''), '')\n        self.assertEqual(bad_type_var_func(1, 1), 1)\n        self.assertEqual(bad_type_var_func('', ''), '')\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func(1.0)\n\n        with self.assertRaises(RuntimeTypeError):\n            bad_type_var_func(1.0, 1)\n\n    def test_covariant_type_var(self):\n        A = typing.TypeVar('A', bound=numbers.Number, covariant=True)\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertEqual(type_var_func(1), 1)\n        self.assertEqual(type_var_func(1.0), 1.0)\n        self.assertEqual(type_var_func(1+1j), 1+1j)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func('bad')\n\n    def test_contravariant_type_var(self):\n        class B:\n            pass\n\n        class C(B):\n            pass\n\n        class D(C):\n            pass\n\n        A = typing.TypeVar('A', bound=C, contravariant=True)\n\n        b = B()\n        c = C()\n        d = D()\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertIs(type_var_func(c), c)\n        self.assertIs(type_var_func(b), b)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func(d)\n\n    def test_bivariant_type_var(self):\n        class B:\n            pass\n\n        class C(B):\n            pass\n\n        class D(C):\n            pass\n\n        A = EnhancedTypeVar('A', bound=C, covariant=True, contravariant=True)\n\n        b = B()\n        c = C()\n        d = D()\n\n        type_var_func = self.get_type_var_func(type_var=A)\n\n        self.assertIs(type_var_func(c), c)\n        self.assertIs(type_var_func(b), b)\n        self.assertIs(type_var_func(d), d)\n\n        with self.assertRaises(RuntimeTypeError):\n            type_var_func('bad')\n\n\nclass ListTypesTests(unittest.TestCase):\n    def setUp(self):\n        @runtime_validation\n        def str_func(x: typing.List[str]) -> str:\n            return x[0]\n        self.str_func = str_func\n\n        @runtime_validation\n        def int_func(x: typing.List[int]) -> int:\n            return x[0]\n        self.int_func = int_func\n\n        def int_str_func(x: typing.List[typing.Union[str, int]]) -> int:\n            return int(x[0])\n        self.union_func = int_str_func\n\n    def test_str_list(self):\n        self.str_func(['a'])\n        self.str_func(['a', 'b', 'c'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func(3)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func('3')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func([1, 2, 3])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func([1, 'b', 5.0])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.str_func(['a', 1, 'b', 5.0])\n\n    def test_int_list(self):\n        self.int_func([1])\n        self.int_func([1, 2, 3])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(5)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func('5')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['1', '2', 'a'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['a', 1, 'b', 5.0])\n\n    def test_union_func(self):\n        self.union_func([1])\n        self.union_func([1, 2, 3])\n        self.union_func(['1'])\n        self.union_func(['1', '2', '3'])\n        self.union_func([1, '2', 3, '4'])\n        self.union_func(['1', 2, '3', 4])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(1)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func('a')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func([[1, 2, 3], '4'])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.int_func(['a', 'b', {3, 4, 5}])\n\n\nclass UnionTypesTests(unittest.TestCase):\n    \"\"\"\n    Test case for Union Types\n    \"\"\"\n\n    def setUp(self):\n        @runtime_validation\n        def test_func(x: typing.Union[float, typing.List[str]]) -> int:\n            return 5\n        @runtime_validation\n        def nest_func(x: typing.Union[float, typing.List[typing.Union[str, int]]]) -> int:\n            return 5\n        self.test_func = test_func\n        self.nest_func = nest_func\n\n    def test_basic_union(self):\n        @runtime_validation\n        def sample(data: typing.Union[int, str]) -> typing.Union[int, str]:\n            return data\n\n        @runtime_validation\n        def sample_bad(data: typing.Any) -> typing.Union[int, str]:\n            return data\n\n        self.assertEqual(sample(1), 1)\n        self.assertEqual(sample(''), '')\n        with self.assertRaises(RuntimeTypeError):\n            sample(b'')\n\n        with self.assertRaises(RuntimeTypeError):\n            sample_bad(1.0)\n\n    def test_good_nested_union(self):\n        self.test_func(5.0)\n        self.test_func(['1', '2', 'a'])\n\n    def test_bad_nested_union(self):\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func('a')\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func([1, 2, 3, 4])\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test_func(['a', 4, 5])\n\n    def test_nested_func_good(self):\n        self.nest_func(5.0)\n        self.nest_func(['a', 'b', 'c'])\n        self.nest_func([1, 2, 3])\n        self.nest_func([1, 'a', 2, 'b'])\n\n    def test_nested_func_bad(self):\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func('a')\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func({'a': 5, 'b':6})\n        with self.assertRaises(RuntimeTypeError):\n            self.nest_func({1, 2, 3, 4})\n\n\nclass ContainerTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the container types - types of unbounded size\n    \"\"\"\n    pass\n\n\nclass IterableTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for iterator and generator support\n    \"\"\"\n    pass\n\n\nclass CallableTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the callable types such as functions\n    \"\"\"\n    def setUp(self):\n        @runtime_validation\n        def test(func: typing.Callable[[int, int], int], x: int) -> int:\n            return func(x, x)\n        @runtime_validation\n        def test_list(func: typing.Callable[[typing.Union[typing.List[typing.Any], int]],\n                                            int]) -> int:\n            return func(5)\n        @runtime_validation\n        def union(func: typing.Callable[[typing.Union[float, int], typing.Optional[str]],\n                                        int]) -> int:\n            return func(5)\n\n        @runtime_validation\n        def any_func_args(func: typing.Callable):\n            return func\n\n        @runtime_validation\n        def any_func_return(func) -> typing.Callable:\n            return func\n\n        self.test = test\n        self.test_list = test_list\n        self.union = union\n        self.any_func_args = any_func_args\n        self.any_func_return = any_func_return\n\n    # TODO: rename this test\n    def test_unrestrained_callable_arguments(self):\n        \"\"\"\n        Verifies that a function which expects any Callable as an argument,\n        would fail if an object of different type is passed\n        \"\"\"\n        callable = lambda x: x\n        self.any_func_args(callable)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.any_func_args('bad_input')\n\n    # TODO: rename this test\n    def test_unrestrained_callable_returns(self):\n        \"\"\"\n        Verifies that a function which expects any Callable as an output,\n        would fail if an object of different type is returned\n        \"\"\"\n        callable = lambda x: x\n        self.any_func_return(callable), callable\n\n        with self.assertRaises(RuntimeTypeError):\n            self.any_func_return('bad_input')\n\n    def test_good_func_arg(self):\n        \"\"\" Test that good arguments pass \"\"\"\n        def good(x: int, y: int) -> int:\n            return int(x * y)\n\n        self.test(good, 5)\n\n    def test_bad_func_return(self):\n        \"\"\"\n        Test that a function being passed in with mismatching return raises\n        \"\"\"\n        def bad_return(x: int, y: int) -> float:\n            return float(x * y)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test(bad_return, 5)\n\n    def test_bad_func_call(self):\n        \"\"\"\n        Test that a function being passed in with mismatching callsig raises\n        \"\"\"\n        def bad_callsig(x: str, y: str) -> int:\n            return int(x + y)\n\n        with self.assertRaises(RuntimeTypeError):\n            self.test(bad_callsig, 5)\n\n    def test_bad_func(self):\n        \"\"\"\n        Test that passing in something that's not a function raises\n        \"\"\"\n        with self.assertRaises(RuntimeTypeError):\n            self.test(5, 5)\n\n    def test_nested_func(self):\n        \"\"\"\n        Test that a function with deeply nested types works\n        \"\"\"\n        def nest_func(x: typing.Union[int, typing.List[typing.Any]]) -> int:\n            return 5\n        self.test_list(nest_func)\n\n    def test_nested_bad_func(self):\n        \"\"\"\n        Test that a function with bad deeply nested types fails\n        \"\"\"\n        def nest_func(x: typing.List[typing.List[int]]) -> int:\n            return 5\n        with self.assertRaises(RuntimeTypeError):\n            self.test_list(nest_func)\n\n    def test_good_union_func(self):\n        def good_union(x: typing.Union[float, int], a: typing.Optional[str]=None) -> int:\n            print(a)\n            return int(x)\n        self.union(good_union)\n\n    def test_bad_union_func(self):\n        def bad_union(x: float, a=None) -> int:\n            return int(x)\n        with self.assertRaises(RuntimeTypeError):\n            self.union(bad_union)\n\n    def test_good_optional_parameter_func(self):\n        def good_param(x: typing.Union[float, int], y: typing.Optional[str] = 'a') -> int:\n            return x\n        self.union(good_param)\n\n    def test_bad_optional_parameter_func(self):\n        def bad_param(x: typing.Union[float, int], y: str = 'b') -> int:\n            return x\n        with self.assertRaises(RuntimeTypeError):\n            self.union(bad_param)\n\n\nclass GenericTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for the generic types\n    \"\"\"\n\n    def test_custom_generic_initialisation(self):\n        \"\"\"\n        Verifies that user defined generics can be initialised\n        \"\"\"\n        T = typing.TypeVar('T')\n\n        class Sample(typing.Generic[T]):\n            pass\n\n        SD = runtime_validation(Sample)\n\n        ST = Sample[int]\n        SDT = SD[int]\n\n        s = Sample()\n        sd = SD()\n        st = ST()\n        sdt = SDT()\n\n        self.assertFalse(hasattr(s, '__enforcer__'))\n        self.assertFalse(hasattr(st, '__enforcer__'))\n        self.assertTrue(hasattr(sd, '__enforcer__'))\n        self.assertTrue(hasattr(sdt, '__enforcer__'))\n\n        self.assertEqual(sd.__enforcer__.signature, Sample)\n        self.assertEqual(sdt.__enforcer__.signature, Sample)\n\n        self.assertEqual(sd.__enforcer__.generic, SD.__enforcer__.generic)\n        self.assertEqual(sdt.__enforcer__.generic, SDT.__enforcer__.generic)\n\n        self.assertEqual(sd.__enforcer__.bound, SD.__enforcer__.bound)\n        self.assertEqual(sdt.__enforcer__.bound, SDT.__enforcer__.bound)\n\n        for hint_name, hint_value in sdt.__enforcer__.hints.items():\n            self.assertEqual(hint_value, SDT.__enforcer__.hints[hint_name])\n\n        self.assertEqual(len(sdt.__enforcer__.hints), len(SDT.__enforcer__.hints))\n    \n    def test_custom_generic_validation(self):\n        \"\"\"\n        Verifies that user defined generic can be used as a type hint\n        \"\"\"\n        T = typing.TypeVar('T')\n\n        @runtime_validation\n        class Sample(typing.Generic[T]):\n            def get(self, data: T) -> T:\n                return data\n\n        @runtime_validation\n        def return_int(data: Sample[int], arg: int) -> int:\n            return data.get(arg)\n\n        @runtime_validation\n        def return_any(data: Sample) -> typing.Any:\n            return data\n\n        good = Sample[int]()\n        bad = Sample[str]()\n        other = Sample()\n        strange = Sample[T]()\n\n        self.assertEqual(return_int(good, 1), 1)\n        self.assertIs(return_any(other), other)\n\n        # TODO: Find out exactly what should be be happening in this case\n        #self.assertIs(return_any(strange), strange)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(bad, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(other, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_int(strange, 1)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_any(good)\n\n        with self.assertRaises(RuntimeTypeError):\n            return_any(bad)\n\n\nclass NestedTypesTests(unittest.TestCase):\n    \"\"\"\n    Tests for special and corner cases when types are deeply nested\n    \"\"\"\n    pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"
        ],
        "test_patch": "",
        "patch_preview": "From 8e6233cdc7d98be2602fcdd3db5dbdc942bd46c5 Mon Sep 17 00:00:00 2001\nFrom: Hillel Wayne <hwayne@gmail.com>\nDate: Fri, 16 Sep 2016 21:20:39 -0500\nSubject: [PATCH 1/2] fix bug when return annotation is missing\n\n---\n enforce/enforcers.py  |  2 ++\n tests/test_enforce.py | 16 +++++++++-------\n 2 files changed, 11 insertions(+), 7 deletions(-)\n\ndiff --git a/enforce/enforcers.py b/enforce/enforcers.py\nindex f0ff0de..2a00afa 100644\n--- a/enforce/enforcers.py\n+++ b/enforce/enforcers.py\n@@ -97,6 +97,8 @"
      },
      "patch": {
        "length": 2526,
        "files_changed": 3,
        "lines_added": 20,
        "lines_deleted": 7,
        "net_change": 13,
        "changed_files": [
          {
            "file": "enforce/enforcers.py",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "tests/test_enforce.py",
            "added": 9,
            "deleted": 7
          },
          {
            "file": "tests/test_enforce.py",
            "added": 9,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 28,
        "total_lines": 5243,
        "total_bytes": 174116,
        "python_files": 23,
        "python_lines": 4828,
        "file_extensions": {
          ".yml": 1,
          ".txt": 1,
          ".pyproj": 1,
          ".sln": 1,
          ".py": 23,
          ".md": 1
        },
        "largest_files": [
          {
            "path": "tests/test_enforce.py",
            "size": 25292,
            "lines": 900,
            "extension": ".py"
          },
          {
            "path": "tests/test_types.py",
            "size": 32887,
            "lines": 787,
            "extension": ".py"
          },
          {
            "path": "tests/test_settings.py",
            "size": 18428,
            "lines": 513,
            "extension": ".py"
          },
          {
            "path": "enforce/nodes.py",
            "size": 15913,
            "lines": 441,
            "extension": ".py"
          },
          {
            "path": "tests/test_decorators.py",
            "size": 10492,
            "lines": 383,
            "extension": ".py"
          },
          {
            "path": "tests/test_enforcers.py",
            "size": 8997,
            "lines": 307,
            "extension": ".py"
          },
          {
            "path": "enforce/types.py",
            "size": 9228,
            "lines": 275,
            "extension": ".py"
          },
          {
            "path": "enforce/enforcers.py",
            "size": 9912,
            "lines": 274,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 8546,
            "lines": 263,
            "extension": ".md"
          },
          {
            "path": "enforce/settings.py",
            "size": 5827,
            "lines": 204,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 28,
        "files_changed_count": 3,
        "files_changed_ratio": 0.10714285714285714,
        "total_lines_in_repo": 5243,
        "lines_added": 20,
        "lines_deleted": 7,
        "net_lines_changed": 13,
        "lines_changed_ratio": 0.005149723440778181,
        "pr_body_length": 313,
        "commit_message_length": 48,
        "python_file_count": 23,
        "python_line_count": 4828
      }
    },
    {
      "tar_file_name": "agronholm#sqlacodegen#pull#226",
      "repo_name": "agronholm#sqlacodegen#pull#226",
      "success": true,
      "error": null,
      "commit": {
        "sha": "47ba1d6bb4de26b0192541f96f0e7438cc43c4df",
        "message": "[pre-commit.ci] pre-commit autoupdate (#225)\n\nupdates:\r\n- [github.com/asottile/pyupgrade: v2.37.2 â†’ v2.37.3](https://github.com/asottile/pyupgrade/compare/v2.37.2...v2.37.3)\r\n\r\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>",
        "author": {
          "name": "pre-commit-ci[bot]",
          "email": "66853113+pre-commit-ci[bot]@users.noreply.github.com",
          "date": "2022-08-01T20:12:43Z"
        },
        "html_url": "https://github.com/agronholm/sqlacodegen/commit/47ba1d6bb4de26b0192541f96f0e7438cc43c4df",
        "api_url": "https://api.github.com/repos/agronholm/sqlacodegen/commits/47ba1d6bb4de26b0192541f96f0e7438cc43c4df"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/agronholm#sqlacodegen#pull#226",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/agronholm#sqlacodegen#pull#226.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/agronholm#sqlacodegen#pull#226/source_code"
      },
      "pr": {
        "number": 226,
        "title": "[pre-commit.ci] pre-commit autoupdate",
        "body": "<!--pre-commit.ci start-->\nupdates:\n- [github.com/csachs/pyproject-flake8: v0.0.1a4 â†’ v0.0.1a5](https://github.com/csachs/pyproject-flake8/compare/v0.0.1a4...v0.0.1a5)\n<!--pre-commit.ci end-->",
        "state": "closed",
        "created_at": "2022-08-15T18:21:30Z",
        "updated_at": "2022-08-15T19:19:36Z",
        "merged_at": "2022-08-15T19:19:35Z",
        "html_url": "https://github.com/agronholm/sqlacodegen/pull/226",
        "user": "pre-commit-ci[bot]",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "agronholm_sqlacodegen-226",
        "repo": "/agronholm/sqlacodegen",
        "base_commit": "47ba1d6bb4de26b0192541f96f0e7438cc43c4df",
        "problem_statement": {},
        "edit_files": [
          ".pre-commit-config.yaml"
        ],
        "oracle_files": [
          "# This is the configuration file for pre-commit (https://pre-commit.com/).\n# To use:\n# * Install pre-commit (https://pre-commit.com/#installation)\n# * Copy this file as \".pre-commit-config.yaml\"\n# * Run \"pre-commit install\".\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: check-toml\n      - id: check-yaml\n      - id: debug-statements\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n        args: [ \"--fix=lf\" ]\n      - id: trailing-whitespace\n\n  - repo: https://github.com/asottile/pyupgrade\n    rev: v2.37.3\n    hooks:\n      - id: pyupgrade\n        args: [ \"--py37-plus\" ]\n\n  - repo: https://github.com/psf/black\n    rev: 22.6.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n\n  - repo: https://github.com/csachs/pyproject-flake8\n    rev: v0.0.1a4\n    hooks:\n      - id: pyproject-flake8\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v0.971\n    hooks:\n      - id: mypy\n        additional_dependencies:\n          - pytest\n          - sqlalchemy[mypy]\n\n  - repo: https://github.com/pre-commit/pygrep-hooks\n    rev: v1.9.0\n    hooks:\n      - id: python-check-blanket-noqa\n      - id: python-check-blanket-type-ignore\n      - id: python-no-eval\n      - id: rst-backticks\n      - id: rst-directive-colons\n      - id: rst-inline-touching-normal\n"
        ],
        "test_patch": "",
        "patch_preview": "From 42d218e1d4f2326c15445af72c334c56756f358e Mon Sep 17 00:00:00 2001\nFrom: \"pre-commit-ci[bot]\"\n <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nDate: Mon, 15 Aug 2022 18:21:29 +0000\nSubject: [PATCH] [pre-commit.ci] pre-commit autoupdate\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nupdates:\n- [github.com/csachs/pyproject-flake8: v0.0.1a4 â†’ v0.0.1a5](https://github.com/csachs/pyproject-flake8/compare/v0.0.1a4...v0.0.1a5)\n---\n .pre-commit-conf"
      },
      "patch": {
        "length": 900,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": ".pre-commit-config.yaml",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 16,
        "total_lines": 5269,
        "total_bytes": 176130,
        "python_files": 9,
        "python_lines": 4740,
        "file_extensions": {
          ".toml": 1,
          "": 1,
          ".cfg": 1,
          ".rst": 3,
          ".py": 9,
          ".typed": 1
        },
        "largest_files": [
          {
            "path": "tests/test_generators.py",
            "size": 82249,
            "lines": 2736,
            "extension": ".py"
          },
          {
            "path": "src/sqlacodegen/generators.py",
            "size": 57611,
            "lines": 1472,
            "extension": ".py"
          },
          {
            "path": "src/sqlacodegen/utils.py",
            "size": 6971,
            "lines": 201,
            "extension": ".py"
          },
          {
            "path": "README.rst",
            "size": 7163,
            "lines": 190,
            "extension": ".rst"
          },
          {
            "path": "tests/test_cli.py",
            "size": 3942,
            "lines": 174,
            "extension": ".py"
          },
          {
            "path": "CHANGES.rst",
            "size": 7360,
            "lines": 173,
            "extension": ".rst"
          },
          {
            "path": "pyproject.toml",
            "size": 2414,
            "lines": 100,
            "extension": ".toml"
          },
          {
            "path": "src/sqlacodegen/models.py",
            "size": 2274,
            "lines": 79,
            "extension": ".py"
          },
          {
            "path": "src/sqlacodegen/cli.py",
            "size": 2550,
            "lines": 75,
            "extension": ".py"
          },
          {
            "path": "CONTRIBUTING.rst",
            "size": 2436,
            "lines": 47,
            "extension": ".rst"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 16,
        "files_changed_count": 1,
        "files_changed_ratio": 0.0625,
        "total_lines_in_repo": 5269,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.0003795786676788764,
        "pr_body_length": 192,
        "commit_message_length": 266,
        "python_file_count": 9,
        "python_line_count": 4740
      }
    },
    {
      "tar_file_name": "agronholm#typeguard#pull#273",
      "repo_name": "agronholm#typeguard#pull#273",
      "success": true,
      "error": null,
      "commit": {
        "sha": "2fa8085cca9662afb1b7f2f37891b5d6935bf8d7",
        "message": "Added classifier for Python 3.11",
        "author": {
          "name": "Alex GrÃ¶nholm",
          "email": "alex.gronholm@nextday.fi",
          "date": "2023-01-01T11:29:24Z"
        },
        "html_url": "https://github.com/agronholm/typeguard/commit/2fa8085cca9662afb1b7f2f37891b5d6935bf8d7",
        "api_url": "https://api.github.com/repos/agronholm/typeguard/commits/2fa8085cca9662afb1b7f2f37891b5d6935bf8d7"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/agronholm#typeguard#pull#273",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/agronholm#typeguard#pull#273.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/agronholm#typeguard#pull#273/source_code"
      },
      "pr": {
        "number": 273,
        "title": "[pre-commit.ci] pre-commit autoupdate",
        "body": "<!--pre-commit.ci start-->\nupdates:\n- [github.com/pre-commit/pre-commit-hooks: v4.3.0 â†’ v4.4.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.3.0...v4.4.0)\n- [github.com/asottile/pyupgrade: v3.0.0 â†’ v3.3.1](https://github.com/asottile/pyupgrade/compare/v3.0.0...v3.3.1)\n- [github.com/pycqa/isort: 5.10.1 â†’ 5.11.4](https://github.com/pycqa/isort/compare/5.10.1...5.11.4)\n- [github.com/psf/black: 22.10.0 â†’ 22.12.0](https://github.com/psf/black/compare/22.10.0...22.12.0)\n- [github.com/csachs/pyproject-flake8: v5.0.4a1.post1 â†’ v6.0.0.post1](https://github.com/csachs/pyproject-flake8/compare/v5.0.4a1.post1...v6.0.0.post1)\n<!--pre-commit.ci end-->",
        "state": "closed",
        "created_at": "2022-10-17T19:06:57Z",
        "updated_at": "2023-01-01T11:32:49Z",
        "merged_at": "2023-01-01T11:32:48Z",
        "html_url": "https://github.com/agronholm/typeguard/pull/273",
        "user": "pre-commit-ci[bot]",
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "commits": 2
      },
      "swebench": {
        "instance_id": "agronholm_typeguard-273",
        "repo": "/agronholm/typeguard",
        "base_commit": "2fa8085cca9662afb1b7f2f37891b5d6935bf8d7",
        "problem_statement": {},
        "edit_files": [
          ".pre-commit-config.yaml"
        ],
        "oracle_files": [
          "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: check-symlinks\n      - id: check-toml\n      - id: check-yaml\n      - id: debug-statements\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n        args: [ \"--fix=lf\" ]\n      - id: trailing-whitespace\n\n  - repo: https://github.com/asottile/pyupgrade\n    rev: v3.0.0\n    hooks:\n      - id: pyupgrade\n        args: [ \"--py37-plus\" ]\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n\n  - repo: https://github.com/psf/black\n    rev: 22.10.0\n    hooks:\n      - id: black\n        exclude: \"^tests/mypy/negative.py\"\n\n  - repo: https://github.com/csachs/pyproject-flake8\n    rev: v5.0.4a1.post1\n    hooks:\n      - id: pyproject-flake8\n"
        ],
        "test_patch": "",
        "patch_preview": "From fa27580ea8100a1c39236e0faf71561870393191 Mon Sep 17 00:00:00 2001\nFrom: \"pre-commit-ci[bot]\"\n <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nDate: Mon, 26 Dec 2022 20:00:19 +0000\nSubject: [PATCH] [pre-commit.ci] pre-commit autoupdate\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nupdates:\n- [github.com/pre-commit/pre-commit-hooks: v4.3.0 â†’ v4.4.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.3.0...v4.4.0)\n- [github.com/asottil"
      },
      "patch": {
        "length": 2011,
        "files_changed": 1,
        "lines_added": 5,
        "lines_deleted": 5,
        "net_change": 0,
        "changed_files": [
          {
            "file": ".pre-commit-config.yaml",
            "added": 5,
            "deleted": 5
          }
        ]
      },
      "issue_comments": [
        {
          "id": 1368421266,
          "body": "\n[![Coverage Status](https://coveralls.io/builds/55553472/badge)](https://coveralls.io/builds/55553472)\n\nCoverage: 89.077%. Remained the same when pulling **c066a6d71e0df4b2fa7bfe9e193212ff1ea996c1 on pre-commit-ci-update-config** into **2fa8085cca9662afb1b7f2f37891b5d6935bf8d7 on master**.\n",
          "user": "coveralls",
          "created_at": "2023-01-01T11:32:37Z",
          "html_url": "https://github.com/agronholm/typeguard/pull/273#issuecomment-1368421266"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 31,
        "total_lines": 4164,
        "total_bytes": 128877,
        "python_files": 22,
        "python_lines": 3392,
        "file_extensions": {
          ".toml": 1,
          "": 1,
          ".rst": 6,
          ".py": 22,
          ".typed": 1
        },
        "largest_files": [
          {
            "path": "tests/test_checkers.py",
            "size": 25282,
            "lines": 835,
            "extension": ".py"
          },
          {
            "path": "src/typeguard/_checkers.py",
            "size": 20476,
            "lines": 634,
            "extension": ".py"
          },
          {
            "path": "src/typeguard/__init__.py",
            "size": 10761,
            "lines": 319,
            "extension": ".py"
          },
          {
            "path": "docs/versionhistory.rst",
            "size": 12222,
            "lines": 306,
            "extension": ".rst"
          },
          {
            "path": "tests/test_typechecked.py",
            "size": 5838,
            "lines": 217,
            "extension": ".py"
          },
          {
            "path": "docs/userguide.rst",
            "size": 7652,
            "lines": 190,
            "extension": ".rst"
          },
          {
            "path": "src/typeguard/importhook.py",
            "size": 5771,
            "lines": 181,
            "extension": ".py"
          },
          {
            "path": "tests/test_importhook.py",
            "size": 4254,
            "lines": 143,
            "extension": ".py"
          },
          {
            "path": "src/typeguard/_utils.py",
            "size": 4554,
            "lines": 141,
            "extension": ".py"
          },
          {
            "path": "src/typeguard/_generators.py",
            "size": 3403,
            "lines": 135,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 31,
        "files_changed_count": 1,
        "files_changed_ratio": 0.03225806451612903,
        "total_lines_in_repo": 4164,
        "lines_added": 5,
        "lines_deleted": 5,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.0024015369836695487,
        "pr_body_length": 658,
        "commit_message_length": 32,
        "python_file_count": 22,
        "python_line_count": 3392
      }
    },
    {
      "tar_file_name": "aparo#pyes#pull#346",
      "repo_name": "aparo#pyes#pull#346",
      "success": true,
      "error": null,
      "commit": {
        "sha": "fe41ab73a22bf23b61d5df6d43fe9bf524c7ed59",
        "message": "Merge pull request #343 from boekkooi/master\n\nBulk create throws invalid BulkOperationException",
        "author": {
          "name": "Alberto Paro",
          "email": "alberto.paro@gmail.com",
          "date": "2013-11-22T12:21:29Z"
        },
        "html_url": "https://github.com/aparo/pyes/commit/fe41ab73a22bf23b61d5df6d43fe9bf524c7ed59",
        "api_url": "https://api.github.com/repos/aparo/pyes/commits/fe41ab73a22bf23b61d5df6d43fe9bf524c7ed59"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/aparo#pyes#pull#346",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/aparo#pyes#pull#346.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/aparo#pyes#pull#346/source_code"
      },
      "pr": {
        "number": 346,
        "title": "Fix bug where mappings with index_options break mappings",
        "body": "This is a bit of a band-aid.  We should add index_options as an actual keyword argument and instance attribute (and anywhere else appropriate).  However given that the mapping attributes can change, we should keep **kwargs to prevent future breakage\n",
        "state": "closed",
        "created_at": "2013-12-05T06:14:03Z",
        "updated_at": "2013-12-05T07:24:20Z",
        "merged_at": "2013-12-05T07:24:20Z",
        "html_url": "https://github.com/aparo/pyes/pull/346",
        "user": "merrellb",
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "aparo_pyes-346",
        "repo": "/aparo/pyes",
        "base_commit": "fe41ab73a22bf23b61d5df6d43fe9bf524c7ed59",
        "problem_statement": {},
        "edit_files": [
          "pyes/mappings.py"
        ],
        "oracle_files": [
          "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nimport threading\nfrom .models import SortedDict, DotDict\n\n_thread_locals = threading.local()\n#store threadsafe data\nfrom .utils import keys_to_string\n\ndef to_bool(value):\n    \"\"\"\n    Convert a value to boolean\n    :param value: the value to convert\n    :type value: any type\n    :return: a boolean value\n    :rtype: a boolean\n    \"\"\"\n    if value is None:\n        return None\n    if isinstance(value, bool):\n        return value\n    elif isinstance(value, basestring):\n        if value==\"no\":\n            return False\n        elif value==\"yes\":\n            return True\n\ncheck_values = {\n    'index': ['no', 'analyzed', 'not_analyzed'],\n    'term_vector': ['no', 'yes', 'with_offsets', 'with_positions', 'with_positions_offsets'],\n    'type': ['float', 'double', 'short', 'integer', 'long'],\n    'store': ['yes', 'no'],\n    'index_analyzer': [],\n    'search_analyzer': [],\n    }\n\n\nclass AbstractField(object):\n    def __init__(self, index=\"not_analyzed\", store=\"no\", boost=1.0,\n                 term_vector=\"no\", omit_norms=True,\n                 omit_term_freq_and_positions=True,\n                 type=None, index_name=None,\n                 path=None,\n                 analyzer=None,\n                 index_analyzer=None,\n                 search_analyzer=None,\n                 name=None):\n        self.store = to_bool(store)\n        self.boost = boost\n        self.term_vector = term_vector\n        self.index = index\n        self.omit_norms = omit_norms\n        self.omit_term_freq_and_positions = omit_term_freq_and_positions\n        self.index_name = index_name\n        self.type = type\n        self.analyzer = analyzer\n        self.index_analyzer = index_analyzer\n        self.search_analyzer = search_analyzer\n        self.name = name\n        self.path = path\n\n    def as_dict(self):\n        result = {\"type\": self.type,\n                  'index': self.index}\n        if self.store != \"no\":\n            if isinstance(self.store, bool):\n                if self.store:\n                    result['store'] = \"yes\"\n                else:\n                    result['store'] = \"no\"\n            else:\n                result['store'] = self.store\n        if self.boost != 1.0:\n            result['boost'] = self.boost\n        if self.term_vector != \"no\":\n            result['term_vector'] = self.term_vector\n        if self.omit_norms != True:\n            result['omit_norms'] = self.omit_norms\n        if self.omit_term_freq_and_positions != True:\n            result['omit_term_freq_and_positions'] = self.omit_term_freq_and_positions\n        if self.index_name:\n            result['index_name'] = self.index_name\n        if self.analyzer:\n            result['analyzer'] = self.analyzer\n        if self.index_analyzer:\n            result['index_analyzer'] = self.index_analyzer\n        if self.search_analyzer:\n            result['search_analyzer'] = self.search_analyzer\n        if self.path is not None:\n            result['path'] = self.path\n\n        return result\n\n    def get_code(self, num=0):\n        data = SortedDict(self.as_dict())\n        if \"store\" in data:\n            data[\"store\"]=to_bool(data[\"store\"])\n        var_name = \"prop_\"+self.name\n        return var_name, var_name+\" = \"+self.__class__.__name__+\"(name=%r, \"%self.name+\", \".join([\"%s=%r\"%(k,v) for k,v in data.items()])+\")\"\n\nclass StringField(AbstractField):\n    def __init__(self, null_value=None, include_in_all=None, *args, **kwargs):\n        super(StringField, self).__init__(*args, **kwargs)\n        self.null_value = null_value\n        self.include_in_all = include_in_all\n        self.type = \"string\"\n\n    def as_dict(self):\n        result = super(StringField, self).as_dict()\n        if self.null_value is not None:\n            result['null_value'] = self.null_value\n        if self.include_in_all is not None:\n            result['include_in_all'] = self.include_in_all\n        return result\n\n\nclass GeoPointField(AbstractField):\n    def __init__(self, null_value=None, include_in_all=None,\n                 lat_lon=None, geohash=None, geohash_precision=None,\n                 normalize_lon=None, normalize_lat=None,\n                 validate_lon=None, validate_lat=None,\n                 *args, **kwargs):\n        super(GeoPointField, self).__init__(*args, **kwargs)\n        self.null_value = null_value\n        self.include_in_all = include_in_all\n        self.lat_lon = lat_lon\n        self.geohash = geohash\n        self.geohash_precision = geohash_precision\n        self.normalize_lon = normalize_lon\n        self.normalize_lat = normalize_lat\n        self.validate_lat = validate_lat\n        self.validate_lon = validate_lon\n        self.type = \"geo_point\"\n\n    def as_dict(self):\n        result = super(GeoPointField, self).as_dict()\n        if self.null_value is not None:\n            result['null_value'] = self.null_value\n        if self.include_in_all is not None:\n            result['include_in_all'] = self.include_in_all\n        if self.lat_lon is not None:\n            result['lat_lon'] = self.lat_lon\n        if self.geohash is not None:\n            result['geohash'] = self.geohash\n        if self.normalize_lon is not None:\n            result['normalize_lon'] = self.normalize_lon\n        if self.normalize_lat is not None:\n            result['normalize_lat'] = self.normalize_lat\n\n        if self.validate_lon is not None:\n            result['validate_lon'] = self.validate_lon\n\n\n        if self.validate_lat is not None:\n            result['validate_lat'] = self.validate_lat\n\n        if self.geohash_precision is not None:\n            try:\n                int(self.geohash_precision)\n            except ValueError:\n                raise ValueError(\"geohash_precision must be an integer\")\n            result['geohash_precision'] = self.geohash_precision\n        return result\n\n\nclass NumericFieldAbstract(AbstractField):\n    def __init__(self, null_value=None, include_in_all=None, precision_step=4,\n                 numeric_resolution=None, ignore_malformed=None, **kwargs):\n        super(NumericFieldAbstract, self).__init__(**kwargs)\n        self.null_value = null_value\n        self.include_in_all = include_in_all\n        self.precision_step = precision_step\n        self.numeric_resolution = numeric_resolution\n        self.ignore_malformed=ignore_malformed\n\n    def as_dict(self):\n        result = super(NumericFieldAbstract, self).as_dict()\n        if self.null_value is not None:\n            result['null_value'] = self.null_value\n        if self.include_in_all is not None:\n            result['include_in_all'] = self.include_in_all\n        if self.precision_step != 4:\n            result['precision_step'] = self.precision_step\n        if self.numeric_resolution:\n            result['numeric_resolution'] = self.numeric_resolution\n        if self.ignore_malformed is not None:\n            result['ignore_malformed'] = self.ignore_malformed\n        return result\n\n\nclass IpField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(IpField, self).__init__(*args, **kwargs)\n        self.type = \"ip\"\n\n\nclass ShortField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(ShortField, self).__init__(*args, **kwargs)\n        self.type = \"short\"\n\n\nclass IntegerField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(IntegerField, self).__init__(*args, **kwargs)\n        self.type = \"integer\"\n\n\nclass LongField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(LongField, self).__init__(*args, **kwargs)\n        self.type = \"long\"\n\n\nclass FloatField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(FloatField, self).__init__(*args, **kwargs)\n        self.type = \"float\"\n\n\nclass DoubleField(NumericFieldAbstract):\n    def __init__(self, *args, **kwargs):\n        super(DoubleField, self).__init__(*args, **kwargs)\n        self.type = \"double\"\n\n\nclass DateField(NumericFieldAbstract):\n    def __init__(self, format=None, **kwargs):\n        super(DateField, self).__init__(**kwargs)\n        self.format = format\n        self.type = \"date\"\n\n    def as_dict(self):\n        result = super(DateField, self).as_dict()\n        if self.format:\n            result['format'] = self.format\n        return result\n\n\nclass BooleanField(AbstractField):\n    def __init__(self, null_value=None, include_in_all=None, *args, **kwargs):\n        super(BooleanField, self).__init__(*args, **kwargs)\n        self.null_value = null_value\n        self.include_in_all = include_in_all\n        self.type = \"boolean\"\n\n    def as_dict(self):\n        result = super(BooleanField, self).as_dict()\n        if self.null_value is not None:\n            result['null_value'] = self.null_value\n        if self.include_in_all is not None:\n            result['include_in_all'] = self.include_in_all\n        return result\n\nclass BinaryField(AbstractField):\n    def __init__(self, *args, **kwargs):\n        kwargs[\"tokenize\"] = False\n        super(BinaryField, self).__init__(*args, **kwargs)\n        self.type = \"binary\"\n\n\n    def as_dict(self):\n        result = super(BinaryField, self).as_dict()\n        return result\n\nclass MultiField(object):\n    def __init__(self, name, type=None, path=None, fields=None):\n        self.name = name\n        self.type = \"multi_field\"\n        self.path = path\n        self.fields = {}\n        if fields:\n            if isinstance(fields, dict):\n                self.fields = dict([(name, get_field(name, data)) for name, data in fields.items()])\n            elif isinstance(fields, list):\n                for field in fields:\n                    self.fields[field.name] = field.as_dict()\n\n    def as_dict(self):\n        result = {\"type\": self.type,\n                  \"fields\": {}}\n        if self.fields:\n            for name, value in self.fields.items():\n                if isinstance(value, dict):\n                    result['fields'][name] = value\n                else:\n                    result['fields'][name] = value.as_dict()\n        if self.path:\n            result['path'] = self.path\n        return result\n\n\nclass AttachmentField(object):\n    \"\"\"An attachment field.\n\n    Requires the mapper-attachments plugin to be installed to be used.\n\n    \"\"\"\n\n    def __init__(self, name, type=None, path=None, fields=None):\n        self.name = name\n        self.type = \"attachment\"\n        self.path = path\n        self.fields = dict([(name, get_field(name, data)) for name, data in fields.items()])\n\n    def as_dict(self):\n        result_fields = dict((name, value.as_dict())\n        for (name, value) in self.fields.items())\n        result = dict(type=self.type, fields=result_fields)\n        if self.path:\n            result['path'] = self.path\n        return result\n\n\nclass ObjectField(object):\n    def __init__(self, name=None, type=None, path=None, properties=None,\n                 dynamic=None, enabled=None, include_in_all=None, dynamic_templates=None,\n                 include_in_parent=None, include_in_root=None,\n                 connection=None, index_name=None, *args, **kwargs):\n        self.name = name\n        self.type = \"object\"\n        self.path = path\n        self.properties = properties\n        self.include_in_all = include_in_all\n        self.dynamic = dynamic\n        self.dynamic_templates = dynamic_templates or []\n        self.enabled = enabled\n        self.include_in_all = include_in_all\n        self.include_in_parent = include_in_parent\n        self.include_in_root = include_in_root\n        self.connection = connection\n        self.index_name = index_name\n        if properties:\n            self.properties = dict([(name, get_field(name, data)) for name, data in properties.items()])\n        else:\n            self.properties = {}\n\n    def add_property(self, prop):\n        \"\"\"\n        Add a property to the object\n        \"\"\"\n        self.properties[prop.name] = prop\n\n    def as_dict(self):\n        result = {\"type\": self.type,\n                  \"properties\": {}}\n        if self.dynamic is not None:\n            result['dynamic'] = self.dynamic\n        if self.enabled is not None:\n            result['enabled'] = self.enabled\n        if self.include_in_all is not None:\n            result['include_in_all'] = self.include_in_all\n        if self.include_in_parent is not None:\n            result['include_in_parent'] = self.include_in_parent\n        if self.include_in_root is not None:\n            result['include_in_root'] = self.include_in_root\n\n        if self.path is not None:\n            result['path'] = self.path\n\n        if self.properties:\n            for name, value in self.properties.items():\n                result['properties'][name] = value.as_dict()\n        return result\n\n    def __str__(self):\n        return str(self.as_dict())\n\n    def save(self):\n        if self.connection is None:\n            raise RuntimeError(\"No connection available\")\n\n        self.connection.put_mapping(doc_type=self.name, mapping=self.as_dict(), indices=self.index_name)\n\n    def get_properties_by_type(self, type, recursive=True, parent_path=\"\"):\n        \"\"\"\n        Returns a sorted list of fields that match the type.\n\n        :param type the type of the field \"string\",\"integer\" or a list of types\n        :param recursive recurse to sub object\n        :returns a sorted list of fields the match the type\n\n        \"\"\"\n        if parent_path:\n            parent_path += \".\"\n\n        if isinstance(type, basestring):\n            if type == \"*\":\n                type = set(MAPPING_NAME_TYPE.keys()) - set([\"nested\", \"multi_field\", \"multifield\"])\n            else:\n                type = [type]\n        properties = []\n        for prop in self.properties.values():\n            if prop.type in type:\n                properties.append((parent_path + prop.name, prop))\n                continue\n            elif prop.type == \"multi_field\" and prop.name in prop.fields and prop.fields[prop.name].type in type:\n                properties.append((parent_path + prop.name, prop))\n                continue\n\n            if not recursive:\n                continue\n            if prop.type in [\"nested\", \"object\"]:\n                properties.extend(\n                    prop.get_properties_by_type(type, recursive=recursive, parent_path=parent_path + prop.name))\n        return sorted(properties)\n\n    def get_available_facets(self):\n        \"\"\"\n        Returns Available facets for the document\n        \"\"\"\n        result = []\n        for k, v in self.properties.items():\n            if isinstance(v, DateField):\n                if not v.tokenize:\n                    result.append((k, \"date\"))\n            elif isinstance(v, NumericFieldAbstract):\n                result.append((k, \"numeric\"))\n            elif isinstance(v, StringField):\n                if not v.tokenize:\n                    result.append((k, \"term\"))\n            elif isinstance(v, GeoPointField):\n                if not v.tokenize:\n                    result.append((k, \"geo\"))\n            elif isinstance(v, ObjectField):\n                for n, t in self.get_available_facets():\n                    result.append((self.name + \".\" + k, t))\n        return result\n\n    def get_code(self, num=1):\n        data = SortedDict(self.as_dict())\n        data.pop(\"properties\", [])\n        var_name =\"obj_%s\"%self.name\n        code= [var_name+\" = \"+self.__class__.__name__+\"(name=%r, \"%self.name+\", \".join([\"%s=%r\"%(k,v) for k,v in data.items()])+\")\"]\n        for name, field in self.properties.items():\n            num+=1\n            vname, vcode = field.get_code(num)\n            code.append(vcode)\n            code.append(\"%s.add_property(%s)\"%(var_name, vname))\n\n        return var_name, u'\\n'.join(code)\n\nclass NestedObject(ObjectField):\n    def __init__(self, *args, **kwargs):\n        super(NestedObject, self).__init__(*args, **kwargs)\n        self.type = \"nested\"\n\n\nclass DocumentObjectField(ObjectField):\n    def __init__(self, _all=None, _boost=None, _id=None,\n                 _index=None, _source=None, _type=None, date_formats=None, _routing=None, _ttl=None,\n                 _parent=None, _timestamp=None, _analyzer=None, _size=None, date_detection=None,\n                 numeric_detection=None, dynamic_date_formats=None, _meta=None, *args, **kwargs):\n        super(DocumentObjectField, self).__init__(*args, **kwargs)\n        self._timestamp = _timestamp\n        self._all = _all\n        self._boost = _boost\n        self._id = _id\n        self._index = _index\n        self._source = _source\n        self._routing = _routing\n        self._ttl = _ttl\n        self._analyzer = _analyzer\n        self._size = _size\n\n        self._type = _type\n        if self._type is None:\n            self._type = {\"store\": \"yes\"}\n\n        self._parent = _parent\n        self.date_detection = date_detection\n        self.numeric_detection = numeric_detection\n        self.dynamic_date_formats = dynamic_date_formats\n        self._meta = DotDict(_meta or {})\n\n\n    def get_meta(self, subtype=None):\n        \"\"\"\n        Return the meta data.\n        \"\"\"\n        if subtype:\n            return DotDict(self._meta.get(subtype, {}))\n        return  self._meta\n\n    def enable_compression(self, threshold=\"5kb\"):\n        self._source.update({\"compress\": True, \"compression_threshold\": threshold})\n\n    def as_dict(self):\n        result = super(DocumentObjectField, self).as_dict()\n        result['_type'] = self._type\n        if self._all is not None:\n            result['_all'] = self._all\n        if self._source is not None:\n            result['_source'] = self._source\n        if self._boost is not None:\n            result['_boost'] = self._boost\n        if self._routing is not None:\n            result['_routing'] = self._routing\n        if self._ttl is not None:\n            result['_ttl'] = self._ttl\n        if self._id is not None:\n            result['_id'] = self._id\n        if self._timestamp is not None:\n            result['_timestamp'] = self._timestamp\n        if self._index is not None:\n            result['_index'] = self._index\n        if self._parent is not None:\n            result['_parent'] = self._parent\n        if self._analyzer is not None:\n            result['_analyzer'] = self._analyzer\n        if self._size is not None:\n            result['_size'] = self._size\n\n        if self.date_detection is not None:\n            result['date_detection'] = self.date_detection\n        if self.numeric_detection is not None:\n            result['numeric_detection'] = self.numeric_detection\n        if self.dynamic_date_formats is not None:\n            result['dynamic_date_formats'] = self.dynamic_date_formats\n\n        return result\n\n    def add_property(self, prop):\n        \"\"\"\n        Add a property to the object\n        \"\"\"\n        self.properties[prop.name] = prop\n\n    def __repr__(self):\n        return u\"<DocumentObjectField:%s>\" % self.name\n\n\n    def save(self):\n        if self.connection is None:\n            raise RuntimeError(\"No connection available\")\n        self.connection.put_mapping(doc_type=self.name, mapping=self.as_dict(), indices=self.index_name)\n\n    def get_code(self, num=1):\n        data = SortedDict(self.as_dict())\n        data.pop(\"properties\", [])\n        var_name =\"doc_%s\"%self.name\n        code= [var_name+\" = \"+self.__class__.__name__+\"(name=%r, \"%self.name+\", \".join([\"%s=%r\"%(k,v) for k,v in data.items()])+\")\"]\n        for name, field in self.properties.items():\n            num+=1\n            vname, vcode = field.get_code(num)\n            code.append(vcode)\n            code.append(\"%s.add_property(%s)\"%(var_name, vname))\n\n        return u'\\n'.join(code)\n\ndef get_field(name, data, default=\"object\", document_object_field=None, is_document=False):\n    \"\"\"\n    Return a valid Field by given data\n    \"\"\"\n    if isinstance(data, AbstractField):\n        return data\n    data = keys_to_string(data)\n    _type = data.get('type', default)\n    if _type == \"string\":\n        return StringField(name=name, **data)\n    elif _type == \"binary\":\n        return BinaryField(name=name, **data)\n    elif _type == \"boolean\":\n        return BooleanField(name=name, **data)\n    elif _type == \"short\":\n        return ShortField(name=name, **data)\n    elif _type == \"integer\":\n        return IntegerField(name=name, **data)\n    elif _type == \"long\":\n        return LongField(name=name, **data)\n    elif _type == \"float\":\n        return FloatField(name=name, **data)\n    elif _type == \"double\":\n        return DoubleField(name=name, **data)\n    elif _type == \"ip\":\n        return IpField(name=name, **data)\n    elif _type == \"date\":\n        return DateField(name=name, **data)\n    elif _type == \"multi_field\":\n        return MultiField(name=name, **data)\n    elif _type == \"geo_point\":\n        return GeoPointField(name=name, **data)\n    elif _type == \"attachment\":\n        return AttachmentField(name=name, **data)\n    elif is_document or _type == \"document\":\n        if document_object_field:\n            return document_object_field(name=name, **data)\n        else:\n            return DocumentObjectField(name=name, **data)\n\n    elif _type == \"object\":\n        if '_timestamp' in data or \"_all\" in data:\n            if document_object_field:\n                return document_object_field(name=name, **data)\n            else:\n                return DocumentObjectField(name=name, **data)\n\n        return ObjectField(name=name, **data)\n    elif _type == \"nested\":\n        return NestedObject(name=name, **data)\n    raise RuntimeError(\"Invalid type: %s\" % _type)\n\n\nclass Mapper(object):\n    def __init__(self, data, connection=None, is_mapping=False, document_object_field=None):\n        \"\"\"\n        Create a mapper object\n\n        :param data: a dict containing the mappings\n        :param connection: a connection object\n        :param is_mapping: if it's a mapping or index/mapping\n        :param document_object_field: the kind of object to be used for document object Field\n        :return:\n        \"\"\"\n        self.indices = {}\n        self.mappings = {}\n        self.is_mapping = is_mapping\n        self.connection = connection\n        self.document_object_field = document_object_field\n        self._process(data)\n\n    def _process(self, data):\n        \"\"\"\n        Process indexer data\n        \"\"\"\n        if self.is_mapping:\n            for docname, docdata in data.items():\n                self.mappings[docname] = get_field(docname, docdata, \"document\",\n                                                   document_object_field=self.document_object_field, is_document=True)\n        else:\n            indices = []\n            for indexname, indexdata in data.items():\n                idata = []\n                for docname, docdata in indexdata.items():\n                    o = get_field(docname, docdata, document_object_field=self.document_object_field, is_document=True)\n                    o.connection = self.connection\n                    o.index_name = indexname\n                    idata.append((docname, o))\n                idata.sort()\n                indices.append((indexname, idata))\n            indices.sort()\n            self.indices = indices\n\n\n    def get_doctypes(self, index, edges=True):\n        \"\"\"\n        Returns a list of doctypes given an index\n        \"\"\"\n        #TODO lazy loading of index in mapping not exists\n        return self.indices[index]\n\n    def get_doctype(self, index, name):\n        \"\"\"\n        Returns a doctype given an index and a name\n        \"\"\"\n        return self.indices[index][name]\n\n    def get_property(self, index, doctype, name):\n        \"\"\"\n        Returns a property of a given type\n\n        :return a mapped property\n        \"\"\"\n\n        return self.indices[index][doctype].properties[name]\n\nMAPPING_NAME_TYPE = {\n    \"attachment\": AttachmentField,\n    \"boolean\": BooleanField,\n    \"date\": DateField,\n    \"double\": DoubleField,\n    \"float\": FloatField,\n    \"geopoint\": GeoPointField,\n    \"integer\": IntegerField,\n    \"int\": IntegerField,\n    \"ip\": IpField,\n    \"long\": LongField,\n    \"multifield\": MultiField,\n    \"nested\": NestedObject,\n    \"short\": ShortField,\n    \"string\": StringField,\n    \"binary\":BinaryField\n}\n\n"
        ],
        "test_patch": "",
        "patch_preview": "From 76fcce76acb5594b454984cebcca32d82de72784 Mon Sep 17 00:00:00 2001\nFrom: merrellb <brian@merrells.org>\nDate: Thu, 5 Dec 2013 01:12:33 -0500\nSubject: [PATCH] Fix bug where new mappings with index_options break mappings\n\nThis is a bit of a band-aid.  We should add index_options as an actual keyword argument and instance attribute (and anywhere else appropriate).  However given that the mapping attributes can change, we should keep **kwargs to prevent future breakage\n---\n pyes/mappings.py | 3 +"
      },
      "patch": {
        "length": 1066,
        "files_changed": 1,
        "lines_added": 2,
        "lines_deleted": 1,
        "net_change": 1,
        "changed_files": [
          {
            "file": "pyes/mappings.py",
            "added": 2,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 413,
        "total_lines": 41004,
        "total_bytes": 2459070,
        "python_files": 71,
        "python_lines": 13182,
        "file_extensions": {
          "": 10,
          ".sh": 2,
          ".cfg": 1,
          ".txt": 4,
          ".in": 1,
          ".py": 71,
          ".rst": 293,
          ".html": 3,
          ".json": 1,
          ".png": 16,
          ".psd": 1,
          ".gif": 3,
          ".conf": 3,
          ".css": 2,
          ".css_t": 2
        },
        "largest_files": [
          {
            "path": "performance/data/names.txt",
            "size": 32022,
            "lines": 2937,
            "extension": ".txt"
          },
          {
            "path": "docs/logo.psd",
            "size": 776053,
            "lines": 2575,
            "extension": ".psd"
          },
          {
            "path": "pyes/es.py",
            "size": 72224,
            "lines": 1943,
            "extension": ".py"
          },
          {
            "path": "pyes/query.py",
            "size": 52500,
            "lines": 1538,
            "extension": ".py"
          },
          {
            "path": "docs/logo_orig.png",
            "size": 122077,
            "lines": 991,
            "extension": ".png"
          },
          {
            "path": "docs/logo.png",
            "size": 115461,
            "lines": 820,
            "extension": ".png"
          },
          {
            "path": "docs/_theme/ADCTheme/static/scrn2.png",
            "size": 121395,
            "lines": 810,
            "extension": ".png"
          },
          {
            "path": "pyes/queryset.py",
            "size": 28302,
            "lines": 803,
            "extension": ".py"
          },
          {
            "path": "docs/_theme/ADCTheme/static/scrn1.png",
            "size": 108046,
            "lines": 783,
            "extension": ".png"
          },
          {
            "path": "docs/_theme/ADCTheme/static/adctheme.css",
            "size": 13347,
            "lines": 743,
            "extension": ".css"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 413,
        "files_changed_count": 1,
        "files_changed_ratio": 0.002421307506053269,
        "total_lines_in_repo": 41004,
        "lines_added": 2,
        "lines_deleted": 1,
        "net_lines_changed": 1,
        "lines_changed_ratio": 7.316359379572725e-05,
        "pr_body_length": 250,
        "commit_message_length": 95,
        "python_file_count": 71,
        "python_line_count": 13182
      }
    },
    {
      "tar_file_name": "areed1192#td-ameritrade-python-api#pull#239",
      "repo_name": "areed1192#td-ameritrade-python-api#pull#239",
      "success": true,
      "error": null,
      "commit": {
        "sha": "3378ca89f464df80a5b651f3e365f2f7d9c758d7",
        "message": "Add update notice",
        "author": {
          "name": "areed",
          "email": "areed@eastridge.com",
          "date": "2022-01-03T15:43:37Z"
        },
        "html_url": "https://github.com/areed1192/td-ameritrade-python-api/commit/3378ca89f464df80a5b651f3e365f2f7d9c758d7",
        "api_url": "https://api.github.com/repos/areed1192/td-ameritrade-python-api/commits/3378ca89f464df80a5b651f3e365f2f7d9c758d7"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/areed1192#td-ameritrade-python-api#pull#239",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/areed1192#td-ameritrade-python-api#pull#239.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/areed1192#td-ameritrade-python-api#pull#239/source_code"
      },
      "pr": {
        "number": 239,
        "title": "import websockets.client",
        "body": "Issues link: https://github.com/areed1192/td-ameritrade-python-api/issues/238",
        "state": "closed",
        "created_at": "2022-04-06T09:05:55Z",
        "updated_at": "2022-09-13T13:53:35Z",
        "merged_at": "2022-09-13T13:53:35Z",
        "html_url": "https://github.com/areed1192/td-ameritrade-python-api/pull/239",
        "user": "saber-be",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "areed1192_td-ameritrade-python-api-239",
        "repo": "/areed1192/td-ameritrade-python-api",
        "base_commit": "3378ca89f464df80a5b651f3e365f2f7d9c758d7",
        "problem_statement": {},
        "edit_files": [
          "td/stream.py"
        ],
        "oracle_files": [
          "import asyncio\nimport csv\nimport io\nimport json\nimport os\nimport textwrap\nimport unicodedata\nimport urllib\n\nfrom typing import List\nfrom typing import Union\n\nimport websockets\n\nfrom td.enums import CSV_FIELD_KEYS\nfrom td.enums import CSV_FIELD_KEYS_LEVEL_2\nfrom td.enums import STREAM_FIELD_IDS\n\n\nclass TDStreamerClient():\n\n    \"\"\"\n        TD Ameritrade Streaming API Client Class.\n\n        Implements a Websocket object that connects to the TD Streaming API, submits requests,\n        handles messages, and streams data back to the user.\n    \"\"\"\n\n    def __init__(self, websocket_url: str, user_principal_data: dict, credentials: dict) -> None:     \n        \"\"\"Initalizes the Streaming Client.\n        \n        Initalizes the Client Object and defines different components that will be needed to\n        make a connection with the TD Streaming API.\n\n        Arguments:\n        ----\n        websocket_url {str} -- The websocket URL that is returned from a Get_User_Prinicpals Request.\n\n        user_principal_data {dict} -- The data that was returned from the \"Get_User_Principals\" request. \n            Contains the info need for the account info.\n\n        credentials {dict} -- A credentials dictionary that is created from the \"create_streaming_session\"\n            method.\n        \n        Usage:\n        ----\n\n            >>> td_session = TDClient(\n                client_id='<CLIENT_ID>',\n                redirect_uri='<REDIRECT_URI>',\n                credentials_path='<CREDENTIALS_PATH>'\n            )\n            >>> td_session.login()\n            >>> td_stream_session = td_session.create_streaming_session()\n\n        \"\"\"\n\n        self.websocket_url = \"wss://{}/ws\".format(websocket_url)\n        self.credentials = credentials\n        self.user_principal_data = user_principal_data\n        self.connection: websockets.WebSocketClientProtocol = None\n        self.file_stream_level_1: io.TextIOWrapper = None\n        self.file_stream_level_2: io.TextIOWrapper = None\n\n        # this will hold all of our requests\n        self.data_requests = {\"requests\": []}\n\n        # this will house all of our field numebrs and keys so that way the user can use names to define the fields they want.\n        self.fields_ids_dictionary = STREAM_FIELD_IDS\n        self.fields_keys_write = CSV_FIELD_KEYS\n        self.fields_keys_write_level_2 = CSV_FIELD_KEYS_LEVEL_2\n        self.approved_writes_level_1 = list(self.fields_keys_write.keys())\n        self.approved_writes_level_2 = list(self.fields_keys_write_level_2.keys())\n\n        self.print_to_console = True\n        self.write_flag = False\n\n        try:\n            self.loop = asyncio.get_event_loop()\n        except websockets.WebSocketException:\n            self.loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self.loop)\n\n        self.unsubscribe_count = 0\n\n    def write_behavior(self, file_path: str, write: str = 'csv', append_mode: bool = True) -> None:        \n        \"\"\"Sets the csv dump location and the append mode.\n\n        Arguments:\n        ----\n\n        file_path {str} -- Specifies where you would like the CSV file to be written to. \n            If nothing is provided then current working directory is used.\n\n        Keyword Arguments:\n        ----\n        \n        write {str} -- Defines where you want to write the streaming data to. Right now can only specify\n            'csv'. (default: {'csv'})\n\n        append_mode {bool} -- Defines whether the write mode should be append or new. If append-mode is True, \n            then all CSV data will go to the existing file. Can either be `True` or `False`. (default: {True})\n\n        Usage:\n        ----\n            >>> td_session = TDClient(\n                client_id='<CLIENT_ID>',\n                redirect_uri='<REDIRECT_URI>',\n                credentials_path='<CREDENTIALS_PATH>'\n            )\n            >>> td_session.login()\n            >>> td_stream_session = td_session.create_streaming_session()\n            >>> td_stream_session.write_behavior(file_path='data_dump.csv')\n        \"\"\"\n\n        if write == 'csv':\n            self.CSV_PATH = file_path\n            self.CSV_PATH_STREAM = self.CSV_PATH.replace(\".csv\", \"_level_2.csv\")\n\n            # Define Storage mode for CSV files.\n            if append_mode == True:\n                self.CSV_APPEND_MODE = 'a+'\n            elif append_mode == False:\n                self.CSV_APPEND_MODE = 'w+'\n\n            self.file_stream_level_1 = open(\n                file=self.CSV_PATH, \n                mode=self.CSV_APPEND_MODE, \n                newline=''\n            )\n\n            self.file_stream_level_2 = open(\n                file=self.CSV_PATH_STREAM,\n                mode=self.CSV_APPEND_MODE,\n                newline=''\n            )\n\n            self.write_flag = True\n\n    def _write_non_chart_services(self, data_content: dict, service_name: str) -> List:\n        \"\"\"Takes a Non-Chart Services and parses the values to write.\n\n        Arguments:\n        ----\n        data_content {dict} -- The content from the stream.\n\n        service_name {str} -- The name of the service the data came from.\n\n        Returns:\n        ----\n        list -- A single row of data.\n        \"\"\"\n\n        all_data = []\n\n        for data_section in data_content:\n            for field_key in data_section:\n                new_key = self.fields_keys_write[service_name][field_key]\n                field_value = data_section[field_key]\n                data = [service_name, field_key, new_key, field_value]\n                all_data.append(data)\n        \n        return all_data\n\n    def _write_chart_services(self, data_content: dict, service_name: str) -> List:\n        \"\"\"Takes a Chart Services and parses the values to write.\n\n        Arguments:\n        ----\n        data_content {dict} -- The content from the stream.\n\n        service_name {str} -- The name of the service the data came from.\n\n        Returns:\n        ----\n        List -- A single row of data.\n        \"\"\"\n\n        all_data = []\n\n        for data_section in data_content:\n            for field_key in data_section:\n\n                if field_key != '3':\n                    new_key = self.fields_keys_write[service_name][field_key]\n                    field_value = data_section[field_key]\n                    data = [service_name, field_key, new_key, field_value]\n                    all_data.append(data)\n\n                elif field_key == '3':\n\n                    for candle in data_section['3']:\n                        for candle_key in candle:\n                            new_key = self.fields_keys_write[service_name][candle_key]\n                            field_value = candle[candle_key]\n                            data = [service_name, candle_key, new_key, field_value]\n                            all_data.append(data)\n\n        return all_data\n\n    def _write_level_two_services(self, data_content: dict, service_name: str) -> List:\n        \"\"\"Takes Level 2 services and parses them so they can be written.\n\n        Arguments:\n        ----\n        data_content {dict} -- The content from the stream.\n\n        service_name {str} -- The name of the service the data came from.\n\n        Returns:\n        ----\n        List -- A single row of data.\n        \"\"\"\n\n        all_data = []\n\n        for service_content in data_content:\n\n            symbol = service_content['key']\n            book_timestamp = service_content['1']\n            book_bid = service_content['2']\n            book_ask = service_content['3']\n\n            content_names = [symbol, service_name]\n            book_data_full = [\n                {'book_type':'bid','book_data':book_bid}, \n                {'book_type':'ask','book_data':book_ask}\n            ]\n\n            for book_dict in book_data_full:\n                book_data = book_dict['book_data']\n                book_type = book_dict['book_type']\n\n                for index, activity_section in enumerate(book_data):                                                           \n                    section_id = str(book_timestamp) + \"_\" + str(index)\n                    price = activity_section['0']\n                    total_size = activity_section['1']\n                    total_count = activity_section['2']\n                    book_data_collection = activity_section['3']\n\n                    for book_data in book_data_collection:                                    \n                        mpid = book_data[\"0\"]\n                        size = book_data[\"1\"]\n                        _time = book_data[\"2\"]\n\n                        data = [\n                            \"book_{}\".format(book_type), section_id, \n                            \"book_{}_price\".format(book_type), price, \n                            \"book_{}_size\".format(book_type), total_size, \n                            \"book_{}_total_count\".format(book_type), total_count, \n                            \"book_{}_section_mpid\".format(book_type), mpid, \n                            \"book_{}_section_size\".format(book_type), size, \n                            \"book_{}_section_time\".format(book_type), _time\n                        ]\n\n                        all_data.append(content_names + data)\n\n        return all_data\n    \n    def _write_active_services(self,data_content: dict, service_name: str) -> List:\n        \"\"\"Takes Level 2 services and parses them so they can be written.\n\n        Arguments:\n        ----\n        data_content {dict} -- The content from the stream.\n\n        service_name {str} -- The name of the service the data came from.\n\n        Returns:\n        ----\n        List -- A single row of data.\n        \"\"\"\n\n        all_data = []    \n\n        for data_section in data_content:\n\n            active_key = data_section['key']\n            active_data = data_section['1']\n\n            active_data_parts = active_data.split(';')\n            active_data_id = active_data_parts[0]\n            active_data_duration = active_data_parts[1]\n            active_data_timestamp = active_data_parts[2]\n            active_data_display_time = active_data_parts[3]\n            active_data_number_of_groups = active_data_parts[4]\n\n            all_data.append([service_name, \"\", 'active-key', active_key])\n            all_data.append([service_name, \"\", 'active-id', active_data_id])\n            all_data.append([service_name, \"\", 'active-duration', active_data_duration])\n            all_data.append([service_name, \"\", 'active-timestamp', active_data_timestamp])\n            all_data.append([service_name, \"\", 'active-display-time', active_data_display_time])\n            all_data.append([service_name, \"\", 'active-group-count', active_data_number_of_groups])\n\n            active_data_groups = active_data_parts[5:]\n\n            for active_data_group in active_data_groups:\n\n                group_split = active_data_group.split(':')\n\n                group_id = group_split[0]\n                group_count = group_split[1]\n                group_total_volume = group_split[2]\n\n                group_id_label = 'active-group-id'\n                group_id_count_label = 'active-group-id-{}-count'.format(group_id)\n                group_id_volume_label = 'active-group-id-{}-volume'.format(group_id)\n\n                all_data.append([service_name, \"\", group_id_label, group_id])\n                all_data.append([service_name, \"\", group_id_count_label, group_count])\n                all_data.append([service_name, \"\", group_id_volume_label, group_total_volume])\n\n                group_symbols = group_split[3:]\n                new_groups = [group_symbols[i:i + 3] for i in range(0, len(group_symbols), 3)]\n\n                for index, group in enumerate(new_groups):\n\n                    group_item_id_label = 'active-group-id-{}-item-{}-symbol'.format(group_id, index + 1)\n                    group_item_volume_label = 'active-group-id-{}-item-{}-volume'.format(group_id, index + 1)\n                    group_item_percent_label = 'active-group-id-{}-item-{}-percent'.format(group_id, index + 1)\n\n                    all_data.append([service_name, \"\", group_item_id_label, group[0]])\n                    all_data.append([service_name, \"\", group_item_volume_label, group[1]])\n                    all_data.append([service_name, \"\", group_item_percent_label, group[2]])\n        \n        return all_data\n\n    async def _write_to_csv(self, data: dict) -> None:\n        \"\"\"Writes the stream to a CSV file.\n\n        Takes the data from a stream, determines which sections can be\n        written and then writes it to a CSV file for further manipulation.\n\n        Arguments:\n        ----\n        data {dict} -- The data stream.\n        \"\"\"\n\n        # Deterimne what part of the message we need to get.\n        if 'data' in data.keys():\n            data = data['data']\n        elif 'snapshot' in data.keys():\n            data = data['snapshot']\n        else:\n            return None\n\n        stream_writer_level_1 = csv.writer(self.file_stream_level_1)\n        stream_writer_level_2 = csv.writer(self.file_stream_level_2)\n\n        for service_result in data:\n\n            # A Service response should have the following keys.\n            service_name = service_result['service']\n            service_timestamp = service_result['timestamp']\n            service_contents = service_result['content']\n\n            approved_level_1 = service_name in self.approved_writes_level_1\n            approved_level_2 = service_name in self.approved_writes_level_2\n            chart_history_service = service_name == 'CHART_HISTORY_FUTURES'\n            active_service = 'ACTIVES_' in service_name\n\n            # Write the non-chart level 1 services.\n            if approved_level_1 and chart_history_service == False and active_service == False:\n\n                # Grab the data\n                new_data = self._write_non_chart_services(data_content=service_contents, service_name=service_name)\n\n                for row in new_data:\n                    new_row = [service_timestamp] + row\n                    stream_writer_level_1.writerow(new_row)  \n\n            # Write the Chart Services.\n            elif approved_level_1 and chart_history_service and active_service == False:\n                \n                # Grab the data\n                new_data = self._write_chart_services(data_content=service_contents, service_name=service_name)\n\n                for row in new_data:\n                    new_row = [service_timestamp] + row\n                    stream_writer_level_1.writerow(new_row)  \n\n            # Write the Active Services.\n            elif approved_level_1 and chart_history_service == False and active_service:\n                \n                # Grab the data\n                new_data = self._write_active_services(data_content=service_contents, service_name=service_name)\n\n                for row in new_data:\n                    new_row = [service_timestamp] + row\n                    stream_writer_level_1.writerow(new_row)  \n\n            # Write the Level 2 Services\n            elif approved_level_2:\n                    \n                # Grab the data\n                new_data = self._write_level_two_services(data_content=service_contents, service_name=service_name)\n\n                for row in new_data:\n                    new_row = [service_timestamp] + row\n                    stream_writer_level_2.writerow(new_row)\n\n    async def unsubscribe(self, service: str) -> dict:\n        \"\"\"Unsubscribe from a service.\n\n        Arguments:\n        ----\n        service {str} -- The name of the service, to unsubscribe from. For example,\n            \"LEVELONE_FUTURES\" or \"QUOTES\".\n\n        Returns:\n        ----\n        dict -- A message from the websocket specifiying whether the unsubscribe command\n            was successful.\n        \"\"\"\n\n        self.unsubscribe_count += 1\n\n        service_count = len(self.data_requests['requests']) + self.unsubscribe_count\n        \n        request = {\n            \"requests\":[\n                {\n                    \"service\": service.upper(), \n                    \"requestid\": service_count, \n                    \"command\": 'UNSUBS',\n                    \"account\": self.user_principal_data['accounts'][0]['accountId'],\n                    \"source\": self.user_principal_data['streamerInfo']['appId']\n                }\n            ]\n        }\n\n        await self._send_message(json.dumps(request))\n\n        return await self._receive_message(return_value=True)\n\n    def _build_login_request(self) -> str:\n        \"\"\"Builds the Login request for the streamer.\n\n        Builds the login request dictionary that will \n        be used as the first service request with the \n        streaming API.\n\n        Returns:\n        ----\n        [str] -- A JSON string with the login details.\n\n        \"\"\"        \n\n        # define a request\n        login_request = {\n            \"requests\": [\n                {\n                    \"service\": \"ADMIN\",\n                    \"requestid\": \"0\",\n                    \"command\": \"LOGIN\",\n                    \"account\": self.user_principal_data['accounts'][0]['accountId'],\n                    \"source\": self.user_principal_data['streamerInfo']['appId'],\n                    \"parameters\": {\n                        \"credential\": urllib.parse.urlencode(self.credentials),\n                        \"token\": self.user_principal_data['streamerInfo']['token'],\n                        \"version\": \"1.0\"\n                    }\n                }\n            ]\n        }\n\n        return json.dumps(login_request)\n\n    def _build_data_request(self) -> str:\n        \"\"\"Builds the data request for the streaming service.\n\n        Takes all the service requests and converts them to a JSON \n        string.\n\n        Returns:\n        ----\n        [str] -- A JSON string with the login details.\n\n        \"\"\"\n\n        return json.dumps(self.data_requests)\n\n    async def build_pipeline(self) -> websockets.WebSocketClientProtocol:\n        \"\"\"Builds a data pipeine for processing data.\n\n        Often we want to take the data we are streaming and\n        use it in other functions or store it in other platforms.\n        This method makes the process of building a pipeline easy\n        by handling all the connection setup and request setup.\n\n        Returns:\n        ----\n        websockets.WebSocketClientProtocol -- The websocket connection.\n        \"\"\"\n\n        # In this case, we don't want things printing to the console.\n        self.print_to_console = False\n\n        # Connect to Websocket.\n        await self._connect()\n\n        # Build the Data Request.\n        await self._send_message(self._build_data_request())\n\n        return self.connection\n\n    async def start_pipeline(self) -> dict:     \n        \"\"\"Recieves the data as it streams in.\n\n        Returns:\n        ----\n        dict -- The data coming from the websocket.\n        \"\"\"\n\n        return await self._receive_message(return_value=True)\n\n    def stream(self, print_to_console: bool = True) -> None:\n        \"\"\"Starts the stream and prints the output to the console.\n\n        Initalizes the stream by building a login request, starting \n        an event loop, creating a connection, passing through the \n        requests, and keeping the loop running.\n\n        Keyword Arguments:\n        ----\n        print_to_console {bool} -- Specifies whether the content is to be printed\n            to the console or not. (default: {True})\n        \"\"\"        \n\n        # Print it to the console.\n        self.print_to_console = print_to_console\n\n        # Connect to the Websocket.\n        self.loop.run_until_complete(self._connect())\n\n        # Send the Request.\n        asyncio.ensure_future(self._send_message(self._build_data_request()))\n\n        # Start Recieving Messages.\n        asyncio.ensure_future(self._receive_message(return_value=False))\n\n        # Keep the Loop going, until an exception is reached.\n        self.loop.run_forever()\n\n    def close_logic(self, logic_type: str) -> bool:\n        \"\"\"Defines how the stream should close.\n\n        Sets the logic to determine how long to keep the server open. \n        If Not specified, Server will remain open forever or until \n        it encounters an error.\n\n        Keyword Arguments:\n        ----\n        logic_type {str} -- Defines what rules to follow to close the conneciton.\n            can be either of the following: ['empty', 'market-hours']\n\n        Returns:\n        ----\n        bool -- Specifiying whether the close logic was set `True`, or\n            wasn't set `False`\n        \"\"\"\n        pass\n\n    async def close_stream(self) -> None:\n        \"\"\"Closes the connection to the streaming service.\"\"\"        \n        \n        # close the connection.\n        await self.connection.close()\n\n        # Define the Message.\n        message = textwrap.dedent(\"\"\"\n        {lin_brk}\n        CLOSING PROCESS INITIATED:\n        {lin_brk}\n        WebSocket Closed: True\n        Event Loop Closed: True\n        {lin_brk}\n        \"\"\").format(lin_brk=\"=\"*80)\n        \n        # Shutdown all asynchronus generators.\n        await self.loop.shutdown_asyncgens()\n\n        # Stop the loop.\n        if self.loop.is_running():\n            self.loop.call_soon_threadsafe(self.loop.stop())\n            print(message)\n            await asyncio.sleep(3)\n\n        # # Once closed, verify it's closed.\n        # if self.loop.is_closed():\n        #     print('Event loop was closed.')\n        # else:            \n        #     print('Event loop was not closed.')\n\n        # # cancel all the task.\n        # for index, task in enumerate(asyncio.Task.all_tasks()):\n            \n        #     # let the user know which task is cancelled.\n        #     print(\"Cancelling Task: {}\".format(index))\n\n        #     # cancel it.\n        #     task.cancel()\n\n        #     try:\n        #         await task\n        #     except asyncio.CancelledError:\n        #         print(\"main(): cancel_me is cancelled now\")\n\n    async def _connect(self) -> websockets.WebSocketClientProtocol:\n        \"\"\"Connects the Client to the TD Websocket.\n\n        Connecting to webSocket server websockets.client.connect \n        returns a WebSocketClientProtocol, which is used to send \n        and receive messages\n\n        Keyword Arguments:\n        ----\n        pipeline_start {bool} -- This is also used to start the data\n            pipeline so, in that case we can handle more tasks here.\n            (default: {True})\n\n        Returns:\n        ---\n        websockets.WebSocketClientProtocol -- The websocket connection.\n        \"\"\"        \n\n        # Grab the login info.\n        login_request = self._build_login_request()\n\n        # Create a connection.\n        self.connection = await websockets.client.connect(self.websocket_url)\n\n        # See if we are connected.\n        is_connected = await self._check_connection()\n\n        # If we are connected then login.\n        if is_connected:\n\n            await self._send_message(login_request)\n\n            while True:\n                \n                # Grab the Response.\n                response = await self._receive_message(return_value=True)\n                responses = response.get('response')\n\n                # If we get a code 3, we had a login error.\n                if responses[0]['content']['code'] == 3:\n                    raise ValueError('LOGIN ERROR: ' + responses[0]['content']['msg'])\n                \n                # see if we had a login response.\n                for r in responses:\n                    if r.get('service') == 'ADMIN' and r.get('command') == 'LOGIN':\n                        return self.connection\n\n    async def _check_connection(self) -> bool:\n        \"\"\"Determines if we have an active connection\n\n        There are multiple times we will need to check the connection \n        of the websocket, this function will help do that.\n\n        Raises:\n        ----\n        ConnectionError: An error is raised if we can't connect to the\n            websocket.\n\n        Returns:\n        ----\n        bool -- True if the connection healthy, False otherwise.\n        \"\"\"        \n\n        # if it's open we can stream.\n        if self.connection.open:\n            print('Connection established. Streaming will begin shortly.')\n            return True\n        elif self.connection.close:\n            print('Connection was never opened and was closed.')\n            return False\n        else:\n            raise ConnectionError\n\n    async def _send_message(self, message: str):\n        \"\"\"Sends a message to webSocket server\n\n        Arguments:\n        ----\n        message {str} -- The JSON string with the\n            data streaming service subscription.\n        \"\"\"        \n\n        await self.connection.send(message)\n\n\n    async def _receive_message(self, return_value: bool = False) -> dict:\n        \"\"\"Recieves and processes the messages as needed.\n\n        Keyword Arguments:\n        ----\n        return_value {bool} -- Specifies whether the messages should be returned\n            back to the calling function or not. (default: {False})\n\n        Returns:\n        ----\n        {dict} -- A python dictionary\n        \"\"\"\n\n        # Keep going until cancelled.\n        while True:\n\n            try:\n                \n                # Grab the Message\n                message = await self.connection.recv()\n\n                # Parse Message\n                message_decoded = await self._parse_json_message(message=message)\n\n                # Write the data if needed.\n                if self.write_flag:\n                    try:\n                        await self._write_to_csv(data = message_decoded)\n                    except:\n                        print('Could not write content to CSV file, closing stream')\n                        await self.close_stream()\n                        break\n\n                if return_value:\n                    return message_decoded\n\n                elif self.print_to_console:\n                    print('='*20)\n                    print('Message Received:')\n                    print('-'*20)\n                    print(message_decoded)\n                    print('-'*20)\n                    print('')         \n\n            except websockets.exceptions.ConnectionClosed:\n\n                # stop the connection if there is an error.\n                await self.close_stream()\n                break           \n\n    async def _parse_json_message(self, message: str) -> dict:\n        \"\"\"Parses incoming messages from the stream\n\n        Arguments:\n        ----\n        message {str} -- The JSON string needing to be parsed.\n\n        Returns:\n        ----\n        dict -- A python dictionary containing the original values.\n        \"\"\"\n\n        try:\n            message_decoded = json.loads(message)\n        except:\n            message = message.encode('utf-8').replace(b'\\xef\\xbf\\xbd', bytes('\"None\"','utf-8')).decode('utf-8')\n            message_decoded = json.loads(message)\n\n        return message_decoded\n\n    async def heartbeat(self) -> None:\n        \"\"\"Sending heartbeat to server every 5 seconds.\"\"\"\n\n        while True:\n            try:\n                await self.connection.send('ping')\n                await asyncio.sleep(5)\n            except websockets.exceptions.ConnectionClosed:\n                self.close_stream()\n                break\n\n    def _new_request_template(self) -> dict:\n        \"\"\"Serves as a template to build new service requests.\n\n        This takes the Request template and populates the required fields\n        for a subscription request.\n\n        Returns:\n        ----\n        {dict} -- The service request with the standard fields filled out.\n        \"\"\"\n\n        # first get the current service request count\n        service_count = len(self.data_requests['requests']) + 1\n\n        request = {\n            \"service\": None, \n            \"requestid\": service_count, \n            \"command\": None,\n            \"account\": self.user_principal_data['accounts'][0]['accountId'],\n            \"source\": self.user_principal_data['streamerInfo']['appId'],\n            \"parameters\": {\n                \"keys\": None, \n                \"fields\": None\n            }\n        }\n\n        return request\n\n    def _validate_argument(self, argument: Union[str, int], endpoint: str) -> Union[List[str], str]:\n        \"\"\"Validate field arguments before submitting request.\n\n        Arguments:\n        ---\n        argument {Union[str, int]} -- Either a single argument or a list of arguments that are\n            fields to be requested.\n        \n        endpoint {str} -- The subscription service the request will be sent to. For example,\n            \"level_one_quote\".\n\n        Returns:\n        ----\n        Union[List[str], str] -- The field or fields that have been validated.\n        \"\"\"        \n\n        # initalize a new list.\n        arg_list = []\n\n        # see if the argument is a list or not.\n        if isinstance(argument, list):\n\n            for arg in argument:\n\n                arg_str = str(arg)\n                key_list = list(self.fields_ids_dictionary[endpoint].keys())\n                val_list = list(self.fields_ids_dictionary[endpoint].values())\n\n                if arg_str in key_list:\n                    arg_list.append(arg_str)\n                elif arg_str in val_list:\n                    key_value = key_list[val_list.index(arg_str)]\n                    arg_list.append(key_value)                  \n\n            return arg_list\n\n        else:\n\n            arg_str = str(argument)\n            key_list = list(self.fields_ids_dictionary[endpoint].keys())\n            val_list = list(self.fields_ids_dictionary[endpoint].values())\n\n            if arg_str in key_list:\n                return arg_str\n            elif arg_str in val_list:\n                key_value = key_list[val_list.index(arg_str)]\n                return key_value\n                \n\n    def quality_of_service(self, qos_level: str) -> None:\n        \"\"\"Quality of Service Subscription.\n        \n        Allows the user to set the speed at which they recieve messages\n        from the TD Server.\n\n        Arguments:\n        ----\n        qos_level {str} -- The Quality of Service level that you wish to set. \n            Ranges from 0 to 5 where 0 is the fastest and 5 is the slowest.\n\n        Raises:\n        ----\n        ValueError: Error if no field is passed through.\n\n        Usage:\n        ----\n            >>> td_session = TDClient(\n                client_id='<CLIENT_ID>',\n                redirect_uri='<REDIRECT_URI>',\n                credentials_path='<CREDENTIALS_PATH>'\n            )\n\n            >>> td_session.login()\n            >>> td_stream_session = td_session.create_streaming_session()\n            >>> td_stream_session.quality_of_service(qos_level='express')\n            >>> td_stream_session.stream()\n        \"\"\"\n        # valdiate argument.\n        qos_level = self._validate_argument(argument=qos_level, endpoint='qos_request')\n\n        if qos_level is not None:\n\n            # Build the request\n            request = self._new_request_template()\n            request['service'] = 'ADMIN'\n            request['command'] = 'QOS'\n            request['parameters']['qoslevel'] = qos_level\n            self.data_requests['requests'].append(request)\n\n        else:\n            raise ValueError('No Quality of Service Level provided.')\n\n    def chart(self, service: str, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"Subscribes to the Chart Service.\n\n        Represents the CHART_EQUITY, CHART_FUTRUES, and CHART_OPTIONS endpoint that can \n        be used to stream info needed to recreate charts.\n\n        Arguments:\n        ---\n        service {str} -- The type of Chart Service you wish to recieve. Can be either \n            `CHART_EQUITY`, `CHART_FUTURES` or `CHART_OPTIONS`\n        \n        symbols {List[str]} -- The symbol you wish to get chart data for.\n        \n        fields {Union[List[str], List[int]]} -- The fields for the request. Can either be a list of \n            keys ['key 1','key 2'] or a list of ints [1, 2, 3]\n\n        Raises:\n        ----\n        ValueError: Error if no field is passed through.\n\n        Usage:\n        ----\n            >>> td_session = TDClient(\n                client_id='<CLIENT_ID>',\n                redirect_uri='<REDIRECT_URI>',\n                credentials_path='<CREDENTIALS_PATH>'\n            )\n\n            >>> td_session.login()\n            >>> td_stream_session = td_session.create_streaming_session()\n        \n            >>> td_stream_session.charts(\n                service='CHART_EQUITY', \n                symbols=['AAPL','MSFT'], \n                fields=[0,1,2,3,4,5,6,7]\n            )\n\n            >>> td_stream_session.charts(\n                service='CHART_OPTIONS', \n                symbols=['AAPL_040920C115'], \n                fields=[0,1,2,3,4,5,6,7]\n            )\n\n            >>> td_stream_session.charts(\n                service='CHART_FUTURES', \n                symbols=['/ES','/CL'], \n                fields=[0,1,2,3,4,5,6,7]\n            )\n\n            >>> td_stream_session.stream()\n        \"\"\"        \n\n        # check to make sure it's a valid Chart Service.\n        service_flag = service in ['CHART_EQUITY', 'CHART_FUTURES', 'CHART_OPTIONS']\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint=service.lower())\n        \n        if service_flag and fields is not None:\n\n            # Build the request\n            request = request = self._new_request_template()\n            request['service'] = service\n            request['command'] = 'SUBS'\n            request['parameters']['keys'] = ','.join(symbols)\n            request['parameters']['fields'] = ','.join(fields)\n            self.data_requests['requests'].append(request)\n\n        else:\n            raise ValueError('ERROR!')\n\n    def actives(self, service: str, venue: str, duration: str) -> None:\n        \"\"\"\n            Represents the ACTIVES endpoint for the TD Streaming API where\n            you can get the most actively traded stocks for a specific exchange.\n\n            NAME: service\n            DESC: The type of Active Service you wish to recieve. Can be one of the following:\n                  [NASDAQ, NYSE, OTCBB, CALLS, OPTS, PUTS, CALLS-DESC, OPTS-DESC, PUTS-DESC]\n            TYPE: String\n\n            NAME: venue\n            DESC: The symbol you wish to get chart data for.\n            TYPE: String\n\n            NAME: duration\n            DESC: Specifies the look back period for collecting most actively traded instrument. Can be either\n                  ['ALL', '60', '300', '600', '1800', '3600'] where the integrers represent number of seconds.\n            TYPE: String\n        \"\"\"\n\n        # check to make sure it's a valid active service.\n        service_flag = service in [\n            'ACTIVES_NASDAQ', 'ACTIVES_NYSE', 'ACTIVES_OPTIONS', 'ACTIVES_OTCBB']\n\n        # check to make sure it's a valid active service venue.\n        venue_flag = venue in ['NASDAQ', 'NYSE', 'OTCBB', 'CALLS',\n                               'OPTS', 'PUTS', 'CALLS-DESC', 'OPTS-DESC', 'PUTS-DESC']\n\n        # check to make sure it's a valid duration\n        duration_flag = duration in ['ALL', '60', '300', '600', '1800', '3600']\n\n        if service_flag and venue_flag and duration_flag:\n\n            # Build the request\n            request = self._new_request_template()\n            request['service'] = service\n            request['command'] = 'SUBS'\n            request['parameters']['keys'] = venue + '-' + duration\n            request['parameters']['fields'] = '1'\n            self.data_requests['requests'].append(request)\n\n        else:\n            raise ValueError('ERROR!')\n\n    def account_activity(self):\n        \"\"\"\n            Represents the ACCOUNT_ACTIVITY endpoint of the TD Streaming API. This service is used to \n            request streaming updates for one or more accounts associated with the logged in User ID. \n            Common usage would involve issuing the OrderStatus API request to get all transactions \n            for an account, and subscribing to ACCT_ACTIVITY to get any updates.     \n        \"\"\"\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'ACCT_ACTIVITY'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = self.user_principal_data['streamerSubscriptionKeys']['keys'][0]['key']\n        request['parameters']['fields'] = '0,1,2,3'\n\n        self.data_requests['requests'].append(request)\n\n    def chart_history_futures(self, symbol: str, frequency: str, start_time: str = None, end_time: str = None, period: str = None) -> None:\n        \"\"\"\n            Represents the CHART HISTORY FUTURES endpoint for the TD Streaming API. Only Futures \n            chart history is available via Streamer Server.\n\n            NAME: symbol\n            DESC: A single futures symbol that you wish to get chart data for.\n            TYPE: String\n\n            NAME: frequency\n            DESC: The frequency at which you want the data to appear. Can be one of the following options:\n                  [m1, m5, m10, m30, h1, d1, w1, n1] where [m=minute, h=hour, d=day, w=week, n=month]\n            TYPE: String\n\n            NAME: period\n            DESC: The period you wish to return historical data for. Can be one of the following options:\n                  [d5, w4, n10, y1, y10] where [d=day, w=week, n=month, y=year]\n            TYPE: String\n\n            NAME: start_time\n            DESC: Start time of chart in milliseconds since Epoch. OPTIONAL\n            TYPE: String\n\n            NAME: end_time\n            DESC: End time of chart in milliseconds since Epoch. OPTIONAL\n            TYPE: String\n        \"\"\"\n\n        # define the valid inputs.\n        valid_frequencies = ['m1', 'm5', 'm10', 'm30', 'h1', 'd1', 'w1', 'n1']\n        valid_periods = ['d1', 'd5', 'w4', 'n10', 'y1', 'y10']\n\n        # validate the frequency input.\n        if frequency not in valid_frequencies:\n            raise ValueError(\n                \"The FREQUENCY you have chosen is not correct please choose a valid option:['m1', 'm5', 'm10', 'm30', 'h1', 'd1', 'w1', 'n1']\")\n\n        # validate the period input.\n        if period not in valid_periods and start_time is None and end_time is None:\n            raise ValueError(\n                \"The PERIOD you have chosen is not correct please choose a valid option:['d5', 'w4', 'n10', 'y1', 'y10']\")\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'CHART_HISTORY_FUTURES'\n        request['command'] = 'GET'\n        request['parameters']['symbol'] = symbol[0]\n        request['parameters']['frequency'] = frequency\n\n\n        # handle the case where we get a start time or end time. DO FURTHER VALIDATION.\n        if start_time is not None or end_time is not None:\n            request['parameters']['END_TIME'] = end_time\n            request['parameters']['START_TIME'] = start_time\n        else:\n            request['parameters']['period'] = period\n\n        del request['parameters']['keys']\n        del request['parameters']['fields']\n\n        request['requestid'] = str(request['requestid'])\n\n        self.data_requests['requests'].append(request)\n\n    def level_one_quotes(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the LEVEL ONE QUOTES endpoint for the TD Streaming API. This\n            will return quotes for a given list of symbols along with specified field information.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_one_quote')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'QUOTE'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_one_options(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the LEVEL ONE OPTIONS endpoint for the TD Streaming API. This\n            will return quotes for a given list of option symbols along with specified field information.\n\n            NAME: symbols\n            DESC: A List of option symbols you wish to stream quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_one_option')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'OPTION'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_one_futures(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the LEVEL ONE FUTURES endpoint for the TD Streaming API. This\n            will return quotes for a given list of futures symbols along with specified field information.\n\n            NAME: symbols\n            DESC: A List of futures symbols you wish to stream quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_one_futures')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'LEVELONE_FUTURES'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_one_forex(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the LEVEL ONE FOREX endpoint for the TD Streaming API. This\n            will return quotes for a given list of forex symbols along with specified field information.\n\n            NAME: symbols\n            DESC: A List of forex symbols you wish to stream quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_one_forex')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'LEVELONE_FOREX'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_one_futures_options(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the LEVEL ONE FUTURES OPTIONS endpoint for the TD Streaming API. This\n            will return quotes for a given list of forex symbols along with specified field information.\n\n            NAME: symbols\n            DESC: A List of forex symbols you wish to stream quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_one_futures_options')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'LEVELONE_FUTURES_OPTIONS'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def news_headline(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the NEWS_HEADLINE endpoint for the TD Streaming API. This endpoint\n            is used to stream news headlines for different instruments.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream news for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>         \n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='news_headline')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'NEWS_HEADLINE'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def timesale(self, service: str, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            Represents the TIMESALE endpoint for the TD Streaming API. The TIMESALE server ID is used to \n            request Time & Sales data for all supported symbols\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time and sales data for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings>         \n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(argument=fields, endpoint='timesale')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = service\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    \"\"\"\n        EXPERIMENTATION SECTION\n\n        NO GUARANTEE THESE WILL WORK.\n    \"\"\"\n\n    def level_two_quotes(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_QUOTES endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_two_quotes')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'LISTED_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_two_options(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_QUOTES_OPTIONS endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(argument=fields, endpoint='level_two_options')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'OPTIONS_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_two_nasdaq(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_QUOTES_NASDAQ endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n        # valdiate argument.\n        fields = self._validate_argument(argument=fields, endpoint='level_two_nasdaq')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'NASDAQ_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def level_two_total_view(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n\n        fields = [str(field) for field in fields]\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'TOTAL_VIEW'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    \"\"\"\n        NOT WORKING\n    \"\"\"\n\n    def _streamer_server(self):\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'STREAMER_SERVER'\n        request['command'] = 'ADMIN'\n        request['parameters'] = {}\n\n        self.data_requests['requests'].append(request)\n\n    def _news_history(self):\n\n        # OFFICIALLY DEAD\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'NEWS'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = 'IBM'\n        request['parameters']['fields'] = 1576828800000\n\n        self.data_requests['requests'].append(request)\n\n    def _level_two_opra(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_OPRA endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'OPRA'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def _level_two_nyse(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_NYSE endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        fields = [str(field) for field in fields]\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'NYSE_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def _level_two_futures_options(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_FUTURES_OPTIONS endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        if fields is not None:\n            fields = [str(field) for field in fields]\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'FUTURES_OPTIONS_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = symbols\n        request['parameters']['fields'] = '0,1,2,3'\n\n        self.data_requests['requests'].append(request)\n\n    def _level_two_futures(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_QUOTES_FUTURES endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(argument=fields, endpoint='level_two_futures')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'FUTURES_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n\n    def _level_two_forex(self, symbols: List[str], fields: Union[List[str], List[int]]) -> None:\n        \"\"\"\n            EXPERIMENTAL: USE WITH CAUTION!\n\n            Represents the LEVEL_TWO_FOREX endpoint for the streaming API. Documentation on this\n            service does not exist, but it appears that we can pass through 1 of 3 fields.\n\n            NAME: symbols\n            DESC: A List of symbols you wish to stream time level two quotes for.\n            TYPE: List<String>\n\n            NAME: fields\n            DESC: The fields you want returned from the Endpoint, can either be the numeric representation\n                  or the key value representation. For more info on fields, refer to the documentation.\n            TYPE: List<Integer> | List<Strings> \n\n        \"\"\"\n\n        # valdiate argument.\n        fields = self._validate_argument(\n            argument=fields, endpoint='level_two_forex')\n\n        # Build the request\n        request = self._new_request_template()\n        request['service'] = 'FOREX_BOOK'\n        request['command'] = 'SUBS'\n        request['parameters']['keys'] = ','.join(symbols)\n        request['parameters']['fields'] = ','.join(fields)\n\n        self.data_requests['requests'].append(request)\n"
        ],
        "test_patch": "",
        "patch_preview": "From c009858db727e4ad0f7e51a8e8c1108384fa6486 Mon Sep 17 00:00:00 2001\nFrom: saber bejestani <saberbejestani@gmail.com>\nDate: Wed, 6 Apr 2022 12:02:27 +0300\nSubject: [PATCH] import websockets.client\n\n---\n td/stream.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/td/stream.py b/td/stream.py\nindex 704f169..d7cf52e 100644\n--- a/td/stream.py\n+++ b/td/stream.py\n@@ -11,7 +11,7 @@\n from typing import Union\n \n import websockets\n-\n+import websockets.client\n from td.enums import CSV"
      },
      "patch": {
        "length": 596,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "td/stream.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 86,
        "total_lines": 90121,
        "total_bytes": 3244341,
        "python_files": 28,
        "python_lines": 8505,
        "file_extensions": {
          "": 1,
          ".txt": 1,
          ".py": 28,
          ".md": 3,
          ".jsonc": 28,
          ".json": 16,
          ".jpg": 6,
          ".pdf": 1,
          ".html": 2
        },
        "largest_files": [
          {
            "path": "samples/responses/sample_historical_prices.jsonc",
            "size": 818887,
            "lines": 36902,
            "extension": ".jsonc"
          },
          {
            "path": "samples/responses/sample_option_chain.jsonc",
            "size": 877888,
            "lines": 25260,
            "extension": ".jsonc"
          },
          {
            "path": "samples/responses/sample_chart_history_futures.json",
            "size": 167099,
            "lines": 8063,
            "extension": ".json"
          },
          {
            "path": "samples/resources/Creating a TD Ameritrade Developer Account.pdf",
            "size": 600401,
            "lines": 4540,
            "extension": ".pdf"
          },
          {
            "path": "td/client.py",
            "size": 77620,
            "lines": 2185,
            "extension": ".py"
          },
          {
            "path": "td/stream.py",
            "size": 58261,
            "lines": 1568,
            "extension": ".py"
          },
          {
            "path": "td/enums.py",
            "size": 24219,
            "lines": 1004,
            "extension": ".py"
          },
          {
            "path": "samples/instructions/photos/td_terms.jpg",
            "size": 112722,
            "lines": 856,
            "extension": ".jpg"
          },
          {
            "path": "samples/instructions/photos/paste_url.jpg",
            "size": 119070,
            "lines": 769,
            "extension": ".jpg"
          },
          {
            "path": "td/orders.py",
            "size": 23800,
            "lines": 617,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 86,
        "files_changed_count": 1,
        "files_changed_ratio": 0.011627906976744186,
        "total_lines_in_repo": 90121,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 2.2192385792434616e-05,
        "pr_body_length": 77,
        "commit_message_length": 17,
        "python_file_count": 28,
        "python_line_count": 8505
      }
    },
    {
      "tar_file_name": "baijum#selenium-python#pull#97",
      "repo_name": "baijum#selenium-python#pull#97",
      "success": true,
      "error": null,
      "commit": {
        "sha": "7e47b0000f8c62a695b974f1ba4c9cbd7039fa35",
        "message": "fix the typo (#96)",
        "author": {
          "name": "Ai-Lin Liou",
          "email": "alin.code@gmail.com",
          "date": "2021-03-18T07:44:55Z"
        },
        "html_url": "https://github.com/baijum/selenium-python/commit/7e47b0000f8c62a695b974f1ba4c9cbd7039fa35",
        "api_url": "https://api.github.com/repos/baijum/selenium-python/commits/7e47b0000f8c62a695b974f1ba4c9cbd7039fa35"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/baijum#selenium-python#pull#97",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/baijum#selenium-python#pull#97.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/baijum#selenium-python#pull#97/source_code"
      },
      "pr": {
        "number": 97,
        "title": "Mention Helium as a beginner-friendly solution",
        "body": "[Helium](https://github.com/mherrmann/selenium-python-helium) is a wrapper around selenium-python that solves many common pain points that especially beginners struggle with. This PR adds a link to Helium. If wanted, I can also supply Chinese and Japanese translations for this change.",
        "state": "closed",
        "created_at": "2021-03-19T07:41:04Z",
        "updated_at": "2021-03-20T01:23:03Z",
        "merged_at": null,
        "html_url": "https://github.com/baijum/selenium-python/pull/97",
        "user": "mherrmann",
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "baijum_selenium-python-97",
        "repo": "/baijum/selenium-python",
        "base_commit": "7e47b0000f8c62a695b974f1ba4c9cbd7039fa35",
        "problem_statement": {},
        "edit_files": [
          "source/index.rst"
        ],
        "oracle_files": [
          "********************\nSelenium with Python\n********************\n\n:Author: `Baiju Muthukadan <https://muthukadan.net>`_\n:License: This document is licensed under a\n  `Creative Commons Attribution-ShareAlike 4.0 International License <http://creativecommons.org/licenses/by-sa/4.0/>`_.\n\n.. note::\n\n   This is not an official documentation.  If you would like to contribute to\n   this documentation, you can `fork this project in GitHub and send pull\n   requests <https://github.com/baijum/selenium-python>`_.  You can also send\n   your feedback to my email: baiju.m.mail AT gmail DOT com.  So far 50+\n   community members have contributed to this project (See the closed pull\n   requests).  I encourage contributors to add more sections and make it an\n   awesome documentation!  If you know any translation of this document, please\n   send a PR to update the below list.\n\n   **Translations:**\n\n   - `Chinese <https://selenium-python-zh.readthedocs.io/en/latest/>`_\n   - `Japanese <https://kurozumi.github.io/selenium-python/index.html>`_\n\n\n.. toctree::\n   :numbered:\n\n   installation\n   getting-started\n   navigating\n   locating-elements\n   waits\n   page-objects\n   api\n   faq\n\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n"
        ],
        "test_patch": "",
        "patch_preview": "From 1c96bdc4654013a763b7a338a49790fe5ee06834 Mon Sep 17 00:00:00 2001\nFrom: Michael Herrmann <michael@herrmann.io>\nDate: Fri, 19 Mar 2021 08:39:14 +0100\nSubject: [PATCH] Mention Helium as a beginner-friendly solution\n\n---\n source/index.rst | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a/source/index.rst b/source/index.rst\nindex 7d51db0..8e906e6 100644\n--- a/source/index.rst\n+++ b/source/index.rst\n@@ -22,6 +22,9 @@ Selenium with Python\n    - `Chinese <https://selenium-python-zh.readthedoc"
      },
      "patch": {
        "length": 837,
        "files_changed": 1,
        "lines_added": 3,
        "lines_deleted": 0,
        "net_change": 3,
        "changed_files": [
          {
            "file": "source/index.rst",
            "added": 3,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [
        {
          "id": 802821939,
          "body": "@mherrmann Your project looks very interesting. But this tutorial may not be the right place to add a link.",
          "user": "baijum",
          "created_at": "2021-03-19T13:10:44Z",
          "html_url": "https://github.com/baijum/selenium-python/pull/97#issuecomment-802821939"
        },
        {
          "id": 802828150,
          "body": "Thank you for your reply. Would you be willing to try it? It would help a lot of people.",
          "user": "mherrmann",
          "created_at": "2021-03-19T13:21:11Z",
          "html_url": "https://github.com/baijum/selenium-python/pull/97#issuecomment-802828150"
        }
      ],
      "issue_comments_count": 2,
      "code_statistics": {
        "total_files": 18,
        "total_lines": 2719,
        "total_bytes": 103527,
        "python_files": 2,
        "python_lines": 289,
        "file_extensions": {
          "": 1,
          ".txt": 3,
          ".py": 2,
          ".rst": 10,
          ".bat": 1,
          ".png": 1
        },
        "largest_files": [
          {
            "path": "source/api.rst",
            "size": 9900,
            "lines": 474,
            "extension": ".rst"
          },
          {
            "path": "source/locating-elements.rst",
            "size": 9052,
            "lines": 295,
            "extension": ".rst"
          },
          {
            "path": "source/getting-started.rst",
            "size": 9305,
            "lines": 277,
            "extension": ".rst"
          },
          {
            "path": "source/conf.py",
            "size": 8682,
            "lines": 275,
            "extension": ".py"
          },
          {
            "path": "source/navigating.rst",
            "size": 8494,
            "lines": 233,
            "extension": ".rst"
          },
          {
            "path": "source/_static/logo.png",
            "size": 25192,
            "lines": 187,
            "extension": ".png"
          },
          {
            "path": "make.bat",
            "size": 4547,
            "lines": 170,
            "extension": ".bat"
          },
          {
            "path": "source/installation.rst",
            "size": 6271,
            "lines": 158,
            "extension": ".rst"
          },
          {
            "path": "source/page-objects.rst",
            "size": 5020,
            "lines": 157,
            "extension": ".rst"
          },
          {
            "path": "source/waits.rst",
            "size": 5120,
            "lines": 144,
            "extension": ".rst"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 18,
        "files_changed_count": 1,
        "files_changed_ratio": 0.05555555555555555,
        "total_lines_in_repo": 2719,
        "lines_added": 3,
        "lines_deleted": 0,
        "net_lines_changed": 3,
        "lines_changed_ratio": 0.0011033468186833395,
        "pr_body_length": 285,
        "commit_message_length": 18,
        "python_file_count": 2,
        "python_line_count": 289
      }
    },
    {
      "tar_file_name": "boramalper#himawaripy#pull#37",
      "repo_name": "boramalper#himawaripy#pull#37",
      "success": true,
      "error": null,
      "commit": {
        "sha": "4a90a382604ecc4cd5050681fc5bacd13235056a",
        "message": "Use 'feh' as a fallback option\n\nhttps://github.com/boramalper/himawaripy/pull/28",
        "author": {
          "name": "Bora M. Alper",
          "email": "bora@boramalper.org",
          "date": "2016-02-05T19:14:18Z"
        },
        "html_url": "https://github.com/boramalper/himawaripy/commit/4a90a382604ecc4cd5050681fc5bacd13235056a",
        "api_url": "https://api.github.com/repos/boramalper/himawaripy/commits/4a90a382604ecc4cd5050681fc5bacd13235056a"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/boramalper#himawaripy#pull#37",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/boramalper#himawaripy#pull#37.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/boramalper#himawaripy#pull#37/source_code"
      },
      "pr": {
        "number": 37,
        "title": "Fix MATE detection.",
        "body": "I get \"MATE\" inside os.environ.get(\"XDG_CURRENT_DESKTOP\"), not os.environ.get(\"DESKTOP_SESSION\") on MATE 1.12.1 on Arch.\n\nI'm guessing this is the problem mentioned in passing from #20 making him set his desktop to mate manually.\n",
        "state": "closed",
        "created_at": "2016-02-06T03:06:18Z",
        "updated_at": "2016-02-06T04:52:35Z",
        "merged_at": "2016-02-06T04:52:35Z",
        "html_url": "https://github.com/boramalper/himawaripy/pull/37",
        "user": "rbong",
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "boramalper_himawaripy-37",
        "repo": "/boramalper/himawaripy",
        "base_commit": "4a90a382604ecc4cd5050681fc5bacd13235056a",
        "problem_statement": {
          "title": "urllib issue",
          "body": "Firstly, I had to manually set the DE string to \"mate\" as it fails to detect the DE in Arch for some reason. \n\nAnyways, I get the following error output:\n\n[tv@Urban-HTPC himawaripy]$ ./himawaripy.py\nUpdating...\nLatest version: 2016/02/05/00:50:00 GMT\n\nDownloading tiles: 64/64 completed\nDownloaded\n\nError while parsing options: Unknown option -type.\nRun 'gconftool-2 --help' to see a full list of available command line options.\nDone!\n"
        },
        "edit_files": [
          "utils.py"
        ],
        "oracle_files": [
          "# http://stackoverflow.com/a/21213358/4466589\n\nimport os\nimport sys\nimport subprocess\nimport re\n\ndef get_desktop_environment():\n    # From http://stackoverflow.com/questions/2035657/what-is-my-current-desktop-environment\n    # and http://ubuntuforums.org/showthread.php?t=652320\n    # and http://ubuntuforums.org/showthread.php?t=652320\n    # and http://ubuntuforums.org/showthread.php?t=1139057\n    if sys.platform in [\"win32\", \"cygwin\"]:\n        return \"windows\"\n    elif sys.platform == \"darwin\":\n        return \"mac\"\n    else: # Most likely either a POSIX system or something not much common\n        desktop_session = os.environ.get(\"DESKTOP_SESSION\")\n        if desktop_session is not None: # Easier to match if we don't have to deal with caracter cases\n            desktop_session = desktop_session.lower()\n            if desktop_session in [\"gnome\", \"unity\", \"cinnamon\", \"mate\", \"xfce4\", \"lxde\", \"fluxbox\", \n                                   \"blackbox\", \"openbox\", \"icewm\", \"jwm\", \"afterstep\",\"trinity\", \"kde\", \"pantheon\",\n                                   \"gnome-classic\"]:\n                return desktop_session\n            ## Special cases ##\n            # Canonical sets $DESKTOP_SESSION to Lubuntu rather than LXDE if using LXDE.\n            # There is no guarantee that they will not do the same with the other desktop environments.\n            elif \"xfce\" in desktop_session or desktop_session.startswith(\"xubuntu\"):\n                return \"xfce4\"\n            elif desktop_session.startswith(\"ubuntu\"):\n                return \"unity\"       \n            elif desktop_session.startswith(\"lubuntu\"):\n                return \"lxde\" \n            elif desktop_session.startswith(\"kubuntu\"): \n                return \"kde\" \n            elif desktop_session.startswith(\"razor\"): # e.g. razorkwin\n                return \"razor-qt\"\n            elif desktop_session.startswith(\"wmaker\"): # e.g. wmaker-common\n                return \"windowmaker\"\n        if os.environ.get('KDE_FULL_SESSION') == 'true':\n            return \"kde\"\n        elif os.environ.get('GNOME_DESKTOP_SESSION_ID'):\n            if not \"deprecated\" in os.environ.get('GNOME_DESKTOP_SESSION_ID'):\n                return \"gnome2\"\n        # From http://ubuntuforums.org/showthread.php?t=652320\n        elif is_running(\"xfce-mcs-manage\"):\n            return \"xfce4\"\n        elif is_running(\"ksmserver\"):\n            return \"kde\"\n\n    # We couldn't detect it so far, so let's try one last time\n    current_desktop = os.environ.get(\"XDG_CURRENT_DESKTOP\")\n    if current_desktop:\n        current_desktop = current_desktop.lower()\n        if current_desktop in [\"gnome\", \"unity\", \"kde\", \"gnome-classic\"]:\n            return current_desktop\n\n        # Special Cases\n        elif current_desktop == \"xfce\":\n            return \"xfce4\"\n        elif current_desktop == \"x-cinnamon\":\n            return \"cinnamon\"\n\ndef has_program(program):\n    try:\n        subprocess.check_output([\"which\", \"--\", program])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n    return \"unknown\"\n\ndef is_running(process):\n    try:\n        subprocess.check_output ([\"pidof\", \"--\", process])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n"
        ],
        "test_patch": "",
        "patch_preview": "From 79f7f7c740a9865590bc46cca9157db45b011f82 Mon Sep 17 00:00:00 2001\nFrom: rbong <roger.bongers@yahoo.ca>\nDate: Fri, 5 Feb 2016 22:08:18 -0500\nSubject: [PATCH] Fixed MATE detection.\n\n---\n utils.py | 12 ++++++------\n 1 file changed, 6 insertions(+), 6 deletions(-)\n\ndiff --git a/utils.py b/utils.py\nindex 90ea748..90163d2 100644\n--- a/utils.py\n+++ b/utils.py\n@@ -18,7 +18,7 @@ def get_desktop_environment():\n         desktop_session = os.environ.get(\"DESKTOP_SESSION\")\n         if desktop_session is"
      },
      "patch": {
        "length": 2228,
        "files_changed": 1,
        "lines_added": 6,
        "lines_deleted": 6,
        "net_change": 0,
        "changed_files": [
          {
            "file": "utils.py",
            "added": 6,
            "deleted": 6
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 5,
        "total_lines": 323,
        "total_bytes": 11928,
        "python_files": 2,
        "python_lines": 179,
        "file_extensions": {
          ".py": 2,
          "": 2,
          ".md": 1
        },
        "largest_files": [
          {
            "path": "README.md",
            "size": 3318,
            "lines": 103,
            "extension": ".md"
          },
          {
            "path": "himawaripy.py",
            "size": 3780,
            "lines": 100,
            "extension": ".py"
          },
          {
            "path": "utils.py",
            "size": 3248,
            "lines": 79,
            "extension": ".py"
          },
          {
            "path": "LICENSE",
            "size": 1105,
            "lines": 21,
            "extension": ""
          },
          {
            "path": "AUTHORS",
            "size": 477,
            "lines": 20,
            "extension": ""
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 5,
        "files_changed_count": 1,
        "files_changed_ratio": 0.2,
        "total_lines_in_repo": 323,
        "lines_added": 6,
        "lines_deleted": 6,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.03715170278637771,
        "pr_body_length": 230,
        "commit_message_length": 80,
        "python_file_count": 2,
        "python_line_count": 179
      }
    },
    {
      "tar_file_name": "brainix#pottery#pull#761",
      "repo_name": "brainix#pottery#pull#761",
      "success": false,
      "error": "object of type 'NoneType' has no len()",
      "commit": {
        "sha": "602598242694d62b31c3a62eac08334aa51d4164",
        "message": "Upgrade requirements (#759)",
        "author": {
          "name": "Rajiv Bakulesh Shah",
          "email": "brainix@gmail.com",
          "date": "2025-01-28T05:59:41Z"
        },
        "html_url": "https://github.com/brainix/pottery/commit/602598242694d62b31c3a62eac08334aa51d4164",
        "api_url": "https://api.github.com/repos/brainix/pottery/commits/602598242694d62b31c3a62eac08334aa51d4164"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/brainix#pottery#pull#761",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/brainix#pottery#pull#761.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/brainix#pottery#pull#761/source_code"
      },
      "pr": {
        "number": 761,
        "title": "Upgrade requirements",
        "body": null,
        "state": "closed",
        "created_at": "2025-02-06T10:14:27Z",
        "updated_at": "2025-02-06T10:34:15Z",
        "merged_at": "2025-02-06T10:34:13Z",
        "html_url": "https://github.com/brainix/pottery/pull/761",
        "user": "brainix",
        "additions": 14,
        "deletions": 8,
        "changed_files": 2,
        "commits": 2
      },
      "swebench": {
        "instance_id": "brainix_pottery-761",
        "repo": "/brainix/pottery",
        "base_commit": "602598242694d62b31c3a62eac08334aa51d4164",
        "problem_statement": {},
        "edit_files": [
          "requirements.txt",
          "requirements-to-freeze.txt",
          "requirements.txt"
        ],
        "oracle_files": [
          "annotated-types==0.7.0\nAuthlib==1.4.0\nbandit==1.8.2\ncertifi==2024.12.14\ncffi==1.17.1\ncharset-normalizer==3.4.1\nclick==8.1.8\ncoverage==7.6.10\ncryptography==44.0.0\ndocutils==0.20.1\ndparse==0.6.4\nfilelock==3.16.1\nflake8==7.1.1\nhiredis==3.1.0\nid==1.5.0\nidna==3.10\niniconfig==2.0.0\nisort==6.0.0\njaraco.classes==3.4.0\njaraco.context==6.0.1\njaraco.functools==4.1.0\nJinja2==3.1.5\nkeyring==25.6.0\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmarshmallow==3.26.0\nmccabe==0.7.0\nmdurl==0.1.2\nmmh3==5.1.0\nmore-itertools==10.6.0\nmypy==1.14.1\nmypy-extensions==1.0.0\nnh3==0.2.20\npackaging==24.2\npbr==6.1.0\npluggy==1.5.0\npsutil==6.1.1\npycodestyle==2.12.1\npycparser==2.22\npydantic==2.9.2\npydantic_core==2.23.4\npyflakes==3.2.0\nPygments==2.19.1\nPyJWT==2.9.0\npytest==8.3.4\npytest-asyncio==0.25.2\npytest-cov==6.0.0\nPyYAML==6.0.2\nreadme_renderer==43.0\nredis==5.3.0b4\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nrfc3986==2.0.0\nrich==13.9.4\nruamel.yaml==0.18.10\nsafety==3.2.14\nsafety-schemas==0.0.10\nsetuptools==75.8.0\nshellingham==1.5.4\nstevedore==5.4.0\ntwine==6.1.0\ntyper==0.15.1\ntypes-cffi==1.16.0.20241221\ntypes-pyOpenSSL==24.1.0.20240722\ntypes-redis==4.6.0.20241004\ntypes-setuptools==75.8.0.20250110\ntyping_extensions==4.12.2\nurllib3==2.3.0\nuvloop==0.21.0\nwheel==0.45.1\n",
          "# --------------------------------------------------------------------------- #\n#   requirements-to-freeze.txt                                                #\n#                                                                             #\n#   Copyright Â© 2015-2024, Rajiv Bakulesh Shah, original author.              #\n#                                                                             #\n#   Licensed under the Apache License, Version 2.0 (the \"License\");           #\n#   you may not use this file except in compliance with the License.          #\n#   You may obtain a copy of the License at:                                  #\n#       http://www.apache.org/licenses/LICENSE-2.0                            #\n#                                                                             #\n#   Unless required by applicable law or agreed to in writing, software       #\n#   distributed under the License is distributed on an \"AS IS\" BASIS,         #\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  #\n#   See the License for the specific language governing permissions and       #\n#   limitations under the License.                                            #\n# --------------------------------------------------------------------------- #\n\n\n# There's a bug in redis-py 3.4.0 that prevents connecting to Redis with\n# authentication (a username and password).  For more info:\n#   https://github.com/andymccurdy/redis-py/issues/1278\n#\n# We need redis-py 4.2.0rc1 or later for aioredis.  For more info:\n#   https://github.com/aio-libs/aioredis-py/tree/19be499015a8cf32580e937cbfd711fd48489eca#-aioredis-is-now-in-redis-py-420rc1-\nredis>=4.2.0rc1\nhiredis\nmmh3\ntyping_extensions\n\npytest\npytest-asyncio\npytest-cov\nuvloop\nmypy\ntypes-redis\n\nflake8\nisort\nbandit\nsafety\n\ntwine\n\n\n# We don't need Requests at the top-level.  However, it's pulled in from\n# something else, and there's a security vulnerability in the version that it\n# pulls in.  For more info:\n#   https://nvd.nist.gov/vuln/detail/CVE-2018-18074\nrequests>=2.20.0\n\n# We don't need urllib3 at the top-level.  However, it's pulled in from\n# something else, and there's a security vulnerability in the version that it\n# pulls in.  For more info:\n#   https://nvd.nist.gov/vuln/detail/CVE-2018-20060\n#   https://nvd.nist.gov/vuln/detail/CVE-2019-11324\nurllib3>=1.24.2\n\n# We don't need docutils at the top-level.  However, it's pulled in from\n# something else, and recent docutils doesn't support Python 3.8.\ndocutils==0.20.1\n",
          "annotated-types==0.7.0\nAuthlib==1.4.0\nbandit==1.8.2\ncertifi==2024.12.14\ncffi==1.17.1\ncharset-normalizer==3.4.1\nclick==8.1.8\ncoverage==7.6.10\ncryptography==44.0.0\ndocutils==0.20.1\ndparse==0.6.4\nfilelock==3.16.1\nflake8==7.1.1\nhiredis==3.1.0\nid==1.5.0\nidna==3.10\niniconfig==2.0.0\nisort==6.0.0\njaraco.classes==3.4.0\njaraco.context==6.0.1\njaraco.functools==4.1.0\nJinja2==3.1.5\nkeyring==25.6.0\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmarshmallow==3.26.0\nmccabe==0.7.0\nmdurl==0.1.2\nmmh3==5.1.0\nmore-itertools==10.6.0\nmypy==1.14.1\nmypy-extensions==1.0.0\nnh3==0.2.20\npackaging==24.2\npbr==6.1.0\npluggy==1.5.0\npsutil==6.1.1\npycodestyle==2.12.1\npycparser==2.22\npydantic==2.9.2\npydantic_core==2.23.4\npyflakes==3.2.0\nPygments==2.19.1\nPyJWT==2.9.0\npytest==8.3.4\npytest-asyncio==0.25.2\npytest-cov==6.0.0\nPyYAML==6.0.2\nreadme_renderer==43.0\nredis==5.3.0b4\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nrfc3986==2.0.0\nrich==13.9.4\nruamel.yaml==0.18.10\nsafety==3.2.14\nsafety-schemas==0.0.10\nsetuptools==75.8.0\nshellingham==1.5.4\nstevedore==5.4.0\ntwine==6.1.0\ntyper==0.15.1\ntypes-cffi==1.16.0.20241221\ntypes-pyOpenSSL==24.1.0.20240722\ntypes-redis==4.6.0.20241004\ntypes-setuptools==75.8.0.20250110\ntyping_extensions==4.12.2\nurllib3==2.3.0\nuvloop==0.21.0\nwheel==0.45.1\n"
        ],
        "test_patch": "",
        "patch_preview": "From fd66558380173f50ea112bc63f073eb26e0d9ace Mon Sep 17 00:00:00 2001\nFrom: Raj Shah <brainix@gmail.com>\nDate: Thu, 6 Feb 2025 02:13:56 -0800\nSubject: [PATCH 1/2] Upgrade requirements\n\n---\n requirements.txt | 12 ++++++------\n 1 file changed, 6 insertions(+), 6 deletions(-)\n\ndiff --git a/requirements.txt b/requirements.txt\nindex ea203c54..a702b368 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,7 +1,7 @@\n annotated-types==0.7.0\n-Authlib==1.4.0\n+Authlib==1.4.1\n bandit==1.8.2\n-certifi=="
      },
      "patch": {
        "length": 2544,
        "files_changed": 3,
        "lines_added": 14,
        "lines_deleted": 8,
        "net_change": 6,
        "changed_files": [
          {
            "file": "requirements.txt",
            "added": 6,
            "deleted": 6
          },
          {
            "file": "requirements-to-freeze.txt",
            "added": 6,
            "deleted": 0
          },
          {
            "file": "requirements.txt",
            "added": 2,
            "deleted": 2
          }
        ]
      },
      "issue_comments": [
        {
          "id": 2639435805,
          "body": "âœ”ï¸ ",
          "user": "brainix",
          "created_at": "2025-02-06T10:33:29Z",
          "html_url": "https://github.com/brainix/pottery/pull/761#issuecomment-2639435805"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 50,
        "total_lines": 10850,
        "total_bytes": 410007,
        "python_files": 41,
        "python_lines": 9105,
        "file_extensions": {
          ".txt": 2,
          "": 2,
          ".ini": 2,
          ".md": 2,
          ".py": 41,
          ".typed": 1
        },
        "largest_files": [
          {
            "path": "README.md",
            "size": 33704,
            "lines": 1113,
            "extension": ".md"
          },
          {
            "path": "pottery/redlock.py",
            "size": 28626,
            "lines": 773,
            "extension": ".py"
          },
          {
            "path": "tests/test_cache.py",
            "size": 22484,
            "lines": 643,
            "extension": ".py"
          },
          {
            "path": "pottery/aioredlock.py",
            "size": 24146,
            "lines": 563,
            "extension": ".py"
          },
          {
            "path": "pottery/list.py",
            "size": 16412,
            "lines": 373,
            "extension": ".py"
          },
          {
            "path": "tests/test_set.py",
            "size": 12889,
            "lines": 372,
            "extension": ".py"
          },
          {
            "path": "pottery/bloom.py",
            "size": 14616,
            "lines": 366,
            "extension": ".py"
          },
          {
            "path": "tests/test_redlock.py",
            "size": 14664,
            "lines": 365,
            "extension": ".py"
          },
          {
            "path": "pottery/cache.py",
            "size": 13622,
            "lines": 350,
            "extension": ".py"
          },
          {
            "path": "tests/test_bloom.py",
            "size": 12528,
            "lines": 348,
            "extension": ".py"
          }
        ]
      }
    },
    {
      "tar_file_name": "chengyumeng#spider163#pull#35",
      "repo_name": "chengyumeng#spider163#pull#35",
      "success": true,
      "error": null,
      "commit": {
        "sha": "6cc66910017fa9ed775f8ddee762da2e66227e1b",
        "message": "Merge pull request #34 from Chengyumeng/develop\n\nåˆ¶å®š2018å¹´ç¬¬äºŒå­£åº¦å¼€å‘è§„åˆ’",
        "author": {
          "name": "Cheng YuMeng",
          "email": "792400644@qq.com",
          "date": "2018-04-05T11:30:36Z"
        },
        "html_url": "https://github.com/chengyumeng/spider163/commit/6cc66910017fa9ed775f8ddee762da2e66227e1b",
        "api_url": "https://api.github.com/repos/chengyumeng/spider163/commits/6cc66910017fa9ed775f8ddee762da2e66227e1b"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/chengyumeng#spider163#pull#35",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/chengyumeng#spider163#pull#35.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/chengyumeng#spider163#pull#35/source_code"
      },
      "pr": {
        "number": 35,
        "title": "é‚®ä»¶ç³»ç»ŸæŽ¥å…¥",
        "body": "ä¸»è¦æ›´æ–°äº†é»˜è®¤é…ç½®ã€å¢žåŠ äº†smtpé‚®ç®±å…¥å£ç­‰",
        "state": "closed",
        "created_at": "2018-04-07T13:33:22Z",
        "updated_at": "2018-04-07T13:40:27Z",
        "merged_at": "2018-04-07T13:40:27Z",
        "html_url": "https://github.com/chengyumeng/spider163/pull/35",
        "user": "chengyumeng",
        "additions": 119,
        "deletions": 0,
        "changed_files": 8,
        "commits": 6
      },
      "swebench": {
        "instance_id": "chengyumeng_spider163-35",
        "repo": "/chengyumeng/spider163",
        "base_commit": "6cc66910017fa9ed775f8ddee762da2e66227e1b",
        "problem_statement": {},
        "edit_files": [
          "doc/2018.Q2.TODO.md",
          "spider163/utils/mail.py",
          "spider163/version.py",
          "spider163/template/spider163.conf",
          "spider163/utils/config.py",
          "spider163/mail/__init__.py",
          "spider163/mail/mail.py",
          "spider163/bin/cli.py"
        ],
        "oracle_files": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        "test_patch": "",
        "patch_preview": "From f260dc9bbb00632f66e72f68c7d46a67379e30d3 Mon Sep 17 00:00:00 2001\nFrom: Chengyumeng <792400644@qq.com>\nDate: Fri, 6 Apr 2018 19:39:23 +0800\nSubject: [PATCH 1/6] =?UTF-8?q?=E6=9B=B4=E6=96=B0=E7=AC=AC=E4=BA=8C?=\n =?UTF-8?q?=E5=AD=A3=E5=BA=A6=E7=A0=94=E5=8F=91=E8=AE=A1=E5=88=92?=\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\n---\n doc/2018.Q2.TODO.md | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a/doc/2018.Q2.TODO.md b/doc/2018.Q2.TODO.md\nindex"
      },
      "patch": {
        "length": 9422,
        "files_changed": 8,
        "lines_added": 119,
        "lines_deleted": 0,
        "net_change": 119,
        "changed_files": [
          {
            "file": "doc/2018.Q2.TODO.md",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "spider163/utils/mail.py",
            "added": 36,
            "deleted": 0
          },
          {
            "file": "spider163/version.py",
            "added": 22,
            "deleted": 0
          },
          {
            "file": "spider163/template/spider163.conf",
            "added": 5,
            "deleted": 0
          },
          {
            "file": "spider163/utils/config.py",
            "added": 8,
            "deleted": 0
          },
          {
            "file": "spider163/mail/__init__.py",
            "added": 0,
            "deleted": 0
          },
          {
            "file": "spider163/mail/mail.py",
            "added": 33,
            "deleted": 0
          },
          {
            "file": "spider163/bin/cli.py",
            "added": 13,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 54,
        "total_lines": 6154,
        "total_bytes": 1194725,
        "python_files": 27,
        "python_lines": 1929,
        "file_extensions": {
          "": 3,
          ".jpeg": 4,
          ".sh": 1,
          ".in": 1,
          ".py": 27,
          ".md": 4,
          ".yaml": 1,
          ".conf": 2,
          ".js": 5,
          ".css": 1,
          ".html": 5
        },
        "largest_files": [
          {
            "path": "logo.jpeg",
            "size": 141929,
            "lines": 974,
            "extension": ".jpeg"
          },
          {
            "path": "wechat.jpeg",
            "size": 121823,
            "lines": 809,
            "extension": ".jpeg"
          },
          {
            "path": "spider163/www/static/img/weixin.jpeg",
            "size": 75928,
            "lines": 653,
            "extension": ".jpeg"
          },
          {
            "path": "spider163/www/static/img/zhifubao.jpeg",
            "size": 77772,
            "lines": 532,
            "extension": ".jpeg"
          },
          {
            "path": "spider163/bin/cli.py",
            "size": 10073,
            "lines": 282,
            "extension": ".py"
          },
          {
            "path": "spider163/spider/comment.py",
            "size": 12675,
            "lines": 278,
            "extension": ".py"
          },
          {
            "path": "spider163/www/static/js/macarons.js",
            "size": 4845,
            "lines": 198,
            "extension": ".js"
          },
          {
            "path": "spider163/www/static/js/spider163.js",
            "size": 4989,
            "lines": 138,
            "extension": ".js"
          },
          {
            "path": "spider163/www/static/js/stat.js",
            "size": 4461,
            "lines": 129,
            "extension": ".js"
          },
          {
            "path": "spider163/www/templates/spider.html",
            "size": 5409,
            "lines": 129,
            "extension": ".html"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 54,
        "files_changed_count": 8,
        "files_changed_ratio": 0.14814814814814814,
        "total_lines_in_repo": 6154,
        "lines_added": 119,
        "lines_deleted": 0,
        "net_lines_changed": 119,
        "lines_changed_ratio": 0.019337016574585635,
        "pr_body_length": 22,
        "commit_message_length": 64,
        "python_file_count": 27,
        "python_line_count": 1929
      }
    },
    {
      "tar_file_name": "chyyuu#ucore_os_docs#pull#16",
      "repo_name": "chyyuu#ucore_os_docs#pull#16",
      "success": true,
      "error": null,
      "commit": {
        "sha": "61d5284699334e353caca1d294e9d9bd742ce54f",
        "message": "merge updates",
        "author": {
          "name": "chyyuu",
          "email": "yuchen@tsinghua.edu.cn",
          "date": "2018-04-15T10:57:28Z"
        },
        "html_url": "https://github.com/chyyuu/ucore_os_docs/commit/61d5284699334e353caca1d294e9d9bd742ce54f",
        "api_url": "https://api.github.com/repos/chyyuu/ucore_os_docs/commits/61d5284699334e353caca1d294e9d9bd742ce54f"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/chyyuu#ucore_os_docs#pull#16",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/chyyuu#ucore_os_docs#pull#16.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/chyyuu#ucore_os_docs#pull#16/source_code"
      },
      "pr": {
        "number": 16,
        "title": " Visual improvement by specifying language for code blocks; typo fix for lab3",
        "body": "- visual improvement, eg. \r\n  > ```\r\n  >     int i = 0;\r\n  > ```\r\n  > ```c\r\n  >     int i = 0;\r\n  > ```\r\n- typo fix, eg. \"ä¸Šè¯‰\"->\"ä¸Šè¿°\"",
        "state": "closed",
        "created_at": "2018-04-16T06:18:31Z",
        "updated_at": "2018-04-16T06:47:33Z",
        "merged_at": "2018-04-16T06:47:33Z",
        "html_url": "https://github.com/chyyuu/ucore_os_docs/pull/16",
        "user": "leefige",
        "additions": 50,
        "deletions": 50,
        "changed_files": 15,
        "commits": 4
      },
      "swebench": {
        "instance_id": "chyyuu_ucore_os_docs-16",
        "repo": "/chyyuu/ucore_os_docs",
        "base_commit": "61d5284699334e353caca1d294e9d9bd742ce54f",
        "problem_statement": {},
        "edit_files": [
          "lab2/lab2_3_3_3_phymem_pagelevel.md",
          "lab2/lab2_3_3_4_phymem_allocation.md",
          "lab2/lab2_3_5_probe_phymem_methods.md",
          "lab2/lab2_3_6_implement_probe_phymem.md",
          "lab2/lab2_3_7_phymemlab_concepts.md",
          "lab0/lab0_2_3_1_4_extend_gcc_asm.md",
          "lab2/lab2_3_3_5_3_setup_paging_map.md",
          "lab2/lab2_3_3_5_4_maping_relations.md",
          "lab2/lab2_3_3_6_self_mapping.md",
          "lab2/lab2_3_6_implement_probe_phymem.md",
          "lab2/lab2_3_7_phymemlab_concepts.md",
          "SUMMARY.md",
          "lab3/lab3_2_2_files.md",
          "lab3/lab3_3_2_labs_steps.md",
          "lab3/lab3_3_3_data_structures.md",
          "lab3/lab3_4_page_fault_handler.md",
          "lab3/lab3_5_2_page_swapping_principles.md"
        ],
        "oracle_files": [
          "###ä»¥é¡µä¸ºå•ä½ç®¡ç†ç‰©ç†å†…å­˜\n\nåœ¨èŽ·å¾—å¯ç”¨ç‰©ç†å†…å­˜èŒƒå›´åŽï¼Œç³»ç»Ÿéœ€è¦å»ºç«‹ç›¸åº”çš„æ•°æ®ç»“æž„æ¥ç®¡ç†ä»¥ç‰©ç†é¡µï¼ˆæŒ‰4KBå¯¹é½ï¼Œä¸”å¤§å°ä¸º4KBçš„ç‰©ç†å†…å­˜å•å…ƒï¼‰ä¸ºæœ€å°å•ä½çš„æ•´ä¸ªç‰©ç†å†…å­˜ï¼Œä»¥é…åˆåŽç»­æ¶‰åŠçš„åˆ†é¡µç®¡ç†æœºåˆ¶ã€‚æ¯ä¸ªç‰©ç†é¡µå¯ä»¥ç”¨ä¸€ä¸ª\nPageæ•°æ®ç»“æž„æ¥è¡¨ç¤ºã€‚ç”±äºŽä¸€ä¸ªç‰©ç†é¡µéœ€è¦å ç”¨ä¸€ä¸ªPageç»“æž„çš„ç©ºé—´ï¼ŒPageç»“æž„åœ¨è®¾è®¡æ—¶é¡»å°½å¯èƒ½å°ï¼Œä»¥å‡å°‘å¯¹å†…å­˜çš„å ç”¨ã€‚Pageçš„å®šä¹‰åœ¨kern/mm/memlayout.hä¸­ã€‚ä»¥é¡µä¸ºå•ä½çš„ç‰©ç†å†…å­˜åˆ†é…ç®¡ç†çš„å®žçŽ°åœ¨kern/default\\_pmm.[ch]ã€‚\n\nä¸ºäº†ä¸Žä»¥åŽçš„åˆ†é¡µæœºåˆ¶é…åˆï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å»ºç«‹å¯¹æ•´ä¸ªè®¡ç®—æœºçš„æ¯ä¸€ä¸ªç‰©ç†é¡µçš„å±žæ€§ç”¨ç»“æž„Pageæ¥è¡¨ç¤ºï¼Œå®ƒåŒ…å«äº†æ˜ å°„æ­¤ç‰©ç†é¡µçš„è™šæ‹Ÿé¡µä¸ªæ•°ï¼Œæè¿°ç‰©ç†é¡µå±žæ€§çš„flagså’ŒåŒå‘é“¾æŽ¥å„ä¸ªPageç»“æž„çš„page\\_linkåŒå‘é“¾è¡¨ã€‚\n```\nstruct Page {\n    int ref;        // page frame's reference counter\n    uint32_t flags; // array of flags that describe the status of the page frame\n    unsigned int property;// the num of free block, used in first fit pm manager\n    list_entry_t page_link;// free list link\n};\n```\nè¿™é‡Œçœ‹çœ‹Pageæ•°æ®ç»“æž„çš„å„ä¸ªæˆå‘˜å˜é‡æœ‰ä½•å…·ä½“å«ä¹‰ã€‚refè¡¨ç¤ºè¿™æ ·é¡µè¢«é¡µè¡¨çš„å¼•ç”¨è®°æ•°ï¼ˆåœ¨â€œå®žçŽ°åˆ†é¡µæœºåˆ¶â€ä¸€èŠ‚ä¼šè®²åˆ°ï¼‰ã€‚å¦‚æžœè¿™ä¸ªé¡µè¢«é¡µè¡¨å¼•ç”¨äº†ï¼Œå³åœ¨æŸé¡µè¡¨ä¸­æœ‰ä¸€ä¸ªé¡µè¡¨é¡¹è®¾ç½®äº†ä¸€ä¸ªè™šæ‹Ÿé¡µåˆ°è¿™ä¸ªPageç®¡ç†çš„ç‰©ç†é¡µçš„æ˜ å°„å…³ç³»ï¼Œå°±ä¼šæŠŠPageçš„refåŠ ä¸€ï¼›åä¹‹ï¼Œè‹¥é¡µè¡¨é¡¹å–æ¶ˆï¼Œå³æ˜ å°„å…³ç³»è§£é™¤ï¼Œå°±ä¼šæŠŠPageçš„refå‡ä¸€ã€‚flagsè¡¨ç¤ºæ­¤ç‰©ç†é¡µçš„çŠ¶æ€æ ‡è®°ï¼Œè¿›ä¸€æ­¥æŸ¥çœ‹kern/mm/memlayout.hä¸­çš„å®šä¹‰ï¼Œå¯ä»¥çœ‹åˆ°ï¼š\n```\n/* Flags describing the status of a page frame */\n#define PG_reserved                 0       // the page descriptor is reserved for kernel or unusable\n#define PG_property                 1       // the member 'property' is valid\n```\nè¿™è¡¨ç¤ºflagsç›®å‰ç”¨åˆ°äº†ä¸¤ä¸ªbitè¡¨ç¤ºé¡µç›®å‰å…·æœ‰çš„ä¸¤ç§å±žæ€§ï¼Œbit\n0è¡¨ç¤ºæ­¤é¡µæ˜¯å¦è¢«ä¿ç•™ï¼ˆreservedï¼‰ï¼Œå¦‚æžœæ˜¯è¢«ä¿ç•™çš„é¡µï¼Œåˆ™bit\n0ä¼šè®¾ç½®ä¸º1ï¼Œä¸”ä¸èƒ½æ”¾åˆ°ç©ºé—²é¡µé“¾è¡¨ä¸­ï¼Œå³è¿™æ ·çš„é¡µä¸æ˜¯ç©ºé—²é¡µï¼Œä¸èƒ½åŠ¨æ€åˆ†é…ä¸Žé‡Šæ”¾ã€‚æ¯”å¦‚ç›®å‰å†…æ ¸ä»£ç å ç”¨çš„ç©ºé—´å°±å±žäºŽè¿™æ ·â€œè¢«ä¿ç•™â€çš„é¡µã€‚åœ¨æœ¬å®žéªŒä¸­ï¼Œbit\n1è¡¨ç¤ºæ­¤é¡µæ˜¯å¦æ˜¯freeçš„ï¼Œå¦‚æžœè®¾ç½®ä¸º1ï¼Œè¡¨ç¤ºè¿™é¡µæ˜¯freeçš„ï¼Œå¯ä»¥è¢«åˆ†é…ï¼›å¦‚æžœè®¾ç½®ä¸º0ï¼Œè¡¨ç¤ºè¿™é¡µå·²ç»è¢«åˆ†é…å‡ºåŽ»äº†ï¼Œä¸èƒ½è¢«å†äºŒæ¬¡åˆ†é…ã€‚å¦å¤–ï¼Œæœ¬å®žéªŒè¿™é‡Œå–çš„åå­—PG\\_propertyæ¯”è¾ƒä¸ç›´è§‚\nï¼Œä¸»è¦æ˜¯æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸åŒçš„é¡µåˆ†é…ç®—æ³•ï¼ˆbest fit, buddy\nsystemç­‰ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªPG\\_propertyå°±æœ‰ä¸åŒçš„å«ä¹‰äº†ã€‚\n\nåœ¨æœ¬å®žéªŒä¸­ï¼ŒPageæ•°æ®ç»“æž„çš„æˆå‘˜å˜é‡propertyç”¨æ¥è®°å½•æŸè¿žç»­å†…å­˜ç©ºé—²å—çš„å¤§å°ï¼ˆå³åœ°å€è¿žç»­çš„ç©ºé—²é¡µçš„ä¸ªæ•°ï¼‰ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ç”¨åˆ°æ­¤æˆå‘˜å˜é‡çš„è¿™ä¸ªPageæ¯”è¾ƒç‰¹æ®Šï¼Œæ˜¯è¿™ä¸ªè¿žç»­å†…å­˜ç©ºé—²å—åœ°å€æœ€å°çš„ä¸€é¡µï¼ˆå³å¤´ä¸€é¡µï¼Œ\nHead\nPageï¼‰ã€‚è¿žç»­å†…å­˜ç©ºé—²å—åˆ©ç”¨è¿™ä¸ªé¡µçš„æˆå‘˜å˜é‡propertyæ¥è®°å½•åœ¨æ­¤å—å†…çš„ç©ºé—²é¡µçš„ä¸ªæ•°ã€‚è¿™é‡ŒåŽ»çš„åå­—propertyä¹Ÿä¸æ˜¯å¾ˆç›´è§‚ï¼ŒåŽŸå› ä¸Žä¸Šé¢ç±»ä¼¼ï¼Œåœ¨ä¸åŒçš„é¡µåˆ†é…ç®—æ³•ä¸­ï¼Œpropertyæœ‰ä¸åŒçš„å«ä¹‰ã€‚\n\nPageæ•°æ®ç»“æž„çš„æˆå‘˜å˜é‡page\\_linkæ˜¯ä¾¿äºŽæŠŠå¤šä¸ªè¿žç»­å†…å­˜ç©ºé—²å—é“¾æŽ¥åœ¨ä¸€èµ·çš„åŒå‘é“¾è¡¨æŒ‡é’ˆï¼ˆå¯å›žé¡¾åœ¨lab0å®žéªŒæŒ‡å¯¼ä¹¦ä¸­æœ‰å…³åŒå‘é“¾è¡¨æ•°æ®ç»“æž„çš„ä»‹ç»ï¼‰ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ç”¨åˆ°æ­¤æˆå‘˜å˜é‡çš„è¿™ä¸ªPageæ¯”è¾ƒç‰¹æ®Šï¼Œæ˜¯è¿™ä¸ªè¿žç»­å†…å­˜ç©ºé—²å—åœ°å€æœ€å°çš„ä¸€é¡µï¼ˆå³å¤´ä¸€é¡µï¼Œ\nHead\nPageï¼‰ã€‚è¿žç»­å†…å­˜ç©ºé—²å—åˆ©ç”¨è¿™ä¸ªé¡µçš„æˆå‘˜å˜é‡page\\_linkæ¥é“¾æŽ¥æ¯”å®ƒåœ°å€å°å’Œå¤§çš„å…¶ä»–è¿žç»­å†…å­˜ç©ºé—²å—ã€‚\n\nåœ¨åˆå§‹æƒ…å†µä¸‹ï¼Œä¹Ÿè®¸è¿™ä¸ªç‰©ç†å†…å­˜çš„ç©ºé—²ç‰©ç†é¡µéƒ½æ˜¯è¿žç»­çš„ï¼Œè¿™æ ·å°±å½¢æˆäº†ä¸€ä¸ªå¤§çš„è¿žç»­å†…å­˜ç©ºé—²å—ã€‚ä½†éšç€ç‰©ç†é¡µçš„åˆ†é…ä¸Žé‡Šæ”¾ï¼Œè¿™ä¸ªå¤§çš„è¿žç»­å†…å­˜ç©ºé—²å—ä¼šåˆ†è£‚ä¸ºä¸€ç³»åˆ—åœ°å€ä¸è¿žç»­çš„å¤šä¸ªå°è¿žç»­å†…å­˜ç©ºé—²å—ï¼Œä¸”æ¯ä¸ªè¿žç»­å†…å­˜ç©ºé—²å—å†…éƒ¨çš„ç‰©ç†é¡µæ˜¯è¿žç»­çš„ã€‚é‚£ä¹ˆä¸ºäº†æœ‰æ•ˆåœ°ç®¡ç†è¿™äº›å°è¿žç»­å†…å­˜ç©ºé—²å—ã€‚æ‰€æœ‰çš„è¿žç»­å†…å­˜ç©ºé—²å—å¯ç”¨ä¸€ä¸ªåŒå‘é“¾è¡¨ç®¡ç†èµ·æ¥ï¼Œä¾¿äºŽåˆ†é…å’Œé‡Šæ”¾ï¼Œä¸ºæ­¤å®šä¹‰äº†ä¸€ä¸ªfree\\_area\\_tæ•°æ®ç»“æž„ï¼ŒåŒ…å«äº†ä¸€ä¸ªlist\\_entryç»“æž„çš„åŒå‘é“¾è¡¨æŒ‡é’ˆå’Œè®°å½•å½“å‰ç©ºé—²é¡µçš„ä¸ªæ•°çš„æ— ç¬¦å·æ•´åž‹å˜é‡nr\\_freeã€‚å…¶ä¸­çš„é“¾è¡¨æŒ‡é’ˆæŒ‡å‘äº†ç©ºé—²çš„ç‰©ç†é¡µã€‚\n```\n/* free_area_t - maintains a doubly linked list to record free (unused) pages */\ntypedef struct {\n            list_entry_t free_list;                                // the list header\n            unsigned int nr_free;                                 // # of free pages in this free list\n} free_area_t;\n```\næœ‰äº†è¿™ä¸¤ä¸ªæ•°æ®ç»“æž„ï¼Œucoreå°±å¯ä»¥ç®¡ç†èµ·æ¥æ•´ä¸ªä»¥é¡µä¸ºå•ä½çš„ç‰©ç†å†…å­˜ç©ºé—´ã€‚æŽ¥ä¸‹æ¥éœ€è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š\n\nâ€¢ ç®¡ç†é¡µçº§ç‰©ç†å†…å­˜ç©ºé—´æ‰€éœ€çš„Pageç»“æž„çš„å†…å­˜ç©ºé—´ä»Žå“ªé‡Œå¼€å§‹ï¼Œå å¤šå¤§ç©ºé—´ï¼Ÿ\nâ€¢ ç©ºé—²å†…å­˜ç©ºé—´çš„èµ·å§‹åœ°å€åœ¨å“ªé‡Œï¼Ÿ\n\nå¯¹äºŽè¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®bootloaderç»™å‡ºçš„å†…å­˜å¸ƒå±€ä¿¡æ¯æ‰¾å‡ºæœ€å¤§çš„ç‰©ç†å†…å­˜åœ°å€maxpaï¼ˆå®šä¹‰åœ¨page\\_initå‡½æ•°ä¸­çš„å±€éƒ¨å˜é‡ï¼‰ï¼Œç”±äºŽx86çš„èµ·å§‹ç‰©ç†å†…å­˜åœ°å€ä¸º0ï¼Œæ‰€ä»¥å¯ä»¥å¾—çŸ¥éœ€è¦ç®¡ç†çš„ç‰©ç†é¡µä¸ªæ•°ä¸º\n```\nnpage = maxpa / PGSIZE\n```\nè¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥é¢„ä¼°å‡ºç®¡ç†é¡µçº§ç‰©ç†å†…å­˜ç©ºé—´æ‰€éœ€çš„Pageç»“æž„çš„å†…å­˜ç©ºé—´æ‰€éœ€çš„å†…å­˜å¤§å°ä¸ºï¼š\n```\nsizeof(struct Page) * npage\n```\nç”±äºŽbootloaderåŠ è½½ucoreçš„ç»“æŸåœ°å€ï¼ˆç”¨å…¨å±€æŒ‡é’ˆå˜é‡endè®°å½•ï¼‰ä»¥ä¸Šçš„ç©ºé—´æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŠŠendæŒ‰é¡µå¤§å°ä¸ºè¾¹ç•ŒåŽ»æ•´åŽï¼Œä½œä¸ºç®¡ç†é¡µçº§ç‰©ç†å†…å­˜ç©ºé—´æ‰€éœ€çš„Pageç»“æž„çš„å†…å­˜ç©ºé—´ï¼Œè®°ä¸ºï¼š\n```\npages = (struct Page *)ROUNDUP((void *)end, PGSIZE);\n```\nä¸ºäº†ç®€åŒ–èµ·è§ï¼Œä»Žåœ°å€0åˆ°åœ°å€pages+ sizeof(struct Page) \\*\nnpage)ç»“æŸçš„ç‰©ç†å†…å­˜ç©ºé—´è®¾å®šä¸ºå·²å ç”¨ç‰©ç†å†…å­˜ç©ºé—´ï¼ˆèµ·å§‹0\\~640KBçš„ç©ºé—´æ˜¯ç©ºé—²çš„ï¼‰ï¼Œåœ°å€pages+\nsizeof(struct Page) \\*\nnpage)ä»¥ä¸Šçš„ç©ºé—´ä¸ºç©ºé—²ç‰©ç†å†…å­˜ç©ºé—´ï¼Œè¿™æ—¶çš„ç©ºé—²ç©ºé—´èµ·å§‹åœ°å€ä¸º\n```\nuintptr_t freemem = PADDR((uintptr_t)pages + sizeof(struct Page) * npage);\n```\nä¸ºæ­¤æˆ‘ä»¬éœ€è¦æŠŠè¿™ä¸¤éƒ¨åˆ†ç©ºé—´ç»™æ ‡è¯†å‡ºæ¥ã€‚é¦–å…ˆï¼Œå¯¹äºŽæ‰€æœ‰ç‰©ç†ç©ºé—´ï¼Œé€šè¿‡å¦‚ä¸‹è¯­å¥å³å¯å®žçŽ°å ç”¨æ ‡è®°ï¼š\n```\nfor (i = 0; i < npage; i ++) {\nSetPageReserved(pages + i);\n}\n````\nç„¶åŽï¼Œæ ¹æ®æŽ¢æµ‹åˆ°çš„ç©ºé—²ç‰©ç†ç©ºé—´ï¼Œé€šè¿‡å¦‚ä¸‹è¯­å¥å³å¯å®žçŽ°ç©ºé—²æ ‡è®°ï¼š\n```\n//èŽ·å¾—ç©ºé—²ç©ºé—´çš„èµ·å§‹åœ°å€beginå’Œç»“æŸåœ°å€end\nâ€¦â€¦\ninit_memmap(pa2page(begin), (end - begin) / PGSIZE);\n```\nå…¶å®žSetPageReservedåªéœ€æŠŠç‰©ç†åœ°å€å¯¹åº”çš„Pageç»“æž„ä¸­çš„flagsæ ‡å¿—è®¾ç½®ä¸ºPG\\_reserved\nï¼Œè¡¨ç¤ºè¿™äº›é¡µå·²ç»è¢«ä½¿ç”¨äº†ï¼Œå°†æ¥ä¸èƒ½è¢«ç”¨äºŽåˆ†é…ã€‚è€Œinit\\_memmapå‡½æ•°åˆ™æ˜¯æŠŠç©ºé—²ç‰©ç†é¡µå¯¹åº”çš„Pageç»“æž„ä¸­çš„flagså’Œå¼•ç”¨è®¡æ•°refæ¸…é›¶ï¼Œå¹¶åŠ åˆ°free\\_area.free\\_listæŒ‡å‘çš„åŒå‘åˆ—è¡¨ä¸­ï¼Œä¸ºå°†æ¥çš„ç©ºé—²é¡µç®¡ç†åšå¥½åˆå§‹åŒ–å‡†å¤‡å·¥ä½œã€‚\n\nå…³äºŽå†…å­˜åˆ†é…çš„æ“ä½œç³»ç»ŸåŽŸç†æ–¹é¢çš„çŸ¥è¯†æœ‰å¾ˆå¤šï¼Œä½†åœ¨æœ¬å®žéªŒä¸­åªå®žçŽ°äº†æœ€ç®€å•çš„å†…å­˜é¡µåˆ†é…ç®—æ³•ã€‚ç›¸åº”çš„å®žçŽ°åœ¨default\\_pmm.cä¸­çš„default\\_alloc\\_pageså‡½æ•°å’Œdefault\\_free\\_pageså‡½æ•°ï¼Œç›¸å…³å®žçŽ°å¾ˆç®€å•ï¼Œè¿™é‡Œå°±ä¸å…·ä½“åˆ†æžäº†ï¼Œç›´æŽ¥çœ‹æºç ï¼Œåº”è¯¥å¾ˆå¥½ç†è§£ã€‚\n\nå…¶å®žå®žéªŒäºŒåœ¨å†…å­˜åˆ†é…å’Œé‡Šæ”¾æ–¹é¢æœ€ä¸»è¦çš„ä½œç”¨æ˜¯å»ºç«‹äº†ä¸€ä¸ªç‰©ç†å†…å­˜é¡µç®¡ç†å™¨æ¡†æž¶ï¼Œè¿™å®žé™…ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°æŒ‡é’ˆåˆ—è¡¨ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š\n```\nstruct pmm_manager {\n            const char *name; //ç‰©ç†å†…å­˜é¡µç®¡ç†å™¨çš„åå­—\n            void (*init)(void); //åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨\n            void (*init_memmap)(struct Page *base, size_t n); //åˆå§‹åŒ–ç®¡ç†ç©ºé—²å†…å­˜é¡µçš„æ•°æ®ç»“æž„\n            struct Page *(*alloc_pages)(size_t n); //åˆ†é…nä¸ªç‰©ç†å†…å­˜é¡µ\n            void (*free_pages)(struct Page *base, size_t n); //é‡Šæ”¾nä¸ªç‰©ç†å†…å­˜é¡µ\n            size_t (*nr_free_pages)(void); //è¿”å›žå½“å‰å‰©ä½™çš„ç©ºé—²é¡µæ•°\n            void (*check)(void); //ç”¨äºŽæ£€æµ‹åˆ†é…/é‡Šæ”¾å®žçŽ°æ˜¯å¦æ­£ç¡®çš„è¾…åŠ©å‡½æ•°\n};\n```\né‡ç‚¹æ˜¯å®žçŽ°init\\_memmap/ alloc\\_pages/\nfree\\_pagesè¿™ä¸‰ä¸ªå‡½æ•°ã€‚å½“å®Œæˆç‰©ç†å†…å­˜é¡µç®¡ç†åˆå§‹åŒ–å·¥ä½œåŽï¼Œè®¡ç®—æœºç³»ç»Ÿçš„å†…å­˜å¸ƒå±€å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](../lab2_figs/image003.png)\nå›¾3 è®¡ç®—æœºç³»ç»Ÿçš„å†…å­˜å¸ƒå±€\n\n",
          "### ç‰©ç†å†…å­˜é¡µåˆ†é…ç®—æ³•å®žçŽ°\n\nå¦‚æžœè¦åœ¨ucoreä¸­å®žçŽ°è¿žç»­ç‰©ç†å†…å­˜åˆ†é…ç®—æ³•ï¼Œåˆ™éœ€è¦è€ƒè™‘çš„äº‹æƒ…æ¯”è¾ƒå¤šï¼Œç›¸å¯¹è¯¾æœ¬ä¸Šçš„ç‰©ç†å†…å­˜åˆ†é…ç®—æ³•æè¿°è¦å¤æ‚ä¸å°‘ã€‚ä¸‹é¢ä»‹ç»ä¸€ä¸‹å¦‚æžœè¦å®žçŽ°ä¸€ä¸ªFirstFitå†…å­˜åˆ†é…ç®—æ³•çš„å¤§è‡´æµç¨‹ã€‚\n\nlab2çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯å®Œæˆfirst\\_fitçš„åˆ†é…ç®—æ³•ã€‚åŽŸç†FirstFitå†…å­˜åˆ†é…ç®—æ³•ä¸Šå¾ˆç®€å•ï¼Œä½†è¦åœ¨ucoreä¸­å®žçŽ°ï¼Œéœ€è¦å……åˆ†äº†è§£å’Œåˆ©ç”¨ucoreå·²æœ‰çš„æ•°æ®ç»“æž„å’Œç›¸å…³æ“ä½œã€å…³é”®çš„ä¸€äº›å…¨å±€å˜é‡ç­‰ã€‚\n\n**å…³é”®æ•°æ®ç»“æž„å’Œå˜é‡**\n\nfirst\\_fitåˆ†é…ç®—æ³•éœ€è¦ç»´æŠ¤ä¸€ä¸ªæŸ¥æ‰¾æœ‰åºï¼ˆåœ°å€æŒ‰ä»Žå°åˆ°å¤§æŽ’åˆ—ï¼‰ç©ºé—²å—ï¼ˆä»¥é¡µä¸ºæœ€å°å•ä½çš„è¿žç»­åœ°å€ç©ºé—´ï¼‰çš„æ•°æ®ç»“æž„ï¼Œè€ŒåŒå‘é“¾è¡¨æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚\n\nlibs/list.hå®šä¹‰äº†å¯æŒ‚æŽ¥ä»»æ„å…ƒç´ çš„é€šç”¨åŒå‘é“¾è¡¨ç»“æž„å’Œå¯¹åº”çš„æ“ä½œï¼Œæ‰€ä»¥éœ€è¦äº†è§£å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ–‡ä»¶æä¾›çš„å„ç§å‡½æ•°ï¼Œä»Žè€Œå¯ä»¥å®Œæˆå¯¹åŒå‘é“¾è¡¨çš„åˆå§‹åŒ–/æ’å…¥/åˆ é™¤ç­‰ã€‚\n\nkern/mm/memlayout.hä¸­å®šä¹‰äº†ä¸€ä¸ª free\\_area\\_t æ•°æ®ç»“æž„ï¼ŒåŒ…å«æˆå‘˜ç»“æž„\n```\n  list_entry_t free_list;         // the list header   ç©ºé—²å—åŒå‘é“¾è¡¨çš„å¤´\n  unsigned int nr_free;           // # of free pages in this free list  ç©ºé—²å—çš„æ€»æ•°ï¼ˆä»¥é¡µä¸ºå•ä½ï¼‰\n```\næ˜¾ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ­¤æ•°æ®ç»“æž„æ¥å®Œæˆå¯¹ç©ºé—²å—çš„ç®¡ç†ã€‚è€Œdefault\\_pmm.cä¸­å®šä¹‰çš„free\\_areaå˜é‡å°±æ˜¯å¹²è¿™ä¸ªäº‹æƒ…çš„ã€‚\n\nkern/mm/pmm.hä¸­å®šä¹‰äº†ä¸€ä¸ªé€šç”¨çš„åˆ†é…ç®—æ³•çš„å‡½æ•°åˆ—è¡¨ï¼Œç”¨pmm\\_manager\nè¡¨ç¤ºã€‚å…¶ä¸­initå‡½æ•°å°±æ˜¯ç”¨æ¥åˆå§‹åŒ–free\\_areaå˜é‡çš„,\nfirst\\_fitåˆ†é…ç®—æ³•å¯ç›´æŽ¥é‡ç”¨default\\_initå‡½æ•°çš„å®žçŽ°ã€‚init\\_memmapå‡½æ•°éœ€è¦æ ¹æ®çŽ°æœ‰çš„å†…å­˜æƒ…å†µæž„å»ºç©ºé—²å—åˆ—è¡¨çš„åˆå§‹çŠ¶æ€ã€‚ä½•æ—¶åº”è¯¥æ‰§è¡Œè¿™ä¸ªå‡½æ•°å‘¢ï¼Ÿ\n\né€šè¿‡åˆ†æžä»£ç ï¼Œå¯ä»¥çŸ¥é“ï¼š\n```\nkern_init --> pmm_init-->page_init-->init_memmap--> pmm_manager->init_memmap\n```\næ‰€ä»¥ï¼Œdefault\\_init\\_memmapéœ€è¦æ ¹æ®page\\_initå‡½æ•°ä¸­ä¼ é€’è¿‡æ¥çš„å‚æ•°ï¼ˆæŸä¸ªè¿žç»­åœ°å€çš„ç©ºé—²å—çš„èµ·å§‹é¡µï¼Œé¡µä¸ªæ•°ï¼‰æ¥å»ºç«‹ä¸€ä¸ªè¿žç»­å†…å­˜ç©ºé—²å—çš„åŒå‘é“¾è¡¨ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªå‡å®špage\\_initå‡½æ•°æ˜¯æŒ‰åœ°å€ä»Žå°åˆ°å¤§çš„é¡ºåºä¼ æ¥çš„è¿žç»­å†…å­˜ç©ºé—²å—çš„ã€‚é“¾è¡¨å¤´æ˜¯free\\_area.free\\_listï¼Œé“¾è¡¨é¡¹æ˜¯Pageæ•°æ®ç»“æž„çš„base-\\>page\\_linkã€‚è¿™æ ·æˆ‘ä»¬å°±ä¾é Pageæ•°æ®ç»“æž„ä¸­çš„æˆå‘˜å˜é‡page\\_linkå½¢æˆäº†è¿žç»­å†…å­˜ç©ºé—²å—åˆ—è¡¨ã€‚\n\n**è®¾è®¡å®žçŽ°**\n\ndefault\\_init\\_memmapå‡½æ•°è®²æ ¹æ®æ¯ä¸ªç‰©ç†é¡µå¸§çš„æƒ…å†µæ¥å»ºç«‹ç©ºé—²é¡µé“¾è¡¨ï¼Œä¸”ç©ºé—²é¡µå—åº”è¯¥æ˜¯æ ¹æ®åœ°å€é«˜ä½Žå½¢æˆä¸€ä¸ªæœ‰åºé“¾è¡¨ã€‚æ ¹æ®ä¸Šè¿°å˜é‡çš„å®šä¹‰ï¼Œdefault\\_init\\_memmapå¯å¤§è‡´å®žçŽ°å¦‚ä¸‹ï¼š\n```\ndefault_init_memmap(struct Page *base, size_t n) {\n    struct Page *p = base;\n    for (; p != base + n; p ++) {\n        p->flags = p->property = 0;\n        set_page_ref(p, 0);\n    }\n    base->property = n;\n    SetPageProperty(base);\n    nr_free += n;\n    list_add(&free_list, &(base->page_link));\n}\n```\nå¦‚æžœè¦åˆ†é…ä¸€ä¸ªé¡µï¼Œé‚£è¦è€ƒè™‘å“ªäº›å‘¢ï¼Ÿè¿™é‡Œå°±éœ€è¦è€ƒè™‘å®žçŽ°default\\_alloc\\_pageså‡½æ•°ï¼Œæ³¨æ„å‚æ•°nè¡¨ç¤ºè¦åˆ†é…nä¸ªé¡µã€‚å¦å¤–ï¼Œéœ€è¦æ³¨æ„å®žçŽ°æ—¶å°½é‡å¤šè€ƒè™‘ä¸€äº›è¾¹ç•Œæƒ…å†µï¼Œè¿™æ ·ç¡®ä¿è½¯ä»¶çš„é²æ£’æ€§ã€‚æ¯”å¦‚\n```\nif (n > nr_free) {\nreturn NULL;\n}\n```\nè¿™æ ·å¯ä»¥ç¡®ä¿åˆ†é…ä¸ä¼šè¶…å‡ºèŒƒå›´ã€‚ä¹Ÿå¯åŠ ä¸€äº›\nassertå‡½æ•°ï¼Œåœ¨æœ‰é”™è¯¯å‡ºçŽ°æ—¶ï¼Œèƒ½å¤Ÿè¿…é€Ÿå‘çŽ°ã€‚æ¯”å¦‚ nåº”è¯¥å¤§äºŽ0ï¼Œæˆ‘ä»¬å°±å¯ä»¥åŠ ä¸Š\n```\nassert(n \\> 0);\n```\nè¿™æ ·åœ¨n<=0çš„æƒ…å†µä¸‹ï¼Œucoreä¼šè¿…é€ŸæŠ¥é”™ã€‚firstfitéœ€è¦ä»Žç©ºé—²é“¾è¡¨å¤´å¼€å§‹æŸ¥æ‰¾æœ€å°çš„åœ°å€ï¼Œé€šè¿‡list\\_nextæ‰¾åˆ°ä¸‹ä¸€ä¸ªç©ºé—²å—å…ƒç´ ï¼Œé€šè¿‡le2pageå®å¯ä»¥æ›´åŠ é“¾è¡¨å…ƒç´ èŽ·å¾—å¯¹åº”çš„PageæŒ‡é’ˆpã€‚é€šè¿‡p-\\>propertyå¯ä»¥äº†è§£æ­¤ç©ºé—²å—çš„å¤§å°ã€‚å¦‚æžœ\\>=nï¼Œè¿™å°±æ‰¾åˆ°äº†ï¼å¦‚æžœ<nï¼Œåˆ™list\\_nextï¼Œç»§ç»­æŸ¥æ‰¾ã€‚ç›´åˆ°list\\_next==\n&free\\_listï¼Œè¿™è¡¨ç¤ºæ‰¾å®Œäº†ä¸€éäº†ã€‚æ‰¾åˆ°åŽï¼Œå°±è¦ä»Žæ–°ç»„ç»‡ç©ºé—²å—ï¼Œç„¶åŽæŠŠæ‰¾åˆ°çš„pageè¿”å›žã€‚æ‰€ä»¥default\\_alloc\\_pageså¯å¤§è‡´å®žçŽ°å¦‚ä¸‹ï¼š\n```\nstatic struct Page *\ndefault_alloc_pages(size_t n) {\n    if (n > nr_free) {\n        return NULL;\n    }\n    struct Page *page = NULL;\n    list_entry_t *le = &free_list;\n    while ((le = list_next(le)) != &free_list) {\n        struct Page *p = le2page(le, page_link);\n        if (p->property >= n) {\n            page = p;\n            break;\n        }\n    }\n    if (page != NULL) {\n        list_del(&(page->page_link));\n        if (page->property > n) {\n            struct Page *p = page + n;\n            p->property = page->property - n;\n            list_add(&free_list, &(p->page_link));\n        }\n        nr_free -= n;\n        ClearPageProperty(page);\n    }\n    return page;\n}\n```\ndefault\\_free\\_pageså‡½æ•°çš„å®žçŽ°å…¶å®žæ˜¯default\\_alloc\\_pagesçš„é€†è¿‡ç¨‹ï¼Œä¸è¿‡éœ€è¦è€ƒè™‘ç©ºé—²å—çš„åˆå¹¶é—®é¢˜ã€‚è¿™é‡Œå°±ä¸å†ç»†è®²äº†ã€‚æ³¨æ„ï¼Œä¸Šè¯‰ä»£ç åªæ˜¯å‚è€ƒè®¾è®¡ï¼Œä¸æ˜¯å®Œæ•´çš„æ­£ç¡®è®¾è®¡ã€‚æ›´è¯¦ç»†çš„è¯´æ˜Žä½äºŽlab2/kernel/mm/default\\_pmm.cçš„æ³¨é‡Šä¸­ã€‚å¸Œæœ›åŒå­¦èƒ½å¤Ÿé¡ºåˆ©å®Œæˆæœ¬å®žéªŒçš„ç¬¬ä¸€éƒ¨åˆ†ã€‚\n",
          "**æŽ¢æµ‹ç‰©ç†å†…å­˜åˆ†å¸ƒå’Œå¤§å°çš„æ–¹æ³•**\n\næ“ä½œç³»ç»Ÿéœ€è¦çŸ¥é“äº†è§£æ•´ä¸ªè®¡ç®—æœºç³»ç»Ÿä¸­çš„ç‰©ç†å†…å­˜å¦‚ä½•åˆ†å¸ƒçš„ï¼Œå“ªäº›è¢«å¯ç”¨ï¼Œå“ªäº›ä¸å¯ç”¨ã€‚å…¶åŸºæœ¬æ–¹æ³•æ˜¯é€šè¿‡BIOSä¸­æ–­è°ƒç”¨æ¥å¸®åŠ©å®Œæˆçš„ã€‚å…¶ä¸­BIOSä¸­æ–­è°ƒç”¨å¿…é¡»åœ¨å®žæ¨¡å¼ä¸‹è¿›è¡Œï¼Œæ‰€ä»¥åœ¨bootloaderè¿›å…¥ä¿æŠ¤æ¨¡å¼å‰å®Œæˆè¿™éƒ¨åˆ†å·¥ä½œç›¸å¯¹æ¯”è¾ƒåˆé€‚ã€‚è¿™äº›éƒ¨åˆ†ç”±boot/bootasm.Sä¸­ä»Žprobe\\_memoryå¤„åˆ°finish\\_probeå¤„çš„ä»£ç éƒ¨åˆ†å®Œæˆå®Œæˆã€‚é€šè¿‡BIOSä¸­æ–­èŽ·å–å†…å­˜å¯è°ƒç”¨å‚æ•°ä¸ºe820hçš„INT\n15h BIOSä¸­æ–­ã€‚BIOSé€šè¿‡ç³»ç»Ÿå†…å­˜æ˜ å°„åœ°å€æè¿°ç¬¦ï¼ˆAddress Range\nDescriptorï¼‰æ ¼å¼æ¥è¡¨ç¤ºç³»ç»Ÿç‰©ç†å†…å­˜å¸ƒå±€ï¼Œå…¶å…·ä½“è¡¨ç¤ºå¦‚ä¸‹ï¼š\n```\nOffset  Size    Description\n00h    8å­—èŠ‚   base address               #ç³»ç»Ÿå†…å­˜å—åŸºåœ°å€\n08h    8å­—èŠ‚   length in bytes            #ç³»ç»Ÿå†…å­˜å¤§å°\n10h    4å­—èŠ‚   type of address range     #å†…å­˜ç±»åž‹\n```\nçœ‹ä¸‹é¢çš„(Values for System Memory Map address type)\n```\nValues for System Memory Map address type:\n01h    memory, available to OS\n02h    reserved, not available (e.g. system ROM, memory-mapped device)\n03h    ACPI Reclaim Memory (usable by OS after reading ACPI tables)\n04h    ACPI NVS Memory (OS is required to save this memory between NVS sessions)\nother  not defined yet -- treat as Reserved\n```\nINT15h BIOSä¸­æ–­çš„è¯¦ç»†è°ƒç”¨å‚æ•°:\n```\neaxï¼še820hï¼šINT 15çš„ä¸­æ–­è°ƒç”¨å‚æ•°ï¼›\nedxï¼š534D4150h (å³4ä¸ªASCIIå­—ç¬¦â€œSMAPâ€) ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç­¾åè€Œå·²ï¼›\nebxï¼šå¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨æˆ–å†…å­˜åŒºåŸŸæ‰«æå®Œæ¯•ï¼Œåˆ™ä¸º0ã€‚ å¦‚æžœä¸æ˜¯ï¼Œåˆ™å­˜æ”¾ä¸Šæ¬¡è°ƒç”¨ä¹‹åŽçš„è®¡æ•°å€¼ï¼›\necxï¼šä¿å­˜åœ°å€èŒƒå›´æè¿°ç¬¦çš„å†…å­˜å¤§å°,åº”è¯¥å¤§äºŽç­‰äºŽ20å­—èŠ‚ï¼›\nes:diï¼šæŒ‡å‘ä¿å­˜åœ°å€èŒƒå›´æè¿°ç¬¦ç»“æž„çš„ç¼“å†²åŒºï¼ŒBIOSæŠŠä¿¡æ¯å†™å…¥è¿™ä¸ªç»“æž„çš„èµ·å§‹åœ°å€ã€‚\n```\næ­¤ä¸­æ–­çš„è¿”å›žå€¼ä¸º:\n```\ncflagsçš„CFä½ï¼šè‹¥INT 15ä¸­æ–­æ‰§è¡ŒæˆåŠŸï¼Œåˆ™ä¸ç½®ä½ï¼Œå¦åˆ™ç½®ä½ï¼›\n\neaxï¼š534D4150h ('SMAP') ï¼›\n\nes:diï¼šæŒ‡å‘ä¿å­˜åœ°å€èŒƒå›´æè¿°ç¬¦çš„ç¼“å†²åŒº,æ­¤æ—¶ç¼“å†²åŒºå†…çš„æ•°æ®å·²ç”±BIOSå¡«å†™å®Œæ¯•\n\nebxï¼šä¸‹ä¸€ä¸ªåœ°å€èŒƒå›´æè¿°ç¬¦çš„è®¡æ•°åœ°å€\n\necx    ï¼šè¿”å›žBIOSå¾€ES:DIå¤„å†™çš„åœ°å€èŒƒå›´æè¿°ç¬¦çš„å­—èŠ‚å¤§å°\n\nahï¼šå¤±è´¥æ—¶ä¿å­˜å‡ºé”™ä»£ç \n```\nè¿™æ ·ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒç”¨INT 15h\nBIOSä¸­æ–­ï¼Œé€’å¢ždiçš„å€¼ï¼ˆ20çš„å€æ•°ï¼‰ï¼Œè®©BIOSå¸®æˆ‘ä»¬æŸ¥æ‰¾å‡ºä¸€ä¸ªä¸€ä¸ªçš„å†…å­˜å¸ƒå±€entryï¼Œå¹¶æ”¾å…¥åˆ°ä¸€ä¸ªä¿å­˜åœ°å€èŒƒå›´æè¿°ç¬¦ç»“æž„çš„ç¼“å†²åŒºä¸­ï¼Œä¾›åŽç»­çš„ucoreè¿›ä¸€æ­¥è¿›è¡Œç‰©ç†å†…å­˜ç®¡ç†ã€‚è¿™ä¸ªç¼“å†²åŒºç»“æž„å®šä¹‰åœ¨memlayout.hä¸­ï¼š\n```\nstruct e820map {\n                  int nr_map;\n                  struct {\n                                    long long addr;\n                                    long long size;\n                                    long type;\n                  } map[E820MAX];\n};\n```\n****\n",
          "**å®žçŽ°ç‰©ç†å†…å­˜æŽ¢æµ‹**\n\nç‰©ç†å†…å­˜æŽ¢æµ‹æ˜¯åœ¨bootasm.Sä¸­å®žçŽ°çš„ï¼Œç›¸å…³ä»£ç å¾ˆçŸ­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n```\nprobe_memory:\n//å¯¹0x8000å¤„çš„32ä½å•å…ƒæ¸…é›¶,å³ç»™ä½äºŽ0x8000å¤„çš„\n//struct e820mapçš„æˆå‘˜å˜é‡nr_mapæ¸…é›¶\n           movl $0, 0x8000\n                  xorl %ebx, %ebx\n//è¡¨ç¤ºè®¾ç½®è°ƒç”¨INT 15h BIOSä¸­æ–­åŽï¼ŒBIOSè¿”å›žçš„æ˜ å°„åœ°å€æè¿°ç¬¦çš„èµ·å§‹åœ°å€\n                  movw $0x8004, %di\nstart_probe:\n                  movl $0xE820, %eax // INT 15çš„ä¸­æ–­è°ƒç”¨å‚æ•°\n//è®¾ç½®åœ°å€èŒƒå›´æè¿°ç¬¦çš„å¤§å°ä¸º20å­—èŠ‚ï¼Œå…¶å¤§å°ç­‰äºŽstruct e820mapçš„æˆå‘˜å˜é‡mapçš„å¤§å°\n                  movl $20, %ecx\n//è®¾ç½®edxä¸º534D4150h (å³4ä¸ªASCIIå­—ç¬¦â€œSMAPâ€)ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¦å®š\n                  movl $SMAP, %edx\n//è°ƒç”¨int 0x15ä¸­æ–­ï¼Œè¦æ±‚BIOSè¿”å›žä¸€ä¸ªç”¨åœ°å€èŒƒå›´æè¿°ç¬¦è¡¨ç¤ºçš„å†…å­˜æ®µä¿¡æ¯\n                  int $0x15\n//å¦‚æžœeflagsçš„CFä½ä¸º0ï¼Œåˆ™è¡¨ç¤ºè¿˜æœ‰å†…å­˜æ®µéœ€è¦æŽ¢æµ‹\n                  jnc cont\n//æŽ¢æµ‹æœ‰é—®é¢˜ï¼Œç»“æŸæŽ¢æµ‹\n                  movw $12345, 0x8000\n                  jmp finish_probe\ncont:\n//è®¾ç½®ä¸‹ä¸€ä¸ªBIOSè¿”å›žçš„æ˜ å°„åœ°å€æè¿°ç¬¦çš„èµ·å§‹åœ°å€\n                  addw $20, %di\n//é€’å¢žstruct e820mapçš„æˆå‘˜å˜é‡nr_map\n                  incl 0x8000\n//å¦‚æžœINT0x15è¿”å›žçš„ebxä¸ºé›¶ï¼Œè¡¨ç¤ºæŽ¢æµ‹ç»“æŸï¼Œå¦åˆ™ç»§ç»­æŽ¢æµ‹\n                  cmpl $0, %ebx\n                  jnz start_probe\nfinish_probe:\n```\nä¸Šè¿°ä»£ç æ­£å¸¸æ‰§è¡Œå®Œæ¯•åŽï¼Œåœ¨0x8000åœ°å€å¤„ä¿å­˜äº†ä»ŽBIOSä¸­èŽ·å¾—çš„å†…å­˜åˆ†å¸ƒä¿¡æ¯ï¼Œæ­¤ä¿¡æ¯æŒ‰ç…§struct\ne820mapçš„è®¾ç½®æ¥è¿›è¡Œå¡«å……ã€‚è¿™éƒ¨åˆ†ä¿¡æ¯å°†åœ¨bootloaderå¯åŠ¨ucoreåŽï¼Œç”±ucoreçš„page\\_initå‡½æ•°æ¥æ ¹æ®struct\ne820mapçš„memmapï¼ˆå®šä¹‰äº†èµ·å§‹åœ°å€ä¸º0x8000ï¼‰æ¥å®Œæˆå¯¹æ•´ä¸ªæœºå™¨ä¸­çš„ç‰©ç†å†…å­˜çš„æ€»ä½“ç®¡ç†ã€‚\n",
          "**é“¾æŽ¥åœ°å€/è™šåœ°å€/ç‰©ç†åœ°å€/åŠ è½½åœ°å€ä»¥åŠedata/end/textçš„å«ä¹‰**\n\n**é“¾æŽ¥è„šæœ¬ç®€ä»‹**\n\nucore\nkernelå„ä¸ªéƒ¨åˆ†ç”±ç»„æˆkernelçš„å„ä¸ª.oæˆ–.aæ–‡ä»¶æž„æˆï¼Œä¸”å„ä¸ªéƒ¨åˆ†åœ¨å†…å­˜ä¸­åœ°å€ä½ç½®ç”±ldå·¥å…·æ ¹æ®kernel.ldé“¾æŽ¥è„šæœ¬ï¼ˆlinker\nscriptï¼‰æ¥è®¾å®šã€‚ldå·¥å…·ä½¿ç”¨å‘½ä»¤-TæŒ‡å®šé“¾æŽ¥è„šæœ¬ã€‚é“¾æŽ¥è„šæœ¬ä¸»è¦ç”¨äºŽè§„å®šå¦‚ä½•æŠŠè¾“å…¥æ–‡ä»¶ï¼ˆå„ä¸ª.oæˆ–.aæ–‡ä»¶ï¼‰å†…çš„sectionæ”¾å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆlab2/bin/kernelï¼Œå³ELFæ ¼å¼çš„ucoreå†…æ ¸ï¼‰å†…ï¼Œ\nå¹¶æŽ§åˆ¶è¾“å‡ºæ–‡ä»¶å†…å„éƒ¨åˆ†åœ¨ç¨‹åºåœ°å€ç©ºé—´å†…çš„å¸ƒå±€ã€‚ä¸‹é¢ç®€å•åˆ†æžä¸€ä¸‹/lab2/tools/kernel.ldï¼Œæ¥äº†è§£ä¸€ä¸‹ucoreå†…æ ¸çš„åœ°å€å¸ƒå±€æƒ…å†µã€‚kernel.ldçš„å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š\n```\n/* Simple linker script for the ucore kernel.\n   See the GNU ld 'info' manual (\"info ld\") to learn the syntax. */\n\nOUTPUT_FORMAT(\"elf32-i386\", \"elf32-i386\", \"elf32-i386\")\nOUTPUT_ARCH(i386)\nENTRY(kern_entry)\n\nSECTIONS {\n    /* Load the kernel at this address: \".\" means the current address */\n    . = 0xC0100000;\n\n    .text : {\n        *(.text .stub .text.* .gnu.linkonce.t.*)\n    }\n\n    PROVIDE(etext = .); /* Define the 'etext' symbol to this value */\n\n    .rodata : {\n        *(.rodata .rodata.* .gnu.linkonce.r.*)\n    }\n\n    /* Include debugging information in kernel memory */\n    .stab : {\n        PROVIDE(__STAB_BEGIN__ = .);\n        *(.stab);\n        PROVIDE(__STAB_END__ = .);\n        BYTE(0)     /* Force the linker to allocate space\n                   for this section */\n    }\n\n    .stabstr : {\n        PROVIDE(__STABSTR_BEGIN__ = .);\n        *(.stabstr);\n        PROVIDE(__STABSTR_END__ = .);\n        BYTE(0)     /* Force the linker to allocate space\n                   for this section */\n    }\n\n    /* Adjust the address for the data segment to the next page */\n    . = ALIGN(0x1000);\n\n    /* The data segment */\n    .data : {\n        *(.data)\n    }\n\n    PROVIDE(edata = .);\n\n    .bss : {\n        *(.bss)\n    }\n\n    PROVIDE(end = .);\n\n    /DISCARD/ : {\n        *(.eh_frame .note.GNU-stack)\n    }\n}\n```\nå…¶å®žä»Žé“¾æŽ¥è„šæœ¬çš„å†…å®¹ï¼Œå¯ä»¥å¤§è‡´çŒœå‡ºå®ƒæŒ‡å®šå‘Šè¯‰é“¾æŽ¥å™¨çš„å„ç§ä¿¡æ¯ï¼š\n\n* å†…æ ¸åŠ è½½åœ°å€ï¼š0xC0100000\n* å…¥å£ï¼ˆèµ·å§‹ä»£ç ï¼‰åœ°å€ï¼š ENTRY(kern\\_entry)\n* cpuæœºå™¨ç±»åž‹ï¼ši386\n\nå…¶æœ€ä¸»è¦çš„ä¿¡æ¯æ˜¯å‘Šè¯‰é“¾æŽ¥å™¨å„è¾“å…¥æ–‡ä»¶çš„å„sectionåº”è¯¥æ€Žä¹ˆç»„åˆï¼šåº”è¯¥ä»Žå“ªä¸ªåœ°å€å¼€å§‹æ”¾ï¼Œå„ä¸ªsectionä»¥ä»€ä¹ˆé¡ºåºæ”¾ï¼Œåˆ†åˆ«æ€Žä¹ˆå¯¹é½ç­‰ç­‰ï¼Œæœ€ç»ˆç»„æˆè¾“å‡ºæ–‡ä»¶çš„å„sectionã€‚é™¤æ­¤ä¹‹å¤–ï¼Œlinker\nscriptè¿˜å¯ä»¥å®šä¹‰å„ç§ç¬¦å·ï¼ˆå¦‚.textã€.dataã€.bssç­‰ï¼‰ï¼Œå½¢æˆæœ€ç»ˆç”Ÿæˆçš„ä¸€å †ç¬¦å·çš„åˆ—è¡¨ï¼ˆç¬¦å·è¡¨ï¼‰ï¼Œæ¯ä¸ªç¬¦å·åŒ…å«äº†ç¬¦å·åå­—ï¼Œç¬¦å·æ‰€å¼•ç”¨çš„å†…å­˜åœ°å€ï¼Œä»¥åŠå…¶ä»–ä¸€äº›å±žæ€§ä¿¡æ¯ã€‚ç¬¦å·å®žé™…ä¸Šå°±æ˜¯ä¸€ä¸ªåœ°å€çš„ç¬¦å·è¡¨ç¤ºï¼Œå…¶æœ¬èº«ä¸å ç”¨çš„ç¨‹åºè¿è¡Œçš„å†…å­˜ç©ºé—´ã€‚\n\n**é“¾æŽ¥åœ°å€/åŠ è½½åœ°å€/è™šåœ°å€/ç‰©ç†åœ°å€**\n\nucore è®¾å®šäº†ucoreè¿è¡Œä¸­çš„è™šåœ°å€ç©ºé—´ï¼Œå…·ä½“è®¾ç½®å¯çœ‹\nlab2/kern/mm/memlayout.h ä¸­æè¿°çš„\"Virtual memory map\n\"å›¾ï¼Œå¯ä»¥äº†è§£è™šåœ°å€å’Œç‰©ç†åœ°å€çš„å¯¹åº”å…³ç³»ã€‚lab2/tools/kernel.ldæè¿°çš„æ˜¯æ‰§è¡Œä»£ç çš„é“¾æŽ¥åœ°å€ï¼ˆlink\\_addrï¼‰ï¼Œæ¯”å¦‚å†…æ ¸èµ·å§‹åœ°å€æ˜¯0xC0100000ï¼Œè¿™æ˜¯ä¸€ä¸ªè™šåœ°å€ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¤ä¸ºé“¾æŽ¥åœ°å€ç­‰äºŽè™šåœ°å€ã€‚åœ¨ucoreå»ºç«‹å†…æ ¸é¡µè¡¨æ—¶ï¼Œè®¾å®šäº†ç‰©ç†åœ°å€å’Œè™šåœ°å€çš„è™šå®žæ˜ å°„å…³ç³»æ˜¯ï¼š\n\nphy addr + 0xC0000000 = virtual addr\n\nå³è™šåœ°å€å’Œç‰©ç†åœ°å€ä¹‹é—´æœ‰ä¸€ä¸ªåç§»ã€‚ä½†boot loaderæŠŠucore\nkernelåŠ è½½åˆ°å†…å­˜æ—¶ï¼Œé‡‡ç”¨çš„æ˜¯åŠ è½½åœ°å€ï¼ˆload\naddrï¼‰ï¼Œè¿™æ˜¯ç”±äºŽucoreè¿˜æ²¡æœ‰è¿è¡Œï¼Œå³è¿˜æ²¡æœ‰å¯åŠ¨é¡µè¡¨æ˜ å°„ï¼Œå¯¼è‡´è¿™æ—¶é‡‡ç”¨çš„å¯»å€æ–¹å¼æ˜¯æ®µå¯»å€æ–¹å¼ï¼Œç”¨çš„æ˜¯boot\nloaderåœ¨åˆå§‹åŒ–é˜¶æ®µè®¾ç½®çš„æ®µæ˜ å°„å…³ç³»ï¼Œå…¶æ˜ å°„å…³ç³»ï¼ˆå¯å‚çœ‹bootasm.Sçš„æœ«å°¾å¤„æœ‰å…³æ®µæè¿°ç¬¦è¡¨çš„å†…å®¹ï¼‰æ˜¯ï¼š\n\nlinear addr = phy addr = virtual addr\n\næŸ¥çœ‹ bootloaderçš„å®žçŽ°ä»£ç  bootmain::bootmain.c\n\nreadseg(ph-\\>p\\_va & 0xFFFFFF, ph-\\>p\\_memsz, ph-\\>p\\_offset);\n\nè¿™é‡Œçš„ph-\\>p\\_va=0xC0XXXXXXï¼Œå°±æ˜¯ldå·¥å…·æ ¹æ®kernel.ldè®¾ç½®çš„é“¾æŽ¥åœ°å€ï¼Œä¸”é“¾æŽ¥åœ°å€ç­‰äºŽè™šåœ°å€ã€‚è€ƒè™‘åˆ°ph-\\>p\\_va\n& 0xFFFFFF == 0x0XXXXXXï¼Œæ‰€ä»¥bootloaderåŠ è½½ucore\nkernelçš„åŠ è½½åœ°å€æ˜¯0x0XXXXXX, è¿™å®žé™…ä¸Šæ˜¯ucoreå†…æ ¸æ‰€åœ¨çš„ç‰©ç†åœ°å€ã€‚ç®€è¨€ä¹‹ï¼š\nOSçš„é“¾æŽ¥åœ°å€ï¼ˆlink addrï¼‰ åœ¨tools/kernel.ldä¸­è®¾ç½®å¥½äº†ï¼Œæ˜¯ä¸€ä¸ªè™šåœ°å€ï¼ˆvirtual\naddrï¼‰ï¼›è€Œucore kernelçš„åŠ è½½åœ°å€ï¼ˆload addrï¼‰åœ¨boot\nloaderä¸­çš„bootmainå‡½æ•°ä¸­æŒ‡å®šï¼Œæ˜¯ä¸€ä¸ªç‰©ç†åœ°å€ã€‚\n\nå°ç»“ä¸€ä¸‹ï¼Œucoreå†…æ ¸çš„é“¾æŽ¥åœ°å€==ucoreå†…æ ¸çš„è™šæ‹Ÿåœ°å€ï¼›boot\nloaderåŠ è½½ucoreå†…æ ¸ç”¨åˆ°çš„åŠ è½½åœ°å€==ucoreå†…æ ¸çš„ç‰©ç†åœ°å€ã€‚\n\n**edata/end/textçš„å«ä¹‰**\n\nåœ¨åŸºäºŽELFæ‰§è¡Œæ–‡ä»¶æ ¼å¼çš„ä»£ç ä¸­ï¼Œå­˜åœ¨ä¸€äº›å¯¹ä»£ç å’Œæ•°æ®çš„è¡¨è¿°ï¼ŒåŸºæœ¬æ¦‚å¿µå¦‚ä¸‹ï¼š\n\n* BSSæ®µï¼ˆbss\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºä¸­æœªåˆå§‹åŒ–çš„å…¨å±€å˜é‡çš„å†…å­˜åŒºåŸŸã€‚BSSæ˜¯è‹±æ–‡Block\nStarted by Symbolçš„ç®€ç§°ã€‚BSSæ®µå±žäºŽé™æ€å†…å­˜åˆ†é…ã€‚\n* æ•°æ®æ®µï¼ˆdata\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºä¸­å·²åˆå§‹åŒ–çš„å…¨å±€å˜é‡çš„ä¸€å—å†…å­˜åŒºåŸŸã€‚æ•°æ®æ®µå±žäºŽé™æ€å†…å­˜åˆ†é…ã€‚\n* ä»£ç æ®µï¼ˆcode segment/text\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºæ‰§è¡Œä»£ç çš„ä¸€å—å†…å­˜åŒºåŸŸã€‚è¿™éƒ¨åˆ†åŒºåŸŸçš„å¤§å°åœ¨ç¨‹åºè¿è¡Œå‰å°±å·²ç»ç¡®å®šï¼Œå¹¶ä¸”å†…å­˜åŒºåŸŸé€šå¸¸å±žäºŽåªè¯»,\næŸäº›æž¶æž„ä¹Ÿå…è®¸ä»£ç æ®µä¸ºå¯å†™ï¼Œå³å…è®¸ä¿®æ”¹ç¨‹åºã€‚åœ¨ä»£ç æ®µä¸­ï¼Œä¹Ÿæœ‰å¯èƒ½åŒ…å«ä¸€äº›åªè¯»çš„å¸¸æ•°å˜é‡ï¼Œä¾‹å¦‚å­—ç¬¦ä¸²å¸¸é‡ç­‰ã€‚\n\nåœ¨lab2/kern/init/init.cçš„kern\\_initå‡½æ•°ä¸­ï¼Œå£°æ˜Žäº†å¤–éƒ¨å…¨å±€å˜é‡ï¼š\n```\nextern char edata[], end[];\n```\nä½†æœå¯»æ‰€æœ‰æºç æ–‡ä»¶\\*.[ch]ï¼Œæ²¡æœ‰å‘çŽ°æœ‰è¿™ä¸¤ä¸ªå˜é‡çš„å®šä¹‰ã€‚é‚£è¿™ä¸¤ä¸ªå˜é‡ä»Žå“ªé‡Œæ¥çš„å‘¢ï¼Ÿå…¶å®žåœ¨lab2/tools/kernel.ldä¸­ï¼Œå¯ä»¥çœ‹åˆ°å¦‚ä¸‹å†…å®¹ï¼š\n```\nâ€¦\n.text : {\n        *(.text .stub .text.* .gnu.linkonce.t.*)\n}\nâ€¦\n    .data : {\n        *(.data)\n}\nâ€¦\nPROVIDE(edata = .);\nâ€¦\n    .bss : {\n        *(.bss)\n}\nâ€¦\nPROVIDE(end = .);\nâ€¦\n```\nè¿™é‡Œçš„â€œ.â€è¡¨ç¤ºå½“å‰åœ°å€ï¼Œâ€œ.textâ€è¡¨ç¤ºä»£ç æ®µèµ·å§‹åœ°å€ï¼Œâ€œ.dataâ€ä¹Ÿæ˜¯ä¸€ä¸ªåœ°å€ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®ƒå³ä»£è¡¨äº†ä»£ç æ®µçš„ç»“æŸåœ°å€ï¼Œä¹Ÿæ˜¯æ•°æ®æ®µçš„èµ·å§‹åœ°å€ã€‚ç±»æŽ¨ä¸‹åŽ»ï¼Œâ€œedataâ€è¡¨ç¤ºæ•°æ®æ®µçš„ç»“æŸåœ°å€ï¼Œâ€œ.bssâ€è¡¨ç¤ºæ•°æ®æ®µçš„ç»“æŸåœ°å€å’ŒBSSæ®µçš„èµ·å§‹åœ°å€ï¼Œè€Œâ€œendâ€è¡¨ç¤ºBSSæ®µçš„ç»“æŸåœ°å€ã€‚\n\nè¿™æ ·å›žå¤´çœ‹kerne\\_initä¸­çš„å¤–éƒ¨å…¨å±€å˜é‡ï¼Œå¯çŸ¥edata[]å’Œ\nend[]è¿™äº›å˜é‡æ˜¯ldæ ¹æ®kernel.ldé“¾æŽ¥è„šæœ¬ç”Ÿæˆçš„å…¨å±€å˜é‡ï¼Œè¡¨ç¤ºç›¸åº”æ®µçš„èµ·å§‹åœ°å€æˆ–ç»“æŸåœ°å€ç­‰ï¼Œå®ƒä»¬ä¸åœ¨ä»»ä½•ä¸€ä¸ª.Sã€.cæˆ–.hæ–‡ä»¶ä¸­å®šä¹‰ã€‚\n\n\n\n",
          "\n##### GCCæ‰©å±•å†…è”æ±‡ç¼–\n\nä½¿ç”¨GCCæ‰©å±•å†…è”æ±‡ç¼–çš„ä¾‹å­å¦‚ä¸‹ï¼š\n\n```c\n#define read_cr0() ({ \\\nunsigned int __dummy; \\\n__asm__( \\\n    \"movl %%cr0,%0\\n\\t\" \\\n    :\"=r\" (__dummy)); \\\n__dummy; \\\n})\n```\n\nå®ƒä»£è¡¨ä»€ä¹ˆå«ä¹‰å‘¢ï¼Ÿè¿™éœ€è¦ä»Žå…¶åŸºæœ¬æ ¼å¼è®²èµ·ã€‚GCCæ‰©å±•å†…è”æ±‡ç¼–çš„åŸºæœ¬æ ¼å¼æ˜¯ï¼š          \n\n```c\nasm [volatile] ( Assembler Template\n   : Output Operands\n   [ : Input Operands\n   [ : Clobbers ] ])\n```\n\nå…¶ä¸­ï¼Œ\\_\\_asm\\_\\_ è¡¨ç¤ºæ±‡ç¼–ä»£ç çš„å¼€å§‹ï¼Œå…¶åŽå¯ä»¥è·Ÿ \\_\\_volatile\\_\\_ï¼ˆè¿™æ˜¯å¯é€‰é¡¹ï¼‰ï¼Œå…¶å«ä¹‰æ˜¯é¿å… â€œasmâ€ æŒ‡ä»¤è¢«åˆ é™¤ã€ç§»åŠ¨æˆ–ç»„åˆï¼Œåœ¨æ‰§è¡Œä»£ç æ—¶ï¼Œå¦‚æžœä¸å¸Œæœ›æ±‡ç¼–è¯­å¥è¢« gcc ä¼˜åŒ–è€Œæ”¹å˜ä½ç½®ï¼Œå°±éœ€è¦åœ¨ asm ç¬¦å·åŽæ·»åŠ  volatile å…³é”®è¯ï¼šasm volatile(...)ï¼›æˆ–è€…æ›´è¯¦ç»†åœ°è¯´æ˜Žä¸ºï¼š\\_\\_asm\\_\\_ \\_\\_volatile\\_\\_(...)ï¼›ç„¶åŽå°±æ˜¯å°æ‹¬å¼§ï¼Œæ‹¬å¼§ä¸­çš„å†…å®¹æ˜¯å…·ä½“çš„å†…è”æ±‡ç¼–æŒ‡ä»¤ä»£ç ã€‚ \"<asm routine>\" ä¸ºæ±‡ç¼–æŒ‡ä»¤éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œ\"movl %%cr0,%0\\n\\t\"ã€‚æ•°å­—å‰åŠ å‰ç¼€ â€œï¼…â€œï¼Œå¦‚ï¼…1ï¼Œï¼…2ç­‰è¡¨ç¤ºä½¿ç”¨å¯„å­˜å™¨çš„æ ·æ¿æ“ä½œæ•°ã€‚å¯ä»¥ä½¿ç”¨çš„æ“ä½œæ•°æ€»æ•°å–å†³äºŽå…·ä½“CPUä¸­é€šç”¨å¯„å­˜å™¨çš„æ•° é‡ï¼Œå¦‚Intelå¯ä»¥æœ‰8ä¸ªã€‚æŒ‡ä»¤ä¸­æœ‰å‡ ä¸ªæ“ä½œæ•°ï¼Œå°±è¯´æ˜Žæœ‰å‡ ä¸ªå˜é‡éœ€è¦ä¸Žå¯„å­˜å™¨ç»“åˆï¼Œç”±gccåœ¨ç¼–è¯‘æ—¶æ ¹æ®åŽé¢è¾“å‡ºéƒ¨åˆ†å’Œè¾“å…¥éƒ¨åˆ†çš„çº¦æŸæ¡ä»¶è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚ç”±äºŽè¿™äº›æ ·æ¿æ“ä½œæ•°çš„å‰ç¼€ä½¿ç”¨äº†â€ï¼…â€œï¼Œå› æ­¤ï¼Œåœ¨ç”¨åˆ°å…·ä½“çš„å¯„å­˜å™¨æ—¶å°±åœ¨å‰é¢åŠ **ä¸¤ä¸ªâ€œï¼…â€**ï¼Œå¦‚**%%cr0**ã€‚è¾“å‡ºéƒ¨åˆ†ï¼ˆoutput operand listï¼‰ï¼Œç”¨ä»¥è§„å®šå¯¹è¾“å‡ºå˜é‡ï¼ˆç›®æ ‡æ“ä½œæ•°ï¼‰å¦‚ä½•ä¸Žå¯„å­˜å™¨ç»“åˆçš„çº¦æŸï¼ˆconstraintï¼‰,è¾“å‡ºéƒ¨åˆ†å¯ä»¥æœ‰å¤šä¸ªçº¦æŸï¼Œäº’ç›¸ä»¥é€—å·åˆ†å¼€ã€‚æ¯ä¸ªçº¦æŸä»¥â€œï¼â€å¼€å¤´ï¼ŒæŽ¥ç€ç”¨ä¸€ä¸ªå­—æ¯æ¥è¡¨ç¤ºæ“ä½œæ•°çš„ç±»åž‹ï¼Œç„¶åŽæ˜¯å…³äºŽå˜é‡ç»“åˆçš„çº¦æŸã€‚ä¾‹å¦‚ï¼Œä¸Šä¾‹ä¸­ï¼š\n\n\t:\"=r\" (__dummy)\n\nâ€œï¼râ€è¡¨ç¤ºç›¸åº”çš„ç›®æ ‡æ“ä½œæ•°ï¼ˆæŒ‡ä»¤éƒ¨åˆ†çš„%0ï¼‰å¯ä»¥ä½¿ç”¨ä»»ä½•ä¸€ä¸ªé€šç”¨å¯„å­˜å™¨ï¼Œå¹¶ä¸”å˜é‡__dummy å­˜æ”¾åœ¨è¿™ä¸ªå¯„å­˜å™¨ä¸­ï¼Œä½†å¦‚æžœæ˜¯ï¼š               \n\n\t:â€œï¼mâ€(__dummy)\n\nâ€œï¼mâ€å°±è¡¨ç¤ºç›¸åº”çš„ç›®æ ‡æ“ä½œæ•°æ˜¯å­˜æ”¾åœ¨å†…å­˜å•å…ƒ__dummyä¸­ã€‚è¡¨ç¤ºçº¦æŸæ¡ä»¶çš„å­—æ¯å¾ˆå¤šï¼Œä¸‹è¡¨ç»™å‡ºå‡ ä¸ªä¸»è¦çš„çº¦æŸå­—æ¯åŠå…¶å«ä¹‰ï¼š\n\n<table>\n\t<tr><td>å­—æ¯</td><td>å«ä¹‰</td></tr>\n\t<tr><td>m, v, o</td><td>å†…å­˜å•å…ƒ</td></tr>\n\t<tr><td>R</td><td>ä»»ä½•é€šç”¨å¯„å­˜å™¨</td>\t</tr>\n\t<tr><td>Q</td><td>å¯„å­˜å™¨eax, ebx, ecx,edxä¹‹ä¸€</td></tr>\n\t<tr><td>I, h</td><td>ç›´æŽ¥æ“ä½œæ•°</td></tr>\n\t<tr><td>E, F</td><td>æµ®ç‚¹æ•°</td></tr>\n\t<tr><td>G</td><td>ä»»æ„</td></tr>\n\t<tr><td>a, b, c, d</td><td>å¯„å­˜å™¨eax/ax/al, ebx/bx/bl, ecx/cx/clæˆ–edx/dx/dl</td></tr>\n\t<tr><td>S, D</td><td>å¯„å­˜å™¨esiæˆ–edi</td></tr>\n\t<tr><td>I</td><td>å¸¸æ•°ï¼ˆ0ï½ž31ï¼‰</td></tr>\n</table>\n\nè¾“å…¥éƒ¨åˆ†ï¼ˆinput  operand listï¼‰ï¼šè¾“å…¥éƒ¨åˆ†ä¸Žè¾“å‡ºéƒ¨åˆ†ç›¸ä¼¼ï¼Œä½†æ²¡æœ‰â€œï¼â€ã€‚å¦‚æžœè¾“å…¥éƒ¨åˆ†ä¸€ä¸ªæ“ä½œæ•°æ‰€è¦æ±‚ä½¿ç”¨çš„å¯„å­˜å™¨ï¼Œä¸Žå‰é¢è¾“å‡ºéƒ¨åˆ†æŸä¸ªçº¦æŸæ‰€è¦æ±‚çš„æ˜¯åŒä¸€ä¸ªå¯„å­˜å™¨ï¼Œé‚£å°±æŠŠå¯¹åº”æ“ä½œæ•°çš„ç¼–å·ï¼ˆå¦‚â€œ1â€ï¼Œâ€œ2â€ç­‰ï¼‰æ”¾åœ¨çº¦æŸæ¡ä»¶ä¸­ã€‚åœ¨åŽé¢çš„ä¾‹å­ä¸­ï¼Œå¯çœ‹åˆ°è¿™ç§æƒ…å†µã€‚ä¿®æ”¹éƒ¨åˆ†ï¼ˆclobber list,ä¹Ÿç§° ä¹±ç åˆ—è¡¨ï¼‰:è¿™éƒ¨åˆ†å¸¸å¸¸ä»¥â€œmemoryâ€ä¸ºçº¦æŸæ¡ä»¶ï¼Œä»¥è¡¨ç¤ºæ“ä½œå®ŒæˆåŽå†…å­˜ä¸­çš„å†…å®¹å·²æœ‰æ”¹å˜ï¼Œå¦‚æžœåŽŸæ¥æŸä¸ªå¯„å­˜å™¨çš„å†…å®¹æ¥è‡ªå†…å­˜ï¼Œé‚£ä¹ˆçŽ°åœ¨å†…å­˜ä¸­è¿™ä¸ªå•å…ƒçš„å†…å®¹å·²ç»æ”¹å˜ã€‚ä¹±ç åˆ—è¡¨é€šçŸ¥ç¼–è¯‘å™¨ï¼Œæœ‰äº›å¯„å­˜å™¨æˆ–å†…å­˜å› å†…è”æ±‡ç¼–å—é€ æˆä¹±ç ï¼Œå¯éšå¼åœ°ç ´åäº†æ¡ä»¶å¯„å­˜å™¨çš„æŸäº›ä½ï¼ˆå­—æ®µï¼‰ã€‚ æ³¨æ„ï¼ŒæŒ‡ä»¤éƒ¨åˆ†ä¸ºå¿…é€‰é¡¹ï¼Œè€Œè¾“å…¥éƒ¨åˆ†ã€è¾“å‡ºéƒ¨åˆ†åŠä¿®æ”¹éƒ¨åˆ†ä¸ºå¯é€‰é¡¹ï¼Œå½“è¾“å…¥éƒ¨åˆ†å­˜åœ¨ï¼Œè€Œè¾“å‡ºéƒ¨åˆ†ä¸å­˜åœ¨æ—¶ï¼Œå†’å·â€œï¼šâ€è¦ä¿ç•™ï¼Œå½“â€œmemoryâ€å­˜åœ¨æ—¶ï¼Œä¸‰ä¸ªå†’å·éƒ½è¦ä¿ç•™ï¼Œä¾‹å¦‚\n\n```c\n#define __cli() __asm__ __volatile__(\"cli\": : :\"memory\")\n```\n\nä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š\n\n```c\nint count=1;\nint value=1;\nint buf[10];\nvoid main()\n{\n    asm(\n        \"cld \\n\\t\"\n        \"rep \\n\\t\"\n        \"stosl\"\n    :\n    : \"c\" (count), \"a\" (value) , \"D\" (buf)\n    );\n}\n```\n\nå¾—åˆ°çš„ä¸»è¦æ±‡ç¼–ä»£ç ä¸ºï¼š\n\n```asm\nmovl count,%ecx\nmovl value,%eax\nmovl buf,%edi\n#APP\ncld\nrep\nstosl\n#NO_APP\n```\n\ncld,rep,stosè¿™å‡ æ¡è¯­å¥çš„åŠŸèƒ½æ˜¯å‘bufä¸­å†™ä¸Šcountä¸ªvalueå€¼ã€‚å†’å·åŽçš„è¯­å¥æŒ‡æ˜Žè¾“å…¥ï¼Œè¾“å‡ºå’Œè¢«æ”¹å˜çš„å¯„å­˜å™¨ã€‚é€šè¿‡å†’å·ä»¥åŽçš„è¯­å¥ï¼Œç¼–è¯‘å™¨å°±çŸ¥é“ä½ çš„æŒ‡ä»¤éœ€è¦å’Œæ”¹å˜å“ªäº›å¯„å­˜å™¨ï¼Œä»Žè€Œå¯ä»¥ä¼˜åŒ–å¯„å­˜å™¨çš„åˆ†é…ã€‚å…¶ä¸­ç¬¦å·\"c\"(count)æŒ‡ç¤ºè¦æŠŠcountçš„å€¼æ”¾å…¥ecxå¯„å­˜å™¨ã€‚ç±»ä¼¼çš„è¿˜æœ‰ï¼š\n\n\ta eax\n\tb ebx\n\tc ecx\n\td edx\n\tS esi\n\tD edi\n\tI å¸¸æ•°å€¼ï¼Œ(0 - 31)\n\tq,r åŠ¨æ€åˆ†é…çš„å¯„å­˜å™¨\n\tg eax,ebx,ecx,edxæˆ–å†…å­˜å˜é‡\n\tA æŠŠeaxå’Œedxåˆæˆä¸€ä¸ª64ä½çš„å¯„å­˜å™¨(use long longs)\n\nä¹Ÿå¯ä»¥è®©gccè‡ªå·±é€‰æ‹©åˆé€‚çš„å¯„å­˜å™¨ã€‚å¦‚ä¸‹é¢çš„ä¾‹å­ï¼š\n\n```c\nasm(\"leal (%1,%1,4),%0\"\n    : \"=r\" (x)\n    : \"0\" (x)\n);\n``` \n\nè¿™æ®µä»£ç åˆ°çš„ä¸»è¦æ±‡ç¼–ä»£ç ä¸ºï¼š\n\n```asm\nmovl x,%eax\n#APP\nleal (%eax,%eax,4),%eax\n#NO_APP\nmovl %eax,x\n```\n\nå‡ ç‚¹è¯´æ˜Žï¼š\n\n* [1] ä½¿ç”¨qæŒ‡ç¤ºç¼–è¯‘å™¨ä»Žeax, ebx, ecx, edxåˆ†é…å¯„å­˜å™¨ã€‚\nä½¿ç”¨ræŒ‡ç¤ºç¼–è¯‘å™¨ä»Žeax, ebx, ecx, edx, esi, ediåˆ†é…å¯„å­˜å™¨ã€‚\n* [2] ä¸å¿…æŠŠç¼–è¯‘å™¨åˆ†é…çš„å¯„å­˜å™¨æ”¾å…¥æ”¹å˜çš„å¯„å­˜å™¨åˆ—è¡¨ï¼Œå› ä¸ºå¯„å­˜å™¨å·²ç»è®°ä½äº†å®ƒä»¬ã€‚\n* [3] \"=\"æ˜¯æ ‡ç¤ºè¾“å‡ºå¯„å­˜å™¨ï¼Œå¿…é¡»è¿™æ ·ç”¨ã€‚\n* [4] æ•°å­—%nçš„ç”¨æ³•ï¼šæ•°å­—è¡¨ç¤ºçš„å¯„å­˜å™¨æ˜¯æŒ‰ç…§å‡ºçŽ°å’Œä»Žå·¦åˆ°å³çš„é¡ºåºæ˜ å°„åˆ°ç”¨\"r\"æˆ–\"q\"è¯·æ±‚çš„å¯„å­˜å™¨ï¼Žå¦‚æžœè¦é‡ç”¨\"r\"æˆ–\"q\"è¯·æ±‚çš„å¯„å­˜å™¨çš„è¯ï¼Œå°±å¯ä»¥ä½¿ç”¨å®ƒä»¬ã€‚\n* [5] å¦‚æžœå¼ºåˆ¶ä½¿ç”¨å›ºå®šçš„å¯„å­˜å™¨çš„è¯ï¼Œå¦‚ä¸ç”¨%1ï¼Œè€Œç”¨ebxï¼Œåˆ™ï¼š\n\n```c\nasm(\"leal (%%ebx,%%ebx,4),%0\"\n    : \"=r\" (x)\n    : \"0\" (x) \n);\n```\n\n> æ³¨æ„è¦ä½¿ç”¨ä¸¤ä¸ª%,å› ä¸ºä¸€ä¸ª%çš„è¯­æ³•å·²ç»è¢«%nç”¨æŽ‰äº†ã€‚\n\nå‚è€ƒï¼š\n- [GCC Manualï¼Œ ç‰ˆæœ¬ä¸º5.0.0 pre-release,6.43èŠ‚ï¼ˆHow to Use Inline Assembly Language in C Codeï¼‰](https://gcc.gnu.org/onlinedocs/gcc.pdf)\n- [GCC-Inline-Assembly-HOWTO](http://www.ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html)\n",
          "###å»ºç«‹è™šæ‹Ÿé¡µå’Œç‰©ç†é¡µå¸§çš„åœ°å€æ˜ å°„å…³ç³»\n\n**å»ºç«‹äºŒçº§é¡µè¡¨**\n\n80368çš„é‡‡ç”¨äº†äºŒçº§é¡µè¡¨æ¥å»ºç«‹çº¿æ€§åœ°å€ä¸Žç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚ç”±äºŽæˆ‘ä»¬å·²ç»å…·æœ‰äº†ä¸€ä¸ªç‰©ç†å†…å­˜é¡µç®¡ç†å™¨default\\_pmm\\_managerï¼Œæ”¯æŒåŠ¨æ€åˆ†é…å’Œé‡Šæ”¾å†…å­˜é¡µçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨å®ƒæ¥èŽ·å¾—æ‰€éœ€çš„ç©ºé—²ç‰©ç†é¡µã€‚åœ¨äºŒçº§é¡µè¡¨ç»“æž„ä¸­ï¼Œé¡µç›®å½•è¡¨å 4KBç©ºé—´ï¼Œå¯é€šè¿‡alloc\\_pageå‡½æ•°èŽ·å¾—ä¸€ä¸ªç©ºé—²ç‰©ç†é¡µä½œä¸ºé¡µç›®å½•è¡¨ï¼ˆPage Directory Tableï¼ŒPDTï¼‰ã€‚åŒç†ï¼Œucoreä¹Ÿé€šè¿‡è¿™ç§ç±»ä¼¼æ–¹å¼èŽ·å¾—ä¸€ä¸ªé¡µè¡¨(Page Table,PT)æ‰€éœ€çš„4KBç©ºé—´ã€‚\n\næ•´ä¸ªé¡µç›®å½•è¡¨å’Œé¡µè¡¨æ‰€å ç©ºé—´å¤§å°å–å†³ä¸ŽäºŒçº§é¡µè¡¨è¦ç®¡ç†å’Œæ˜ å°„çš„ç‰©ç†é¡µæ•°ã€‚å‡å®šå½“å‰ç‰©ç†å†…å­˜0~16MBï¼Œæ¯ç‰©ç†é¡µï¼ˆä¹Ÿç§°Page Frameï¼‰å¤§å°ä¸º4KBï¼Œåˆ™æœ‰4096ä¸ªç‰©ç†é¡µï¼Œä¹Ÿå°±æ„å‘³è¿™æœ‰4ä¸ªé¡µç›®å½•é¡¹å’Œ4096ä¸ªé¡µè¡¨é¡¹éœ€è¦è®¾ç½®ã€‚ä¸€ä¸ªé¡µç›®å½•é¡¹(Page Directory Entryï¼ŒPDE)å’Œä¸€ä¸ªé¡µè¡¨é¡¹(Page Table Entryï¼ŒPTE)å 4Bã€‚å³ä½¿æ˜¯4ä¸ªé¡µç›®å½•é¡¹ä¹Ÿéœ€è¦ä¸€ä¸ªå®Œæ•´çš„é¡µç›®å½•è¡¨ï¼ˆå 4KBï¼‰ã€‚è€Œ4096ä¸ªé¡µè¡¨é¡¹éœ€è¦16KBï¼ˆå³4096*4Bï¼‰çš„ç©ºé—´ï¼Œä¹Ÿå°±æ˜¯4ä¸ªç‰©ç†é¡µï¼Œ16KBçš„ç©ºé—´ã€‚æ‰€ä»¥å¯¹16MBç‰©ç†é¡µå»ºç«‹ä¸€ä¸€æ˜ å°„çš„16MBè™šæ‹Ÿé¡µï¼Œéœ€è¦5ä¸ªç‰©ç†é¡µï¼Œå³20KBçš„ç©ºé—´æ¥å½¢æˆäºŒçº§é¡µè¡¨ã€‚\n\nä¸ºæŠŠ0\\~KERNSIZEï¼ˆæ˜Žç¡®ucoreè®¾å®šå®žé™…ç‰©ç†å†…å­˜ä¸èƒ½è¶…è¿‡KERNSIZEå€¼ï¼Œå³0x38000000å­—èŠ‚ï¼Œ896MBï¼Œ3670016ä¸ªç‰©ç†é¡µï¼‰çš„ç‰©ç†åœ°å€ä¸€ä¸€æ˜ å°„åˆ°é¡µç›®å½•é¡¹å’Œé¡µè¡¨é¡¹çš„å†…å®¹ï¼Œå…¶å¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š\n\n1. å…ˆé€šè¿‡alloc\\_pageèŽ·å¾—ä¸€ä¸ªç©ºé—²ç‰©ç†é¡µï¼Œç”¨äºŽé¡µç›®å½•è¡¨ï¼›\n2. è°ƒç”¨boot\\_map\\_segmentå‡½æ•°å»ºç«‹ä¸€ä¸€æ˜ å°„å…³ç³»ï¼Œå…·ä½“å¤„ç†è¿‡ç¨‹ä»¥é¡µä¸ºå•ä½è¿›è¡Œè®¾ç½®ï¼Œå³\n```\nvirt addr = phy addr + 0xC0000000\n```\nè®¾ä¸€ä¸ª32bitçº¿æ€§åœ°å€laæœ‰ä¸€ä¸ªå¯¹åº”çš„32bitç‰©ç†åœ°å€paï¼Œå¦‚æžœåœ¨ä»¥laçš„é«˜10ä½ä¸ºç´¢å¼•å€¼çš„é¡µç›®å½•é¡¹ä¸­çš„å­˜åœ¨ä½ï¼ˆPTE\\_Pï¼‰ä¸º0ï¼Œè¡¨ç¤ºç¼ºå°‘å¯¹åº”çš„é¡µè¡¨ç©ºé—´ï¼Œåˆ™å¯é€šè¿‡alloc\\_pageèŽ·å¾—ä¸€ä¸ªç©ºé—²ç‰©ç†é¡µç»™é¡µè¡¨ï¼Œé¡µè¡¨èµ·å§‹ç‰©ç†åœ°å€æ˜¯æŒ‰4096å­—èŠ‚å¯¹é½çš„ï¼Œè¿™æ ·å¡«å†™é¡µç›®å½•é¡¹çš„å†…å®¹ä¸º\n```\n  é¡µç›®å½•é¡¹å†…å®¹ = (é¡µè¡¨èµ·å§‹ç‰©ç†åœ°å€ & ~0x0FFF) | PTE_U | PTE_W | PTE_P\n```\nè¿›ä¸€æ­¥å¯¹äºŽé¡µè¡¨ä¸­ä»¥çº¿æ€§åœ°å€laçš„ä¸­10ä½ä¸ºç´¢å¼•å€¼å¯¹åº”é¡µè¡¨é¡¹çš„å†…å®¹ä¸º\n```\n  é¡µè¡¨é¡¹å†…å®¹ = (pa & ~0x0FFF) | PTE_P | PTE_W\n```\nå…¶ä¸­ï¼š\n\n* PTE\\_Uï¼šä½3ï¼Œè¡¨ç¤ºç”¨æˆ·æ€çš„è½¯ä»¶å¯ä»¥è¯»å–å¯¹åº”åœ°å€çš„ç‰©ç†å†…å­˜é¡µå†…å®¹\n* PTE\\_Wï¼šä½2ï¼Œè¡¨ç¤ºç‰©ç†å†…å­˜é¡µå†…å®¹å¯å†™\n* PTE\\_Pï¼šä½1ï¼Œè¡¨ç¤ºç‰©ç†å†…å­˜é¡µå­˜åœ¨\n\nucore\nçš„å†…å­˜ç®¡ç†ç»å¸¸éœ€è¦æŸ¥æ‰¾é¡µè¡¨ï¼šç»™å®šä¸€ä¸ªè™šæ‹Ÿåœ°å€ï¼Œæ‰¾å‡ºè¿™ä¸ªè™šæ‹Ÿåœ°å€åœ¨äºŒçº§é¡µè¡¨ä¸­å¯¹åº”çš„é¡¹ã€‚é€šè¿‡æ›´æ”¹æ­¤é¡¹çš„å€¼å¯ä»¥æ–¹ä¾¿åœ°å°†è™šæ‹Ÿåœ°å€æ˜ å°„åˆ°å¦å¤–çš„é¡µä¸Šã€‚å¯å®Œæˆæ­¤åŠŸèƒ½çš„è¿™ä¸ªå‡½æ•°æ˜¯get\\_pteå‡½æ•°ã€‚å®ƒçš„åŽŸåž‹ä¸º\n```\npte_t  *get_pte (pde_t *pgdir,  uintptr_t la, bool  create)\n```\nä¸‹é¢çš„è°ƒç”¨å…³ç³»å›¾å¯ä»¥æ¯”è¾ƒå¥½åœ°çœ‹å‡ºget\\_pteåœ¨å®žçŽ°ä¸Šè¯‰æµç¨‹ä¸­çš„ä½ç½®ï¼š\n\n![](../lab2_figs/image007.png)\n\nå›¾6 get\\_pteè°ƒç”¨å…³ç³»å›¾\n\nè¿™é‡Œæ¶‰åŠåˆ°ä¸‰ä¸ªç±»åž‹pte tã€pde tå’Œuintptr\ntã€‚é€šè¿‡å‚è§mm/mmlayout.hå’Œlibs/types.hï¼Œå¯çŸ¥å®ƒä»¬å…¶å®žéƒ½æ˜¯unsigned\nintç±»åž‹ã€‚åœ¨æ­¤åšåŒºåˆ†ï¼Œæ˜¯ä¸ºäº†åˆ†æ¸…æ¦‚å¿µã€‚\n\npde\\_tå…¨ç§°ä¸º page directory\nentryï¼Œä¹Ÿå°±æ˜¯ä¸€çº§é¡µè¡¨çš„è¡¨é¡¹ï¼ˆæ³¨æ„ï¼špgdirå®žé™…ä¸æ˜¯è¡¨\né¡¹ï¼Œè€Œæ˜¯ä¸€çº§é¡µè¡¨æœ¬èº«ã€‚å®žé™…ä¸Šåº”è¯¥æ–°å®šä¹‰ä¸€ä¸ªç±»åž‹pgd\\_tæ¥è¡¨ç¤ºä¸€çº§é¡µè¡¨æœ¬èº«ï¼‰ã€‚pte\ntå…¨ ç§°ä¸º page table entryï¼Œè¡¨ç¤ºäºŒçº§é¡µè¡¨çš„è¡¨é¡¹ã€‚uintptr\ntè¡¨ç¤ºä¸ºçº¿æ€§åœ°å€ï¼Œç”±äºŽæ®µå¼ç®¡ç†åªåšç›´æŽ¥æ˜ å°„ï¼Œæ‰€ä»¥å®ƒä¹Ÿæ˜¯é€»è¾‘åœ°å€ã€‚\n\npgdirç»™å‡ºé¡µè¡¨èµ·å§‹åœ°å€ã€‚é€šè¿‡æŸ¥æ‰¾è¿™ä¸ªé¡µè¡¨ï¼Œæˆ‘ä»¬éœ€è¦ç»™å‡ºäºŒçº§é¡µè¡¨ä¸­å¯¹åº”é¡¹çš„åœ°å€ã€‚\nè™½ç„¶ç›®å‰æˆ‘ä»¬åªæœ‰boot\\_pgdirä¸€ä¸ªé¡µè¡¨ï¼Œä½†æ˜¯å¼•å…¥è¿›ç¨‹çš„æ¦‚å¿µä¹‹åŽæ¯ä¸ªè¿›ç¨‹éƒ½ä¼šæœ‰è‡ªå·±çš„é¡µ\nè¡¨ã€‚\n\næœ‰å¯èƒ½æ ¹æœ¬å°±æ²¡æœ‰å¯¹åº”çš„äºŒçº§é¡µè¡¨çš„æƒ…å†µï¼Œæ‰€ä»¥äºŒçº§é¡µè¡¨ä¸å¿…è¦ä¸€å¼€å§‹å°±åˆ†é…ï¼Œè€Œæ˜¯ç­‰åˆ°éœ€è¦çš„æ—¶å€™å†æ·»åŠ å¯¹åº”çš„äºŒçº§é¡µè¡¨ã€‚å¦‚æžœåœ¨æŸ¥æ‰¾äºŒçº§é¡µè¡¨é¡¹æ—¶ï¼Œå‘çŽ°å¯¹åº”çš„äºŒçº§é¡µè¡¨ä¸å­˜åœ¨ï¼Œåˆ™éœ€è¦æ ¹æ®createå‚æ•°çš„å€¼æ¥å¤„ç†æ˜¯å¦åˆ›å»ºæ–°çš„äºŒçº§é¡µè¡¨ã€‚å¦‚æžœcreateå‚æ•°ä¸º0ï¼Œåˆ™get\\_pteè¿”å›žNULLï¼›å¦‚æžœcreateå‚æ•°ä¸ä¸º0ï¼Œåˆ™get\\_pteéœ€è¦ç”³è¯·ä¸€ä¸ªæ–°çš„ç‰©ç†é¡µï¼ˆé€šè¿‡alloc\\_pageæ¥å®žçŽ°ï¼Œå¯åœ¨mm/pmm.hä¸­æ‰¾åˆ°å®ƒçš„å®šä¹‰ï¼‰ï¼Œå†åœ¨ä¸€çº§é¡µè¡¨ä¸­æ·»åŠ é¡µç›®å½•é¡¹æŒ‡å‘è¡¨ç¤ºäºŒçº§é¡µè¡¨çš„æ–°ç‰©ç†é¡µã€‚æ³¨æ„ï¼Œæ–°ç”³è¯·çš„é¡µå¿…é¡»å…¨éƒ¨è®¾å®šä¸ºé›¶ï¼Œå› ä¸ºè¿™ä¸ªé¡µæ‰€ä»£è¡¨çš„è™šæ‹Ÿåœ°å€éƒ½æ²¡æœ‰è¢«æ˜ å°„ã€‚\n\nå½“å»ºç«‹ä»Žä¸€çº§é¡µè¡¨åˆ°äºŒçº§é¡µè¡¨çš„æ˜ å°„æ—¶ï¼Œéœ€è¦æ³¨æ„è®¾ç½®æŽ§åˆ¶ä½ã€‚è¿™é‡Œåº”è¯¥è®¾ç½®åŒæ—¶è®¾ç½®\nä¸ŠPTE\\_Uã€PTE\\_Wå’ŒPTE\\_Pï¼ˆå®šä¹‰å¯åœ¨mm/mmu.hï¼‰ã€‚å¦‚æžœåŽŸæ¥å°±æœ‰äºŒçº§é¡µè¡¨ï¼Œæˆ–è€…æ–°å»ºç«‹äº†é¡µè¡¨ï¼Œåˆ™åªéœ€è¿”å›žå¯¹åº”é¡¹çš„åœ°å€å³å¯ã€‚\n\nè™šæ‹Ÿåœ°å€åªæœ‰æ˜ å°„ä¸Šäº†ç‰©ç†é¡µæ‰å¯ä»¥æ­£å¸¸çš„è¯»å†™ã€‚åœ¨å®Œæˆæ˜ å°„ç‰©ç†é¡µçš„è¿‡ç¨‹ä¸­ï¼Œé™¤äº†è¦è±¡ä¸Šé¢é‚£æ ·åœ¨é¡µè¡¨çš„å¯¹åº”è¡¨é¡¹ä¸Šå¡«ä¸Šç›¸åº”çš„ç‰©ç†åœ°å€å¤–ï¼Œè¿˜è¦è®¾ç½®æ­£ç¡®çš„æŽ§åˆ¶ä½ã€‚æœ‰å…³\nx86 ä¸­é¡µè¡¨æŽ§åˆ¶ä½çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚ç…§ã€ŠIntelÂ® 64 and IA-32 Architectures\nSoftware Developer â€™s Manual â€“ Volume 3Aã€‹4.11 èŠ‚ã€‚\n\nåªæœ‰å½“ä¸€çº§äºŒçº§é¡µè¡¨çš„é¡¹éƒ½è®¾ç½®äº†ç”¨æˆ·å†™æƒé™åŽï¼Œç”¨æˆ·æ‰èƒ½å¯¹å¯¹åº”çš„ç‰©ç†åœ°å€è¿›è¡Œè¯»å†™ã€‚\næ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨ä¸€çº§é¡µè¡¨å…ˆç»™ç”¨æˆ·å†™æƒé™ï¼Œå†åœ¨äºŒçº§é¡µè¡¨ä¸Šé¢æ ¹æ®éœ€è¦é™åˆ¶ç”¨æˆ·çš„æƒé™ï¼Œå¯¹ç‰©ç†é¡µè¿›è¡Œä¿æŠ¤ã€‚ç”±äºŽä¸€ä¸ªç‰©ç†é¡µå¯èƒ½è¢«æ˜ å°„åˆ°ä¸åŒçš„è™šæ‹Ÿåœ°å€ä¸ŠåŽ»ï¼ˆè­¬å¦‚ä¸€å—å†…å­˜åœ¨ä¸åŒè¿›ç¨‹\né—´å…±äº«ï¼‰ï¼Œå½“è¿™ä¸ªé¡µéœ€è¦åœ¨ä¸€ä¸ªåœ°å€ä¸Šè§£é™¤æ˜ å°„æ—¶ï¼Œæ“ä½œç³»ç»Ÿä¸èƒ½ç›´æŽ¥æŠŠè¿™ä¸ªé¡µå›žæ”¶ï¼Œè€Œæ˜¯è¦å…ˆçœ‹çœ‹å®ƒè¿˜æœ‰æ²¡æœ‰æ˜ å°„åˆ°åˆ«çš„è™šæ‹Ÿåœ°å€ä¸Šã€‚è¿™æ˜¯é€šè¿‡æŸ¥æ‰¾ç®¡ç†è¯¥ç‰©ç†é¡µçš„Pageæ•°æ®ç»“æž„çš„æˆå‘˜å˜é‡refï¼ˆç”¨æ¥è¡¨ç¤ºè™šæ‹Ÿé¡µåˆ°ç‰©ç†é¡µçš„æ˜ å°„å…³ç³»çš„ä¸ªæ•°ï¼‰æ¥å®žçŽ°çš„ï¼Œå¦‚æžœrefä¸º0äº†ï¼Œè¡¨ç¤ºæ²¡æœ‰è™šæ‹Ÿé¡µåˆ°ç‰©ç†é¡µçš„æ˜ å°„å…³ç³»äº†ï¼Œå°±å¯ä»¥æŠŠè¿™ä¸ªç‰©ç†é¡µç»™å›žæ”¶äº†ï¼Œä»Žè€Œè¿™ä¸ªç‰©ç†é¡µæ˜¯freeçš„äº†ï¼Œå¯ä»¥å†è¢«åˆ†é…ã€‚page\\_insertå‡½æ•°å°†ç‰©ç†é¡µæ˜ å°„åœ¨äº†é¡µè¡¨ä¸Šã€‚å¯å‚çœ‹page\\_insertå‡½æ•°çš„å®žçŽ°æ¥äº†è§£ucoreå†…æ ¸æ˜¯å¦‚ä½•ç»´æŠ¤è¿™ä¸ªå˜é‡çš„ã€‚å½“ä¸éœ€è¦å†è®¿é—®è¿™å—è™šæ‹Ÿåœ°å€æ—¶ï¼Œå¯ä»¥æŠŠè¿™å—ç‰©ç†é¡µå›žæ”¶å¹¶åœ¨å°†æ¥ç”¨åœ¨å…¶ä»–åœ°æ–¹ã€‚å–æ¶ˆæ˜ å°„ç”±page\\_removeæ¥åšï¼Œè¿™å…¶å®žæ˜¯page\ninsertçš„é€†æ“ä½œã€‚\n\nå»ºç«‹å¥½ä¸€ä¸€æ˜ å°„çš„äºŒçº§é¡µè¡¨ç»“æž„åŽï¼ŒæŽ¥ä¸‹æ¥å°±è¦ä½¿èƒ½åˆ†é¡µæœºåˆ¶äº†ï¼Œè¿™ä¸»è¦æ˜¯é€šè¿‡enable\\_pagingå‡½æ•°å®žçŽ°çš„ï¼Œè¿™ä¸ªå‡½æ•°ä¸»è¦åšäº†ä¸¤ä»¶äº‹ï¼š\n\n1. é€šè¿‡lcr3æŒ‡ä»¤æŠŠé¡µç›®å½•è¡¨çš„èµ·å§‹åœ°å€å­˜å…¥CR3å¯„å­˜å™¨ä¸­ï¼›\n\n2. é€šè¿‡lcr0æŒ‡ä»¤æŠŠcr0ä¸­çš„CR0\\_PGæ ‡å¿—ä½è®¾ç½®ä¸Šã€‚\n\næ‰§è¡Œå®Œenable\\_pagingå‡½æ•°åŽï¼Œè®¡ç®—æœºç³»ç»Ÿè¿›å…¥äº†åˆ†é¡µæ¨¡å¼ï¼ä½†åˆ°è¿™ä¸€æ­¥è¿˜æ²¡å»ºç«‹å¥½å®Œæ•´çš„æ®µé¡µå¼æ˜ å°„ã€‚è¿˜è®°å¾—ucoreåœ¨æœ€å¼€å§‹é€šè¿‡kern\\_entryå‡½æ•°è®¾ç½®äº†ä¸´æ—¶çš„æ–°æ®µæ˜ å°„æœºåˆ¶å—ï¼Ÿè¿™ä¸ªä¸´æ—¶çš„æ–°æ®µæ˜ å°„ä¸æ˜¯æœ€ç®€å•çš„å¯¹ç­‰æ˜ å°„ï¼Œå¯¼è‡´è™šæ‹Ÿåœ°å€å’Œçº¿æ€§åœ°å€ä¸ç›¸ç­‰ã€‚è¿™é‡Œéœ€è¦æ³¨æ„ï¼šåˆšè¿›å…¥åˆ†é¡µæ¨¡å¼çš„æ—¶åˆ»æ˜¯ä¸€ä¸ªè¿‡æ¸¡è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡æ¸¡è¿‡ç¨‹ä¸­ï¼Œè™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ä¸ºï¼š\n```\nvirt addr = linear addr + 0xC0000000 = phy addr + 2 * 0xC0000000\n```\nè€Œæˆ‘ä»¬å¸Œæœ›çš„æ®µé¡µå¼æ˜ å°„çš„æœ€ç»ˆæ˜ å°„å…³ç³»ä¸ºï¼š\n```\n virt addr = linear addr = phy addr + 0xC0000000\n```\nè¿™é‡Œæœ€ç»ˆçš„æ®µæ˜ å°„æ˜¯ç®€å•çš„æ®µå¯¹ç­‰æ˜ å°„ï¼ˆvirt addr = linear addrï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´æ®µæ˜ å°„å…³ç³»ï¼Œå³é‡æ–°è®¾ç½®æ–°çš„GDTï¼Œå»ºç«‹å¯¹ç­‰æ®µæ˜ å°„ã€‚åœ¨è¿™ä¸ªç‰¹æ®Šçš„é˜¶æ®µï¼Œå¦‚æžœä¸æŠŠæ®µæ˜ å°„å…³ç³»æ”¹ä¸ºvirt addr = linear addrï¼Œåˆ™é€šè¿‡æ®µé¡µå¼ä¸¤æ¬¡åœ°å€è½¬æ¢åŽï¼Œæ— æ³•å¾—åˆ°æ­£ç¡®çš„ç‰©ç†åœ°å€ã€‚ä¸ºæ­¤æˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥è°ƒç”¨gdt\\_initå‡½æ•°ï¼Œæ ¹æ®æ–°çš„gdtå…¨å±€æ®µæè¿°ç¬¦è¡¨å†…å®¹ï¼ˆgdtå®šä¹‰ä½äºŽpmm.cä¸­ï¼‰ï¼Œæ¢å¤ç®€å•çš„æ®µå¯¹ç­‰æ˜ å°„å…³ç³»ï¼Œå³ä½¿å¾—virt addr = linear addrã€‚è¿™æ ·åœ¨æ‰§è¡Œå®Œgdt\\_initåŽï¼Œé€šè¿‡çš„æ®µæœºåˆ¶å’Œé¡µæœºåˆ¶å®žçŽ°çš„åœ°å€æ˜ å°„å…³ç³»ä¸ºï¼š\n```\nvirt addr=linear addr = phy addr +0xC0000000\n```\nè¿™é‡Œå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåœ¨è°ƒç”¨enable\\_pageå‡½æ•°åˆ°æ‰§è¡Œgdt\\_initå‡½æ•°ä¹‹å‰ï¼Œå†…æ ¸ä½¿ç”¨çš„è¿˜æ˜¯æ—§çš„æ®µè¡¨æ˜ å°„ï¼Œå³ï¼š\n```\nvirt addr = linear addr + 0xC0000000 = phy addr + 2 * 0xC0000000\n```\nå¦‚ä½•ä¿è¯æ­¤æ—¶å†…æ ¸ä¾ç„¶èƒ½å¤Ÿæ­£å¸¸å·¥ä½œå‘¢ï¼Ÿå…¶å®žåªéœ€è®©indexä¸º0çš„é¡µç›®å½•é¡¹çš„å†…å®¹ç­‰äºŽä»¥ç´¢å¼•å€¼ä¸º(KERNBASE>>22)çš„ç›®å½•è¡¨é¡¹çš„å†…å®¹å³å¯ã€‚ç›®å‰å†…æ ¸å¤§å°ä¸è¶…è¿‡\n4M ï¼ˆå®žé™…ä¸Šæ˜¯3Mï¼Œå› ä¸ºå†…æ ¸ä»Ž 0x100000å¼€å§‹ç¼–å€ï¼‰ï¼Œè¿™æ ·å°±åªéœ€è¦è®©é¡µè¡¨åœ¨0\\~4MBçš„çº¿æ€§åœ°å€ä¸ŽKERNBASE \\~ KERNBASE+4MBçš„çº¿æ€§åœ°å€èŽ·å¾—ç›¸åŒçš„æ˜ å°„å³å¯ï¼Œéƒ½æ˜ å°„åˆ° 0\\~4MBçš„ç‰©ç†åœ°å€ç©ºé—´ï¼Œå…·ä½“å®žçŽ°åœ¨pmm.cä¸­pmm\\_initå‡½æ•°çš„è¯­å¥ï¼š\n```\nboot_pgdir[0] = boot_pgdir[PDX(KERNBASE)];\n```\nå®žé™…ä¸Šè¿™ç§æ˜ å°„ä¹Ÿé™åˆ¶äº†å†…æ ¸çš„å¤§å°ã€‚å½“å†…æ ¸å¤§å°è¶…è¿‡é¢„æœŸçš„3MB\nå°±å¯èƒ½å¯¼è‡´æ‰“å¼€åˆ†é¡µä¹‹åŽå†…æ ¸crashï¼Œåœ¨åŽé¢çš„è¯•éªŒä¸­ï¼Œä¹Ÿçš„ç¡®å‡ºçŽ°äº†è¿™ç§æƒ…å†µã€‚è§£å†³æ–¹æ³•åŒæ ·ç®€å•ï¼Œå°±æ˜¯æ‹·è´æ›´å¤šçš„é«˜åœ°å€å¯¹åº”çš„é¡µç›®å½•é¡¹å†…å®¹åˆ°ä½Žåœ°å€å¯¹åº”çš„é¡µç›®å½•é¡¹ä¸­å³å¯ã€‚\n\nå½“æ‰§è¡Œå®Œæ¯•gdt\\_initå‡½æ•°åŽï¼Œæ–°çš„æ®µé¡µå¼æ˜ å°„å·²ç»å»ºç«‹å¥½äº†ï¼Œä¸Šé¢çš„0\\~4MBçš„çº¿æ€§åœ°å€ä¸Ž0\\~4MBçš„ç‰©ç†åœ°å€ä¸€ä¸€æ˜ å°„å…³ç³»å·²ç»æ²¡æœ‰ç”¨äº†ã€‚\næ‰€ä»¥å¯ä»¥é€šè¿‡å¦‚ä¸‹è¯­å¥è§£é™¤è¿™ä¸ªè€çš„æ˜ å°„å…³ç³»ã€‚\n```\nboot_pgdir[0] = 0;\n```\nåœ¨page\\_initå‡½æ•°å»ºç«‹å®Œå®žçŽ°ç‰©ç†å†…å­˜ä¸€ä¸€æ˜ å°„å’Œé¡µç›®å½•è¡¨è‡ªæ˜ å°„çš„é¡µç›®å½•è¡¨å’Œé¡µè¡¨åŽï¼Œä¸€æ—¦ä½¿èƒ½åˆ†é¡µæœºåˆ¶ï¼Œåˆ™ucoreçœ‹åˆ°çš„å†…æ ¸è™šæ‹Ÿåœ°å€ç©ºé—´å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](../lab2_figs/image008.png)\n\nå›¾7 ä½¿èƒ½åˆ†é¡µæœºåˆ¶åŽçš„è™šæ‹Ÿåœ°å€ç©ºé—´å›¾\n",
          "### ç³»ç»Ÿæ‰§è¡Œä¸­åœ°å€æ˜ å°„çš„å››ä¸ªé˜¶æ®µ\nåŽŸç†è¯¾ä¸Šè®²åˆ°äº†é¡µæ˜ å°„ï¼Œæ®µæ˜ å°„ï¼Œä»¥åŠæ®µé¡µå¼æ˜ å°„å…³ç³»ï¼Œä½†å¯¹å¦‚ä½•å»ºç«‹æ®µé¡µå¼æ˜ å°„å…³ç³»æ²¡æœ‰è¯¦è¯´ã€‚å…¶å®žï¼Œåœ¨lab1å’Œlab2ä¸­éƒ½ä¼šæ¶‰åŠå¦‚ä½•å»ºç«‹æ˜ å°„å…³ç³»çš„æ“ä½œã€‚åœ¨lab1ä¸­ï¼Œæˆ‘ä»¬å·²ç»ç¢°åˆ°åˆ°äº†ç®€å•çš„æ®µæ˜ å°„ï¼Œå³å¯¹ç­‰æ˜ å°„å…³ç³»ï¼Œä¿è¯äº†ç‰©ç†åœ°å€å’Œè™šæ‹Ÿåœ°å€ç›¸ç­‰ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡å»ºç«‹å…¨å±€æ®µæè¿°ç¬¦è¡¨ï¼Œè®©æ¯ä¸ªæ®µçš„åŸºå€ä¸º0ï¼Œä»Žè€Œç¡®å®šäº†å¯¹ç­‰æ˜ å°„å…³ç³»ã€‚åœ¨lab2ä¸­ï¼Œç”±äºŽåœ¨æ®µåœ°å€æ˜ å°„çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¼•å…¥äº†é¡µåœ°å€æ˜ å°„ï¼Œå½¢æˆäº†ç»„åˆå¼çš„æ®µé¡µå¼åœ°å€æ˜ å°„ã€‚è¿™ç§æ–¹å¼è™½ç„¶æ›´åŠ çµæ´»äº†ï¼Œä½†å®žçŽ°ç¨å¾®å¤æ‚äº†ä¸€äº›ã€‚åœ¨lab2ä¸­ï¼Œä¸ºäº†å»ºç«‹æ­£ç¡®çš„åœ°å€æ˜ å°„å…³ç³»ï¼Œldåœ¨é“¾æŽ¥é˜¶æ®µç”Ÿæˆäº†ucore OSæ‰§è¡Œä»£ç çš„è™šæ‹Ÿåœ°å€ï¼Œè€Œbootloaderä¸Žucore OSååŒå·¥ä½œï¼Œé€šè¿‡åœ¨è¿è¡Œæ—¶å¯¹åœ°å€æ˜ å°„çš„ä¸€ç³»åˆ—â€œè…¾æŒªè½¬ç§»â€ï¼Œä»Žè®¡ç®—æœºåŠ ç”µï¼Œå¯åŠ¨æ®µå¼ç®¡ç†æœºåˆ¶ï¼Œå¯åŠ¨æ®µé¡µå¼ç®¡ç†æœºåˆ¶ï¼Œåœ¨æ®µé¡µå¼ç®¡ç†æœºåˆ¶ä¸‹è¿è¡Œè¿™æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œè™šåœ°å€åˆ°ç‰©ç†åœ°å€çš„æ˜ å°„äº§ç”Ÿäº†å¤šæ¬¡å˜åŒ–ï¼Œå®žçŽ°äº†æœ€ç»ˆçš„æ®µé¡µå¼æ˜ å°„å…³ç³»ï¼š\n``` \n virt addr = linear addr = phy addr + 0xC0000000  \n```\n\nä¸‹é¢ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•ä¸€æ­¥ä¸€æ­¥å®žçŽ°çš„ã€‚è§‚å¯Ÿä¸€ä¸‹é“¾æŽ¥è„šæœ¬ï¼Œå³tools/kernel.ldæ–‡ä»¶åœ¨lab1å’Œlab2ä¸­çš„åŒºåˆ«ã€‚åœ¨lab1ä¸­ï¼š\n```\nENTRY(kern_init)\n\nSECTIONS {\n            /* Load the kernel at this address: \".\" means the current address */\n            . = 0x100000;\n\n            .text : {\n                       *(.text .stub .text.* .gnu.linkonce.t.*)\n            }\n```\nè¿™æ„å‘³ç€åœ¨lab1ä¸­é€šè¿‡ldå·¥å…·å½¢æˆçš„ucoreçš„èµ·å§‹è™šæ‹Ÿåœ°å€ä»Ž0x100000å¼€å§‹ï¼Œæ³¨æ„ï¼šè¿™ä¸ªåœ°å€æ˜¯è™šæ‹Ÿåœ°å€ã€‚ä½†ç”±äºŽlab1ä¸­å»ºç«‹çš„æ®µåœ°å€æ˜ å°„å…³ç³»ä¸ºå¯¹ç­‰å…³ç³»ï¼Œæ‰€ä»¥ucoreçš„ç‰©ç†åœ°å€ä¹Ÿæ˜¯0x100000ï¼Œè€Œucoreçš„å…¥å£å‡½æ•°kern\\_initçš„èµ·å§‹åœ°å€ã€‚æ‰€ä»¥åœ¨lab1ä¸­è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»å¦‚ä¸‹ï¼š\n```\n lab1ï¼š virt addr = linear addr = phy addr\n```\n\nåœ¨lab2ä¸­ï¼š\n```\nENTRY(kern_entry)\n\nSECTIONS {\n            /* Load the kernel at this address: \".\" means the current address */\n            . = 0xC0100000;\n\n            .text : {\n                        *(.text .stub .text.* .gnu.linkonce.t.*)\n            }\n```\nè¿™æ„å‘³ç€lab2ä¸­é€šè¿‡ldå·¥å…·å½¢æˆçš„ucoreçš„èµ·å§‹è™šæ‹Ÿåœ°å€ä»Ž0xC0100000å¼€å§‹ï¼Œæ³¨æ„ï¼šè¿™ä¸ªåœ°å€ä¹Ÿæ˜¯è™šæ‹Ÿåœ°å€ã€‚å…¥å£å‡½æ•°ä¸ºkern\\_entryå‡½æ•°ï¼ˆåœ¨kern/init/entry.Sä¸­ï¼‰ã€‚è¿™ä¸Žlab1æœ‰å¾ˆå¤§å·®åˆ«ã€‚ä½†å…¶å®žåœ¨lab1å’Œlab2ä¸­ï¼ŒbootloaderæŠŠucoreéƒ½æ”¾åœ¨äº†èµ·å§‹ç‰©ç†åœ°å€ä¸º0x100000çš„ç‰©ç†å†…å­˜ç©ºé—´ã€‚è¿™å®žé™…ä¸Šè¯´æ˜Žäº†ucoreåœ¨lab1å’Œlab2ä¸­é‡‡ç”¨çš„åœ°å€æ˜ å°„ä¸åŒã€‚lab2åœ¨ä¸åŒé˜¶æ®µæœ‰ä¸åŒçš„è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚\n\n**ç¬¬ä¸€ä¸ªé˜¶æ®µ**æ˜¯bootloaderé˜¶æ®µï¼Œå³ä»Žbootloaderçš„startå‡½æ•°ï¼ˆåœ¨boot/bootasm.Sä¸­ï¼‰åˆ°æ‰§è¡Œucore kernelçš„kern_\\entryå‡½æ•°ä¹‹å‰ï¼Œå…¶è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ä¸Žlab1çš„ä¸€æ ·ï¼Œå³ï¼š\n```\n lab2 stage 1ï¼š virt addr = linear addr = phy addr\n```\n\n**ç¬¬äºŒä¸ªé˜¶æ®µ**ä»Žä»Žkern_\\entryå‡½æ•°å¼€å§‹ï¼Œåˆ°æ‰§è¡Œenable_pageå‡½æ•°ï¼ˆåœ¨kern/mm/pmm.cä¸­ï¼‰ä¹‹å‰å†æ¬¡æ›´æ–°äº†æ®µæ˜ å°„ï¼Œè¿˜æ²¡æœ‰å¯åŠ¨é¡µæ˜ å°„æœºåˆ¶ã€‚ç”±äºŽgccç¼–è¯‘å‡ºçš„è™šæ‹Ÿèµ·å§‹åœ°å€ä»Ž0xC0100000å¼€å§‹ï¼Œucoreè¢«bootloaderæ”¾ç½®åœ¨ä»Žç‰©ç†åœ°å€0x100000å¤„å¼€å§‹çš„ç‰©ç†å†…å­˜ä¸­ã€‚æ‰€ä»¥å½“kern\\_entryå‡½æ•°å®Œæˆæ–°çš„æ®µæ˜ å°„å…³ç³»åŽï¼Œä¸”ucoreåœ¨æ²¡æœ‰å»ºç«‹å¥½é¡µæ˜ å°„æœºåˆ¶å‰ï¼ŒCPUæŒ‰ç…§ucoreä¸­çš„è™šæ‹Ÿåœ°å€æ‰§è¡Œï¼Œèƒ½å¤Ÿè¢«åˆ†æ®µæœºåˆ¶æ˜ å°„åˆ°æ­£ç¡®çš„ç‰©ç†åœ°å€ä¸Šï¼Œç¡®ä¿ucoreè¿è¡Œæ­£ç¡®ã€‚è¿™æ—¶çš„è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ä¸ºï¼š \n``` \n lab2 stage 2ï¼š virt addr - 0xC0000000 = linear addr = phy addr \n```\næ³¨æ„æ­¤æ—¶CPUåœ¨å¯»å€æ—¶è¿˜æ˜¯åªé‡‡ç”¨äº†åˆ†æ®µæœºåˆ¶ã€‚æœ€åŽåŽå¹¶ä½¿èƒ½åˆ†é¡µæ˜ å°„æœºåˆ¶ï¼ˆè¯·æŸ¥çœ‹lab2/kern/mm/pmm.cä¸­çš„enable\\_pagingå‡½æ•°ï¼‰ï¼Œä¸€æ—¦æ‰§è¡Œå®Œenable\\_pagingå‡½æ•°ä¸­çš„åŠ è½½cr0æŒ‡ä»¤ï¼ˆå³è®©CPUä½¿èƒ½åˆ†é¡µæœºåˆ¶ï¼‰ï¼Œåˆ™æŽ¥ä¸‹æ¥çš„è®¿é—®æ˜¯åŸºäºŽæ®µé¡µå¼çš„æ˜ å°„å…³ç³»äº†ã€‚\n\n**ç¬¬ä¸‰ä¸ªé˜¶æ®µ**ä»Ženable_pageå‡½æ•°å¼€å§‹ï¼Œåˆ°æ‰§è¡Œgdt_initå‡½æ•°ï¼ˆåœ¨kern/mm/pmm.cä¸­ï¼‰ä¹‹å‰ï¼Œå¯åŠ¨äº†é¡µæ˜ å°„æœºåˆ¶ï¼Œä½†æ²¡æœ‰ç¬¬ä¸‰æ¬¡æ›´æ–°æ®µæ˜ å°„ã€‚è¿™æ—¶çš„è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»æ¯”è¾ƒå¾®å¦™ï¼š \n``` \n lab2 stage 3:  virt addr - 0xC0000000 = linear addr  = phy addr + 0xC0000000 # çº¿æ€§åœ°å€åœ¨0~4MBä¹‹å¤–çš„ä¸‰è€…æ˜ å°„å…³ç³»\n                virt addr - 0xC0000000 = linear addr  = phy addr # çº¿æ€§åœ°å€åœ¨0~4MBä¹‹å†…çš„ä¸‰è€…æ˜ å°„å…³ç³»\n```\nè¯·æ³¨æ„`pmm_init`å‡½æ•°ä¸­çš„ä¸€æ¡è¯­å¥ï¼š\n```\n boot_pgdir[0] = boot_pgdir[PDX(KERNBASE)];\n```\nå°±æ˜¯ç”¨æ¥å»ºç«‹ç‰©ç†åœ°å€åœ¨0~4MBä¹‹å†…çš„ä¸‰ä¸ªåœ°å€é—´çš„ä¸´æ—¶æ˜ å°„å…³ç³»`virt addr - 0xC0000000 = linear addr = phy addr`ã€‚\n\n**ç¬¬å››ä¸ªé˜¶æ®µ**ä»Žgdt_initå‡½æ•°å¼€å§‹ï¼Œç¬¬ä¸‰æ¬¡æ›´æ–°äº†æ®µæ˜ å°„ï¼Œå½¢æˆäº†æ–°çš„æ®µé¡µå¼æ˜ å°„æœºåˆ¶ï¼Œå¹¶ä¸”å–æ¶ˆäº†ä¸´æ—¶æ˜ å°„å…³ç³»ï¼Œå³æ‰§è¡Œè¯­å¥â€œboot\\_pgdir**[**0**]** **=**\n0**;**â€æŠŠboot\\_pgdir[0]çš„ç¬¬ä¸€ä¸ªé¡µç›®å½•è¡¨é¡¹ï¼ˆ0\\~4MBï¼‰æ¸…é›¶æ¥å–æ¶ˆä¸´æ—¶çš„é¡µæ˜ å°„å…³ç³»ã€‚è¿™æ—¶å½¢æˆäº†æˆ‘ä»¬æœŸæœ›çš„è™šæ‹Ÿåœ°å€ï¼Œçº¿æ€§åœ°å€ä»¥åŠç‰©ç†åœ°å€ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼š \n``` \n lab2 stage 4ï¼š virt addr = linear addr = phy addr + 0xC0000000  \n```\n\n",
          "### è‡ªæ˜ å°„æœºåˆ¶\n\nè¿™æ˜¯æ‰©å±•çŸ¥è¯†ã€‚\nä¸Šä¸€å°èŠ‚è®²è¿°äº†é€šè¿‡boot\\_map\\_segmentå‡½æ•°å»ºç«‹äº†åŸºäºŽä¸€ä¸€æ˜ å°„å…³ç³»çš„é¡µç›®å½•è¡¨é¡¹å’Œé¡µè¡¨é¡¹ï¼Œè¿™é‡Œçš„æ˜ å°„å…³ç³»ä¸ºï¼š\n\nvirtual addr (KERNBASE\\~KERNBASE+KMEMSIZE) = physical\\_addr\n(0\\~KMEMSIZE)\n\nè¿™æ ·åªè¦ç»™å‡ºä¸€ä¸ªè™šåœ°å€å’Œä¸€ä¸ªç‰©ç†åœ°å€ï¼Œå°±å¯ä»¥è®¾ç½®ç›¸åº”PDEå’ŒPTEï¼Œå°±å¯å®Œæˆæ­£ç¡®çš„æ˜ å°„å…³ç³»ã€‚\n\nå¦‚æžœæˆ‘ä»¬è¿™æ—¶éœ€è¦æŒ‰è™šæ‹Ÿåœ°å€çš„åœ°å€é¡ºåºæ˜¾ç¤ºæ•´ä¸ªé¡µç›®å½•è¡¨å’Œé¡µè¡¨çš„å†…å®¹ï¼Œåˆ™è¦æŸ¥æ‰¾é¡µç›®å½•è¡¨çš„é¡µç›®å½•è¡¨é¡¹å†…å®¹ï¼Œæ ¹æ®é¡µç›®å½•è¡¨é¡¹å†…å®¹æ‰¾åˆ°é¡µè¡¨çš„ç‰©ç†åœ°å€ï¼Œå†è½¬æ¢æˆå¯¹åº”çš„è™šåœ°å€ï¼Œç„¶åŽè®¿é—®é¡µè¡¨çš„è™šåœ°å€ï¼Œæœç´¢æ•´ä¸ªé¡µè¡¨çš„æ¯ä¸ªé¡µç›®å½•é¡¹ã€‚è¿™æ ·è¿‡ç¨‹æ¯”è¾ƒç¹çã€‚\n\næˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªç®€æ´çš„æ–¹æ³•æ¥å®žçŽ°è¿™ä¸ªæŸ¥æ‰¾ã€‚ucoreåšäº†ä¸€ä¸ªå¾ˆå·§å¦™çš„åœ°å€è‡ªæ˜ å°„è®¾è®¡ï¼ŒæŠŠé¡µç›®å½•è¡¨å’Œé¡µè¡¨æ”¾åœ¨ä¸€ä¸ªè¿žç»­çš„4MBè™šæ‹Ÿåœ°å€ç©ºé—´ä¸­ï¼Œå¹¶è®¾ç½®é¡µç›®å½•è¡¨è‡ªèº«çš„è™šåœ°å€<--\\>ç‰©ç†åœ°å€æ˜ å°„å…³ç³»ã€‚è¿™æ ·åœ¨å·²çŸ¥é¡µç›®å½•è¡¨èµ·å§‹è™šåœ°å€çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¿žç»­æ‰«æè¿™ç‰¹å®šçš„4MBè™šæ‹Ÿåœ°å€ç©ºé—´ï¼Œå°±å¾ˆå®¹æ˜“è®¿é—®æ¯ä¸ªé¡µç›®å½•è¡¨é¡¹å’Œé¡µè¡¨é¡¹å†…å®¹ã€‚\n\nå…·ä½“è€Œè¨€ï¼Œucoreæ˜¯è¿™æ ·è®¾è®¡çš„ï¼Œé¦–å…ˆè®¾ç½®äº†ä¸€ä¸ªå¸¸é‡ï¼ˆmemlayout.hï¼‰ï¼š\n\nVPT=0xFAC00000ï¼Œ è¿™ä¸ªåœ°å€çš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸ºï¼š\n\n1111 1010 1100 0000 0000 0000 0000 0000\n\né«˜10ä½ä¸º1111 1010\n11ï¼Œå³10è¿›åˆ¶çš„1003ï¼Œä¸­é—´10ä½ä¸º0ï¼Œä½Ž12ä½ä¹Ÿä¸º0ã€‚åœ¨pmm.cä¸­æœ‰ä¸¤ä¸ªå…¨å±€åˆå§‹åŒ–å˜é‡\n\npte\\_t \\* const vpt = (pte\\_t \\*)VPT;\n\npde\\_t \\* const vpd = (pde\\_t \\*)PGADDR(PDX(VPT), PDX(VPT), 0);\n\nå¹¶åœ¨pmm\\_initå‡½æ•°æ‰§è¡Œäº†å¦‚ä¸‹è¯­å¥ï¼š\n\nboot\\_pgdir[PDX(VPT)] = PADDR(boot\\_pgdir) | PTE\\_P | PTE\\_W;\n\nè¿™äº›å˜é‡å’Œè¯­å¥æœ‰ä½•ç‰¹æ®Šå«ä¹‰å‘¢ï¼Ÿå…¶å®žvpdå˜é‡çš„å€¼å°±æ˜¯é¡µç›®å½•è¡¨çš„èµ·å§‹è™šåœ°å€0xFAFEB000ï¼Œä¸”å®ƒçš„é«˜10ä½å’Œä¸­10ä½æ˜¯ç›¸ç­‰çš„ï¼Œéƒ½æ˜¯10è¿›åˆ¶çš„1003ã€‚å½“æ‰§è¡Œäº†ä¸Šè¿°è¯­å¥ï¼Œå°±ç¡®ä¿äº†vpdå˜é‡çš„å€¼å°±æ˜¯é¡µç›®å½•è¡¨çš„èµ·å§‹è™šåœ°å€ï¼Œä¸”vptæ˜¯é¡µç›®å½•è¡¨ä¸­ç¬¬ä¸€ä¸ªç›®å½•è¡¨é¡¹æŒ‡å‘çš„é¡µè¡¨çš„èµ·å§‹è™šåœ°å€ã€‚æ­¤æ—¶æè¿°å†…æ ¸è™šæ‹Ÿç©ºé—´çš„é¡µç›®å½•è¡¨çš„è™šåœ°å€ä¸º0xFAFEB000ï¼Œå¤§å°ä¸º4KBã€‚é¡µè¡¨çš„ç†è®ºè¿žç»­è™šæ‹Ÿåœ°å€ç©ºé—´0xFAC00000\\~0xFB000000ï¼Œå¤§å°ä¸º4MBã€‚å› ä¸ºè¿™ä¸ªè¿žç»­åœ°å€ç©ºé—´çš„å¤§å°ä¸º4MBï¼Œå¯æœ‰1Mä¸ªPTEï¼Œå³å¯æ˜ å°„4GBçš„åœ°å€ç©ºé—´ã€‚\n\nä½†ucoreå®žé™…ä¸Šä¸ä¼šç”¨å®Œè¿™ä¹ˆå¤šé¡¹ï¼Œåœ¨memlayout.hä¸­å®šä¹‰äº†å¸¸é‡\n```\n#define KERNBASE 0xC0000000\n#define KMEMSIZE 0x38000000 // the maximum amount of physical memory\n#define KERNTOP (KERNBASE + KMEMSIZE)\n```\n\nè¡¨ç¤ºucoreåªæ”¯æŒ896MBçš„ç‰©ç†å†…å­˜ç©ºé—´ï¼Œè¿™ä¸ª896MBåªæ˜¯ä¸€ä¸ªè®¾å®šï¼Œå¯ä»¥æ ¹æ®æƒ…å†µæ”¹å˜ã€‚åˆ™æœ€å¤§çš„å†…æ ¸è™šåœ°å€ä¸ºå¸¸é‡\n```\n#define KERNTOP (KERNBASE + KMEMSIZE)=0xF8000000\n```\n\næ‰€ä»¥æœ€å¤§å†…æ ¸è™šåœ°å€KERNTOPçš„é¡µç›®å½•é¡¹è™šåœ°å€ä¸º\n```\nvpd+0xF8000000/0x400000*4=0xFAFEB000+0x3E0*4=0xFAFEBF80\n```\n\næœ€å¤§å†…æ ¸è™šåœ°å€KERNTOPçš„é¡µè¡¨é¡¹è™šåœ°å€ä¸ºï¼š\n```\nvpt+0xF8000000/0x1000*4=0xFAC00000+0xF8000*4=0xFAFE0000\n```\n\n> éœ€è¦æ³¨æ„ï¼Œé¡µç›®å½•é¡¹å’Œé¡µè¡¨é¡¹æ˜¯4å­—èŠ‚å¯¹é½çš„ã€‚ä»Žä¸Šé¢çš„è®¾ç½®å¯ä»¥çœ‹å‡ºKERNTOP/4MåŽçš„å€¼æ˜¯4å­—èŠ‚å¯¹é½çš„ï¼Œæ‰€ä»¥è¿™æ ·ç®—å‡ºæ¥çš„é¡µç›®å½•é¡¹å’Œé¡µè¡¨é¡¹åœ°å€çš„æœ€åŽä¸¤ä½ä¸€å®šæ˜¯0ã€‚\n\nåœ¨pmm.cä¸­çš„å‡½æ•°print\\_pgdirå°±æ˜¯åŸºäºŽucoreçš„é¡µè¡¨è‡ªæ˜ å°„æ–¹å¼å®Œæˆäº†å¯¹æ•´ä¸ªé¡µç›®å½•è¡¨å’Œé¡µè¡¨çš„å†…å®¹æ‰«æå’Œæ‰“å°ã€‚æ³¨æ„ï¼Œè¿™é‡Œä¸ä¼šå‡ºçŽ°æŸä¸ªé¡µè¡¨çš„è™šåœ°å€ä¸Žé¡µç›®å½•è¡¨è™šåœ°å€ç›¸åŒçš„æƒ…å†µã€‚\n\nprint\\_pgdirå‡½æ•°ä½¿å¾— ucore å…·å¤‡å’Œ qemu çš„info pgç›¸åŒçš„åŠŸèƒ½ï¼Œå³print pgdirèƒ½\nå¤Ÿä»Žå†…å­˜ä¸­ï¼Œå°†å½“å‰é¡µè¡¨å†…æœ‰æ•ˆæ•°æ®ï¼ˆPTE\\_Pï¼‰å°å‡ºæ¥ã€‚æ‹·è´å‡ºçš„æ ¼å¼å¦‚ä¸‹æ‰€ç¤º:\n```\nPDE(0e0)  c0000000-f8000000  38000000  urw\n|-- PTE(38000) c0000000-f8000000  38000000 -rw\nPDE(001)  fac00000-fb000000  00400000  -rw\n|-- PTE(000e0)  faf00000-fafe0000  000e0000  urw\n|-- PTE(00001)  fafeb000-fafec000  00001000  -rw\n```\nä¸Šé¢ä¸­çš„æ•°å­—åŒ…æ‹¬æ‹¬å·é‡Œçš„ï¼Œéƒ½æ˜¯åå…­è¿›åˆ¶ã€‚\n\nä¸»è¦çš„åŠŸèƒ½æ˜¯ä»Žé¡µè¡¨ä¸­å°†å…·å¤‡ç›¸åŒæƒé™çš„ PDE å’Œ PTE\né¡¹ç›®ç»„ç»‡èµ·æ¥ã€‚æ¯”å¦‚ä¸Šè¡¨ä¸­ï¼š\n```\nPDE(0e0) c0000000-f8000000 38000000 urw\n```\nâ€¢ PDE(0e0)ï¼š0e0è¡¨ç¤º PDE è¡¨ä¸­ç›¸é‚»çš„ 224 é¡¹å…·æœ‰ç›¸åŒçš„æƒé™ï¼›\nâ€¢ c0000000-f8000000ï¼šè¡¨ç¤º PDE è¡¨ä¸­,è¿™ç›¸é‚»çš„ä¸¤é¡¹æ‰€æ˜ å°„çš„çº¿æ€§åœ°å€çš„èŒƒå›´ï¼›\nâ€¢ 38000000ï¼šåŒæ ·è¡¨ç¤ºèŒƒå›´ï¼Œå³f8000000å‡åŽ»c0000000çš„ç»“æžœï¼›\nâ€¢ urwï¼šPDE è¡¨ä¸­æ‰€ç»™å‡ºçš„æƒé™ä½ï¼Œuè¡¨ç¤ºç”¨æˆ·å¯è¯»ï¼Œå³PTE\\_Uï¼Œrè¡¨ç¤ºPTE\\_Pï¼Œwè¡¨ç¤ºç”¨\næˆ·å¯å†™ï¼Œå³PTE\\_Wã€‚\n```\nPDE(001) fac00000-fb000000 00400000 -rw\n```\nè¡¨ç¤ºä»… 1 æ¡è¿žç»­çš„ PDE è¡¨é¡¹å…·å¤‡ç›¸åŒçš„å±žæ€§ã€‚ç›¸åº”çš„ï¼Œåœ¨è¿™æ¡è¡¨é¡¹ä¸­éåŽ†æ‰¾åˆ° 2\nç»„ PTE è¡¨é¡¹ï¼Œè¾“å‡ºå¦‚ä¸‹:\n```\n|-- PTE(000e0) faf00000-fafe0000 000e0000 urw\n|-- PTE(00001) fafeb000-fafec000 00001000 -rw\n```\næ³¨æ„ï¼š\n1. PTE ä¸­è¾“å‡ºçš„æƒé™æ˜¯ PTE è¡¨ä¸­çš„æ•°æ®ç»™å‡ºçš„ï¼Œå¹¶æ²¡æœ‰å’Œ PDE\nè¡¨ä¸­æƒé™åšä¸Žè¿ç®—ã€‚\n2.\næ•´ä¸ªprint\\_pgdirå‡½æ•°å¼ºè°ƒä¸¤ç‚¹ï¼šç¬¬ä¸€æ˜¯ç›¸åŒæƒé™ï¼Œç¬¬äºŒæ˜¯è¿žç»­ã€‚\n3.\nprint\\_pgdirä¸­ç”¨åˆ°äº†vptå’Œvpdä¸¤ä¸ªå˜é‡ã€‚å¯ä»¥å‚\nè€ƒVPTå’ŒPGADDRä¸¤ä¸ªå®ã€‚\n\nè‡ªæ˜ å°„æœºåˆ¶è¿˜å¯æ–¹ä¾¿ç”¨æˆ·æ€ç¨‹åºè®¿é—®é¡µè¡¨ã€‚å› ä¸ºé¡µè¡¨æ˜¯å†…æ ¸ç»´æŠ¤çš„ï¼Œç”¨æˆ·ç¨‹åºå¾ˆéš¾çŸ¥é“è‡ªå·±é¡µè¡¨çš„æ˜ å°„ç»“æž„ã€‚VPT\nå®žé™…ä¸Šåœ¨å†…æ ¸åœ°å€ç©ºé—´çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åŒæ ·çš„æ–¹å¼å®žçŽ°ä¸€ä¸ªç”¨æˆ·åœ°å€ç©ºé—´çš„æ˜ å°„ï¼ˆæ¯”å¦‚\npgdir[UVPT] = PADDR(pgdir) | PTE\\_P | PTE\\_Uï¼Œæ³¨æ„ï¼Œè¿™é‡Œä¸èƒ½ç»™å†™æƒé™ï¼Œå¹¶ä¸”\npgdir æ˜¯æ¯ä¸ªè¿›ç¨‹çš„ page tableï¼Œä¸æ˜¯\nboot\\_pgdirï¼‰ï¼Œè¿™æ ·ï¼Œç”¨æˆ·ç¨‹åºå°±å¯ä»¥ç”¨å’Œå†…æ ¸ä¸€æ ·çš„ print\\_pgdir\nå‡½æ•°éåŽ†è‡ªå·±çš„é¡µè¡¨ç»“æž„äº†ã€‚\n",
          "**å®žçŽ°ç‰©ç†å†…å­˜æŽ¢æµ‹**\n\nç‰©ç†å†…å­˜æŽ¢æµ‹æ˜¯åœ¨bootasm.Sä¸­å®žçŽ°çš„ï¼Œç›¸å…³ä»£ç å¾ˆçŸ­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n```\nprobe_memory:\n//å¯¹0x8000å¤„çš„32ä½å•å…ƒæ¸…é›¶,å³ç»™ä½äºŽ0x8000å¤„çš„\n//struct e820mapçš„æˆå‘˜å˜é‡nr_mapæ¸…é›¶\n           movl $0, 0x8000\n                  xorl %ebx, %ebx\n//è¡¨ç¤ºè®¾ç½®è°ƒç”¨INT 15h BIOSä¸­æ–­åŽï¼ŒBIOSè¿”å›žçš„æ˜ å°„åœ°å€æè¿°ç¬¦çš„èµ·å§‹åœ°å€\n                  movw $0x8004, %di\nstart_probe:\n                  movl $0xE820, %eax // INT 15çš„ä¸­æ–­è°ƒç”¨å‚æ•°\n//è®¾ç½®åœ°å€èŒƒå›´æè¿°ç¬¦çš„å¤§å°ä¸º20å­—èŠ‚ï¼Œå…¶å¤§å°ç­‰äºŽstruct e820mapçš„æˆå‘˜å˜é‡mapçš„å¤§å°\n                  movl $20, %ecx\n//è®¾ç½®edxä¸º534D4150h (å³4ä¸ªASCIIå­—ç¬¦â€œSMAPâ€)ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¦å®š\n                  movl $SMAP, %edx\n//è°ƒç”¨int 0x15ä¸­æ–­ï¼Œè¦æ±‚BIOSè¿”å›žä¸€ä¸ªç”¨åœ°å€èŒƒå›´æè¿°ç¬¦è¡¨ç¤ºçš„å†…å­˜æ®µä¿¡æ¯\n                  int $0x15\n//å¦‚æžœeflagsçš„CFä½ä¸º0ï¼Œåˆ™è¡¨ç¤ºè¿˜æœ‰å†…å­˜æ®µéœ€è¦æŽ¢æµ‹\n                  jnc cont\n//æŽ¢æµ‹æœ‰é—®é¢˜ï¼Œç»“æŸæŽ¢æµ‹\n                  movw $12345, 0x8000\n                  jmp finish_probe\ncont:\n//è®¾ç½®ä¸‹ä¸€ä¸ªBIOSè¿”å›žçš„æ˜ å°„åœ°å€æè¿°ç¬¦çš„èµ·å§‹åœ°å€\n                  addw $20, %di\n//é€’å¢žstruct e820mapçš„æˆå‘˜å˜é‡nr_map\n                  incl 0x8000\n//å¦‚æžœINT0x15è¿”å›žçš„ebxä¸ºé›¶ï¼Œè¡¨ç¤ºæŽ¢æµ‹ç»“æŸï¼Œå¦åˆ™ç»§ç»­æŽ¢æµ‹\n                  cmpl $0, %ebx\n                  jnz start_probe\nfinish_probe:\n```\nä¸Šè¿°ä»£ç æ­£å¸¸æ‰§è¡Œå®Œæ¯•åŽï¼Œåœ¨0x8000åœ°å€å¤„ä¿å­˜äº†ä»ŽBIOSä¸­èŽ·å¾—çš„å†…å­˜åˆ†å¸ƒä¿¡æ¯ï¼Œæ­¤ä¿¡æ¯æŒ‰ç…§struct\ne820mapçš„è®¾ç½®æ¥è¿›è¡Œå¡«å……ã€‚è¿™éƒ¨åˆ†ä¿¡æ¯å°†åœ¨bootloaderå¯åŠ¨ucoreåŽï¼Œç”±ucoreçš„page\\_initå‡½æ•°æ¥æ ¹æ®struct\ne820mapçš„memmapï¼ˆå®šä¹‰äº†èµ·å§‹åœ°å€ä¸º0x8000ï¼‰æ¥å®Œæˆå¯¹æ•´ä¸ªæœºå™¨ä¸­çš„ç‰©ç†å†…å­˜çš„æ€»ä½“ç®¡ç†ã€‚\n",
          "**é“¾æŽ¥åœ°å€/è™šåœ°å€/ç‰©ç†åœ°å€/åŠ è½½åœ°å€ä»¥åŠedata/end/textçš„å«ä¹‰**\n\n**é“¾æŽ¥è„šæœ¬ç®€ä»‹**\n\nucore\nkernelå„ä¸ªéƒ¨åˆ†ç”±ç»„æˆkernelçš„å„ä¸ª.oæˆ–.aæ–‡ä»¶æž„æˆï¼Œä¸”å„ä¸ªéƒ¨åˆ†åœ¨å†…å­˜ä¸­åœ°å€ä½ç½®ç”±ldå·¥å…·æ ¹æ®kernel.ldé“¾æŽ¥è„šæœ¬ï¼ˆlinker\nscriptï¼‰æ¥è®¾å®šã€‚ldå·¥å…·ä½¿ç”¨å‘½ä»¤-TæŒ‡å®šé“¾æŽ¥è„šæœ¬ã€‚é“¾æŽ¥è„šæœ¬ä¸»è¦ç”¨äºŽè§„å®šå¦‚ä½•æŠŠè¾“å…¥æ–‡ä»¶ï¼ˆå„ä¸ª.oæˆ–.aæ–‡ä»¶ï¼‰å†…çš„sectionæ”¾å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆlab2/bin/kernelï¼Œå³ELFæ ¼å¼çš„ucoreå†…æ ¸ï¼‰å†…ï¼Œ\nå¹¶æŽ§åˆ¶è¾“å‡ºæ–‡ä»¶å†…å„éƒ¨åˆ†åœ¨ç¨‹åºåœ°å€ç©ºé—´å†…çš„å¸ƒå±€ã€‚ä¸‹é¢ç®€å•åˆ†æžä¸€ä¸‹/lab2/tools/kernel.ldï¼Œæ¥äº†è§£ä¸€ä¸‹ucoreå†…æ ¸çš„åœ°å€å¸ƒå±€æƒ…å†µã€‚kernel.ldçš„å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š\n```\n/* Simple linker script for the ucore kernel.\n   See the GNU ld 'info' manual (\"info ld\") to learn the syntax. */\n\nOUTPUT_FORMAT(\"elf32-i386\", \"elf32-i386\", \"elf32-i386\")\nOUTPUT_ARCH(i386)\nENTRY(kern_entry)\n\nSECTIONS {\n    /* Load the kernel at this address: \".\" means the current address */\n    . = 0xC0100000;\n\n    .text : {\n        *(.text .stub .text.* .gnu.linkonce.t.*)\n    }\n\n    PROVIDE(etext = .); /* Define the 'etext' symbol to this value */\n\n    .rodata : {\n        *(.rodata .rodata.* .gnu.linkonce.r.*)\n    }\n\n    /* Include debugging information in kernel memory */\n    .stab : {\n        PROVIDE(__STAB_BEGIN__ = .);\n        *(.stab);\n        PROVIDE(__STAB_END__ = .);\n        BYTE(0)     /* Force the linker to allocate space\n                   for this section */\n    }\n\n    .stabstr : {\n        PROVIDE(__STABSTR_BEGIN__ = .);\n        *(.stabstr);\n        PROVIDE(__STABSTR_END__ = .);\n        BYTE(0)     /* Force the linker to allocate space\n                   for this section */\n    }\n\n    /* Adjust the address for the data segment to the next page */\n    . = ALIGN(0x1000);\n\n    /* The data segment */\n    .data : {\n        *(.data)\n    }\n\n    PROVIDE(edata = .);\n\n    .bss : {\n        *(.bss)\n    }\n\n    PROVIDE(end = .);\n\n    /DISCARD/ : {\n        *(.eh_frame .note.GNU-stack)\n    }\n}\n```\nå…¶å®žä»Žé“¾æŽ¥è„šæœ¬çš„å†…å®¹ï¼Œå¯ä»¥å¤§è‡´çŒœå‡ºå®ƒæŒ‡å®šå‘Šè¯‰é“¾æŽ¥å™¨çš„å„ç§ä¿¡æ¯ï¼š\n\n* å†…æ ¸åŠ è½½åœ°å€ï¼š0xC0100000\n* å…¥å£ï¼ˆèµ·å§‹ä»£ç ï¼‰åœ°å€ï¼š ENTRY(kern\\_entry)\n* cpuæœºå™¨ç±»åž‹ï¼ši386\n\nå…¶æœ€ä¸»è¦çš„ä¿¡æ¯æ˜¯å‘Šè¯‰é“¾æŽ¥å™¨å„è¾“å…¥æ–‡ä»¶çš„å„sectionåº”è¯¥æ€Žä¹ˆç»„åˆï¼šåº”è¯¥ä»Žå“ªä¸ªåœ°å€å¼€å§‹æ”¾ï¼Œå„ä¸ªsectionä»¥ä»€ä¹ˆé¡ºåºæ”¾ï¼Œåˆ†åˆ«æ€Žä¹ˆå¯¹é½ç­‰ç­‰ï¼Œæœ€ç»ˆç»„æˆè¾“å‡ºæ–‡ä»¶çš„å„sectionã€‚é™¤æ­¤ä¹‹å¤–ï¼Œlinker\nscriptè¿˜å¯ä»¥å®šä¹‰å„ç§ç¬¦å·ï¼ˆå¦‚.textã€.dataã€.bssç­‰ï¼‰ï¼Œå½¢æˆæœ€ç»ˆç”Ÿæˆçš„ä¸€å †ç¬¦å·çš„åˆ—è¡¨ï¼ˆç¬¦å·è¡¨ï¼‰ï¼Œæ¯ä¸ªç¬¦å·åŒ…å«äº†ç¬¦å·åå­—ï¼Œç¬¦å·æ‰€å¼•ç”¨çš„å†…å­˜åœ°å€ï¼Œä»¥åŠå…¶ä»–ä¸€äº›å±žæ€§ä¿¡æ¯ã€‚ç¬¦å·å®žé™…ä¸Šå°±æ˜¯ä¸€ä¸ªåœ°å€çš„ç¬¦å·è¡¨ç¤ºï¼Œå…¶æœ¬èº«ä¸å ç”¨çš„ç¨‹åºè¿è¡Œçš„å†…å­˜ç©ºé—´ã€‚\n\n**é“¾æŽ¥åœ°å€/åŠ è½½åœ°å€/è™šåœ°å€/ç‰©ç†åœ°å€**\n\nucore è®¾å®šäº†ucoreè¿è¡Œä¸­çš„è™šåœ°å€ç©ºé—´ï¼Œå…·ä½“è®¾ç½®å¯çœ‹\nlab2/kern/mm/memlayout.h ä¸­æè¿°çš„\"Virtual memory map\n\"å›¾ï¼Œå¯ä»¥äº†è§£è™šåœ°å€å’Œç‰©ç†åœ°å€çš„å¯¹åº”å…³ç³»ã€‚lab2/tools/kernel.ldæè¿°çš„æ˜¯æ‰§è¡Œä»£ç çš„é“¾æŽ¥åœ°å€ï¼ˆlink\\_addrï¼‰ï¼Œæ¯”å¦‚å†…æ ¸èµ·å§‹åœ°å€æ˜¯0xC0100000ï¼Œè¿™æ˜¯ä¸€ä¸ªè™šåœ°å€ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¤ä¸ºé“¾æŽ¥åœ°å€ç­‰äºŽè™šåœ°å€ã€‚åœ¨ucoreå»ºç«‹å†…æ ¸é¡µè¡¨æ—¶ï¼Œè®¾å®šäº†ç‰©ç†åœ°å€å’Œè™šåœ°å€çš„è™šå®žæ˜ å°„å…³ç³»æ˜¯ï¼š\n\nphy addr + 0xC0000000 = virtual addr\n\nå³è™šåœ°å€å’Œç‰©ç†åœ°å€ä¹‹é—´æœ‰ä¸€ä¸ªåç§»ã€‚ä½†boot loaderæŠŠucore\nkernelåŠ è½½åˆ°å†…å­˜æ—¶ï¼Œé‡‡ç”¨çš„æ˜¯åŠ è½½åœ°å€ï¼ˆload\naddrï¼‰ï¼Œè¿™æ˜¯ç”±äºŽucoreè¿˜æ²¡æœ‰è¿è¡Œï¼Œå³è¿˜æ²¡æœ‰å¯åŠ¨é¡µè¡¨æ˜ å°„ï¼Œå¯¼è‡´è¿™æ—¶é‡‡ç”¨çš„å¯»å€æ–¹å¼æ˜¯æ®µå¯»å€æ–¹å¼ï¼Œç”¨çš„æ˜¯boot\nloaderåœ¨åˆå§‹åŒ–é˜¶æ®µè®¾ç½®çš„æ®µæ˜ å°„å…³ç³»ï¼Œå…¶æ˜ å°„å…³ç³»ï¼ˆå¯å‚çœ‹bootasm.Sçš„æœ«å°¾å¤„æœ‰å…³æ®µæè¿°ç¬¦è¡¨çš„å†…å®¹ï¼‰æ˜¯ï¼š\n\nlinear addr = phy addr = virtual addr\n\næŸ¥çœ‹ bootloaderçš„å®žçŽ°ä»£ç  bootmain::bootmain.c\n\nreadseg(ph-\\>p\\_va & 0xFFFFFF, ph-\\>p\\_memsz, ph-\\>p\\_offset);\n\nè¿™é‡Œçš„ph-\\>p\\_va=0xC0XXXXXXï¼Œå°±æ˜¯ldå·¥å…·æ ¹æ®kernel.ldè®¾ç½®çš„é“¾æŽ¥åœ°å€ï¼Œä¸”é“¾æŽ¥åœ°å€ç­‰äºŽè™šåœ°å€ã€‚è€ƒè™‘åˆ°ph-\\>p\\_va\n& 0xFFFFFF == 0x0XXXXXXï¼Œæ‰€ä»¥bootloaderåŠ è½½ucore\nkernelçš„åŠ è½½åœ°å€æ˜¯0x0XXXXXX, è¿™å®žé™…ä¸Šæ˜¯ucoreå†…æ ¸æ‰€åœ¨çš„ç‰©ç†åœ°å€ã€‚ç®€è¨€ä¹‹ï¼š\nOSçš„é“¾æŽ¥åœ°å€ï¼ˆlink addrï¼‰ åœ¨tools/kernel.ldä¸­è®¾ç½®å¥½äº†ï¼Œæ˜¯ä¸€ä¸ªè™šåœ°å€ï¼ˆvirtual\naddrï¼‰ï¼›è€Œucore kernelçš„åŠ è½½åœ°å€ï¼ˆload addrï¼‰åœ¨boot\nloaderä¸­çš„bootmainå‡½æ•°ä¸­æŒ‡å®šï¼Œæ˜¯ä¸€ä¸ªç‰©ç†åœ°å€ã€‚\n\nå°ç»“ä¸€ä¸‹ï¼Œucoreå†…æ ¸çš„é“¾æŽ¥åœ°å€==ucoreå†…æ ¸çš„è™šæ‹Ÿåœ°å€ï¼›boot\nloaderåŠ è½½ucoreå†…æ ¸ç”¨åˆ°çš„åŠ è½½åœ°å€==ucoreå†…æ ¸çš„ç‰©ç†åœ°å€ã€‚\n\n**edata/end/textçš„å«ä¹‰**\n\nåœ¨åŸºäºŽELFæ‰§è¡Œæ–‡ä»¶æ ¼å¼çš„ä»£ç ä¸­ï¼Œå­˜åœ¨ä¸€äº›å¯¹ä»£ç å’Œæ•°æ®çš„è¡¨è¿°ï¼ŒåŸºæœ¬æ¦‚å¿µå¦‚ä¸‹ï¼š\n\n* BSSæ®µï¼ˆbss\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºä¸­æœªåˆå§‹åŒ–çš„å…¨å±€å˜é‡çš„å†…å­˜åŒºåŸŸã€‚BSSæ˜¯è‹±æ–‡Block\nStarted by Symbolçš„ç®€ç§°ã€‚BSSæ®µå±žäºŽé™æ€å†…å­˜åˆ†é…ã€‚\n* æ•°æ®æ®µï¼ˆdata\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºä¸­å·²åˆå§‹åŒ–çš„å…¨å±€å˜é‡çš„ä¸€å—å†…å­˜åŒºåŸŸã€‚æ•°æ®æ®µå±žäºŽé™æ€å†…å­˜åˆ†é…ã€‚\n* ä»£ç æ®µï¼ˆcode segment/text\nsegmentï¼‰ï¼šæŒ‡ç”¨æ¥å­˜æ”¾ç¨‹åºæ‰§è¡Œä»£ç çš„ä¸€å—å†…å­˜åŒºåŸŸã€‚è¿™éƒ¨åˆ†åŒºåŸŸçš„å¤§å°åœ¨ç¨‹åºè¿è¡Œå‰å°±å·²ç»ç¡®å®šï¼Œå¹¶ä¸”å†…å­˜åŒºåŸŸé€šå¸¸å±žäºŽåªè¯»,\næŸäº›æž¶æž„ä¹Ÿå…è®¸ä»£ç æ®µä¸ºå¯å†™ï¼Œå³å…è®¸ä¿®æ”¹ç¨‹åºã€‚åœ¨ä»£ç æ®µä¸­ï¼Œä¹Ÿæœ‰å¯èƒ½åŒ…å«ä¸€äº›åªè¯»çš„å¸¸æ•°å˜é‡ï¼Œä¾‹å¦‚å­—ç¬¦ä¸²å¸¸é‡ç­‰ã€‚\n\nåœ¨lab2/kern/init/init.cçš„kern\\_initå‡½æ•°ä¸­ï¼Œå£°æ˜Žäº†å¤–éƒ¨å…¨å±€å˜é‡ï¼š\n```\nextern char edata[], end[];\n```\nä½†æœå¯»æ‰€æœ‰æºç æ–‡ä»¶\\*.[ch]ï¼Œæ²¡æœ‰å‘çŽ°æœ‰è¿™ä¸¤ä¸ªå˜é‡çš„å®šä¹‰ã€‚é‚£è¿™ä¸¤ä¸ªå˜é‡ä»Žå“ªé‡Œæ¥çš„å‘¢ï¼Ÿå…¶å®žåœ¨lab2/tools/kernel.ldä¸­ï¼Œå¯ä»¥çœ‹åˆ°å¦‚ä¸‹å†…å®¹ï¼š\n```\nâ€¦\n.text : {\n        *(.text .stub .text.* .gnu.linkonce.t.*)\n}\nâ€¦\n    .data : {\n        *(.data)\n}\nâ€¦\nPROVIDE(edata = .);\nâ€¦\n    .bss : {\n        *(.bss)\n}\nâ€¦\nPROVIDE(end = .);\nâ€¦\n```\nè¿™é‡Œçš„â€œ.â€è¡¨ç¤ºå½“å‰åœ°å€ï¼Œâ€œ.textâ€è¡¨ç¤ºä»£ç æ®µèµ·å§‹åœ°å€ï¼Œâ€œ.dataâ€ä¹Ÿæ˜¯ä¸€ä¸ªåœ°å€ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®ƒå³ä»£è¡¨äº†ä»£ç æ®µçš„ç»“æŸåœ°å€ï¼Œä¹Ÿæ˜¯æ•°æ®æ®µçš„èµ·å§‹åœ°å€ã€‚ç±»æŽ¨ä¸‹åŽ»ï¼Œâ€œedataâ€è¡¨ç¤ºæ•°æ®æ®µçš„ç»“æŸåœ°å€ï¼Œâ€œ.bssâ€è¡¨ç¤ºæ•°æ®æ®µçš„ç»“æŸåœ°å€å’ŒBSSæ®µçš„èµ·å§‹åœ°å€ï¼Œè€Œâ€œendâ€è¡¨ç¤ºBSSæ®µçš„ç»“æŸåœ°å€ã€‚\n\nè¿™æ ·å›žå¤´çœ‹kerne\\_initä¸­çš„å¤–éƒ¨å…¨å±€å˜é‡ï¼Œå¯çŸ¥edata[]å’Œ\nend[]è¿™äº›å˜é‡æ˜¯ldæ ¹æ®kernel.ldé“¾æŽ¥è„šæœ¬ç”Ÿæˆçš„å…¨å±€å˜é‡ï¼Œè¡¨ç¤ºç›¸åº”æ®µçš„èµ·å§‹åœ°å€æˆ–ç»“æŸåœ°å€ç­‰ï¼Œå®ƒä»¬ä¸åœ¨ä»»ä½•ä¸€ä¸ª.Sã€.cæˆ–.hæ–‡ä»¶ä¸­å®šä¹‰ã€‚\n\n\n\n",
          "# Summary\n\n* [Introduction](README.md)\n\n## Lab 0\n\n* [Lab 0](lab0.md)\n  * [å®žéªŒç›®çš„](lab0/lab0_1_goals.md)\n  * [å‡†å¤‡çŸ¥è¯†](lab0/lab0_2_prepare.md)\n    * [äº†è§£OSå®žéªŒ](lab0/lab0_2_1_about_labs.md)\n    * [è®¾ç½®å®žéªŒçŽ¯å¢ƒ](lab0/lab0_2_2_environment.md)\n      * [å¼€å‘OSå®žéªŒçš„ç®€å•æ­¥éª¤](lab0/lab0_2_2_1_lab_steps.md)\n      * [é€šè¿‡è™šæ‹Ÿæœºä½¿ç”¨Linuxå®žéªŒçŽ¯å¢ƒï¼ˆæŽ¨èï¼šæœ€å®¹æ˜“çš„å®žéªŒçŽ¯å¢ƒå®‰è£…æ–¹æ³•ï¼‰](lab0/lab0_2_2_2_vm_experiment.md)\n      * [å®‰è£…ä½¿ç”¨Linuxå®žéªŒçŽ¯å¢ƒï¼ˆé€‚åˆå¸Œæœ›è‡ªå·±å®‰è£…Linuxç³»ç»Ÿçš„åŒå­¦ï¼‰](lab0/lab0_2_2_3_install.md)\n      * [å®žéªŒä¸­å¯èƒ½ä½¿ç”¨çš„è½¯ä»¶](lab0/lab0_2_2_3_1_softwares.md)\n    * [äº†è§£ç¼–ç¨‹å¼€å‘è°ƒè¯•çš„åŸºæœ¬å·¥å…·](lab0/lab0_2_3_tools.md)\n      * [gccçš„åŸºæœ¬ç”¨æ³•](lab0/lab0_2_3_1_gcc_usage.md)\n        * [ç¼–è¯‘ç®€å•çš„ C ç¨‹åº](lab0/lab0_2_3_1_1_compile_c_prog.md)\n        * [AT&Tæ±‡ç¼–åŸºæœ¬è¯­æ³•](lab0/lab0_2_3_1_2_att_asm.md)\n        * [GCCåŸºæœ¬å†…è”æ±‡ç¼–](lab0/lab0_2_3_1_3_gcc_inline_asm.md)\n        * [GCCæ‰©å±•å†…è”æ±‡ç¼–](lab0/lab0_2_3_1_4_extend_gcc_asm.md)\n      * [makeå’ŒMakefile](lab0/lab0_2_3_2_make_makefile.md)\n      * [gdbä½¿ç”¨](lab0/lab0_2_3_3_gdb.md)\n      * [è¿›ä¸€æ­¥çš„ç›¸å…³å†…å®¹](lab0/lab0_2_3_4_further.md)\n    * [åŸºäºŽç¡¬ä»¶æ¨¡æ‹Ÿå™¨å®žçŽ°æºç çº§è°ƒè¯•](lab0/lab0_2_4_debug_with_emulator.md)\n      * [å®‰è£…ç¡¬ä»¶æ¨¡æ‹Ÿå™¨QEMU](lab0/lab0_2_4_1_install_qemu.md)\n        * [Linuxè¿è¡ŒçŽ¯å¢ƒ](lab0/lab0_2_4_1_1_linux_runtime.md)\n        * [LinuxçŽ¯å¢ƒä¸‹çš„æºç çº§å®‰è£…è¿‡ç¨‹](lab0/lab0_2_4_1_2_linux_source_install.md)\n          * [èŽ·å¾—å¹¶åº”ç”¨ä¿®æ”¹](lab0/lab0_2_4_1_2_1_patch_qemu.md)\n          * [é…ç½®ã€ç¼–è¯‘å’Œå®‰è£…](lab0/lab0_2_4_1_2_2_configure_make_install_qemu.md)\n      * [ä½¿ç”¨ç¡¬ä»¶æ¨¡æ‹Ÿå™¨QEMU](lab0/lab0_2_4_2_qemu_usage.md)\n        * [è¿è¡Œå‚æ•°](lab0/lab0_2_4_2_1_qemu_runtime_arguments.md)\n        * [å¸¸ç”¨è°ƒè¯•å‘½ä»¤](lab0/lab0_2_4_2_2_qemu_monitor_debug.md)\n      * [åŸºäºŽqemuå†…å»ºæ¨¡å¼è°ƒè¯•ucore]\n      * [ç»“åˆgdbå’Œqemuæºç çº§è°ƒè¯•ucore](lab0/lab0_2_4_4_gdb_qemu_debug_ucore.md)\n        * [ç¼–è¯‘å¯è°ƒè¯•çš„ç›®æ ‡æ–‡ä»¶](lab0/lab0_2_4_4_1_make_obj.md)\n        * [ucore ä»£ç ç¼–è¯‘](lab0/lab0_2_4_4_2_ucore_make.md)\n        * [ä½¿ç”¨è¿œç¨‹è°ƒè¯•](lab0/lab0_2_4_4_3_remote_debug.md)\n        * [ä½¿ç”¨gdbé…ç½®æ–‡ä»¶](lab0/lab0_2_4_4_4_gdb_config_file.md)\n        * [åŠ è½½è°ƒè¯•ç›®æ ‡](lab0/lab0_2_4_4_5_load_debug_target.md)\n        * [è®¾å®šè°ƒè¯•ç›®æ ‡æž¶æž„](lab0/lab0_2_4_4_6_set_debug_arch.md)\n    * [äº†è§£å¤„ç†å™¨ç¡¬ä»¶](lab0/lab0_2_5_cpu_hardware.md)\n      * [Intel 80386è¿è¡Œæ¨¡å¼](lab0/lab0_2_5_1_intel_80386_modes.md)\n      * [Intel 80386å†…å­˜æž¶æž„](lab0/lab0_2_5_2_intel_80386_mem.md)\n      * [Intel 80386å¯„å­˜å™¨](lab0/lab0_2_5_3_intel_80386_registers.md)\n    * [äº†è§£ucoreç¼–ç¨‹æ–¹æ³•å’Œé€šç”¨æ•°æ®ç»“æž„](lab0/lab0_2_6_ucore_programming.md)\n      * [é¢å‘å¯¹è±¡ç¼–ç¨‹æ–¹æ³•](lab0/lab0_2_6_1_oop.md)\n      * [é€šç”¨æ•°æ®ç»“æž„åŒå‘å¾ªçŽ¯é“¾è¡¨](lab0/lab0_2_6_2_generic_data_structure.md)\n        * [åŒå‘å¾ªçŽ¯é“¾è¡¨](lab0/lab0_2_6_2_1_linked_list.md)\n  * [é™„å½•A.ucoreå®žéªŒä¸­çš„å¸¸ç”¨å·¥å…·](lab0/lab0_ref_ucore-tools.md)\n  * [é™„å½•B.ucoreå®žéªŒå‚è€ƒèµ„æ–™](lab0/lab0_ref_ucore-resource.md)\n\n## Lab 1\n\n* [Lab 1](lab1.md)\n  * [å®žéªŒç›®çš„](lab1/lab1_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab1/lab1_2_labs.md)\n    * [ç»ƒä¹ ](lab1/lab1_2_1_exercise.md)\n      * [ç»ƒä¹ 1](lab1/lab1_2_1_1_ex1.md)\n      * [ç»ƒä¹ 2](lab1/lab1_2_1_2_ex2.md)\n      * [ç»ƒä¹ 3](lab1/lab1_2_1_3_ex3.md)\n      * [ç»ƒä¹ 4](lab1/lab1_2_1_4_ex4.md)\n      * [ç»ƒä¹ 5](lab1/lab1_2_1_5_ex5.md)\n      * [ç»ƒä¹ 6](lab1/lab1_2_1_6_ex6.md)\n      * [æ‰©å±•ç»ƒä¹ ](lab1/lab1_2_1_7_ex7.md)\n    * [é¡¹ç›®ç»„æˆ](lab1/lab1_2_2_files.md)\n  * [ä»Žæœºå™¨å¯åŠ¨åˆ°æ“ä½œç³»ç»Ÿè¿è¡Œçš„è¿‡ç¨‹](lab1/lab1_3_booting.md)\n    * [BIOSå¯åŠ¨è¿‡ç¨‹](lab1/lab1_3_1_bios_booting.md)\n    * [bootloaderå¯åŠ¨è¿‡ç¨‹](lab1/lab1_3_2_bootloader.md)\n      * [ä¿æŠ¤æ¨¡å¼å’Œåˆ†æ®µæœºåˆ¶](lab1/lab1_3_2_1_protection_mode.md)\n      * [åœ°å€ç©ºé—´](lab1/lab1_3_2_2_address_space.md)\n      * [ç¡¬ç›˜è®¿é—®æ¦‚è¿°](lab1/lab1_3_2_3_dist_accessing.md)\n      * [ELFæ–‡ä»¶æ ¼å¼æ¦‚è¿°](lab1/lab1_3_2_4_elf.md)\n    * [æ“ä½œç³»ç»Ÿå¯åŠ¨è¿‡ç¨‹](lab1/lab1_3_3_booting_os.md)\n      * [å‡½æ•°å †æ ˆ](lab1/lab1_3_3_1_function_stack.md)\n      * [ä¸­æ–­ä¸Žå¼‚å¸¸](lab1/lab1_3_3_2_interrupt_exception.md)\n      * [lab1ä¸­å¯¹ä¸­æ–­çš„å¤„ç†å®žçŽ°](lab1/lab1_3_3_3_lab1_interrupt.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab1/lab1_4_lab_requirement.md)\n  * [é™„å½•Aâ€œå…³äºŽA20 Gateâ€](lab1/lab1_appendix_a20.md)\n  * [é™„å½•Bâ€œç¬¬ä¸€æ¡æ‰§è¡ŒæŒ‡ä»¤â€](lab1/lab1_5_appendix.md)\n\n## Lab 2\n\n* [Lab 2 ç‰©ç†å†…å­˜ç®¡ç†](lab2.md)\n  * [å®žéªŒç›®çš„](lab2/lab2_3_1_phymemlab_goal.md)\n  * [å®žéªŒå†…å®¹](lab2/lab2_3_2_phymemlab_contents.md)\n    * [ç»ƒä¹ ](lab2/lab2_3_2_1_phymemlab_exercise.md)\n    * [é¡¹ç›®ç»„æˆ](lab2/lab2_3_2_2_phymemlab_files.md)\n  * [ç‰©ç†å†…å­˜ç®¡ç†](lab2/lab2_3_3_phymem_manage.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab2/lab2_3_3_1_phymemlab_overview.md)\n    * [æŽ¢æµ‹ç³»ç»Ÿç‰©ç†å†…å­˜å¸ƒå±€](lab2/lab2_3_3_2_search_phymem_layout.md)\n    * [ä»¥é¡µä¸ºå•ä½ç®¡ç†ç‰©ç†å†…å­˜](lab2/lab2_3_3_3_phymem_pagelevel.md)\n    * [ç‰©ç†å†…å­˜é¡µåˆ†é…ç®—æ³•å®žçŽ°](lab2/lab2_3_3_4_phymem_allocation.md)\n    * [å®žçŽ°åˆ†é¡µæœºåˆ¶](lab2/lab2_3_3_5_paging.md)\n      * [æ®µé¡µå¼ç®¡ç†åŸºæœ¬æ¦‚å¿µ](lab2/lab2_3_3_5_1_segment_and_paging.md)\n      * [å»ºç«‹æ®µé¡µå¼ç®¡ç†ä¸­éœ€è¦è€ƒè™‘çš„å…³é”®é—®é¢˜](lab2/lab2_3_3_5_2_key_problems_in_seg_page.md)\n      * [ç³»ç»Ÿæ‰§è¡Œä¸­åœ°å€æ˜ å°„çš„å››ä¸ªé˜¶æ®µ](lab2/lab2_3_3_5_4_maping_relations.md)\n      * [å»ºç«‹è™šæ‹Ÿé¡µå’Œç‰©ç†é¡µå¸§çš„åœ°å€æ˜ å°„å…³ç³»](lab2/lab2_3_3_5_3_setup_paging_map.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab2/lab2_3_4_phymemlab_require.md)\n  * [é™„å½•A. æŽ¢æµ‹ç‰©ç†å†…å­˜åˆ†å¸ƒå’Œå¤§å°çš„æ–¹æ³•](lab2/lab2_3_5_probe_phymem_methods.md)\n  * [é™„å½•B. å®žçŽ°ç‰©ç†å†…å­˜æŽ¢æµ‹](lab2/lab2_3_6_implement_probe_phymem.md)\n  * [é™„å½•C. é“¾æŽ¥åœ°å€/è™šåœ°å€/ç‰©ç†åœ°å€/åŠ è½½åœ°å€ä»¥åŠedata/end/textçš„å«ä¹‰](lab2/lab2_3_7_phymemlab_concepts.md)\n  * [é™„å½•D. è‡ªæ˜ å°„æœºåˆ¶](lab2/lab2_3_3_6_self_mapping.md)\n\n## Lab 3\n\n* [Lab 3](lab3.md)\n  * [å®žéªŒç›®çš„](lab3/lab3_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab3/lab3_2_lab2.md)\n    * [ç»ƒä¹ ](lab3/lab3_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab3/lab3_2_2_files.md)\n  * [è™šæ‹Ÿå†…å­˜ç®¡ç†](lab3/lab3_3_vmm.md)\n    * [åŸºæœ¬åŽŸç†æ¦‚è¿°](lab3/lab3_3_1_vmm_principles.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab3/lab3_3_2_labs_steps.md)\n    * [å…³é”®æ•°æ®ç»“æž„å’Œç›¸å…³å‡½æ•°åˆ†æž](lab3/lab3_3_3_data_structures.md)\n  * [Page Faultå¼‚å¸¸å¤„ç†](lab3/lab3_4_page_fault_handler.md)\n  * [é¡µé¢ç½®æ¢æœºåˆ¶çš„å®žçŽ°](lab3/lab3_5_swapping.md)\n    * [é¡µæ›¿æ¢ç®—æ³•](lab3/lab3_5_1_page_swapping.md)\n    * [é¡µé¢ç½®æ¢æœºåˆ¶å®žéªŒæŠ¥å‘Šè¦æ±‚](lab3/lab3_5_2_page_swapping_principles.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab3/lab3_6_labs_requirement.md)\n\n## Lab 4\n\n* [Lab 4](lab4.md)\n  * [å®žéªŒç›®çš„](lab4/lab4_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab4/lab4_2_labs.md)\n    * [ç»ƒä¹ ](lab4/lab4_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab4/lab4_2_2_files.md)\n  * [å†…æ ¸çº¿ç¨‹ç®¡ç†](lab4/lab4_3_kernel_thread_management.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab4/lab4_3_1_lab_steps.md)\n    * [è®¾è®¡å…³é”®æ•°æ®ç»“æž„ -- è¿›ç¨‹æŽ§åˆ¶å—](lab4/lab4_3_2_pcb.md)\n    * [åˆ›å»ºå¹¶æ‰§è¡Œå†…æ ¸çº¿ç¨‹](lab4/lab4_3_3_create_exec_kernel_thread.md)\n      * [åˆ›å»ºç¬¬0ä¸ªå†…æ ¸çº¿ç¨‹idleproc](lab4/lab4_3_3_1_create_kthread_idleproc.md)\n      * [åˆ›å»ºç¬¬1ä¸ªå†…æ ¸çº¿ç¨‹initproc](lab4/lab4_3_3_2_create_kthread_initproc.md)\n      * [è°ƒåº¦å¹¶æ‰§è¡Œå†…æ ¸çº¿ç¨‹initproc](lab4/lab4_3_3_3_sched_run_kthread.md)          \n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab4/lab4_4_labs_requirement.md)\n  * [é™„å½•Aï¼šå®žéªŒå››çš„å‚è€ƒè¾“å‡º](lab4/lab4_5_appendix_a.md)\n  * [é™„å½•Bï¼šã€åŽŸç†ã€‘è¿›ç¨‹çš„å±žæ€§ä¸Žç‰¹å¾è§£æž](lab4/lab4_6_appendix_b.md)\n\n## Lab 5\n\n* [Lab 5](lab5.md)\n  * [å®žéªŒç›®çš„](lab5/lab5_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab5/lab5_2_lab2.md)\n    * [ç»ƒä¹ ](lab5/lab5_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab5/lab5_2_2_files.md)\n  * [ç”¨æˆ·è¿›ç¨‹ç®¡ç†](lab5/lab5_3_user_process.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab5/lab5_3_1_lab_steps.md)\n    * [åˆ›å»ºç”¨æˆ·è¿›ç¨‹](lab5/lab5_3_2_create_user_process.md)\n    * [è¿›ç¨‹é€€å‡ºå’Œç­‰å¾…è¿›ç¨‹](lab5/lab5_3_3_process_exit_wait.md)\n    * [ç³»ç»Ÿè°ƒç”¨å®žçŽ°](lab5/lab5_3_4_syscall.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab5/lab5_4_lab_requirement.md)\n  * [é™„å½• Aï¼šã€åŽŸç†ã€‘ç”¨æˆ·è¿›ç¨‹çš„ç‰¹å¾](lab5/lab5_5_appendix.md)\n\n## Lab 6\n\n* [Lab 6](lab6.md)\n  * [å®žéªŒç›®çš„](lab6/lab6_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab6/lab6_2_labs.md)\n    * [ç»ƒä¹ ](lab6/lab6_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab6/lab6_2_2_files.md)\n  * [è°ƒåº¦æ¡†æž¶å’Œè°ƒåº¦ç®—æ³•è®¾è®¡ä¸Žå®žçŽ°](lab6/lab6_3_scheduler_design.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab6/lab6_3_1_exercises.md)\n    * [è¿›ç¨‹çŠ¶æ€](lab6/lab6_3_3_process_state.md)\n    * [è¿›ç¨‹è°ƒåº¦å®žçŽ°](lab6/lab6_3_4_process_implement.md)\n      * [å†…æ ¸æŠ¢å ç‚¹](lab6/lab6_3_4_1_kernel_preempt_point.md)\n      * [è¿›ç¨‹åˆ‡æ¢è¿‡ç¨‹](lab6/lab6_3_4_2_process_switch.md)\n    * [è°ƒåº¦æ¡†æž¶å’Œè°ƒåº¦ç®—æ³•](lab6/lab6_3_5_scheduler_framework.md)\n      * [è®¾è®¡æ€è·¯](lab6/lab6_3_5_1_designed.md)\n      * [æ•°æ®ç»“æž„](lab6/lab6_3_5_2_data_structure.md)\n      * [è°ƒåº¦ç‚¹çš„ç›¸å…³å…³é”®å‡½æ•°](lab6/lab6_3_5_3_scheduler_point_functions.md)\n      * [RR è°ƒåº¦ç®—æ³•å®žçŽ°](lab6/lab6_3_5_4_RR.md)\n    * [Stride Scheduling](lab6/lab6_3_6_stride_scheduling.md)\n      * [åŸºæœ¬æ€è·¯](lab6/lab6_3_6_1_basic_method.md)\n      * [ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—å®žçŽ° Stride Scheduling](lab6/lab6_3_6_2_priority_queue.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab6/lab6_4_labs_requirement.md)\n\n## Lab 7\n\n* [Lab 7](lab7.md)\n  * [å®žéªŒç›®çš„](lab7/lab7_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab7/lab7_2_labs.md)\n    * [ç»ƒä¹ ](lab7/lab7_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab7/lab7_2_2_files.md)\n  * [åŒæ­¥äº’æ–¥æœºåˆ¶çš„è®¾è®¡ä¸Žå®žçŽ°](lab7/lab7_3_synchronization_implement.md)\n    * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab7/lab7_3_1_experiment.md)\n    * [åŒæ­¥äº’æ–¥æœºåˆ¶çš„åº•å±‚æ”¯æ’‘](lab7/lab7_3_2_synchronization_basic_support.md)\n      * [è®¡æ—¶å™¨](lab7/lab7_3_2_1_timer.md)\n      * [å±è”½ä¸Žä½¿èƒ½ä¸­æ–­](lab7/lab7_3_2_2_interrupt.md)\n      * [ç­‰å¾…é˜Ÿåˆ—](lab7/lab7_3_2_3_waitqueue.md)\n    * [ä¿¡å·é‡](lab7/lab7_3_3_semaphore.md)\n    * [ç®¡ç¨‹å’Œæ¡ä»¶å˜é‡](lab7/lab7_3_4_monitors.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab7/lab7_4_lab_requirement.md)\n  * [é™„å½•](lab7/lab7_5_appendix.md)\n\n## Lab 8\n\n* [Lab 8](lab8.md)\n  * [å®žéªŒç›®çš„](lab8/lab8_1_goals.md)\n  * [å®žéªŒå†…å®¹](lab8/lab8_2_labs.md)\n    * [ç»ƒä¹ ](lab8/lab8_2_1_exercises.md)\n    * [é¡¹ç›®ç»„æˆ](lab8/lab8_2_2_files.md)\n  * [æ–‡ä»¶ç³»ç»Ÿè®¾è®¡ä¸Žå®žçŽ°](lab8/lab8_3_fs_design_implement.md)\n    * [ucore æ–‡ä»¶ç³»ç»Ÿæ€»ä½“ä»‹ç»](lab8/lab8_3_1_ucore_fs_introduction.md)\n    * [é€šç”¨æ–‡ä»¶ç³»ç»Ÿè®¿é—®æŽ¥å£](lab8/lab8_3_2_fs_interface.md)\n    * [æ–‡ä»¶ç³»ç»ŸæŠ½è±¡å±‚ - VFS](lab8/lab8_3_4_fs_abstract.md)\n      * [file & diræŽ¥å£](lab8/lab8_3_4_1_file_dir_interface.md)\n      * [inode æŽ¥å£ ](lab8/lab8_3_4_2_inode_interface.md)\n    * [Simple FS æ–‡ä»¶ç³»ç»Ÿ](lab8/lab8_3_3_sfs.md)\n      * [æ–‡ä»¶ç³»ç»Ÿçš„å¸ƒå±€](lab8/lab8_3_3_1_fs_layout.md)\n      * [ç´¢å¼•èŠ‚ç‚¹](lab8/lab8_3_3_2_inode.md)\n  * [è®¾å¤‡å±‚æ–‡ä»¶ IO å±‚](lab8/lab8_3_5_dev_file_io_layer.md)\n    * [å…³é”®æ•°æ®ç»“æž„](lab8/lab8_3_5_1_data_structure.md)\n    * [stdoutè®¾å¤‡æ–‡ä»¶](lab8/lab8_3_5_2_stdout_dev_file.md)\n    * [stdin è®¾å¤‡æ–‡ä»¶](lab8/lab8_3_5_3_stdin_dev_file.md)\n  * [å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°](lab8/lab8_3_6_labs_steps.md)\n  * [æ–‡ä»¶æ“ä½œå®žçŽ°](lab8/lab8_3_7_file_op_implement.md)\n    * [æ‰“å¼€æ–‡ä»¶](lab8/lab8_3_7_1_file_open.md)\n    * [è¯»æ–‡ä»¶](lab8/lab8_3_7_2_file_read.md)\n  * [å®žéªŒæŠ¥å‘Šè¦æ±‚](lab8/lab8_4_lab_requirement.md)\n\n",
          "### é¡¹ç›®ç»„æˆ\n\nè¡¨1ï¼šå®žéªŒä¸‰æ–‡ä»¶åˆ—è¡¨\n\n```\n|-- boot\n|-- kern\n| |-- driver\n| | |-- â€¦\n| | |-- ide.c\n| | \\`-- ide.h\n| |-- fs\n| | |-- fs.h\n| | |-- swapfs.c\n| | \\`-- swapfs.h\n| |-- init\n| | |-- â€¦\n| | \\`-- init.c\n| |-- mm\n| | |-- default\\_pmm.c\n| | |-- default\\_pmm.h\n| | |-- memlayout.h\n| | |-- mmu.h\n| | |-- pmm.c\n| | |-- pmm.h\n| | |-- swap.c\n| | |-- swap.h\n| | |-- swap\\_fifo.c\n| | |-- swap\\_fifo.h\n| | |-- vmm.c\n| | \\`-- vmm.h\n| |-- sync\n| \\`-- trap\n| |-- trap.c\n| \\`-- â€¦\n|-- libs\n| |-- list.h\n| \\`-- â€¦\n\\`-- tools\n```\n\nç›¸å¯¹ä¸Žå®žéªŒäºŒï¼Œå®žéªŒä¸‰ä¸»è¦æ”¹åŠ¨å¦‚ä¸‹ï¼š\n\n* kern/mm/default\\_pmm.[ch]ï¼šå®žçŽ°åŸºäºŽstruct pmm\\_managerç±»æ¡†æž¶çš„Fist-Fitç‰©ç†å†…å­˜åˆ†é…å‚è€ƒå®žçŽ°ï¼ˆåˆ†é…æœ€å°å•ä½ä¸ºé¡µï¼Œå³4096å­—èŠ‚ï¼‰ï¼Œç›¸å…³åˆ†é…é¡µå’Œé‡Šæ”¾é¡µç­‰å®žçŽ°ä¼šé—´æŽ¥è¢«kmalloc/kfreeç­‰å‡½æ•°ä½¿ç”¨ã€‚\n* kern/mm/pmm.[ch]ï¼špmm.hå®šä¹‰ç‰©ç†å†…å­˜åˆ†é…ç±»æ¡†æž¶struct pmm\\_managerã€‚pmm.cåŒ…å«äº†å¯¹æ­¤ç‰©ç†å†…å­˜åˆ†é…ç±»æ¡†æž¶çš„è®¿é—®ï¼Œä»¥åŠä¸Žå»ºç«‹ã€ä¿®æ”¹ã€è®¿é—®é¡µè¡¨ç›¸å…³çš„å„ç§å‡½æ•°å®žçŽ°ã€‚åœ¨æœ¬å®žéªŒä¸­ä¼šç”¨åˆ°kmalloc/kfreeç­‰å‡½æ•°ã€‚\n* libs/list.hï¼šå®šä¹‰äº†é€šç”¨åŒå‘é“¾è¡¨ç»“æž„ä»¥åŠç›¸å…³çš„æŸ¥æ‰¾ã€æ’å…¥ç­‰åŸºæœ¬æ“ä½œï¼Œè¿™æ˜¯å»ºç«‹åŸºäºŽé“¾è¡¨æ–¹æ³•çš„ç‰©ç†å†…å­˜ç®¡ç†ï¼ˆä»¥åŠå…¶ä»–å†…æ ¸åŠŸèƒ½ï¼‰çš„åŸºç¡€ã€‚åœ¨lab0æ–‡æ¡£ä¸­æœ‰ç›¸å…³æè¿°ã€‚å…¶ä»–æœ‰ç±»ä¼¼åŒå‘é“¾è¡¨éœ€æ±‚çš„å†…æ ¸åŠŸèƒ½æ¨¡å—å¯ç›´æŽ¥ä½¿ç”¨list.hä¸­å®šä¹‰çš„å‡½æ•°ã€‚åœ¨æœ¬å®žéªŒä¸­ä¼šå¤šæ¬¡ç”¨åˆ°æ’å…¥ï¼Œåˆ é™¤ç­‰æ“ä½œå‡½æ•°ã€‚\n* kern/driver/ide.[ch]ï¼šå®šä¹‰å’Œå®žçŽ°äº†å†…å­˜é¡µswapæœºåˆ¶æ‰€éœ€çš„ç£ç›˜æ‰‡åŒºçš„è¯»å†™æ“ä½œæ”¯æŒï¼›åœ¨æœ¬å®žéªŒä¸­ä¼šæ¶‰åŠé€šè¿‡swapfs\\_\\*å‡½æ•°é—´æŽ¥ä½¿ç”¨æ–‡ä»¶ä¸­çš„å‡½æ•°ã€‚æ•…äº†è§£å³å¯ã€‚\n* kern/fs/\\*ï¼šå®šä¹‰å’Œå®žçŽ°äº†å†…å­˜é¡µswapæœºåˆ¶æ‰€éœ€ä»Žç£ç›˜è¯»æ•°æ®åˆ°å†…å­˜é¡µå’Œå†™å†…å­˜æ•°æ®åˆ°ç£ç›˜ä¸ŠåŽ»çš„å‡½æ•° swapfs\\_read/swapfs\\_writeã€‚åœ¨æœ¬å®žéªŒä¸­ä¼šæ¶‰åŠä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°ã€‚\n* kern/mm/memlayout.hï¼šä¿®æ”¹äº†struct Pageï¼Œå¢žåŠ äº†ä¸¤é¡¹pra\\_\\*æˆå‘˜ç»“æž„ï¼Œå…¶ä¸­pra\\_page\\_linkå¯ä»¥ç”¨æ¥å»ºç«‹æè¿°å„ä¸ªé¡µè®¿é—®æƒ…å†µï¼ˆæ¯”å¦‚æ ¹æ®è®¿é—®å…ˆåŽï¼‰çš„é“¾è¡¨ã€‚åœ¨æœ¬å®žéªŒä¸­ä¼šæ¶‰åŠä½¿ç”¨è¿™ä¸¤ä¸ªæˆå‘˜ç»“æž„ï¼Œä»¥åŠle2pageç­‰å®ã€‚\n* kern/mm/vmm.[ch]ï¼švmm.hæè¿°äº†mm\\_structï¼Œvma\\_structç­‰è¡¨è¿°å¯è®¿é—®çš„è™šå­˜åœ°å€è®¿é—®çš„ä¸€äº›ä¿¡æ¯ï¼Œä¸‹é¢ä¼šè¿›ä¸€æ­¥è¯¦ç»†è®²è§£ã€‚vmm.cæ¶‰åŠmm,vmaç»“æž„æ•°æ®çš„åˆ›å»º/é”€æ¯/æŸ¥æ‰¾/æ’å…¥ç­‰å‡½æ•°ï¼Œè¿™äº›å‡½æ•°åœ¨check\\_vmaã€check\\_vmmç­‰ä¸­è¢«ä½¿ç”¨ï¼Œç†è§£å³å¯ã€‚è€Œpage\nfaultå¤„ç†ç›¸å…³çš„do\\_pgfaultå‡½æ•°æ˜¯æœ¬æ¬¡å®žéªŒéœ€è¦æ¶‰åŠå®Œæˆçš„ã€‚\n* kern/mm/swap.[ch]ï¼šå®šä¹‰äº†å®žçŽ°é¡µæ›¿æ¢ç®—æ³•ç±»æ¡†æž¶struct swap\\_managerã€‚swap.cåŒ…å«äº†å¯¹æ­¤é¡µæ›¿æ¢ç®—æ³•ç±»æ¡†æž¶çš„åˆå§‹åŒ–ã€é¡µæ¢å…¥/æ¢å‡ºç­‰å„ç§å‡½æ•°å®žçŽ°ã€‚é‡ç‚¹æ˜¯è¦ç†è§£ä½•æ—¶è°ƒç”¨swap\\_outå’Œswap\\_inå‡½æ•°ã€‚å’Œå¦‚ä½•åœ¨æ­¤æ¡†æž¶ä¸‹è¿žæŽ¥å…·ä½“çš„é¡µæ›¿æ¢ç®—æ³•å®žçŽ°ã€‚check\\_swapå‡½æ•°ä»¥åŠè¢«æ­¤å‡½æ•°è°ƒç”¨çš„\\_fifo\\_check\\_swapå‡½æ•°å®Œæˆäº†å¯¹æœ¬æ¬¡å®žéªŒä¸­çš„ç»ƒä¹ 2ï¼šFIFOé¡µæ›¿æ¢ç®—æ³•åŸºæœ¬æ­£ç¡®æ€§çš„æ£€æŸ¥ï¼Œå¯äº†è§£ï¼Œä¾¿äºŽçŸ¥é“ä¸ºä½•äº§ç”Ÿé”™è¯¯ã€‚\n* kern/mm/swap\\_fifo.[ch]ï¼šFIFOé¡µæ›¿æ¢ç®—æ³•çš„åŸºäºŽé¡µæ›¿æ¢ç®—æ³•ç±»æ¡†æž¶struct swap\\_managerçš„ç®€åŒ–å®žçŽ°ï¼Œä¸»è¦è¢«swap.cçš„ç›¸å…³å‡½æ•°è°ƒç”¨ã€‚é‡ç‚¹æ˜¯\\_fifo\\_map\\_swappableå‡½æ•°ï¼ˆå¯ç”¨äºŽå»ºç«‹é¡µè®¿é—®å±žæ€§å’Œå…³ç³»ï¼Œæ¯”å¦‚è®¿é—®æ—¶é—´çš„å…ˆåŽé¡ºåºï¼‰å’Œ\\_fifo\\_swap\\_out\\_victimå‡½æ•°ï¼ˆå¯ç”¨äºŽå®žçŽ°æŒ‘é€‰å‡ºè¦æ¢å‡ºçš„é¡µï¼‰ï¼Œå½“ç„¶æ¢å‡ºå“ªä¸ªé¡µéœ€è¦å€ŸåŠ©äºŽfifo\\_map\\_swappableå‡½æ•°å»ºç«‹çš„æŸç§å±žæ€§å…³ç³»ï¼Œå·²é€‰å‡ºåˆé€‚çš„é¡µã€‚\n* kern/mm/mmu.hï¼šå…¶ä¸­å®šä¹‰é¢ä¹Ÿé¡µè¡¨é¡¹çš„å„ç§å±žæ€§ä½ï¼Œæ¯”å¦‚PTE\\_P\\\\PET\\_D\\\\PET\\_Aç­‰ï¼Œå¯¹äºŽå®žçŽ°æ‰©å±•å®žéªŒçš„clockç®—æ³•ä¼šæœ‰å¸®åŠ©ã€‚\n\næœ¬æ¬¡å®žéªŒçš„ä¸»è¦ç»ƒä¹ é›†ä¸­åœ¨vmm.cä¸­çš„do\\_pgfaultå‡½æ•°å’Œswap\\_fifo.cä¸­çš„\\_fifo\\_map\\_swappableå‡½æ•°ã€\\_fifo\\_swap\\_out\\_victimå‡½æ•°ã€‚\n\n#### ç¼–è¯‘æ‰§è¡Œ\n\nç¼–è¯‘å¹¶è¿è¡Œä»£ç çš„å‘½ä»¤å¦‚ä¸‹ï¼š\n\n```\nmake\nmake qemu\n```\n\nåˆ™å¯ä»¥å¾—åˆ°å¦‚é™„å½•æ‰€ç¤ºçš„æ˜¾ç¤ºå†…å®¹ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸æ˜¯æ ‡å‡†ç­”æ¡ˆè¾“å‡ºï¼‰\n\n```\n$ make qemu\n(THU.CST) os is loading ...\n\nSpecial kernel symbols:\n  entry  0xc010002a (phys)\n  etext  0xc01081c3 (phys)\n  edata  0xc011fac8 (phys)\n  end    0xc0120cf0 (phys)\nKernel executable memory footprint: 132KB\nebp:0xc011ef48 eip:0xc0100a51 args:0x00010094 0x00000000 0xc011ef78 0xc01000b8 \n    kern/debug/kdebug.c:308: print_stackframe+21\nebp:0xc011ef58 eip:0xc0100d4f args:0x00000000 0x00000000 0x00000000 0xc011efc8 \n    kern/debug/kmonitor.c:129: mon_backtrace+10\nebp:0xc011ef78 eip:0xc01000b8 args:0x00000000 0xc011efa0 0xffff0000 0xc011efa4 \n    kern/init/init.c:56: grade_backtrace2+19\nebp:0xc011ef98 eip:0xc01000d9 args:0x00000000 0xffff0000 0xc011efc4 0x0000002a \n    kern/init/init.c:61: grade_backtrace1+27\nebp:0xc011efb8 eip:0xc01000f5 args:0x00000000 0xc010002a 0xffff0000 0xc010006d \n    kern/init/init.c:66: grade_backtrace0+19\nebp:0xc011efd8 eip:0xc0100115 args:0x00000000 0x00000000 0x00000000 0xc0108200 \n    kern/init/init.c:71: grade_backtrace+26\nebp:0xc011eff8 eip:0xc010007a args:0x00000000 0x00000000 0x0000ffff 0x40cf9a00 \n    kern/init/init.c:31: kern_init+79\nmemory management: default_pmm_manager\ne820map:\n  memory: 0009fc00, [00000000, 0009fbff], type = 1.\n  memory: 00000400, [0009fc00, 0009ffff], type = 2.\n  memory: 00010000, [000f0000, 000fffff], type = 2.\n  memory: 07ee0000, [00100000, 07fdffff], type = 1.\n  memory: 00020000, [07fe0000, 07ffffff], type = 2.\n  memory: 00040000, [fffc0000, ffffffff], type = 2.\ncheck_alloc_page() succeeded!\ncheck_pgdir() succeeded!\ncheck_boot_pgdir() succeeded!\n-------------------- BEGIN --------------------\nPDE(0e0) c0000000-f8000000 38000000 urw\n  |-- PTE(38000) c0000000-f8000000 38000000 -rw\nPDE(001) fac00000-fb000000 00400000 -rw\n  |-- PTE(000e0) faf00000-fafe0000 000e0000 urw\n  |-- PTE(00001) fafeb000-fafec000 00001000 -rw\n--------------------- END ---------------------\ncheck_vma_struct() succeeded!\npage fault at 0x00000100: K/W [no page found].\ncheck_pgfault() succeeded!\ncheck_vmm() succeeded.\nide 0:      10000(sectors), 'QEMU HARDDISK'.\nide 1:     262144(sectors), 'QEMU HARDDISK'.\nSWAP: manager = fifo swap manager\nBEGIN check_swap: count 31966, total 31966\nsetup Page Table for vaddr 0X1000, so alloc a page\nsetup Page Table vaddr 0~4MB OVER!\nset up init env for check_swap begin!\npage fault at 0x00001000: K/W [no page found].\npage fault at 0x00002000: K/W [no page found].\npage fault at 0x00003000: K/W [no page found].\npage fault at 0x00004000: K/W [no page found].\nset up init env for check_swap over!\nwrite Virt Page c in fifo_check_swap\nwrite Virt Page a in fifo_check_swap\nwrite Virt Page d in fifo_check_swap\nwrite Virt Page b in fifo_check_swap\nwrite Virt Page e in fifo_check_swap\npage fault at 0x00005000: K/W [no page found].\nswap_out: i 0, store page in vaddr 0x1000 to disk swap entry 2\nwrite Virt Page b in fifo_check_swap\nwrite Virt Page a in fifo_check_swap\npage fault at 0x00001000: K/W [no page found].\nswap_out: i 0, store page in vaddr 0x2000 to disk swap entry 3\nswap_in: load disk swap entry 2 with swap_page in vadr 0x1000\nwrite Virt Page b in fifo_check_swap\npage fault at 0x00002000: K/W [no page found].\nswap_out: i 0, store page in vaddr 0x3000 to disk swap entry 4\nswap_in: load disk swap entry 3 with swap_page in vadr 0x2000\nwrite Virt Page c in fifo_check_swap\npage fault at 0x00003000: K/W [no page found].\nswap_out: i 0, store page in vaddr 0x4000 to disk swap entry 5\nswap_in: load disk swap entry 4 with swap_page in vadr 0x3000\nwrite Virt Page d in fifo_check_swap\npage fault at 0x00004000: K/W [no page found].\nswap_out: i 0, store page in vaddr 0x5000 to disk swap entry 6\nswap_in: load disk swap entry 5 with swap_page in vadr 0x4000\ncount is 7, total is 7\ncheck_swap() succeeded!\n++ setup timer interrupts\n100 ticks\n100 ticks\n100 ticks\n100 ticks\n```\n",
          "### å®žéªŒæ‰§è¡Œæµç¨‹æ¦‚è¿°\n\næœ¬æ¬¡å®žéªŒä¸»è¦å®Œæˆucoreå†…æ ¸å¯¹è™šæ‹Ÿå†…å­˜çš„ç®¡ç†å·¥ä½œã€‚å…¶æ€»ä½“è®¾è®¡æ€è·¯è¿˜æ˜¯æ¯”è¾ƒç®€å•ï¼Œå³é¦–å…ˆå®Œæˆåˆå§‹åŒ–è™šæ‹Ÿå†…å­˜ç®¡ç†æœºåˆ¶ï¼Œå³éœ€è¦è®¾ç½®å¥½å“ªäº›é¡µéœ€è¦æ”¾åœ¨ç‰©ç†å†…å­˜ä¸­ï¼Œå“ªäº›é¡µä¸éœ€è¦æ”¾åœ¨ç‰©ç†å†…å­˜ä¸­ï¼Œè€Œæ˜¯å¯è¢«æ¢å‡ºåˆ°ç¡¬ç›˜ä¸Šï¼Œå¹¶æ¶‰åŠå®Œå–„å»ºç«‹é¡µè¡¨æ˜ å°„ã€é¡µè®¿é—®å¼‚å¸¸å¤„ç†æ“ä½œç­‰å‡½æ•°å®žçŽ°ã€‚ç„¶åŽå°±æ‰§è¡Œä¸€ç»„è®¿å­˜æµ‹è¯•ï¼Œçœ‹çœ‹æˆ‘ä»¬å»ºç«‹çš„é¡µè¡¨é¡¹æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®å®Œæˆè™šå®žåœ°å€æ˜ å°„ï¼Œæ˜¯å¦æ­£ç¡®æè¿°äº†è™šæ‹Ÿå†…å­˜é¡µåœ¨ç‰©ç†å†…å­˜ä¸­è¿˜æ˜¯åœ¨ç¡¬ç›˜ä¸Šï¼Œæ˜¯å¦èƒ½å¤Ÿæ­£ç¡®æŠŠè™šæ‹Ÿå†…å­˜é¡µåœ¨ç‰©ç†å†…å­˜å’Œç¡¬ç›˜ä¹‹é—´è¿›è¡Œä¼ é€’ï¼Œæ˜¯å¦æ­£ç¡®å®žçŽ°äº†é¡µé¢æ›¿æ¢ç®—æ³•ç­‰ã€‚lab3çš„æ€»ä½“æ‰§è¡Œæµç¨‹å¦‚ä¸‹ã€‚\n\né¦–å…ˆæ˜¯åˆå§‹åŒ–è¿‡ç¨‹ã€‚å‚è€ƒucoreæ€»æŽ§å‡½æ•°initçš„ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°åœ¨è°ƒç”¨å®Œæˆè™šæ‹Ÿå†…å­˜åˆå§‹åŒ–çš„vmm\\_initå‡½æ•°ä¹‹å‰ï¼Œéœ€è¦é¦–å…ˆè°ƒç”¨pmm\\_initå‡½æ•°å®Œæˆç‰©ç†å†…å­˜çš„ç®¡ç†ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬lab2å·²ç»å®Œæˆçš„å†…å®¹ã€‚æŽ¥ç€æ˜¯æ‰§è¡Œä¸­æ–­å’Œå¼‚å¸¸ç›¸å…³çš„åˆå§‹åŒ–å·¥ä½œï¼Œå³è°ƒç”¨pic\\_initå‡½æ•°å’Œidt\\_initå‡½æ•°ç­‰ï¼Œè¿™äº›å·¥ä½œä¸Žlab1çš„ä¸­æ–­å¼‚å¸¸åˆå§‹åŒ–å·¥ä½œçš„å†…å®¹æ˜¯ç›¸åŒçš„ã€‚\n\nåœ¨è°ƒç”¨å®Œidt\\_initå‡½æ•°ä¹‹åŽï¼Œå°†è¿›ä¸€æ­¥è°ƒç”¨ä¸‰ä¸ªlab3ä¸­æ‰æœ‰çš„æ–°å‡½æ•°vmm\\_initã€ide\\_initå’Œswap\\_initã€‚è¿™ä¸‰ä¸ªå‡½æ•°è®¾è®¡äº†æœ¬æ¬¡å®žéªŒä¸­çš„ä¸¤ä¸ªç»ƒä¹ ã€‚ç¬¬ä¸€ä¸ªå‡½æ•°vmm\\_initæ˜¯æ£€æŸ¥æˆ‘ä»¬çš„ç»ƒä¹ 1æ˜¯å¦æ­£ç¡®å®žçŽ°äº†ã€‚ä¸ºäº†è¡¨è¿°ä¸åœ¨ç‰©ç†å†…å­˜ä¸­çš„â€œåˆæ³•â€è™šæ‹Ÿé¡µï¼Œéœ€è¦æœ‰æ•°æ®ç»“æž„æ¥æè¿°è¿™æ ·çš„é¡µï¼Œä¸ºæ­¤ucoreå»ºç«‹äº†mm\\_structå’Œvma\\_structæ•°æ®ç»“æž„ï¼ˆæŽ¥ä¸‹æ¥çš„å°èŠ‚ä¸­æœ‰è¿›ä¸€æ­¥è¯¦ç»†æè¿°ï¼‰ï¼Œå‡å®šæˆ‘ä»¬å·²ç»æè¿°å¥½äº†è¿™æ ·çš„â€œåˆæ³•â€è™šæ‹Ÿé¡µï¼Œå½“ucoreè®¿é—®è¿™äº›â€œåˆæ³•â€è™šæ‹Ÿé¡µæ—¶ï¼Œä¼šç”±äºŽæ²¡æœ‰è™šå®žåœ°å€æ˜ å°„è€Œäº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸ã€‚å¦‚æžœæˆ‘ä»¬æ­£ç¡®å®žçŽ°äº†ç»ƒä¹ 1ï¼Œåˆ™do\\_pgfaultå‡½æ•°ä¼šç”³è¯·ä¸€ä¸ªç©ºé—²ç‰©ç†é¡µï¼Œå¹¶å»ºç«‹å¥½è™šå®žæ˜ å°„å…³ç³»ï¼Œä»Žè€Œä½¿å¾—è¿™æ ·çš„â€œåˆæ³•â€è™šæ‹Ÿé¡µæœ‰å®žé™…çš„ç‰©ç†é¡µå¸§å¯¹åº”ã€‚è¿™æ ·ç»ƒä¹ 1å°±ç®—å®Œæˆäº†ã€‚\n\nide\\_initå’Œswap\\_initæ˜¯ä¸ºç»ƒä¹ 2å‡†å¤‡çš„ã€‚ç”±äºŽé¡µé¢ç½®æ¢ç®—æ³•çš„å®žçŽ°å­˜åœ¨å¯¹ç¡¬ç›˜æ•°æ®å—çš„è¯»å†™ï¼Œæ‰€ä»¥ide\\_initå°±æ˜¯å®Œæˆå¯¹ç”¨äºŽé¡µæ¢å…¥æ¢å‡ºçš„ç¡¬ç›˜ï¼ˆç®€ç§°swapç¡¬ç›˜ï¼‰çš„åˆå§‹åŒ–å·¥ä½œã€‚å®Œæˆide\\_initå‡½æ•°åŽï¼Œucoreå°±å¯ä»¥å¯¹è¿™ä¸ªswapç¡¬ç›˜è¿›è¡Œè¯»å†™æ“ä½œäº†ã€‚swap\\_initå‡½æ•°é¦–å…ˆå»ºç«‹swap\\_managerï¼Œswap\\_manageræ˜¯å®Œæˆé¡µé¢æ›¿æ¢è¿‡ç¨‹çš„ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼Œå…¶ä¸­åŒ…å«äº†é¡µé¢ç½®æ¢ç®—æ³•çš„å®žçŽ°ï¼ˆå…·ä½“å†…å®¹å¯å‚è€ƒ5å°èŠ‚ï¼‰ã€‚ç„¶åŽä¼šè¿›ä¸€æ­¥è°ƒç”¨æ‰§è¡Œcheck\\_swapå‡½æ•°åœ¨å†…æ ¸ä¸­åˆ†é…ä¸€äº›é¡µï¼Œæ¨¡æ‹Ÿå¯¹è¿™äº›é¡µçš„è®¿é—®ï¼Œè¿™ä¼šäº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸ã€‚å¦‚æžœæˆ‘ä»¬æ­£ç¡®å®žçŽ°äº†ç»ƒä¹ 2ï¼Œå°±å¯é€šè¿‡do\\_pgfaultæ¥è°ƒç”¨swap\\_map\\_swappableå‡½æ•°æ¥æŸ¥è¯¢è¿™äº›é¡µçš„è®¿é—®æƒ…å†µå¹¶é—´æŽ¥è°ƒç”¨å®žçŽ°é¡µé¢ç½®æ¢ç®—æ³•çš„ç›¸å…³å‡½æ•°ï¼ŒæŠŠâ€œä¸å¸¸ç”¨â€çš„é¡µæ¢å‡ºåˆ°ç£ç›˜ä¸Šã€‚\n\nucoreåœ¨å®žçŽ°ä¸Šè¿°æŠ€æœ¯æ—¶ï¼Œéœ€è¦è§£å†³ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼š\n\n1. å½“ç¨‹åºè¿è¡Œä¸­è®¿é—®å†…å­˜äº§ç”Ÿpage\nfaultå¼‚å¸¸æ—¶ï¼Œå¦‚ä½•åˆ¤å®šè¿™ä¸ªå¼•èµ·å¼‚å¸¸çš„è™šæ‹Ÿåœ°å€å†…å­˜è®¿é—®æ˜¯è¶Šç•Œã€å†™åªè¯»é¡µçš„â€œéžæ³•åœ°å€â€è®¿é—®è¿˜æ˜¯ç”±äºŽæ•°æ®è¢«ä¸´æ—¶æ¢å‡ºåˆ°ç£ç›˜ä¸Šæˆ–è¿˜æ²¡æœ‰åˆ†é…å†…å­˜çš„â€œåˆæ³•åœ°å€â€è®¿é—®ï¼Ÿ\n2. ä½•æ—¶è¿›è¡Œè¯·æ±‚è°ƒé¡µ/é¡µæ¢å…¥æ¢å‡ºå¤„ç†ï¼Ÿ\n3. å¦‚ä½•åœ¨çŽ°æœ‰ucoreçš„åŸºç¡€ä¸Šå®žçŽ°é¡µæ›¿æ¢ç®—æ³•ï¼Ÿ\n\næŽ¥ä¸‹æ¥å°†è¿›ä¸€æ­¥åˆ†æžå®Œæˆlab3ä¸»è¦æ³¨æ„çš„å…³é”®é—®é¢˜å’Œæ¶‰åŠçš„å…³é”®æ•°æ®ç»“æž„ã€‚\n",
          "### å…³é”®æ•°æ®ç»“æž„å’Œç›¸å…³å‡½æ•°åˆ†æž\n\nå¯¹äºŽç¬¬ä¸€ä¸ªé—®é¢˜çš„å‡ºçŽ°ï¼Œåœ¨äºŽå®žéªŒäºŒä¸­æœ‰å…³å†…å­˜çš„æ•°æ®ç»“æž„å’Œç›¸å…³æ“ä½œéƒ½æ˜¯ç›´æŽ¥é’ˆå¯¹å®žé™…å­˜åœ¨çš„èµ„æº--ç‰©ç†å†…å­˜ç©ºé—´çš„ç®¡ç†ï¼Œæ²¡æœ‰ä»Žä¸€èˆ¬åº”ç”¨ç¨‹åºå¯¹å†…å­˜çš„â€œéœ€æ±‚â€è€ƒè™‘ï¼Œå³éœ€è¦æœ‰ç›¸å…³çš„æ•°æ®ç»“æž„å’Œæ“ä½œæ¥ä½“çŽ°ä¸€èˆ¬åº”ç”¨ç¨‹åºå¯¹è™šæ‹Ÿå†…å­˜çš„â€œéœ€æ±‚â€ã€‚ä¸€èˆ¬åº”ç”¨ç¨‹åºçš„å¯¹è™šæ‹Ÿå†…å­˜çš„â€œéœ€æ±‚â€ä¸Žç‰©ç†å†…å­˜ç©ºé—´çš„â€œä¾›ç»™â€æ²¡æœ‰ç›´æŽ¥çš„å¯¹åº”å…³ç³»ï¼Œucoreæ˜¯é€šè¿‡page\nfaultå¼‚å¸¸å¤„ç†æ¥é—´æŽ¥å®Œæˆè¿™äºŒè€…ä¹‹é—´çš„è¡”æŽ¥ã€‚\n\npage\\_faultå‡½æ•°ä¸çŸ¥é“å“ªäº›æ˜¯â€œåˆæ³•â€çš„è™šæ‹Ÿé¡µï¼ŒåŽŸå› æ˜¯ucoreè¿˜ç¼ºå°‘ä¸€å®šçš„æ•°æ®ç»“æž„æ¥æè¿°è¿™ç§ä¸åœ¨ç‰©ç†å†…å­˜ä¸­çš„â€œåˆæ³•â€è™šæ‹Ÿé¡µã€‚ä¸ºæ­¤ucoreé€šè¿‡å»ºç«‹mm\\_structå’Œvma\\_structæ•°æ®ç»“æž„ï¼Œæè¿°äº†ucoreæ¨¡æ‹Ÿåº”ç”¨ç¨‹åºè¿è¡Œæ‰€éœ€çš„åˆæ³•å†…å­˜ç©ºé—´ã€‚å½“è®¿é—®å†…å­˜äº§ç”Ÿpage\nfaultå¼‚å¸¸æ—¶ï¼Œå¯èŽ·å¾—è®¿é—®çš„å†…å­˜çš„æ–¹å¼ï¼ˆè¯»æˆ–å†™ï¼‰ä»¥åŠå…·ä½“çš„è™šæ‹Ÿå†…å­˜åœ°å€ï¼Œè¿™æ ·ucoreå°±å¯ä»¥æŸ¥è¯¢æ­¤åœ°å€ï¼Œçœ‹æ˜¯å¦å±žäºŽvma\\_structæ•°æ®ç»“æž„ä¸­æè¿°çš„åˆæ³•åœ°å€èŒƒå›´ä¸­ï¼Œå¦‚æžœåœ¨ï¼Œåˆ™å¯æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œè¯·æ±‚è°ƒé¡µ/é¡µæ¢å…¥æ¢å‡ºå¤„ç†ï¼ˆè¿™å°±æ˜¯ç»ƒä¹ 2æ¶‰åŠçš„éƒ¨åˆ†ï¼‰ï¼›å¦‚æžœä¸åœ¨ï¼Œåˆ™æŠ¥é”™ã€‚mm\\_structå’Œvma\\_structæ•°æ®ç»“æž„ç»“åˆé¡µè¡¨è¡¨ç¤ºè™šæ‹Ÿåœ°å€ç©ºé—´å’Œç‰©ç†åœ°å€ç©ºé—´çš„ç¤ºæ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š\n\nå›¾ è™šæ‹Ÿåœ°å€ç©ºé—´å’Œç‰©ç†åœ°å€ç©ºé—´çš„ç¤ºæ„å›¾\n\n![image](../lab3_figs/image001.png)   \n\nåœ¨ucoreä¸­æè¿°åº”ç”¨ç¨‹åºå¯¹è™šæ‹Ÿå†…å­˜â€œéœ€æ±‚â€çš„æ•°æ®ç»“æž„æ˜¯vma\\_structï¼ˆå®šä¹‰åœ¨vmm.hä¸­ï¼‰ï¼Œä»¥åŠé’ˆå¯¹vma\\_structçš„å‡½æ•°æ“ä½œã€‚è¿™é‡ŒæŠŠä¸€ä¸ªvma\\_structç»“æž„çš„å˜é‡ç®€ç§°ä¸ºvmaå˜é‡ã€‚vma\\_structçš„å®šä¹‰å¦‚ä¸‹ï¼š\n\n```\nstruct vma_struct {\n    // the set of vma using the same PDT\n    struct mm_struct *vm_mm;\n    uintptr_t vm_start; // start addr of vma\n    uintptr_t vm_end; // end addr of vma\n    uint32_t vm_flags; // flags of vma\n    //linear list link which sorted by start addr of vma\n    list_entry_t list_link;\n};\n```\n\nvm\\_startå’Œvm\\_endæè¿°äº†ä¸€ä¸ªè¿žç»­åœ°å€çš„è™šæ‹Ÿå†…å­˜ç©ºé—´çš„èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼Œè¿™ä¸¤ä¸ªå€¼éƒ½åº”è¯¥æ˜¯PGSIZE å¯¹é½çš„ï¼Œè€Œä¸”æè¿°çš„æ˜¯ä¸€ä¸ªåˆç†çš„åœ°å€ç©ºé—´èŒƒå›´ï¼ˆå³ä¸¥æ ¼ç¡®ä¿ vm\\_start < vm\\_endçš„å…³ç³»ï¼‰ï¼›list\\_linkæ˜¯ä¸€ä¸ªåŒå‘é“¾è¡¨ï¼ŒæŒ‰ç…§ä»Žå°åˆ°å¤§çš„é¡ºåºæŠŠä¸€ç³»åˆ—ç”¨vma\\_structè¡¨ç¤ºçš„è™šæ‹Ÿå†…å­˜ç©ºé—´é“¾æŽ¥èµ·æ¥ï¼Œå¹¶ä¸”è¿˜è¦æ±‚è¿™äº›é“¾èµ·æ¥çš„vma\\_structåº”è¯¥æ˜¯ä¸ç›¸äº¤çš„ï¼Œå³vmaä¹‹é—´çš„åœ°å€ç©ºé—´æ— äº¤é›†ï¼›vm\\_flagsè¡¨ç¤ºäº†è¿™ä¸ªè™šæ‹Ÿå†…å­˜ç©ºé—´çš„å±žæ€§ï¼Œç›®å‰çš„å±žæ€§åŒ…æ‹¬ï¼š\n\n```\n#define VM_READ 0x00000001 //åªè¯»\n#define VM_WRITE 0x00000002 //å¯è¯»å†™\n#define VM_EXEC 0x00000004 //å¯æ‰§è¡Œ\n```   \n\nvm\\_mmæ˜¯ä¸€ä¸ªæŒ‡é’ˆï¼ŒæŒ‡å‘ä¸€ä¸ªæ¯”vma\\_structæ›´é«˜çš„æŠ½è±¡å±‚æ¬¡çš„æ•°æ®ç»“æž„mm\\_structï¼Œè¿™é‡ŒæŠŠä¸€ä¸ªmm\\_structç»“æž„çš„å˜é‡ç®€ç§°ä¸ºmmå˜é‡ã€‚è¿™ä¸ªæ•°æ®ç»“æž„è¡¨ç¤ºäº†åŒ…å«æ‰€æœ‰è™šæ‹Ÿå†…å­˜ç©ºé—´çš„å…±åŒå±žæ€§ï¼Œå…·ä½“å®šä¹‰å¦‚ä¸‹  \n\n```  \nstruct mm_struct {\n    // linear list link which sorted by start addr of vma\n    list_entry_t mmap_list;\n    // current accessed vma, used for speed purpose\n    struct vma_struct *mmap_cache;\n    pde_t *pgdir; // the PDT of these vma\n    int map_count; // the count of these vma\n    void *sm_priv; // the private data for swap manager\n};\n``` \n\nmmap\\_listæ˜¯åŒå‘é“¾è¡¨å¤´ï¼Œé“¾æŽ¥äº†æ‰€æœ‰å±žäºŽåŒä¸€é¡µç›®å½•è¡¨çš„è™šæ‹Ÿå†…å­˜ç©ºé—´ï¼Œmmap\\_cacheæ˜¯æŒ‡å‘å½“å‰æ­£åœ¨ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜ç©ºé—´ï¼Œç”±äºŽæ“ä½œç³»ç»Ÿæ‰§è¡Œçš„â€œå±€éƒ¨æ€§â€åŽŸç†ï¼Œå½“å‰æ­£åœ¨ç”¨åˆ°çš„è™šæ‹Ÿå†…å­˜ç©ºé—´åœ¨æŽ¥ä¸‹æ¥çš„æ“ä½œä¸­å¯èƒ½è¿˜ä¼šç”¨åˆ°ï¼Œè¿™æ—¶å°±ä¸éœ€è¦æŸ¥é“¾è¡¨ï¼Œè€Œæ˜¯ç›´æŽ¥ä½¿ç”¨æ­¤æŒ‡é’ˆå°±å¯æ‰¾åˆ°ä¸‹ä¸€æ¬¡è¦ç”¨åˆ°çš„è™šæ‹Ÿå†…å­˜ç©ºé—´ã€‚ç”±äºŽmmap_cache çš„å¼•å…¥ï¼Œå¯ä½¿å¾— mm\\_struct æ•°æ®ç»“æž„çš„æŸ¥è¯¢åŠ é€Ÿ 30% ä»¥ä¸Šã€‚pgdir\næ‰€æŒ‡å‘çš„å°±æ˜¯ mm\\_structæ•°æ®ç»“æž„æ‰€ç»´æŠ¤çš„é¡µè¡¨ã€‚é€šè¿‡è®¿é—®pgdirå¯ä»¥æŸ¥æ‰¾æŸè™šæ‹Ÿåœ°å€å¯¹åº”çš„é¡µè¡¨é¡¹æ˜¯å¦å­˜åœ¨ä»¥åŠé¡µè¡¨é¡¹çš„å±žæ€§ç­‰ã€‚map\\_countè®°å½•mmap\\_list é‡Œé¢é“¾æŽ¥çš„ vma\\_structçš„ä¸ªæ•°ã€‚sm_privæŒ‡å‘ç”¨æ¥é“¾æŽ¥è®°å½•é¡µè®¿é—®æƒ…å†µçš„é“¾è¡¨å¤´ï¼Œè¿™å»ºç«‹äº†mm\\_structå’ŒåŽç»­è¦è®²åˆ°çš„swap\\_managerä¹‹é—´çš„è”ç³»ã€‚\n\næ¶‰åŠvma_structçš„æ“ä½œå‡½æ•°ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªï¼š\n\n* vma_create--åˆ›å»ºvma\n* insert_vma_struct--æ’å…¥ä¸€ä¸ªvma\n* find_vma--æŸ¥è¯¢vmaã€‚\n\nvma\\_createå‡½æ•°æ ¹æ®è¾“å…¥å‚æ•°vm\\_startã€vm\\_endã€vm\\_flagsæ¥åˆ›å»ºå¹¶åˆå§‹åŒ–æè¿°ä¸€ä¸ªè™šæ‹Ÿå†…å­˜ç©ºé—´çš„vma\\_structç»“æž„å˜é‡ã€‚insert\\_vma\\_structå‡½æ•°å®ŒæˆæŠŠä¸€ä¸ªvmaå˜é‡æŒ‰ç…§å…¶ç©ºé—´ä½ç½®[vma-\\>vm\\_start,vma-\\>vm\\_end]ä»Žå°åˆ°å¤§çš„é¡ºåºæ’å…¥åˆ°æ‰€å±žçš„mmå˜é‡ä¸­çš„mmap\\_liståŒå‘é“¾è¡¨ä¸­ã€‚find\\_vmaæ ¹æ®è¾“å…¥å‚æ•°addrå’Œmmå˜é‡ï¼ŒæŸ¥æ‰¾åœ¨mmå˜é‡ä¸­çš„mmap\\_liståŒå‘é“¾è¡¨ä¸­æŸä¸ªvmaåŒ…å«æ­¤addrï¼Œå³vma-\\>vm\\_start<=addr <vma-\\>endã€‚è¿™ä¸‰ä¸ªå‡½æ•°ä¸ŽåŽç»­è®²åˆ°çš„page faultå¼‚å¸¸å¤„ç†æœ‰ç´§å¯†è”ç³»ã€‚\n\næ¶‰åŠmm\\_structçš„æ“ä½œå‡½æ•°æ¯”è¾ƒç®€å•ï¼Œåªæœ‰mm\\_createå’Œmm\\_destroyä¸¤ä¸ªå‡½æ•°ï¼Œä»Žå­—é¢æ„æ€å°±å¯ä»¥çœ‹å‡ºæ˜¯æ˜¯å®Œæˆmm\\_structç»“æž„çš„å˜é‡åˆ›å»ºå’Œåˆ é™¤ã€‚åœ¨mm\\_createä¸­ç”¨kmallocåˆ†é…äº†ä¸€å—ç©ºé—´ï¼Œæ‰€ä»¥åœ¨mm\\_destroyä¸­ä¹Ÿè¦å¯¹åº”è¿›è¡Œé‡Šæ”¾ã€‚åœ¨ucoreè¿è¡Œè¿‡ç¨‹ä¸­ï¼Œä¼šäº§ç”Ÿæè¿°è™šæ‹Ÿå†…å­˜ç©ºé—´çš„vma\\_structç»“æž„ï¼Œæ‰€ä»¥åœ¨mm\\_destroyä¸­ä¹Ÿè¦è¿›å¯¹è¿™äº›mmap\\_listä¸­çš„vmaè¿›è¡Œé‡Šæ”¾ã€‚\n",
          "## Page Faultå¼‚å¸¸å¤„ç†\n\nå®žçŽ°è™šå­˜ç®¡ç†çš„ä¸€ä¸ªå…³é”®æ˜¯page faultå¼‚å¸¸å¤„ç†ï¼Œå…¶è¿‡ç¨‹ä¸­ä¸»è¦æ¶‰åŠåˆ°å‡½æ•° -- do\\_pgfaultçš„å…·ä½“å®žçŽ°ã€‚æ¯”å¦‚ï¼Œåœ¨ç¨‹åºçš„æ‰§è¡Œè¿‡ç¨‹ä¸­ç”±äºŽæŸç§åŽŸå› ï¼ˆé¡µæ¡†ä¸å­˜åœ¨/å†™åªè¯»é¡µç­‰ï¼‰è€Œä½¿ CPU æ— æ³•æœ€ç»ˆè®¿é—®åˆ°ç›¸åº”çš„ç‰©ç†å†…å­˜å•å…ƒï¼Œå³æ— æ³•å®Œæˆä»Žè™šæ‹Ÿåœ°å€åˆ°ç‰©ç†åœ°å€æ˜ å°„æ—¶ï¼ŒCPU ä¼šäº§ç”Ÿä¸€æ¬¡é¡µè®¿é—®å¼‚å¸¸ï¼Œä»Žè€Œéœ€è¦è¿›è¡Œç›¸åº”çš„é¡µè®¿é—®å¼‚å¸¸çš„ä¸­æ–­æœåŠ¡ä¾‹ç¨‹ã€‚è¿™ä¸ªé¡µè®¿é—®å¼‚å¸¸å¤„ç†çš„æ—¶æœºè¢«æ“ä½œç³»ç»Ÿå……åˆ†åˆ©ç”¨æ¥å®Œæˆè™šå­˜ç®¡ç†ï¼Œå³å®žçŽ°â€œæŒ‰éœ€è°ƒé¡µâ€/â€œé¡µæ¢å…¥æ¢å‡ºâ€å¤„ç†çš„æ‰§è¡Œæ—¶æœºã€‚å½“ç›¸å…³å¤„ç†å®ŒæˆåŽï¼Œé¡µè®¿é—®å¼‚å¸¸æœåŠ¡ä¾‹ç¨‹ä¼šè¿”å›žåˆ°äº§ç”Ÿå¼‚å¸¸çš„æŒ‡ä»¤å¤„é‡æ–°æ‰§è¡Œï¼Œä½¿å¾—åº”ç”¨è½¯ä»¶å¯ä»¥ç»§ç»­æ­£å¸¸è¿è¡Œä¸‹åŽ»ã€‚\n\nå…·ä½“è€Œè¨€ï¼Œå½“å¯åŠ¨åˆ†é¡µæœºåˆ¶ä»¥åŽï¼Œå¦‚æžœä¸€æ¡æŒ‡ä»¤æˆ–æ•°æ®çš„è™šæ‹Ÿåœ°å€æ‰€å¯¹åº”çš„ç‰©ç†é¡µæ¡†ä¸åœ¨å†…å­˜ä¸­æˆ–è€…è®¿é—®çš„ç±»åž‹æœ‰é”™è¯¯ï¼ˆæ¯”å¦‚å†™ä¸€ä¸ªåªè¯»é¡µæˆ–ç”¨æˆ·æ€ç¨‹åºè®¿é—®å†…æ ¸æ€çš„æ•°æ®ç­‰ï¼‰ï¼Œå°±ä¼šå‘ç”Ÿé¡µè®¿é—®å¼‚å¸¸ã€‚äº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸çš„åŽŸå› ä¸»è¦æœ‰ï¼š\n\n*  ç›®æ ‡é¡µå¸§ä¸å­˜åœ¨ï¼ˆé¡µè¡¨é¡¹å…¨ä¸º0ï¼Œå³è¯¥çº¿æ€§åœ°å€ä¸Žç‰©ç†åœ°å€å°šæœªå»ºç«‹æ˜ å°„æˆ–è€…å·²ç»æ’¤é”€)ï¼›\n*  ç›¸åº”çš„ç‰©ç†é¡µå¸§ä¸åœ¨å†…å­˜ä¸­ï¼ˆé¡µè¡¨é¡¹éžç©ºï¼Œä½†Presentæ ‡å¿—ä½=0ï¼Œæ¯”å¦‚åœ¨swapåˆ†åŒºæˆ–ç£ç›˜æ–‡ä»¶ä¸Š)ï¼Œè¿™åœ¨æœ¬æ¬¡å®žéªŒä¸­ä¼šå‡ºçŽ°ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢ä»‹ç»æ¢é¡µæœºåˆ¶å®žçŽ°æ—¶è¿›ä¸€æ­¥è®²è§£å¦‚ä½•å¤„ç†ï¼›\n*  ä¸æ»¡è¶³è®¿é—®æƒé™(æ­¤æ—¶é¡µè¡¨é¡¹Pæ ‡å¿—=1ï¼Œä½†ä½Žæƒé™çš„ç¨‹åºè¯•å›¾è®¿é—®é«˜æƒé™çš„åœ°å€ç©ºé—´ï¼Œæˆ–è€…æœ‰ç¨‹åºè¯•å›¾å†™åªè¯»é¡µé¢).\n\nå½“å‡ºçŽ°ä¸Šé¢æƒ…å†µä¹‹ä¸€ï¼Œé‚£ä¹ˆå°±ä¼šäº§ç”Ÿé¡µé¢page faultï¼ˆ\\#PFï¼‰å¼‚å¸¸ã€‚CPUä¼šæŠŠäº§ç”Ÿå¼‚å¸¸çš„çº¿æ€§åœ°å€å­˜å‚¨åœ¨CR2ä¸­ï¼Œå¹¶ä¸”æŠŠè¡¨ç¤ºé¡µè®¿é—®å¼‚å¸¸ç±»åž‹çš„å€¼ï¼ˆç®€ç§°é¡µè®¿é—®å¼‚å¸¸é”™è¯¯ç ï¼ŒerrorCodeï¼‰ä¿å­˜åœ¨ä¸­æ–­æ ˆä¸­ã€‚\n\n>[æç¤º]é¡µè®¿é—®å¼‚å¸¸é”™è¯¯ç æœ‰32ä½ã€‚ä½0ä¸ºï¼‘è¡¨ç¤ºå¯¹åº”ç‰©ç†é¡µä¸å­˜åœ¨ï¼›ä½ï¼‘ä¸ºï¼‘è¡¨ç¤ºå†™å¼‚å¸¸ï¼ˆæ¯”å¦‚å†™äº†åªè¯»é¡µï¼›ä½ï¼’ä¸ºï¼‘è¡¨ç¤ºè®¿é—®æƒé™å¼‚å¸¸ï¼ˆæ¯”å¦‚ç”¨æˆ·æ€ç¨‹åºè®¿é—®å†…æ ¸ç©ºé—´çš„æ•°æ®ï¼‰\n\n>[æç¤º]ã€€CR2æ˜¯é¡µæ•…éšœçº¿æ€§åœ°å€å¯„å­˜å™¨ï¼Œä¿å­˜æœ€åŽä¸€æ¬¡å‡ºçŽ°é¡µæ•…éšœçš„å…¨32ä½çº¿æ€§åœ°å€ã€‚CR2ç”¨äºŽå‘ç”Ÿé¡µå¼‚å¸¸æ—¶æŠ¥å‘Šå‡ºé”™ä¿¡æ¯ã€‚å½“å‘ç”Ÿé¡µå¼‚å¸¸æ—¶ï¼Œå¤„ç†å™¨æŠŠå¼•èµ·é¡µå¼‚å¸¸çš„çº¿æ€§åœ°å€ä¿å­˜åœ¨CR2ä¸­ã€‚æ“ä½œç³»ç»Ÿä¸­å¯¹åº”çš„ä¸­æ–­æœåŠ¡ä¾‹ç¨‹å¯ä»¥æ£€æŸ¥CR2çš„å†…å®¹ï¼Œä»Žè€ŒæŸ¥å‡ºçº¿æ€§åœ°å€ç©ºé—´ä¸­çš„å“ªä¸ªé¡µå¼•èµ·æœ¬æ¬¡å¼‚å¸¸ã€‚\n\näº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸åŽï¼ŒCPUç¡¬ä»¶å’Œè½¯ä»¶éƒ½ä¼šåšä¸€äº›äº‹æƒ…æ¥åº”å¯¹æ­¤äº‹ã€‚é¦–å…ˆé¡µè®¿é—®å¼‚å¸¸ä¹Ÿæ˜¯ä¸€ç§å¼‚å¸¸ï¼Œæ‰€ä»¥é’ˆå¯¹ä¸€èˆ¬å¼‚å¸¸çš„ç¡¬ä»¶å¤„ç†æ“ä½œæ˜¯å¿…é¡»è¦åšçš„ï¼Œå³CPUåœ¨å½“å‰å†…æ ¸æ ˆä¿å­˜å½“å‰è¢«æ‰“æ–­çš„ç¨‹åºçŽ°åœºï¼Œå³ä¾æ¬¡åŽ‹å…¥å½“å‰è¢«æ‰“æ–­ç¨‹åºä½¿ç”¨çš„EFLAGSï¼ŒCSï¼ŒEIPï¼ŒerrorCodeï¼›ç”±äºŽé¡µè®¿é—®å¼‚å¸¸çš„ä¸­æ–­å·æ˜¯0xEï¼ŒCPUæŠŠå¼‚å¸¸ä¸­æ–­å·0xEå¯¹åº”çš„ä¸­æ–­æœåŠ¡ä¾‹ç¨‹çš„åœ°å€ï¼ˆvectors.Sä¸­çš„æ ‡å·vector14å¤„ï¼‰åŠ è½½åˆ°CSå’ŒEIPå¯„å­˜å™¨ä¸­ï¼Œå¼€å§‹æ‰§è¡Œä¸­æ–­æœåŠ¡ä¾‹ç¨‹ã€‚è¿™æ—¶ucoreå¼€å§‹å¤„ç†å¼‚å¸¸ä¸­æ–­ï¼Œé¦–å…ˆéœ€è¦ä¿å­˜ç¡¬ä»¶æ²¡æœ‰ä¿å­˜çš„å¯„å­˜å™¨ã€‚åœ¨vectors.Sä¸­çš„æ ‡å·vector14å¤„å…ˆæŠŠä¸­æ–­å·åŽ‹å…¥å†…æ ¸æ ˆï¼Œç„¶åŽå†åœ¨trapentry.Sä¸­çš„æ ‡å·\\_\\_alltrapså¤„æŠŠDSã€ESå’Œå…¶ä»–é€šç”¨å¯„å­˜å™¨éƒ½åŽ‹æ ˆã€‚è‡ªæ­¤ï¼Œè¢«æ‰“æ–­çš„ç¨‹åºæ‰§è¡ŒçŽ°åœºï¼ˆcontextï¼‰è¢«ä¿å­˜åœ¨å†…æ ¸æ ˆä¸­ã€‚æŽ¥ä¸‹æ¥ï¼Œåœ¨trap.cçš„trapå‡½æ•°å¼€å§‹äº†ä¸­æ–­æœåŠ¡ä¾‹ç¨‹çš„å¤„ç†æµç¨‹ï¼Œå¤§è‡´è°ƒç”¨å…³ç³»ä¸ºï¼š\n\n> trap--\\> trap\\_dispatch--\\>pgfault\\_handler--\\>do\\_pgfault\n\nä¸‹é¢éœ€è¦å…·ä½“åˆ†æžä¸€ä¸‹do\\_pgfaultå‡½æ•°ã€‚do\\_pgfaultçš„è°ƒç”¨å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\nå›¾ do\\_pgfaultçš„è°ƒç”¨å…³ç³»å›¾\n\n![image](../lab3_figs/image002.png)\n\näº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸åŽï¼ŒCPUæŠŠå¼•èµ·é¡µè®¿é—®å¼‚å¸¸çš„çº¿æ€§åœ°å€è£…åˆ°å¯„å­˜å™¨CR2ä¸­ï¼Œå¹¶ç»™å‡ºäº†å‡ºé”™ç errorCodeï¼Œè¯´æ˜Žäº†é¡µè®¿é—®å¼‚å¸¸çš„ç±»åž‹ã€‚ucoreã€€OSä¼šæŠŠè¿™ä¸ªå€¼ä¿å­˜åœ¨struct trapframe ä¸­tf\\_erræˆå‘˜å˜é‡ä¸­ã€‚è€Œä¸­æ–­æœåŠ¡ä¾‹ç¨‹ä¼šè°ƒç”¨é¡µè®¿é—®å¼‚å¸¸å¤„ç†å‡½æ•°do\\_pgfaultè¿›è¡Œå…·ä½“å¤„ç†ã€‚è¿™é‡Œçš„é¡µè®¿é—®å¼‚å¸¸å¤„ç†æ˜¯å®žçŽ°æŒ‰éœ€åˆ†é¡µã€é¡µæ¢å…¥æ¢å‡ºæœºåˆ¶çš„å…³é”®ä¹‹å¤„ã€‚\n\nucoreä¸­do\\_pgfaultå‡½æ•°æ˜¯å®Œæˆé¡µè®¿é—®å¼‚å¸¸å¤„ç†çš„ä¸»è¦å‡½æ•°ï¼Œå®ƒæ ¹æ®ä»ŽCPUçš„æŽ§åˆ¶å¯„å­˜å™¨CR2ä¸­èŽ·å–çš„é¡µè®¿é—®å¼‚å¸¸çš„ç‰©ç†åœ°å€ä»¥åŠæ ¹æ®errorCodeçš„é”™è¯¯ç±»åž‹æ¥æŸ¥æ‰¾æ­¤åœ°å€æ˜¯å¦åœ¨æŸä¸ªVMAçš„åœ°å€èŒƒå›´å†…ä»¥åŠæ˜¯å¦æ»¡è¶³æ­£ç¡®çš„è¯»å†™æƒé™ï¼Œå¦‚æžœåœ¨æ­¤èŒƒå›´å†…å¹¶ä¸”æƒé™ä¹Ÿæ­£ç¡®ï¼Œè¿™è®¤ä¸ºè¿™æ˜¯ä¸€æ¬¡åˆæ³•è®¿é—®ï¼Œä½†æ²¡æœ‰å»ºç«‹è™šå®žå¯¹åº”å…³ç³»ã€‚æ‰€ä»¥éœ€è¦åˆ†é…ä¸€ä¸ªç©ºé—²çš„å†…å­˜é¡µï¼Œå¹¶ä¿®æ”¹é¡µè¡¨å®Œæˆè™šåœ°å€åˆ°ç‰©ç†åœ°å€çš„æ˜ å°„ï¼Œåˆ·æ–°TLBï¼Œç„¶åŽè°ƒç”¨iretä¸­æ–­ï¼Œè¿”å›žåˆ°äº§ç”Ÿé¡µè®¿é—®å¼‚å¸¸çš„æŒ‡ä»¤å¤„é‡æ–°æ‰§è¡Œæ­¤æŒ‡ä»¤ã€‚å¦‚æžœè¯¥è™šåœ°å€ä¸åœ¨æŸVMAèŒƒå›´å†…ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€æ¬¡éžæ³•è®¿é—®ã€‚\n",
          "### é¡µé¢ç½®æ¢æœºåˆ¶\n\nå¦‚æžœè¦å®žçŽ°é¡µé¢ç½®æ¢æœºåˆ¶ï¼Œåªè€ƒè™‘é¡µæ›¿æ¢ç®—æ³•çš„è®¾è®¡ä¸Žå®žçŽ°æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œè¿˜éœ€è€ƒè™‘å…¶ä»–é—®é¢˜ï¼š\n\n* å“ªäº›é¡µå¯ä»¥è¢«æ¢å‡ºï¼Ÿ\n* ä¸€ä¸ªè™šæ‹Ÿçš„é¡µå¦‚ä½•ä¸Žç¡¬ç›˜ä¸Šçš„æ‰‡åŒºå»ºç«‹å¯¹åº”å…³ç³»ï¼Ÿ\n* ä½•æ—¶è¿›è¡Œæ¢å…¥å’Œæ¢å‡ºæ“ä½œï¼Ÿ\n* å¦‚ä½•è®¾è®¡æ•°æ®ç»“æž„ä»¥æ”¯æŒé¡µæ›¿æ¢ç®—æ³•ï¼Ÿ\n* å¦‚ä½•å®Œæˆé¡µçš„æ¢å…¥æ¢å‡ºæ“ä½œï¼Ÿ\n\nè¿™äº›é—®é¢˜åœ¨ä¸‹é¢ä¼šé€ä¸€è¿›è¡Œåˆ†æžã€‚æ³¨æ„ï¼Œåœ¨å®žéªŒä¸‰ä¸­ä»…å®žçŽ°äº†ç®€å•çš„é¡µé¢ç½®æ¢æœºåˆ¶ï¼Œä½†çŽ°åœ¨è¿˜æ²¡æœ‰æ¶‰åŠå®žéªŒå››å’Œå®žéªŒäº”æ‰å®žçŽ°çš„å†…æ ¸çº¿ç¨‹å’Œç”¨æˆ·è¿›ç¨‹ï¼Œæ‰€ä»¥è¿˜æ— æ³•é€šè¿‡å†…æ ¸çº¿ç¨‹æœºåˆ¶å®žçŽ°ä¸€ä¸ªå®Œæ•´æ„ä¹‰ä¸Šçš„è™šæ‹Ÿå†…å­˜é¡µé¢ç½®æ¢åŠŸèƒ½ã€‚\n\n#### 1. å¯ä»¥è¢«æ¢å‡ºçš„é¡µ\n\nåœ¨æ“ä½œç³»ç»Ÿçš„è®¾è®¡ä¸­ï¼Œä¸€ä¸ªåŸºæœ¬çš„åŽŸåˆ™æ˜¯ï¼šå¹¶éžæ‰€æœ‰çš„ç‰©ç†é¡µéƒ½å¯ä»¥äº¤æ¢å‡ºåŽ»çš„ï¼Œåªæœ‰æ˜ å°„åˆ°ç”¨æˆ·ç©ºé—´ä¸”è¢«ç”¨æˆ·ç¨‹åºç›´æŽ¥è®¿é—®çš„é¡µé¢æ‰èƒ½è¢«äº¤æ¢ï¼Œè€Œè¢«å†…æ ¸ç›´æŽ¥ä½¿ç”¨çš„å†…æ ¸ç©ºé—´çš„é¡µé¢ä¸èƒ½è¢«æ¢å‡ºã€‚è¿™é‡Œé¢çš„åŽŸå› æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæ“ä½œç³»ç»Ÿæ˜¯æ‰§è¡Œçš„å…³é”®ä»£ç ï¼Œéœ€è¦ä¿è¯è¿è¡Œçš„é«˜æ•ˆæ€§å’Œå®žæ—¶æ€§ï¼Œå¦‚æžœåœ¨æ“ä½œç³»ç»Ÿæ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå‘ç”Ÿäº†ç¼ºé¡µçŽ°è±¡ï¼Œåˆ™æ“ä½œç³»ç»Ÿä¸å¾—ä¸ç­‰å¾ˆé•¿æ—¶é—´ï¼ˆç¡¬ç›˜çš„è®¿é—®é€Ÿåº¦æ¯”å†…å­˜çš„è®¿é—®é€Ÿåº¦æ…¢2\\~3ä¸ªæ•°é‡çº§ï¼‰ï¼Œè¿™å°†å¯¼è‡´æ•´ä¸ªç³»ç»Ÿè¿è¡Œä½Žæ•ˆã€‚è€Œä¸”ï¼Œä¸éš¾æƒ³è±¡ï¼Œå¤„ç†ç¼ºé¡µè¿‡ç¨‹æ‰€ç”¨åˆ°çš„å†…æ ¸ä»£ç æˆ–è€…æ•°æ®å¦‚æžœè¢«æ¢å‡ºï¼Œæ•´ä¸ªå†…æ ¸éƒ½é¢ä¸´å´©æºƒçš„å±é™©ã€‚\n\nä½†åœ¨å®žéªŒä¸‰å®žçŽ°çš„ucoreä¸­ï¼Œæˆ‘ä»¬åªæ˜¯å®žçŽ°äº†æ¢å…¥æ¢å‡ºæœºåˆ¶ï¼Œè¿˜æ²¡æœ‰è®¾è®¡ç”¨æˆ·æ€æ‰§è¡Œçš„ç¨‹åºï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨å®žéªŒä¸‰ä¸­ä»…ä»…é€šè¿‡æ‰§è¡Œcheck\\_swapå‡½æ•°åœ¨å†…æ ¸ä¸­åˆ†é…ä¸€äº›é¡µï¼Œæ¨¡æ‹Ÿå¯¹è¿™äº›é¡µçš„è®¿é—®ï¼Œç„¶åŽé€šè¿‡do\\_pgfaultæ¥è°ƒç”¨swap\\_map\\_swappableå‡½æ•°æ¥æŸ¥è¯¢è¿™äº›é¡µçš„è®¿é—®æƒ…å†µå¹¶é—´æŽ¥è°ƒç”¨ç›¸å…³å‡½æ•°ï¼Œæ¢å‡ºâ€œä¸å¸¸ç”¨â€çš„é¡µåˆ°ç£ç›˜ä¸Šã€‚\n\n#### 2. è™šå­˜ä¸­çš„é¡µä¸Žç¡¬ç›˜ä¸Šçš„æ‰‡åŒºä¹‹é—´çš„æ˜ å°„å…³ç³»\n\nå¦‚æžœä¸€ä¸ªé¡µè¢«ç½®æ¢åˆ°äº†ç¡¬ç›˜ä¸Šï¼Œé‚£æ“ä½œç³»ç»Ÿå¦‚ä½•èƒ½ç®€æ·æ¥è¡¨ç¤ºè¿™ç§æƒ…å†µå‘¢ï¼Ÿåœ¨ucoreçš„è®¾è®¡ä¸Šï¼Œå……åˆ†åˆ©ç”¨äº†é¡µè¡¨ä¸­çš„PTEæ¥è¡¨ç¤ºè¿™ç§æƒ…å†µï¼šå½“ä¸€ä¸ªPTEç”¨æ¥æè¿°ä¸€èˆ¬æ„ä¹‰ä¸Šçš„ç‰©ç†é¡µæ—¶ï¼Œæ˜¾ç„¶å®ƒåº”è¯¥ç»´æŠ¤å„ç§æƒé™å’Œæ˜ å°„å…³ç³»ï¼Œä»¥åŠåº”è¯¥æœ‰PTE\\_Pæ ‡è®°ï¼›ä½†å½“å®ƒç”¨æ¥æè¿°ä¸€ä¸ªè¢«ç½®æ¢å‡ºåŽ»çš„ç‰©ç†é¡µæ—¶ï¼Œå®ƒè¢«ç”¨æ¥ç»´æŠ¤è¯¥ç‰©ç†é¡µä¸Ž swap ç£ç›˜ä¸Šæ‰‡åŒºçš„æ˜ å°„å…³ç³»ï¼Œå¹¶ä¸”è¯¥ PTE ä¸åº”è¯¥ç”± MMU å°†å®ƒè§£é‡Šæˆç‰©ç†é¡µæ˜ å°„(å³æ²¡æœ‰ PTE\\_P æ ‡è®°)ï¼Œä¸Žæ­¤åŒæ—¶å¯¹åº”çš„æƒé™åˆ™äº¤ç”± mm\\_struct æ¥ç»´æŠ¤ï¼Œå½“å¯¹ä½äºŽè¯¥é¡µçš„å†…å­˜åœ°å€è¿›è¡Œè®¿é—®çš„æ—¶å€™ï¼Œå¿…ç„¶å¯¼è‡´ page faultï¼Œç„¶åŽucoreèƒ½å¤Ÿæ ¹æ® PTE æè¿°çš„ swap é¡¹å°†ç›¸åº”çš„ç‰©ç†é¡µé‡æ–°å»ºç«‹èµ·æ¥ï¼Œå¹¶æ ¹æ®è™šå­˜æ‰€æè¿°çš„æƒé™é‡æ–°è®¾ç½®å¥½ PTE ä½¿å¾—å†…å­˜è®¿é—®èƒ½å¤Ÿç»§ç»­æ­£å¸¸è¿›è¡Œã€‚\n\nå¦‚æžœä¸€ä¸ªé¡µï¼ˆ4KB/é¡µï¼‰è¢«ç½®æ¢åˆ°äº†ç¡¬ç›˜æŸ8ä¸ªæ‰‡åŒºï¼ˆ0.5KB/æ‰‡åŒºï¼‰ï¼Œè¯¥PTEçš„æœ€ä½Žä½--presentä½åº”è¯¥ä¸º0 ï¼ˆå³ PTE\\_P æ ‡è®°ä¸ºç©ºï¼Œè¡¨ç¤ºè™šå®žåœ°å€æ˜ å°„å…³ç³»ä¸å­˜åœ¨ï¼‰ï¼ŒæŽ¥ä¸‹æ¥çš„7ä½æš‚æ—¶ä¿ç•™ï¼Œå¯ä»¥ç”¨ä½œå„ç§æ‰©å±•ï¼›è€ŒåŽŸæ¥ç”¨æ¥è¡¨ç¤ºé¡µå¸§å·çš„é«˜24ä½åœ°å€ï¼Œæ°å¥½å¯ä»¥ç”¨æ¥è¡¨ç¤ºæ­¤é¡µåœ¨ç¡¬ç›˜ä¸Šçš„èµ·å§‹æ‰‡åŒºçš„ä½ç½®ï¼ˆå…¶ä»Žç¬¬å‡ ä¸ªæ‰‡åŒºå¼€å§‹ï¼‰ã€‚ä¸ºäº†åœ¨é¡µè¡¨é¡¹ä¸­åŒºåˆ« 0 å’Œ swap åˆ†åŒºçš„æ˜ å°„ï¼Œå°† swap åˆ†åŒºçš„ä¸€ä¸ª page ç©ºå‡ºæ¥ä¸ç”¨ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªé«˜24ä½ä¸ä¸º0ï¼Œè€Œæœ€ä½Žä½ä¸º0çš„PTEè¡¨ç¤ºäº†ä¸€ä¸ªæ”¾åœ¨ç¡¬ç›˜ä¸Šçš„é¡µçš„èµ·å§‹æ‰‡åŒºå·ï¼ˆè§swap.hä¸­å¯¹swap\\_entry\\_tçš„æè¿°ï¼‰ï¼š\n\n```\nswap_entry_t\n-------------------------\n| offset | reserved | 0 |\n-------------------------\n24 bits 7 bits 1 bit\n```\n\nè€ƒè™‘åˆ°ç¡¬ç›˜çš„æœ€å°è®¿é—®å•ä½æ˜¯ä¸€ä¸ªæ‰‡åŒºï¼Œè€Œä¸€ä¸ªæ‰‡åŒºçš„å¤§å°ä¸º512ï¼ˆ2\\^8ï¼‰å­—èŠ‚ï¼Œæ‰€ä»¥éœ€è¦8ä¸ªè¿žç»­æ‰‡åŒºæ‰èƒ½æ”¾ç½®ä¸€ä¸ª4KBçš„é¡µã€‚åœ¨ucoreä¸­ï¼Œç”¨äº†ç¬¬äºŒä¸ªIDEç¡¬ç›˜æ¥ä¿å­˜è¢«æ¢å‡ºçš„æ‰‡åŒºï¼Œæ ¹æ®å®žéªŒä¸‰çš„è¾“å‡ºä¿¡æ¯\n\n```\nâ€œide 1: 262144(sectors), 'QEMU HARDDISK'.â€\n```\n\næˆ‘ä»¬å¯ä»¥çŸ¥é“å®žéªŒä¸‰å¯ä»¥ä¿å­˜262144/8=32768ä¸ªé¡µï¼Œå³128MBçš„å†…å­˜ç©ºé—´ã€‚swap\nåˆ†åŒºçš„å¤§å°æ˜¯ swapfs\\_init é‡Œé¢æ ¹æ®ç£ç›˜é©±åŠ¨çš„æŽ¥å£è®¡ç®—å‡ºæ¥çš„ï¼Œç›®å‰ ucore\né‡Œé¢è¦æ±‚ swap ç£ç›˜è‡³å°‘åŒ…å« 1000 ä¸ª pageï¼Œå¹¶ä¸”è‡³å¤šèƒ½ä½¿ç”¨ 1<<24 ä¸ªpageã€‚\n\n#### 3. æ‰§è¡Œæ¢å…¥æ¢å‡ºçš„æ—¶æœº\n\nåœ¨å®žéªŒä¸‰ä¸­ï¼Œ check\\_mm\\_structå˜é‡è¿™ä¸ªæ•°æ®ç»“æž„è¡¨ç¤ºäº†ç›®å‰\nucoreè®¤ä¸ºåˆæ³•çš„æ‰€æœ‰è™šæ‹Ÿå†…å­˜ç©ºé—´é›†åˆï¼Œè€Œmmä¸­çš„æ¯ä¸ªvmaè¡¨ç¤ºäº†ä¸€æ®µåœ°å€è¿žç»­çš„åˆæ³•è™šæ‹Ÿç©ºé—´ã€‚å½“ucoreæˆ–åº”ç”¨ç¨‹åºè®¿é—®åœ°å€æ‰€åœ¨çš„é¡µä¸åœ¨å†…å­˜æ—¶ï¼Œå°±ä¼šäº§ç”Ÿpage faultå¼‚å¸¸ï¼Œå¼•èµ·è°ƒç”¨do\\_pgfaultå‡½æ•°ï¼Œæ­¤å‡½æ•°ä¼šåˆ¤æ–­äº§ç”Ÿè®¿é—®å¼‚å¸¸çš„åœ°å€å±žäºŽcheck\\_mm\\_structæŸä¸ªvmaè¡¨ç¤ºçš„åˆæ³•è™šæ‹Ÿåœ°å€ç©ºé—´ï¼Œä¸”ä¿å­˜åœ¨ç¡¬ç›˜swapæ–‡ä»¶ä¸­ï¼ˆå³å¯¹åº”çš„PTEçš„é«˜24ä½ä¸ä¸º0ï¼Œè€Œæœ€ä½Žä½ä¸º0ï¼‰ï¼Œåˆ™æ˜¯æ‰§è¡Œé¡µæ¢å…¥çš„æ—¶æœºï¼Œå°†è°ƒç”¨swap\\_inå‡½æ•°å®Œæˆé¡µé¢æ¢å…¥ã€‚\n\næ¢å‡ºé¡µé¢çš„æ—¶æœºç›¸å¯¹å¤æ‚ä¸€äº›ï¼Œé’ˆå¯¹ä¸åŒçš„ç­–ç•¥æœ‰ä¸åŒçš„æ—¶æœºã€‚ucoreç›®å‰å¤§è‡´æœ‰ä¸¤ç§ç­–ç•¥ï¼Œå³ç§¯æžæ¢å‡ºç­–ç•¥å’Œæ¶ˆæžæ¢å‡ºç­–ç•¥ã€‚ç§¯æžæ¢å‡ºç­–ç•¥æ˜¯æŒ‡æ“ä½œç³»ç»Ÿå‘¨æœŸæ€§åœ°ï¼ˆæˆ–åœ¨ç³»ç»Ÿä¸å¿™çš„æ—¶å€™ï¼‰ä¸»åŠ¨æŠŠæŸäº›è®¤ä¸ºâ€œä¸å¸¸ç”¨â€çš„é¡µæ¢å‡ºåˆ°ç¡¬ç›˜ä¸Šï¼Œä»Žè€Œç¡®ä¿ç³»ç»Ÿä¸­æ€»æœ‰ä¸€å®šæ•°é‡çš„ç©ºé—²é¡µå­˜åœ¨ï¼Œè¿™æ ·å½“éœ€è¦ç©ºé—²é¡µæ—¶ï¼ŒåŸºæœ¬ä¸Šèƒ½å¤ŸåŠæ—¶æ»¡è¶³éœ€æ±‚ï¼›æ¶ˆæžæ¢å‡ºç­–ç•¥æ˜¯æŒ‡ï¼Œåªæ˜¯å½“è¯•å›¾å¾—åˆ°ç©ºé—²é¡µæ—¶ï¼Œå‘çŽ°å½“å‰æ²¡æœ‰ç©ºé—²çš„ç‰©ç†é¡µå¯ä¾›åˆ†é…ï¼Œè¿™æ—¶æ‰å¼€å§‹æŸ¥æ‰¾â€œä¸å¸¸ç”¨â€é¡µé¢ï¼Œå¹¶æŠŠä¸€ä¸ªæˆ–å¤šä¸ªè¿™æ ·çš„é¡µæ¢å‡ºåˆ°ç¡¬ç›˜ä¸Šã€‚\n\nåœ¨å®žéªŒä¸‰ä¸­çš„åŸºæœ¬ç»ƒä¹ ä¸­ï¼Œæ”¯æŒä¸Šè¿°çš„ç¬¬äºŒç§æƒ…å†µã€‚å¯¹äºŽç¬¬ä¸€ç§ç§¯æžæ¢å‡ºç­–ç•¥ï¼Œå³æ¯éš”1ç§’æ‰§è¡Œä¸€æ¬¡çš„å®žçŽ°ç§¯æžçš„æ¢å‡ºç­–ç•¥ï¼Œå¯è€ƒè™‘åœ¨æ‰©å±•ç»ƒä¹ ä¸­å®žçŽ°ã€‚å¯¹äºŽç¬¬äºŒç§æ¶ˆæžçš„æ¢å‡ºç­–ç•¥ï¼Œåˆ™æ˜¯åœ¨ucoreè°ƒç”¨alloc\\_pageså‡½æ•°èŽ·å–ç©ºé—²é¡µæ—¶ï¼Œæ­¤å‡½æ•°å¦‚æžœå‘çŽ°æ— æ³•ä»Žç‰©ç†å†…å­˜é¡µåˆ†é…å™¨èŽ·å¾—ç©ºé—²é¡µï¼Œå°±ä¼šè¿›ä¸€æ­¥è°ƒç”¨swap\\_outå‡½æ•°æ¢å‡ºæŸé¡µï¼Œå®žçŽ°ä¸€ç§æ¶ˆæžçš„æ¢å‡ºç­–ç•¥ã€‚\n\n#### 4. é¡µæ›¿æ¢ç®—æ³•çš„æ•°æ®ç»“æž„è®¾è®¡\n\nåˆ°å®žéªŒäºŒä¸ºæ­¢ï¼Œæˆ‘ä»¬çŸ¥é“ç›®å‰è¡¨ç¤ºå†…å­˜ä¸­ç‰©ç†é¡µä½¿ç”¨æƒ…å†µçš„å˜é‡æ˜¯åŸºäºŽæ•°æ®ç»“æž„Pageçš„å…¨å±€å˜é‡pagesæ•°ç»„ï¼Œpagesçš„æ¯ä¸€é¡¹è¡¨ç¤ºäº†è®¡ç®—æœºç³»ç»Ÿä¸­ä¸€ä¸ªç‰©ç†é¡µçš„ä½¿ç”¨æƒ…å†µã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†é¡µå¯è¢«æ¢å‡ºæˆ–å·²è¢«æ¢å‡ºçš„æƒ…å†µï¼Œå¯å¯¹Pageæ•°æ®ç»“æž„è¿›è¡Œæ‰©å±•ï¼š\n\n```\nstruct Page {  \nâ€¦â€¦   \nlist\\_entry\\_t pra\\_page\\_link;   \nuintptr\\_t pra\\_vaddr;   \n};  \n```\n\npra\\_page\\_linkå¯ç”¨æ¥æž„é€ æŒ‰é¡µçš„ç¬¬ä¸€æ¬¡è®¿é—®æ—¶é—´è¿›è¡ŒæŽ’åºçš„ä¸€ä¸ªé“¾è¡¨ï¼Œè¿™ä¸ªé“¾è¡¨çš„å¼€å§‹è¡¨ç¤ºç¬¬ä¸€æ¬¡è®¿é—®æ—¶é—´æœ€è¿‘çš„é¡µï¼Œé“¾è¡¨ç»“å°¾è¡¨ç¤ºç¬¬ä¸€æ¬¡è®¿é—®æ—¶é—´æœ€è¿œçš„é¡µã€‚å½“ç„¶é“¾è¡¨å¤´å¯ä»¥å°±å¯è®¾ç½®ä¸ºpra\\_list\\_headï¼ˆå®šä¹‰åœ¨swap\\_fifo.cä¸­ï¼‰ï¼Œæž„é€ çš„æ—¶æœºæ˜¯åœ¨page faultå‘ç”ŸåŽï¼Œè¿›è¡Œdo\\_pgfaultå‡½æ•°æ—¶ã€‚pra\\_vaddrå¯ä»¥ç”¨æ¥è®°å½•æ­¤ç‰©ç†é¡µå¯¹åº”çš„è™šæ‹Ÿé¡µèµ·å§‹åœ°å€ã€‚\n\nå½“ä¸€ä¸ªç‰©ç†é¡µ ï¼ˆstruct Pageï¼‰ éœ€è¦è¢« swap å‡ºåŽ»çš„æ—¶å€™ï¼Œé¦–å…ˆéœ€è¦ç¡®ä¿å®ƒå·²ç»åˆ†é…äº†ä¸€ä¸ªä½äºŽç£ç›˜ä¸Šçš„swap pageï¼ˆç”±è¿žç»­çš„8ä¸ªæ‰‡åŒºç»„æˆï¼‰ã€‚è¿™é‡Œä¸ºäº†ç®€åŒ–è®¾è®¡ï¼Œåœ¨swap\\_checkå‡½æ•°ä¸­å»ºç«‹äº†æ¯ä¸ªè™šæ‹Ÿé¡µå”¯ä¸€å¯¹åº”çš„swap pageï¼Œå…¶å¯¹åº”å…³ç³»è®¾å®šä¸ºï¼šè™šæ‹Ÿé¡µå¯¹åº”çš„PTEçš„ç´¢å¼•å€¼ = swap pageçš„æ‰‡åŒºèµ·å§‹ä½ç½®\\*8ã€‚\n\nä¸ºäº†å®žçŽ°å„ç§é¡µæ›¿æ¢ç®—æ³•ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¡µæ›¿æ¢ç®—æ³•çš„ç±»æ¡†æž¶swap\\_manager:\n\n```\nstruct swap_manager  \n{  \n    const char *name;  \n    /* Global initialization for the swap manager */  \n    int (*init) (void);  \n    /* Initialize the priv data inside mm_struct */  \n    int (*init_mm) (struct mm_struct *mm);  \n    /* Called when tick interrupt occured */  \n    int (*tick_event) (struct mm_struct *mm);  \n    /* Called when map a swappable page into the mm_struct */  \n    int (*map_swappable) (struct mm_struct *mm, uintptr_t addr, struct Page *page, int swap_in);   \n    /* When a page is marked as shared, this routine is called to delete the addr entry from the swap manager */\n    int (*set_unswappable) (struct mm_struct *mm, uintptr_t addr);  \n    /* Try to swap out a page, return then victim */  \n    int (*swap_out_victim) (struct mm_struct *mm, struct Page *ptr_page, int in_tick);  \n    /* check the page relpacement algorithm */  \n    int (*check\\_swap)(void);   \n};  \n``` \n\nè¿™é‡Œå…³é”®çš„ä¸¤ä¸ªå‡½æ•°æŒ‡é’ˆæ˜¯map\\_swappableå’Œswap\\_out\\_vistimï¼Œå‰ä¸€ä¸ªå‡½æ•°ç”¨äºŽè®°å½•é¡µè®¿é—®æƒ…å†µç›¸å…³å±žæ€§ï¼ŒåŽä¸€ä¸ªå‡½æ•°ç”¨äºŽæŒ‘é€‰éœ€è¦æ¢å‡ºçš„é¡µã€‚æ˜¾ç„¶ç¬¬äºŒä¸ªå‡½æ•°ä¾èµ–äºŽç¬¬ä¸€ä¸ªå‡½æ•°è®°å½•çš„é¡µè®¿é—®æƒ…å†µã€‚tick\\_eventå‡½æ•°æŒ‡é’ˆä¹Ÿå¾ˆé‡è¦ï¼Œç»“åˆå®šæ—¶äº§ç”Ÿçš„ä¸­æ–­ï¼Œå¯ä»¥å®žçŽ°ä¸€ç§ç§¯æžçš„æ¢é¡µç­–ç•¥ã€‚\n\n#### 5. swap_checkçš„æ£€æŸ¥å®žçŽ°\n\nä¸‹é¢å…·ä½“è®²è¿°ä¸€ä¸‹å®žéªŒä¸‰ä¸­å®žçŽ°ç½®æ¢ç®—æ³•çš„é¡µé¢ç½®æ¢çš„æ£€æŸ¥æ‰§è¡Œé€»è¾‘ï¼Œä¾¿äºŽå¤§å®¶å®žçŽ°ç»ƒä¹ 2ã€‚å®žéªŒä¸‰çš„æ£€æŸ¥è¿‡ç¨‹åœ¨å‡½æ•°swap\\_checkï¼ˆkern/mm/swap.cä¸­ï¼‰ä¸­ï¼Œå…¶å¤§è‡´æµç¨‹å¦‚ä¸‹ã€‚\n\n1. è°ƒç”¨mm\\_createå»ºç«‹mmå˜é‡ï¼Œå¹¶è°ƒç”¨vma\\_createåˆ›å»ºvmaå˜é‡ï¼Œè®¾ç½®åˆæ³•çš„è®¿é—®èŒƒå›´ä¸º4KB\\~24KBï¼›\n2. è°ƒç”¨free\\_pageç­‰æ“ä½œï¼Œæ¨¡æ‹Ÿå½¢æˆä¸€ä¸ªåªæœ‰4ä¸ªç©ºé—² physical pageï¼›å¹¶è®¾ç½®äº†ä»Ž4KB\\~24KBçš„è¿žç»­5ä¸ªè™šæ‹Ÿé¡µçš„è®¿é—®æ“ä½œï¼›\n3. è®¾ç½®è®°å½•ç¼ºé¡µæ¬¡æ•°çš„å˜é‡pgfault\\_num=0ï¼Œæ‰§è¡Œcheck\\_content\\_setå‡½æ•°ï¼Œä½¿å¾—èµ·å§‹åœ°å€åˆ†åˆ«å¯¹èµ·å§‹åœ°å€ä¸º0x1000, 0x2000, 0x3000, 0x4000çš„è™šæ‹Ÿé¡µæŒ‰æ—¶é—´é¡ºåºå…ˆåŽå†™æ“ä½œè®¿é—®ï¼Œç”±äºŽä¹‹å‰æ²¡æœ‰å»ºç«‹é¡µè¡¨ï¼Œæ‰€ä»¥ä¼šäº§ç”Ÿpage faultå¼‚å¸¸ï¼Œå¦‚æžœå®Œæˆç»ƒä¹ 1ï¼Œåˆ™è¿™äº›ä»Ž4KB\\~20KBçš„4è™šæ‹Ÿé¡µä¼šä¸Žucoreä¿å­˜çš„4ä¸ªç‰©ç†é¡µå¸§å»ºç«‹æ˜ å°„å…³ç³»ï¼›\n4. ç„¶åŽå¯¹è™šé¡µå¯¹åº”çš„æ–°äº§ç”Ÿçš„é¡µè¡¨é¡¹è¿›è¡Œåˆæ³•æ€§æ£€æŸ¥ï¼›\n5. ç„¶åŽè¿›å…¥æµ‹è¯•é¡µæ›¿æ¢ç®—æ³•çš„ä¸»ä½“ï¼Œæ‰§è¡Œå‡½æ•°check\\_content\\_accessï¼Œå¹¶è¿›ä¸€æ­¥è°ƒç”¨åˆ°\\_fifo\\_check\\_swapå‡½æ•°ï¼Œå¦‚æžœé€šè¿‡äº†æ‰€æœ‰çš„assertã€‚è¿™è¿›ä¸€æ­¥è¡¨ç¤ºFIFOé¡µæ›¿æ¢ç®—æ³•åŸºæœ¬æ­£ç¡®å®žçŽ°ï¼›\n6. æœ€åŽæ¢å¤ucoreçŽ¯å¢ƒã€‚\n"
        ],
        "test_patch": "",
        "patch_preview": "From 012434e9ee8c214e0059409c0af67e7269ba175c Mon Sep 17 00:00:00 2001\nFrom: leefige <templar_git@163.com>\nDate: Sat, 24 Mar 2018 10:45:05 +0800\nSubject: [PATCH 1/3] Visual improvement by specifying language for code\n blocks; typo fix.\n\n---\n lab2/lab2_3_3_3_phymem_pagelevel.md     | 34 ++++++++++++-------------\n lab2/lab2_3_3_4_phymem_allocation.md    | 14 +++++-----\n lab2/lab2_3_5_probe_phymem_methods.md   |  6 ++---\n lab2/lab2_3_6_implement_probe_phymem.md |  2 +-\n lab2/lab2_3_7_phymemlab_conc"
      },
      "patch": {
        "length": 20190,
        "files_changed": 17,
        "lines_added": 56,
        "lines_deleted": 56,
        "net_change": 0,
        "changed_files": [
          {
            "file": "lab2/lab2_3_3_3_phymem_pagelevel.md",
            "added": 17,
            "deleted": 17
          },
          {
            "file": "lab2/lab2_3_3_4_phymem_allocation.md",
            "added": 7,
            "deleted": 7
          },
          {
            "file": "lab2/lab2_3_5_probe_phymem_methods.md",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "lab2/lab2_3_6_implement_probe_phymem.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab2/lab2_3_7_phymemlab_concepts.md",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "lab0/lab0_2_3_1_4_extend_gcc_asm.md",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "lab2/lab2_3_3_5_3_setup_paging_map.md",
            "added": 4,
            "deleted": 4
          },
          {
            "file": "lab2/lab2_3_3_5_4_maping_relations.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab2/lab2_3_3_6_self_mapping.md",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "lab2/lab2_3_6_implement_probe_phymem.md",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "lab2/lab2_3_7_phymemlab_concepts.md",
            "added": 2,
            "deleted": 2
          },
          {
            "file": "SUMMARY.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab3/lab3_2_2_files.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab3/lab3_3_2_labs_steps.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab3/lab3_3_3_data_structures.md",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "lab3/lab3_4_page_fault_handler.md",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "lab3/lab3_5_2_page_swapping_principles.md",
            "added": 5,
            "deleted": 5
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 233,
        "total_lines": 17151,
        "total_bytes": 1826609,
        "python_files": 1,
        "python_lines": 25,
        "file_extensions": {
          ".json": 1,
          ".md": 192,
          ".sh": 1,
          ".png": 37,
          ".py": 1,
          "": 1
        },
        "largest_files": [
          {
            "path": "lab8_figs/image002.png",
            "size": 107546,
            "lines": 759,
            "extension": ".png"
          },
          {
            "path": "lab8_figs/image001.png",
            "size": 113278,
            "lines": 698,
            "extension": ".png"
          },
          {
            "path": "lab0_figs/image001.png",
            "size": 97038,
            "lines": 646,
            "extension": ".png"
          },
          {
            "path": "lab1_figs/image012.png",
            "size": 72749,
            "lines": 569,
            "extension": ".png"
          },
          {
            "path": "lab3_figs/image001.png",
            "size": 44760,
            "lines": 553,
            "extension": ".png"
          },
          {
            "path": "lab2_figs/image004.png",
            "size": 67544,
            "lines": 530,
            "extension": ".png"
          },
          {
            "path": "lab0_figs/image002.png",
            "size": 65187,
            "lines": 456,
            "extension": ".png"
          },
          {
            "path": "lab1_figs/image011.png",
            "size": 55970,
            "lines": 431,
            "extension": ".png"
          },
          {
            "path": "lab2_figs/image003.png",
            "size": 48307,
            "lines": 380,
            "extension": ".png"
          },
          {
            "path": "lab1_figs/image006.png",
            "size": 51963,
            "lines": 337,
            "extension": ".png"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 233,
        "files_changed_count": 17,
        "files_changed_ratio": 0.07296137339055794,
        "total_lines_in_repo": 17151,
        "lines_added": 56,
        "lines_deleted": 56,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.006530231473383477,
        "pr_body_length": 131,
        "commit_message_length": 13,
        "python_file_count": 1,
        "python_line_count": 25
      }
    },
    {
      "tar_file_name": "codelion#optillm#pull#208",
      "repo_name": "codelion#optillm#pull#208",
      "success": true,
      "error": null,
      "commit": {
        "sha": "7904463799373a478ac37166b1a5af594bbdd353",
        "message": "Merge pull request #207 from codelion/fix-cot-reflection\n\nFix cot reflection",
        "author": {
          "name": "Asankhaya Sharma",
          "email": "codelion@users.noreply.github.com",
          "date": "2025-07-07T05:33:30Z"
        },
        "html_url": "https://github.com/algorithmicsuperintelligence/optillm/commit/7904463799373a478ac37166b1a5af594bbdd353",
        "api_url": "https://api.github.com/repos/algorithmicsuperintelligence/optillm/commits/7904463799373a478ac37166b1a5af594bbdd353"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/codelion#optillm#pull#208",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/codelion#optillm#pull#208.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/codelion#optillm#pull#208/source_code"
      },
      "pr": {
        "number": 208,
        "title": "Add majority voting plugin for candidate selection",
        "body": "Introduces a plugin that generates multiple candidate solutions using the OpenAI API and selects the most frequent answer via majority voting. Includes answer extraction, normalization, and a summary of the voting process. Useful for tasks with discrete answers such as math, coding, and multiple choice problems.\r\n\r\nfixes #201 \r\nsuperseeds #83 ",
        "state": "closed",
        "created_at": "2025-07-08T12:27:46Z",
        "updated_at": "2025-07-10T22:04:52Z",
        "merged_at": "2025-07-10T22:04:50Z",
        "html_url": "https://github.com/codelion/optillm/pull/208",
        "user": "codelion",
        "additions": 997,
        "deletions": 200,
        "changed_files": 9,
        "commits": 6
      },
      "swebench": {
        "instance_id": "codelion_optillm-208",
        "repo": "/codelion/optillm",
        "base_commit": "7904463799373a478ac37166b1a5af594bbdd353",
        "problem_statement": {
          "title": "Implement majority vote for parallel inferences",
          "body": "We already have best of n, we can implement a majority vote plugin that samples k times and then chooses the response that is in the majority. This will help compare other techniques to it."
        },
        "edit_files": [
          "optillm/plugins/majority_voting_plugin.py",
          "optillm/plugins/majority_voting_plugin.py",
          "scripts/eval_optillmbench.py",
          "optillm/inference.py",
          "optillm/thinkdeeper_mlx.py",
          "scripts/eval_optillmbench.py",
          "scripts/eval_aime_benchmark.py",
          "scripts/eval_optillmbench.py",
          "optillm/bon.py",
          "optillm/moa.py",
          "optillm/plugins/majority_voting_plugin.py",
          "optillm/__init__.py",
          "setup.py"
        ],
        "oracle_files": [
          "",
          "",
          "#!/usr/bin/env python3\nimport argparse\nimport time\nimport json\nimport os\nfrom typing import Dict, List, Any, Tuple\nimport datasets\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm import tqdm\nimport logging\nfrom datetime import datetime\nimport re\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define the approaches to test\n# Each approach is (name, description)\nAPPROACHES = [\n    (\"none\", \"Baseline without any optimization\"),\n    (\"leap\", \"LEAP Approach\"),\n    (\"rto\", \"Round Trip Optimization\"),\n    (\"cot_reflection\", \"Chain of Thought with Reflection\"),\n    (\"self_consistency\", \"Self Consistency Check\"),\n    (\"plansearch\", \"Planning with Search\"),\n    (\"re2\", \"ReRead Approach\"),\n    (\"z3\", \"Z3 Solver for Mathematical Problems\"),\n    (\"coc\", \"Chain of Code\"),\n    (\"executecode\" , \"Execute Code\"),\n    (\"spl\", \"System Prompt Learning\")\n]\n\ndef load_optillm_bench() -> datasets.Dataset:\n    \"\"\"Load the OptiLLM Bench dataset.\"\"\"\n    try:\n        dataset = load_dataset(\"codelion/optillmbench\")\n        return dataset[\"test\"]  # We use the test split for evaluation\n    except Exception as e:\n        logger.error(f\"Error loading dataset: {e}\")\n        raise\n\ndef extract_gsm8k_answer(text: str) -> float:\n    \"\"\"Extract numerical answer after ### from GSM8K responses.\"\"\"\n    match = re.search(r'###\\s*(-?\\d*\\.?\\d+)', text)\n    if match:\n        try:\n            return float(match.group(1))\n        except ValueError:\n            return None\n    return None\n\ndef remove_thinking_blocks(text: str) -> str:\n    \"\"\"\n    Remove <think>...</think> blocks from the response.\n    If there's a </think> tag, only keep the content after it.\n    \"\"\"\n    if not text:\n        return text\n        \n    # Check if there's a thinking block\n    if '</think>' in text:\n        # Get everything after the last </think> tag\n        parts = text.split('</think>')\n        return parts[-1].strip()\n    \n    # If no thinking blocks, return original text\n    return text\n\ndef extract_choice_index_from_question(question: str, answer: str) -> int:\n    \"\"\"\n    Extract the index of the correct answer from a multiple-choice question.\n    \n    Args:\n        question: The question text containing choices\n        answer: The correct answer (just the text, no index)\n    \n    Returns:\n        int: The index of the correct answer, or -1 if not found\n    \"\"\"\n    # Look for a pattern like \"N. answer\" in the question\n    answer_clean = answer.strip().lower()\n    \n    # Debug logging for critical examples\n    logger.debug(f\"Looking for answer: '{answer_clean}' in question\")\n    \n    # Check for \"Choices:\" marker in the question\n    if \"choices:\" in question.lower():\n        # Split the question by lines after \"Choices:\"\n        choices_section = question.lower().split(\"choices:\")[1].strip()\n        \n        # Log the choices section\n        logger.debug(f\"Choices section: '{choices_section}'\")\n        \n        # Try different approaches to extract choices\n        \n        # 1. If it's all on one line, use a more comprehensive regex\n        if '\\n' not in choices_section:\n            # This pattern matches \"N. text\" where N is a digit and text is any text up to the next number or end\n            all_choices = re.findall(r'(\\d+)\\s*\\.\\s*([^0-9.]+?)(?=\\s*\\d+\\s*\\.|$)', choices_section)\n            \n            logger.debug(f\"Single line choices found: {all_choices}\")\n            \n            for idx, choice_text in all_choices:\n                choice_text_clean = choice_text.strip()\n                if choice_text_clean.lower() == answer_clean:\n                    logger.debug(f\"Found match at index {idx}: '{choice_text_clean}'\")\n                    return int(idx)\n        \n        # 2. Try splitting by newlines\n        choices = choices_section.split(\"\\n\")\n        \n        for i, choice in enumerate(choices):\n            choice = choice.strip()\n            if not choice:\n                continue\n                \n            logger.debug(f\"Checking choice {i}: '{choice}'\")\n            \n            # Try to extract the index and choice text\n            match = re.match(r'\\s*(\\d+)\\s*\\.\\s*(.*)', choice)\n            if match:\n                idx = int(match.group(1))\n                choice_text = match.group(2).strip()\n                \n                logger.debug(f\"Parsed choice: index={idx}, text='{choice_text}'\")\n                \n                if choice_text.lower() == answer_clean:\n                    logger.debug(f\"Found exact match at index {idx}\")\n                    return idx\n        \n        # 3. Fallback: just look for any occurrence of the number followed by the answer\n        pattern = r'(\\d+)\\s*\\.\\s*' + re.escape(answer_clean)\n        match = re.search(pattern, choices_section)\n        if match:\n            logger.debug(f\"Fallback match found at index {match.group(1)}\")\n            return int(match.group(1))\n    \n    logger.debug(\"No match found for answer in choices\")\n    return -1\n\ndef is_numeric_only_response(response: str) -> Tuple[bool, int]:\n    \"\"\"\n    Check if the response is just a numeric value, possibly with whitespace and newlines.\n    \n    Args:\n        response: The response text to check\n        \n    Returns:\n        Tuple of (is_numeric, value)\n    \"\"\"\n    # Strip all whitespace, including newlines\n    clean_response = re.sub(r'\\s', '', response)\n    \n    # Check if it's just a number\n    if clean_response.isdigit():\n        return True, int(clean_response)\n    \n    return False, -1\n\ndef evaluate_response(response: str, ground_truth: str, category: str, question: str = None) -> bool:\n    \"\"\"\n    Evaluate if the response matches the ground truth based on category.\n    \n    Args:\n        response: Model's response\n        ground_truth: Correct answer\n        category: Problem category (gsm8k, mmlu_math, boolq, aqua_rat)\n        question: Original question text, needed for MMLU evaluation\n    \n    Returns:\n        bool: Whether the response is correct\n    \"\"\"\n    if not response or not ground_truth:\n        return False\n    \n    # First, remove any thinking blocks\n    response = remove_thinking_blocks(response)\n        \n    if category == \"gsm8k\":\n        # Extract numerical answers after ### and compare\n        response_num = extract_gsm8k_answer(response)\n        ground_truth_num = extract_gsm8k_answer(ground_truth)\n        \n        if response_num is None or ground_truth_num is None:\n            return False\n            \n        # Compare with small tolerance for floating point\n        return abs(response_num - ground_truth_num) < 1e-6\n    elif category == \"mmlu_math\":\n        # Special handling for MMLU-math multiple choice questions\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        \n        # Case 1: Exact match of answer text\n        if response_clean == ground_truth_clean:\n            logger.debug(\"Exact text match\")\n            return True\n            \n        # For other cases, we need to find what index corresponds to the ground truth\n        if question:\n            correct_index = extract_choice_index_from_question(question, ground_truth)\n            \n            if correct_index >= 0:\n                # Case 2: Check if response is just the digit (most common LLM response for indices)\n                is_numeric, value = is_numeric_only_response(response)\n                if is_numeric and value == correct_index:\n                    logger.debug(f\"Numeric match: response '{response}' -> {value} matches index {correct_index}\")\n                    return True\n                \n                # Case 3: Check if response is \"index. answer\"\n                if re.search(fr\"{correct_index}\\s*\\.\\s*{re.escape(ground_truth_clean)}\", response_clean):\n                    logger.debug(\"Pattern match for 'index. answer'\")\n                    return True\n                \n                # Case 4: Check if response contains both the index and the answer text\n                if str(correct_index) in response_clean and ground_truth_clean in response_clean:\n                    logger.debug(\"Contains both index and answer\")\n                    return True\n        \n        return False\n    else:\n        # For boolq and aqua_rat, exact match is required\n        # Clean up both strings for comparison\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        return response_clean == ground_truth_clean\n\ndef get_prompt_for_category(question: str, category: str) -> str:\n    \"\"\"\n    Generate appropriate prompt based on category.\n    \"\"\"\n    if category == \"gsm8k\":\n        return (\n            f\"Solve this math problem step by step. After solving, provide the final \"\n            f\"numerical answer after '### ' (three hash symbols and a space).\\n\\n\"\n            f\"Question: {question}\\n\\n\"\n            f\"Show your work, then give the final answer after '### '.\"\n        )\n    elif category == \"mmlu_math\":\n        return (\n            f\"Solve this math problem. Provide only the answer with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"boolq\":\n        return (\n            f\"Answer this yes/no question with only 'yes' or 'no'.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"aqua_rat\":\n        return (\n            f\"Choose the correct answer. Provide only the letter choice with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    else:\n        return f\"Question: {question}\"\n\ndef evaluate_model(\n    client: OpenAI,\n    model: str,\n    dataset: datasets.Dataset,\n    approach: str,\n    max_samples: int = None\n) -> Tuple[Dict[str, float], List[Dict[str, Any]]]:\n    \"\"\"\n    Evaluate a model on the dataset using a specific approach.\n    Returns metrics and detailed results.\n    \"\"\"\n    metrics = {\n        \"total_correct\": 0,\n        \"total_time\": 0,\n        \"samples\": 0,\n    }\n    \n    # Initialize category-specific metrics\n    category_metrics = {}\n    \n    # Detailed results for each example\n    detailed_results = []\n    \n    # Prepare the dataset\n    examples = dataset if max_samples is None else dataset.select(range(max_samples))\n    \n    # Create model name with approach\n    full_model_name = f\"{approach}-{model}\" if approach != \"none\" else model\n    \n    for example in tqdm(examples, desc=f\"Evaluating {approach}\"):\n        try:\n            # Get appropriate prompt for the category\n            prompt = get_prompt_for_category(example['question'], example['category'])\n            \n            # Record start time\n            start_time = time.time()\n            \n            # Make API call\n            response = client.chat.completions.create(\n                model=full_model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant focused on providing precise answers in the requested format.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.2,\n                max_tokens=4096,\n                extra_body= {\"spl_learning\": False},\n            )\n            \n            # Calculate time taken\n            time_taken = time.time() - start_time\n            \n            # Get the response text\n            response_text = response.choices[0].message.content\n            \n            # Also store the raw response for reference\n            raw_response = response_text\n            \n            # Process the response to remove thinking blocks\n            processed_response = remove_thinking_blocks(response_text)\n            \n            # Evaluate the processed response\n            is_correct = evaluate_response(\n                processed_response,\n                example['answer'],\n                example['category'],\n                example['question']  # Pass the question for MMLU evaluation\n            )\n            \n            # Update metrics\n            metrics[\"total_correct\"] += int(is_correct)\n            metrics[\"total_time\"] += time_taken\n            metrics[\"samples\"] += 1\n            \n            # Update category metrics\n            if example['category'] not in category_metrics:\n                category_metrics[example['category']] = {\n                    \"correct\": 0,\n                    \"total\": 0,\n                    \"time\": 0\n                }\n            category_metrics[example['category']][\"correct\"] += int(is_correct)\n            category_metrics[example['category']][\"total\"] += 1\n            category_metrics[example['category']][\"time\"] += time_taken\n            \n            # Check if thinking blocks were removed\n            has_thinking = '</think>' in raw_response\n            \n            # Record detailed result\n            detailed_results.append({\n                \"id\": example['id'],\n                \"category\": example['category'],\n                \"correct\": is_correct,\n                \"time_taken\": time_taken,\n                \"raw_response\": raw_response,\n                \"processed_response\": processed_response if has_thinking else None,\n                \"has_thinking\": has_thinking,\n                \"ground_truth\": example['answer']\n            })\n            \n        except Exception as e:\n            logger.error(f\"Error processing example {example['id']}: {e}\")\n            continue\n    \n    # Calculate final metrics\n    final_metrics = {\n        \"accuracy\": metrics[\"total_correct\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"average_time\": metrics[\"total_time\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"total_time\": metrics[\"total_time\"],\n        \"total_samples\": metrics[\"samples\"],\n    }\n    \n    # Add category-specific metrics\n    for category, cat_metrics in category_metrics.items():\n        final_metrics[f\"{category}_accuracy\"] = cat_metrics[\"correct\"] / cat_metrics[\"total\"]\n        final_metrics[f\"{category}_average_time\"] = cat_metrics[\"time\"] / cat_metrics[\"total\"]\n    \n    return final_metrics, detailed_results\n\ndef save_results(metrics: Dict[str, float], detailed_results: List[Dict[str, Any]], \n                model: str, approach: str, output_dir: str):\n    \"\"\"Save evaluation results to files.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create model-specific directory\n    model_dir = os.path.join(output_dir, model.replace('/', '_'))\n    os.makedirs(model_dir, exist_ok=True)\n    \n    base_filename = os.path.join(model_dir, f\"{approach}_{timestamp}\")\n    \n    # Save metrics\n    with open(f\"{base_filename}_metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    \n    # Save detailed results\n    with open(f\"{base_filename}_detailed.json\", \"w\") as f:\n        json.dump(detailed_results, f, indent=2)\n    \n    # Create a summary DataFrame for easier analysis\n    df = pd.DataFrame([\n        {k: v for k, v in result.items() if k != 'raw_response' and k != 'processed_response'}\n        for result in detailed_results\n    ])\n    df.to_csv(f\"{base_filename}_summary.csv\", index=False)\n    \n    logger.info(f\"Results saved to {base_filename}_*\")\n\ndef generate_report(all_metrics: Dict[str, Dict[str, float]], output_dir: str):\n    \"\"\"Generate a comprehensive report comparing all approaches.\"\"\"\n    report = []\n    \n    # Header\n    report.append(\"# OptiLLM Bench Evaluation Report\")\n    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    \n    # Overall Results Table\n    report.append(\"## Overall Results\")\n    headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\", \"Total Time (s)\"]\n    rows = []\n    \n    for approach, metrics in all_metrics.items():\n        rows.append([\n            approach,\n            f\"{metrics['accuracy']*100:.2f}%\",\n            f\"{metrics['average_time']:.2f}\",\n            f\"{metrics['total_time']:.2f}\"\n        ])\n    \n    # Convert to DataFrame for nice formatting\n    df = pd.DataFrame(rows, columns=headers)\n    report.append(df.to_markdown())\n    \n    # Category-wise Results\n    report.append(\"\\n## Results by Category\")\n    categories = [\"gsm8k\", \"mmlu_math\", \"boolq\", \"aqua_rat\"]\n    \n    for category in categories:\n        report.append(f\"\\n### {category.upper()}\")\n        headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\"]\n        rows = []\n        \n        for approach, metrics in all_metrics.items():\n            if f\"{category}_accuracy\" in metrics:\n                rows.append([\n                    approach,\n                    f\"{metrics[f'{category}_accuracy']*100:.2f}%\",\n                    f\"{metrics[f'{category}_average_time']:.2f}\"\n                ])\n        \n        df = pd.DataFrame(rows, columns=headers)\n        report.append(df.to_markdown())\n    \n    # Save report\n    report_path = f\"{output_dir}/evaluation_report.md\"\n    with open(report_path, \"w\") as f:\n        f.write(\"\\n\\n\".join(report))\n    \n    logger.info(f\"Report saved to {report_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate a model on OptiLLM Bench\")\n    parser.add_argument(\"--model\", required=True, help=\"Model identifier\")\n    parser.add_argument(\"--base-url\", default=\"http://localhost:8000/v1\", \n                        help=\"Base URL for API endpoint\")\n    parser.add_argument(\"--max-samples\", type=int, help=\"Maximum number of samples to evaluate\")\n    parser.add_argument(\"--output-dir\", default=\"results\", \n                        help=\"Directory to save results\")\n    parser.add_argument(\"--approaches\", nargs=\"+\", \n                        help=\"Specific approaches to evaluate (default: all)\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    args = parser.parse_args()\n    \n    # Set debug logging if specified\n    if args.debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    # Create output directory\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Get API key from environment\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable must be set\")\n    \n    # Initialize OpenAI client\n    client = OpenAI(\n        api_key=api_key,\n        base_url=args.base_url\n    )\n    \n    # Load dataset\n    dataset = load_optillm_bench()\n    \n    # Determine which approaches to evaluate\n    approaches_to_test = (\n        [a[0] for a in APPROACHES if a[0] in args.approaches]\n        if args.approaches\n        else [a[0] for a in APPROACHES]\n    )\n    \n    # Store all metrics for final report\n    all_metrics = {}\n    \n    # Evaluate each approach\n    for approach in approaches_to_test:\n        logger.info(f\"Evaluating approach: {approach}\")\n        \n        try:\n            metrics, detailed_results = evaluate_model(\n                client,\n                args.model,\n                dataset,\n                approach,\n                args.max_samples\n            )\n            \n            all_metrics[approach] = metrics\n            \n            # Save results for this approach\n            save_results(metrics, detailed_results, args.model, approach, \n                        args.output_dir)\n            \n            logger.info(f\"Completed evaluation for {approach}\")\n            logger.info(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n            logger.info(f\"Average time per sample: {metrics['average_time']:.2f}s\")\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating approach {approach}: {e}\")\n            continue\n    \n    # Generate final report\n    generate_report(all_metrics, args.output_dir)\n\nif __name__ == \"__main__\":\n    main()",
          "import os\nimport torch\nimport logging\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple, Any, Union\nfrom dataclasses import dataclass\nfrom collections import OrderedDict, defaultdict\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport math\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\nfrom peft import PeftModel, PeftConfig\nimport bitsandbytes as bnb\nfrom scipy.stats import entropy\nfrom functools import lru_cache\nimport time\nimport threading\nimport traceback\nimport platform\nimport sys\n\nfrom optillm.cot_decoding import cot_decode\nfrom optillm.entropy_decoding import entropy_decode\nfrom optillm.thinkdeeper import thinkdeeper_decode\nfrom optillm.autothink import autothink_decode\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# MLX Support for Apple Silicon\ntry:\n    import mlx.core as mx\n    from mlx_lm import load as mlx_load, generate as mlx_generate\n    from mlx_lm.tokenizer_utils import TokenizerWrapper\n    MLX_AVAILABLE = True\n    logger.info(\"MLX framework available\")\nexcept ImportError:\n    MLX_AVAILABLE = False\n    logger.debug(\"MLX framework not available - falling back to PyTorch\")\n\n@dataclass\nclass ModelConfig:\n    base_model_id: str\n    adapter_ids: Optional[List[str]] = None\n    batch_size: int = 32\n    max_cache_size: int = 5\n    quantization_bits: int = 4\n    device_preference: Optional[str] = None\n    # Default generation parameters\n    max_new_tokens: int = 4096\n    do_sample: bool = True\n    top_p: float = 0.9\n    top_k: int = 50\n    temperature: float = 0.7\n    num_return_sequences: int = 1\n    repetition_penalty: float = 1.0\n    pad_token_id: Optional[int] = None\n    logprobs: bool = False\n    # Advanced parameters\n    use_memory_efficient_attention: bool = True\n    enable_prompt_caching: bool = True\n    dynamic_temperature: bool = False\n\n\n@dataclass\nclass LogProbsResult:\n    \"\"\"Container for logprobs calculation results\"\"\"\n    tokens: List[str]\n    token_logprobs: List[float]\n    top_logprobs: List[Dict[str, float]]\n    bytes_per_token: List[List[int]]\n\nclass LogProbsCalculator:\n    \"\"\"Handles calculation of log probabilities for generated tokens\"\"\"\n    \n    def __init__(self, tokenizer, model):\n        self.tokenizer = tokenizer\n        self.model = model\n        \n    def _get_bytes_for_token(self, token: str) -> List[int]:\n        \"\"\"Get UTF-8 bytes for a token\"\"\"\n        try:\n            return list(token.encode('utf-8'))\n        except UnicodeEncodeError:\n            return []\n\n    def _get_top_alternatives(\n        self,\n        logits: torch.Tensor,\n        actual_token_id: int,\n        num_alternatives: int\n    ) -> Dict[str, float]:\n        \"\"\"Calculate top alternative tokens and their logprobs\"\"\"\n        probs = F.softmax(logits, dim=-1)\n        logprobs = torch.log(probs)\n        \n        # Get top tokens excluding the actual token\n        top_values, top_indices = torch.topk(logprobs, k=num_alternatives + 1)\n        \n        alternatives = {}\n        for value, idx in zip(top_values, top_indices):\n            token = self.tokenizer.decode([idx])\n            if idx != actual_token_id:  # Skip the actual token\n                alternatives[token] = value.item()\n                if len(alternatives) >= num_alternatives:\n                    break\n                    \n        return alternatives\n\n    def calculate_logprobs(\n        self,\n        input_ids: torch.Tensor,\n        generated_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        num_alternatives: int = 5\n    ) -> LogProbsResult:\n        \"\"\"Calculate log probabilities for a sequence of tokens\"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            # Get model outputs for the entire sequence\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n            logits = outputs.logits\n            \n            # Calculate softmax and log probabilities\n            probs = F.softmax(logits, dim=-1)\n            logprobs = torch.log(probs)\n            \n            # Process each position\n            all_tokens = []\n            all_token_logprobs = []\n            all_top_logprobs = []\n            all_bytes = []\n            \n            sequence_length = generated_ids.shape[-1]\n            \n            for pos in range(sequence_length - 1):  # -1 because we look at next token\n                next_token_id = generated_ids[0, pos + 1]\n                current_logits = logits[0, pos]\n                \n                # Get token and its logprob\n                token = self.tokenizer.decode([next_token_id])\n                token_logprob = logprobs[0, pos, next_token_id].item()\n                \n                # Get top alternative tokens\n                top_logprobs = self._get_top_alternatives(\n                    current_logits,\n                    next_token_id,\n                    num_alternatives\n                )\n                \n                # Get bytes for token\n                token_bytes = self._get_bytes_for_token(token)\n                \n                all_tokens.append(token)\n                all_token_logprobs.append(token_logprob)\n                all_top_logprobs.append(top_logprobs)\n                all_bytes.append(token_bytes)\n            \n            # Add None for the last token\n            all_tokens.append(self.tokenizer.decode([generated_ids[0, -1]]))\n            all_token_logprobs.append(None)\n            all_top_logprobs.append(None)\n            all_bytes.append(self._get_bytes_for_token(all_tokens[-1]))\n            \n            return LogProbsResult(\n                tokens=all_tokens,\n                token_logprobs=all_token_logprobs,\n                top_logprobs=all_top_logprobs,\n                bytes_per_token=all_bytes\n            )\n\n# MLX Support Functions and Classes\n\ndef is_apple_silicon() -> bool:\n    \"\"\"Check if running on Apple Silicon\"\"\"\n    return platform.system() == \"Darwin\" and platform.machine() == \"arm64\"\n\ndef should_use_mlx(model_id: str) -> bool:\n    \"\"\"Determine if a model should use MLX instead of PyTorch\"\"\"\n    if not MLX_AVAILABLE or not is_apple_silicon():\n        return False\n    \n    # Models that should use MLX\n    mlx_patterns = [\n        \"mlx-community/\",\n        \"mlx-\",\n        \"-mlx-\"\n    ]\n    \n    # Known problematic models that should prefer MLX on Apple Silicon\n    problematic_models = [\n        \"Qwen/Qwen3-\",\n        \"google/gemma-3-\",\n        \"google/gemma3-\"\n    ]\n    \n    model_lower = model_id.lower()\n    \n    # Direct MLX model detection\n    for pattern in mlx_patterns:\n        if pattern.lower() in model_lower:\n            return True\n    \n    # Problematic model detection\n    for pattern in problematic_models:\n        if pattern.lower() in model_lower:\n            logger.warning(f\"Model {model_id} detected as potentially problematic with MPS backend\")\n            suggested_mlx = suggest_mlx_alternative(model_id)\n            logger.warning(f\"Consider using MLX model: {suggested_mlx}\")\n            # Don't auto-switch, but recommend\n            return False\n    \n    return False\n\ndef suggest_mlx_alternative(model_id: str) -> str:\n    \"\"\"Suggest MLX alternative for a given model\"\"\"\n    mlx_alternatives = {\n        # Qwen3 models\n        \"Qwen/Qwen3-0.6B\": \"mlx-community/Qwen3-0.6B-4bit\",\n        \"Qwen/Qwen3-1.7B\": \"mlx-community/Qwen3-1.7B-4bit\",\n        \"Qwen/Qwen3-4B\": \"mlx-community/Qwen3-4B-4bit\",\n        \"Qwen/Qwen3-8B\": \"mlx-community/Qwen3-8B-4bit\",\n        \"Qwen/Qwen3-14B\": \"mlx-community/Qwen3-14B-4bit\",\n        \"Qwen/Qwen3-32B\": \"mlx-community/Qwen3-32B-4bit\",\n        \n        # Gemma 3 models  \n        \"google/gemma-3-1b-it\": \"mlx-community/gemma-3-1b-it-4bit\",\n        \"google/gemma-3-4b-it\": \"mlx-community/gemma-3-4b-it-4bit\",\n        \"google/gemma-3-12b-it\": \"mlx-community/gemma-3-12b-it-4bit\",\n        \"google/gemma-3-27b-it\": \"mlx-community/gemma-3-27b-it-4bit\",\n    }\n    \n    return mlx_alternatives.get(model_id, f\"mlx-community/{model_id.split('/')[-1]}-4bit\")\n\n@dataclass\nclass MLXModelConfig:\n    \"\"\"Configuration for MLX models\"\"\"\n    model_id: str\n    max_new_tokens: int = 4096\n    temperature: float = 0.7\n    top_p: float = 0.9\n    repetition_penalty: float = 1.0\n    enable_prompt_caching: bool = True\n\nclass MLXInferencePipeline:\n    \"\"\"MLX-based inference pipeline that mirrors PyTorch pipeline interface\"\"\"\n    \n    def __init__(self, model_config: MLXModelConfig, cache_manager):\n        self.model_config = model_config\n        self.cache_manager = cache_manager\n        self.last_used = time.time()\n        \n        if not MLX_AVAILABLE:\n            raise RuntimeError(\"MLX framework not available. Install with: pip install mlx-lm\")\n        \n        if not is_apple_silicon():\n            raise RuntimeError(\"MLX framework is only supported on Apple Silicon\")\n        \n        try:\n            logger.info(f\"Loading MLX model: {model_config.model_id}\")\n            self.model, self.tokenizer = self._load_mlx_model(model_config.model_id)\n            logger.info(\"MLX model loaded successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to load MLX model: {str(e)}\")\n            raise\n    \n    def _load_mlx_model(self, model_id: str):\n        \"\"\"Load MLX model and tokenizer with caching\"\"\"\n        def _load_model():\n            start_time = time.time()\n            logger.info(f\"Loading MLX model: {model_id}\")\n            \n            try:\n                model, tokenizer = mlx_load(model_id)\n                load_time = time.time() - start_time\n                logger.info(f\"MLX model loaded in {load_time:.2f}s\")\n                return model, tokenizer\n            except Exception as e:\n                logger.error(f\"Error loading MLX model {model_id}: {str(e)}\")\n                raise\n        \n        return self.cache_manager.get_or_load_model(f\"mlx_{model_id}\", _load_model)\n    \n    def generate(\n        self,\n        prompt: str,\n        generation_params: Optional[Dict[str, Any]] = None\n    ) -> Tuple[List[str], List[int], List[Optional[Dict]]]:\n        \"\"\"Generate text using MLX\"\"\"\n        start_time = time.time()\n        \n        if generation_params is None:\n            generation_params = {}\n        \n        # Extract parameters with defaults\n        max_tokens = generation_params.get(\"max_new_tokens\", self.model_config.max_new_tokens)\n        temperature = generation_params.get(\"temperature\", self.model_config.temperature)\n        top_p = generation_params.get(\"top_p\", self.model_config.top_p)\n        repetition_penalty = generation_params.get(\"repetition_penalty\", self.model_config.repetition_penalty)\n        num_return_sequences = generation_params.get(\"num_return_sequences\", 1)\n        \n        # Handle seed\n        if generation_params.get(\"seed\") is not None:\n            mx.random.seed(generation_params[\"seed\"])\n        \n        responses = []\n        token_counts = []\n        logprobs_results = []\n        \n        # Generate multiple sequences if requested\n        for _ in range(num_return_sequences):\n            try:\n                logger.debug(f\"Generating with MLX: max_tokens={max_tokens}, temp={temperature}\")\n                \n                # Use robust MLX generation with multiple fallback approaches\n                response = self._robust_mlx_generate(\n                    prompt, max_tokens, temperature, top_p, repetition_penalty\n                )\n                \n                responses.append(response)\n                \n                # Count tokens (approximate) - check if response is string\n                if isinstance(response, str):\n                    token_count = len(self.tokenizer.encode(response))\n                else:\n                    # Sometimes MLX returns just the new tokens, get the actual text\n                    token_count = len(response) if hasattr(response, '__len__') else 0\n                token_counts.append(token_count)\n                \n                # MLX doesn't provide logprobs by default\n                logprobs_results.append(None)\n                \n            except Exception as e:\n                logger.error(f\"Error during MLX generation: {str(e)}\")\n                logger.error(f\"MLX generation parameters: max_tokens={max_tokens}, temp={temperature}, top_p={top_p}\")\n                responses.append(\"\")\n                token_counts.append(0)\n                logprobs_results.append(None)\n        \n        generation_time = time.time() - start_time\n        logger.info(f\"MLX generation completed in {generation_time:.2f}s\")\n        \n        return responses, token_counts, logprobs_results\n    \n    def _robust_mlx_generate(self, prompt: str, max_tokens: int, temperature: float, top_p: float, repetition_penalty: float) -> str:\n        \"\"\"Robust MLX generation with multiple parameter combinations\"\"\"\n        \n        # Try different parameter combinations based on MLX-LM version\n        parameter_combinations = [\n            # Version 1: Current style with positional args and temp\n            {\n                \"style\": \"positional_temp\",\n                \"args\": (self.model, self.tokenizer, prompt),\n                \"kwargs\": {\n                    \"max_tokens\": max_tokens,\n                    \"temp\": temperature,\n                    \"top_p\": top_p,\n                    \"repetition_penalty\": repetition_penalty,\n                    \"verbose\": False\n                }\n            },\n            # Version 2: All keyword arguments with temp\n            {\n                \"style\": \"keyword_temp\", \n                \"args\": (),\n                \"kwargs\": {\n                    \"model\": self.model,\n                    \"tokenizer\": self.tokenizer,\n                    \"prompt\": prompt,\n                    \"max_tokens\": max_tokens,\n                    \"temp\": temperature,\n                    \"top_p\": top_p,\n                    \"repetition_penalty\": repetition_penalty,\n                    \"verbose\": False\n                }\n            },\n            # Version 3: Using temperature instead of temp\n            {\n                \"style\": \"positional_temperature\",\n                \"args\": (self.model, self.tokenizer, prompt),\n                \"kwargs\": {\n                    \"max_tokens\": max_tokens,\n                    \"temperature\": temperature,\n                    \"top_p\": top_p,\n                    \"repetition_penalty\": repetition_penalty,\n                    \"verbose\": False\n                }\n            },\n            # Version 4: Minimal parameters only\n            {\n                \"style\": \"minimal\",\n                \"args\": (self.model, self.tokenizer, prompt),\n                \"kwargs\": {\n                    \"max_tokens\": max_tokens,\n                    \"temp\": temperature,\n                    \"verbose\": False\n                }\n            },\n            # Version 5: Just essential parameters\n            {\n                \"style\": \"essential\",\n                \"args\": (self.model, self.tokenizer, prompt),\n                \"kwargs\": {\n                    \"max_tokens\": max_tokens\n                }\n            }\n        ]\n        \n        last_error = None\n        \n        for combo in parameter_combinations:\n            try:\n                logger.debug(f\"Trying MLX generation with style: {combo['style']}\")\n                response = mlx_generate(*combo[\"args\"], **combo[\"kwargs\"])\n                logger.debug(f\"Successfully generated with style: {combo['style']}\")\n                return response\n                \n            except Exception as e:\n                last_error = e\n                logger.debug(f\"Failed with style {combo['style']}: {str(e)}\")\n                continue\n        \n        # If all combinations failed, raise the last error\n        raise RuntimeError(f\"All MLX generation methods failed. Last error: {str(last_error)}\")\n    \n    def format_chat_prompt(self, system_prompt: str, user_prompt: str) -> str:\n        \"\"\"Format the prompt according to model's chat template\"\"\"\n        if hasattr(self.tokenizer, 'apply_chat_template'):\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n            try:\n                return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            except Exception as e:\n                logger.warning(f\"Failed to apply chat template: {e}, using fallback\")\n                return f\"System: {system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n        else:\n            return f\"System: {system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n\nclass MLXManager:\n    \"\"\"Manager for MLX models and operations\"\"\"\n    \n    def __init__(self, cache_manager):\n        self.cache_manager = cache_manager\n        self.available = MLX_AVAILABLE and is_apple_silicon()\n        \n        if self.available:\n            logger.info(\"MLX manager initialized - Apple Silicon detected\")\n        else:\n            logger.debug(\"MLX manager not available - requires Apple Silicon and mlx-lm\")\n    \n    def create_pipeline(self, model_id: str, **kwargs) -> MLXInferencePipeline:\n        \"\"\"Create an MLX inference pipeline\"\"\"\n        if not self.available:\n            raise RuntimeError(\"MLX not available on this platform\")\n        \n        config = MLXModelConfig(\n            model_id=model_id,\n            **kwargs\n        )\n        \n        return MLXInferencePipeline(config, self.cache_manager)\n    \n    def is_mlx_model(self, model_id: str) -> bool:\n        \"\"\"Check if model should use MLX\"\"\"\n        return should_use_mlx(model_id)\n\nclass MemoryEfficientAttention(nn.Module):\n    \"\"\"\n    Memory-efficient attention using linear attention mechanism.\n    Supports automatic fallback to optimized implementations when available.\n    \"\"\"\n    def __init__(\n        self,\n        hidden_size: int,\n        num_attention_heads: int,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_attention_heads = num_attention_heads\n        self.head_dim = hidden_size // num_attention_heads\n        self.scale = self.head_dim ** -0.5\n        \n        # Standard projections with bias\n        self.q_proj = nn.Linear(hidden_size, hidden_size)\n        self.k_proj = nn.Linear(hidden_size, hidden_size)\n        self.v_proj = nn.Linear(hidden_size, hidden_size)\n        self.o_proj = nn.Linear(hidden_size, hidden_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Try to import optimized attention implementations\n        self.optimized_attention = None\n        \n        # Try Flash Attention\n        try:\n            from flash_attn import flash_attn_func\n            self.optimized_attention = flash_attn_func\n            print(\"Using Flash Attention\")\n        except ImportError:\n            pass\n            \n        # Try xFormers\n        if self.optimized_attention is None:\n            try:\n                import xformers.ops as xops\n                self.optimized_attention = xops.memory_efficient_attention\n                print(\"Using xFormers attention\")\n            except ImportError:\n                pass\n\n    def _linear_attention(\n        self,\n        q: torch.Tensor,\n        k: torch.Tensor,\n        v: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        causal: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"Implements linear attention for more memory efficiency\"\"\"\n        # Scale query\n        q = q * self.scale\n        \n        # Apply mask if provided\n        if attention_mask is not None:\n            # Convert boolean mask to float mask if needed\n            if attention_mask.dtype == torch.bool:\n                attention_mask = attention_mask.float()\n            k = k * attention_mask.unsqueeze(-1)\n        \n        if causal:\n            # Handle causal attention\n            batch_size, num_heads, seq_length, head_dim = q.shape\n            positions = torch.arange(seq_length, device=q.device)\n            causal_mask = positions.view(1, 1, -1, 1) <= positions.view(1, 1, 1, -1)\n            k = k * causal_mask.float()\n        \n        # Linear attention computation\n        context = torch.matmul(k.transpose(-2, -1), v)\n        out = torch.matmul(q, context)\n        \n        return out\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        causal: bool = False,\n    ) -> torch.Tensor:\n        batch_size, seq_length, _ = hidden_states.size()\n        \n        # Project to q, k, v\n        q = self.q_proj(hidden_states)\n        k = self.k_proj(hidden_states)\n        v = self.v_proj(hidden_states)\n        \n        # Reshape for attention\n        q = q.view(batch_size, seq_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n        \n        # Try using optimized attention if available\n        if self.optimized_attention is not None and hidden_states.device.type == 'cuda':\n            # Handle attention mask for optimized implementations\n            if attention_mask is not None:\n                if attention_mask.dtype != torch.bool:\n                    attention_mask = attention_mask > 0\n                attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n            \n            try:\n                attn_output = self.optimized_attention(\n                    q, k, v,\n                    attn_mask=attention_mask,\n                    causal=causal,\n                    scale=self.scale\n                )\n            except Exception as e:\n                print(f\"Optimized attention failed, falling back to linear attention: {e}\")\n                attn_output = self._linear_attention(q, k, v, attention_mask, causal)\n        else:\n            # Use linear attention for CPU/MPS or when optimized attention is not available\n            attn_output = self._linear_attention(q, k, v, attention_mask, causal)\n        \n        # Reshape and project back\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, seq_length, self.hidden_size)\n        attn_output = self.o_proj(attn_output)\n        \n        return attn_output\n\nclass PromptCache:\n    \"\"\"Advanced caching system for frequent prompts and responses\"\"\"\n    \n    def __init__(self, max_size: int = 1000):\n        self.max_size = max_size\n        self.cache = OrderedDict()\n        self.prompt_stats = defaultdict(lambda: {\"count\": 0, \"success_rate\": 0.0})\n        \n    @lru_cache(maxsize=128)\n    def _compute_prompt_signature(self, prompt: str) -> str:\n        \"\"\"Compute a signature for semantic similarity matching\"\"\"\n        # Simple but effective signature based on key content words\n        words = set(prompt.lower().split())\n        return \" \".join(sorted(list(words)))\n    \n    def get_cached_response(self, prompt: str, temperature: float, top_p: float) -> Optional[str]:\n        \"\"\"Get cached response with fuzzy matching\"\"\"\n        signature = self._compute_prompt_signature(prompt)\n        \n        if signature in self.cache:\n            cached_item = self.cache[signature]\n            if abs(cached_item[\"temperature\"] - temperature) < 0.1 and abs(cached_item[\"top_p\"] - top_p) < 0.1:\n                self.prompt_stats[signature][\"count\"] += 1\n                return cached_item[\"response\"]\n        \n        return None\n    \n    def add_to_cache(self, prompt: str, response: str, temperature: float, top_p: float):\n        \"\"\"Add response to cache with metadata\"\"\"\n        signature = self._compute_prompt_signature(prompt)\n        \n        self.cache[signature] = {\n            \"response\": response,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"timestamp\": torch.cuda.current_timestamp() if torch.cuda.is_available() else 0\n        }\n        \n        if len(self.cache) > self.max_size:\n            self.cache.popitem(last=False)\n    \n    def update_stats(self, prompt: str, success: bool):\n        \"\"\"Update prompt success statistics\"\"\"\n        signature = self._compute_prompt_signature(prompt)\n        stats = self.prompt_stats[signature]\n        stats[\"count\"] += 1\n        stats[\"success_rate\"] = (stats[\"success_rate\"] * (stats[\"count\"] - 1) + float(success)) / stats[\"count\"]\n\nclass DynamicTemperature:\n    \"\"\"Implements dynamic temperature scaling based on input characteristics\"\"\"\n    \n    def __init__(self):\n        self.token_entropy_cache = {}\n    \n    def _compute_token_entropy(self, tokens: List[int]) -> float:\n        \"\"\"Compute token distribution entropy\"\"\"\n        token_counts = np.bincount(tokens)\n        probabilities = token_counts / len(tokens)\n        return entropy(probabilities)\n    \n    def get_optimal_temperature(self, prompt: str, tokenizer: AutoTokenizer, base_temperature: float) -> float:\n        \"\"\"Calculate optimal temperature based on prompt characteristics\"\"\"\n        tokens = tokenizer.encode(prompt)\n        \n        # Calculate entropy-based scaling\n        token_entropy = self._compute_token_entropy(tokens)\n        \n        # Scale temperature based on prompt entropy and length\n        length_factor = np.clip(len(tokens) / 100, 0.5, 2.0)\n        entropy_factor = np.clip(token_entropy / 4.0, 0.5, 1.5)\n        \n        optimal_temperature = base_temperature * length_factor * entropy_factor\n        return np.clip(optimal_temperature, 0.1, 2.0)\n\nclass CacheManager:\n    \"\"\"\n    Singleton cache manager for models and tokenizers.\n    Thread-safe but minimizes lock contention.\n    \"\"\"\n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    instance = super().__new__(cls)\n                    instance._initialized = False\n                    cls._instance = instance\n        return cls._instance\n    \n    def __init__(self, max_size: int = 5):\n        if self._initialized:\n            return\n            \n        with self._lock:\n            if not self._initialized:\n                logger.info(\"Initializing CacheManager singleton\")\n                self.max_size = max_size\n                self.model_cache = OrderedDict()\n                self.tokenizer_cache = OrderedDict()\n                self.adapter_cache = OrderedDict()\n                self.model_adapter_map = {}  # Maps model ID to list of loaded adapter IDs\n                self.cache_stats = defaultdict(lambda: {\"hits\": 0, \"misses\": 0})\n                self._initialized = True\n                logger.info(\"CacheManager singleton initialized\")\n    \n    def get_or_load_model(self, model_key: str, loader_fn) -> Tuple[Any, Any]:\n        \"\"\"Get or load model and tokenizer with minimal locking.\"\"\"\n        cached_model = cached_tokenizer = None\n        cache_hit = False\n        \n        with self._lock:\n            if model_key in self.model_cache and model_key in self.tokenizer_cache:\n                cached_model = self.model_cache[model_key]\n                cached_tokenizer = self.tokenizer_cache[model_key]\n                self.model_cache.move_to_end(model_key)\n                self.tokenizer_cache.move_to_end(model_key)\n                self.cache_stats[model_key][\"hits\"] += 1\n                cache_hit = True\n                logger.debug(f\"Cache hit for model: {model_key}\")\n\n        if cache_hit:\n            return cached_model, cached_tokenizer\n\n        logger.info(f\"Loading model and tokenizer: {model_key}\")\n        model, tokenizer = loader_fn()\n        \n        with self._lock:\n            if model_key in self.model_cache and model_key in self.tokenizer_cache:\n                cached_model = self.model_cache[model_key]\n                cached_tokenizer = self.tokenizer_cache[model_key]\n                self.cache_stats[model_key][\"hits\"] += 1\n                logger.debug(f\"Using already cached model: {model_key}\")\n                return cached_model, cached_tokenizer\n            \n            self.model_cache[model_key] = model\n            self.tokenizer_cache[model_key] = tokenizer\n            self.cache_stats[model_key][\"misses\"] += 1\n            self.model_adapter_map[model_key] = []  # Initialize empty adapter list for new model\n            \n            self._cleanup_caches()\n            \n            logger.info(f\"Successfully cached model and tokenizer: {model_key}\")\n            return model, tokenizer\n    \n    def get_or_load_adapter(self, model_key: str, adapter_key: str, loader_fn):\n        \"\"\"Get or load adapter with enhanced caching.\"\"\"\n        cache_key = f\"{model_key}_{adapter_key}\"\n        \n        with self._lock:\n            if cache_key in self.adapter_cache:\n                adapter = self.adapter_cache[cache_key]\n                self.adapter_cache.move_to_end(cache_key)\n                logger.debug(f\"Cache hit for adapter: {cache_key}\")\n                return adapter\n        \n        adapter = loader_fn()\n        \n        with self._lock:\n            self.adapter_cache[cache_key] = adapter\n            if model_key not in self.model_adapter_map:\n                self.model_adapter_map[model_key] = []\n            if adapter_key not in self.model_adapter_map[model_key]:\n                self.model_adapter_map[model_key].append(adapter_key)\n            self._cleanup_caches()\n            logger.info(f\"Successfully cached adapter: {cache_key}\")\n            return adapter\n    \n    def get_model_adapters(self, model_key: str) -> List[str]:\n        \"\"\"Get list of adapter IDs loaded for a specific model.\"\"\"\n        with self._lock:\n            return self.model_adapter_map.get(model_key, [])\n    \n    def _cleanup_caches(self):\n        \"\"\"Clean up caches if they exceed max size.\"\"\"\n        while len(self.model_cache) > self.max_size:\n            model_key, model = self.model_cache.popitem(last=False)\n            if hasattr(model, 'cpu'):\n                model.cpu()\n            # Clean up associated adapters\n            if model_key in self.model_adapter_map:\n                for adapter_id in self.model_adapter_map[model_key]:\n                    cache_key = f\"{model_key}_{adapter_id}\"\n                    if cache_key in self.adapter_cache:\n                        self.adapter_cache.pop(cache_key)\n                self.model_adapter_map.pop(model_key)\n            \n        while len(self.tokenizer_cache) > self.max_size:\n            self.tokenizer_cache.popitem(last=False)\n            \n        # Cleanup orphaned adapters\n        valid_cache_keys = {\n            f\"{model_key}_{adapter_id}\"\n            for model_key, adapter_ids in self.model_adapter_map.items()\n            for adapter_id in adapter_ids\n        }\n        \n        orphaned_adapters = [\n            key for key in self.adapter_cache.keys()\n            if key not in valid_cache_keys\n        ]\n        \n        for key in orphaned_adapters:\n            adapter = self.adapter_cache.pop(key)\n            if hasattr(adapter, 'cpu'):\n                adapter.cpu()\n                \n        torch.cuda.empty_cache()\n\n    @classmethod\n    def get_instance(cls, max_size: int = 5) -> 'CacheManager':\n        \"\"\"Alternative way to get the singleton instance.\"\"\"\n        if cls._instance is None:\n            return cls(max_size)\n        return cls._instance\n\nclass DeviceManager:\n    def __init__(self):\n        self.available_devices = self._detect_devices()\n        self.device_stats = {device: {'memory_used': 0, 'active_models': 0} for device in self.available_devices}\n\n    def _detect_devices(self) -> List[str]:\n        devices = ['cpu']\n        if torch.cuda.is_available():\n            devices.extend([f'cuda:{i}' for i in range(torch.cuda.device_count())])\n        if torch.backends.mps.is_available():\n            devices.append('mps')\n        return devices\n\n    def get_optimal_device(self, model_size: int = 0) -> str:\n        if not self.available_devices:\n            return 'cpu'\n\n        # Prefer CUDA devices if available\n        cuda_devices = [d for d in self.available_devices if 'cuda' in d]\n        if cuda_devices:\n            # Find CUDA device with most free memory\n            max_free_memory = 0\n            optimal_device = cuda_devices[0]\n            \n            for device in cuda_devices:\n                idx = int(device.split(':')[1])\n                free_memory = torch.cuda.get_device_properties(idx).total_memory - torch.cuda.memory_allocated(idx)\n                if free_memory > max_free_memory:\n                    max_free_memory = free_memory\n                    optimal_device = device\n            \n            return optimal_device\n        \n        # Fall back to MPS if available\n        if 'mps' in self.available_devices:\n            return 'mps'\n        \n        return 'cpu'\n\n    def track_device_usage(self, device: str, memory_delta: int):\n        if device in self.device_stats:\n            self.device_stats[device]['memory_used'] += memory_delta\n\nclass ModelManager:\n    def __init__(self, cache_manager: CacheManager, device_manager: DeviceManager):\n        self.cache_manager = cache_manager\n        self.device_manager = device_manager\n        \n    def quantize_model(self, model):\n        \"\"\"Quantize model to 4-bit precision using bitsandbytes\"\"\"\n        def _replace_linear_layers(module):\n            for name, child in module.named_children():\n                if isinstance(child, torch.nn.Linear):\n                    setattr(module, name, bnb.nn.Linear4bit(\n                        child.in_features,\n                        child.out_features,\n                        bias=child.bias is not None,\n                        compute_dtype=torch.float16\n                    ))\n                else:\n                    _replace_linear_layers(child)\n                    \n        _replace_linear_layers(model)\n        return model\n\n    def load_base_model(self, model_id: str, quantize: bool = True) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n        def _load_model():\n            logger.info(f\"Loading base model: {model_id}\")\n            \n            device = self.device_manager.get_optimal_device()\n            logger.info(f\"Using device: {device}\")\n            \n            # Load tokenizer\n            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n            \n            # Base kwargs for model loading\n            model_kwargs = {\n                \"trust_remote_code\": True,\n                \"device_map\": \"auto\" if 'cuda' in device else device\n            }\n            \n            # Configure device-specific optimizations\n            if 'cuda' in device:\n                compute_capability = torch.cuda.get_device_capability(0)\n                if compute_capability[0] >= 8:\n                    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n                elif compute_capability[0] >= 7:\n                    model_kwargs[\"torch_dtype\"] = torch.float16\n                    \n                # Check for flash attention availability\n                try:\n                    import flash_attn\n                    has_flash_attn = True\n                    logger.info(\"Flash Attention 2 is available\")\n                    model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n                except ImportError:\n                    has_flash_attn = False\n                    logger.info(\"Flash Attention 2 is not installed - falling back to default attention\")\n                    \n            elif 'mps' in device:\n                # MPS supports FP16\n                model_kwargs[\"torch_dtype\"] = torch.float16\n                # model_kwargs[\"torch_dtype\"] = torch.float32\n                logger.info(\"Using MPS device with float16 precision\")\n            else:\n                # CPU can use FP16 if available\n                if hasattr(torch.cpu, 'has_fp16') and torch.cpu.has_fp16:\n                    model_kwargs[\"torch_dtype\"] = torch.float16\n                    logger.info(\"Using CPU device with float16 precision\")\n                else:\n                    model_kwargs[\"torch_dtype\"] = torch.float32\n                    logger.info(\"Using CPU device with float32 precision - FP16 not supported\")\n            \n            # Load model with configured optimizations\n            try:\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_id,\n                    **model_kwargs\n                )\n            except Exception as e:\n                if \"attn_implementation\" in model_kwargs:\n                    logger.warning(f\"Failed to load model with Flash Attention: {e}\")\n                    logger.info(\"Retrying without Flash Attention...\")\n                    model_kwargs.pop(\"attn_implementation\")\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_id,\n                        **model_kwargs\n                    )\n                elif model_kwargs[\"torch_dtype\"] == torch.float16:\n                    # If FP16 fails, fallback to FP32\n                    logger.warning(f\"Failed to load model with FP16: {e}\")\n                    logger.info(\"Falling back to FP32...\")\n                    model_kwargs[\"torch_dtype\"] = torch.float32\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_id,\n                        **model_kwargs\n                    )\n            \n            logger.info(f\"Model loaded successfully with dtype: {model_kwargs['torch_dtype']}\")\n            \n            # Only apply quantization for CUDA devices when not using mixed precision\n            if quantize and 'cuda' in device and model_kwargs[\"torch_dtype\"] == torch.float32:\n                model = self.quantize_model(model)\n            \n            return model, tokenizer\n            \n        return self.cache_manager.get_or_load_model(model_id, _load_model)\n\nclass LoRAManager:\n    \"\"\"LoRA manager with enhanced error handling and caching\"\"\"\n    \n    def __init__(self, cache_manager: CacheManager):\n        self.cache_manager = cache_manager\n        self.loaded_adapters = {}\n        self.adapter_names = {}  # Maps adapter_id to valid adapter name\n\n    def _get_adapter_name(self, adapter_id: str) -> str:\n        \"\"\"Create a valid adapter name from adapter_id.\"\"\"\n        if adapter_id in self.adapter_names:\n            return self.adapter_names[adapter_id]\n            \n        name = adapter_id.replace('.', '_').replace('-', '_')\n        name = ''.join(c if c.isalnum() or c == '_' else '' for c in name)\n        if name[0].isdigit():\n            name = f\"adapter_{name}\"\n            \n        self.adapter_names[adapter_id] = name\n        return name\n\n    def validate_adapter(self, adapter_id: str) -> bool:\n        \"\"\"Validate if adapter exists and is compatible\"\"\"\n        try:\n            config = PeftConfig.from_pretrained(\n                adapter_id,\n                trust_remote_code=True,\n                use_auth_token=os.getenv(\"HF_TOKEN\")\n            )\n            return True\n        except Exception as e:\n            logger.error(f\"Error validating adapter {adapter_id}: {str(e)}\")\n            return False\n\n    def load_adapter(self, base_model: PreTrainedModel, adapter_id: str) -> PreTrainedModel:\n        \"\"\"Load a LoRA adapter with enhanced caching\"\"\"\n        model_key = base_model.config._name_or_path\n        \n        def _load_adapter():\n            logger.info(f\"Loading LoRA adapter: {adapter_id}\")\n            \n            if not self.validate_adapter(adapter_id):\n                error_msg = f\"Adapter {adapter_id} not found or is not compatible\"\n                logger.error(error_msg)\n                raise ValueError(error_msg)\n            \n            try:\n                adapter_name = self._get_adapter_name(adapter_id)\n                \n                config = PeftConfig.from_pretrained(\n                    adapter_id,\n                    trust_remote_code=True,\n                    use_auth_token=os.getenv(\"HF_TOKEN\")\n                )\n                \n                model = base_model\n                model.add_adapter(\n                    config,\n                    adapter_name = adapter_name,\n                )\n                \n                if model not in self.loaded_adapters:\n                    self.loaded_adapters[model] = []\n                if adapter_id not in self.loaded_adapters[model]:\n                    self.loaded_adapters[model].append(adapter_id)\n                \n                return model\n                \n            except Exception as e:\n                error_msg = f\"Failed to load adapter {adapter_id}: {str(e)}\"\n                logger.error(error_msg)\n                raise RuntimeError(error_msg) from e\n\n        return self.cache_manager.get_or_load_adapter(model_key, adapter_id, _load_adapter)\n\n    def set_active_adapter(self, model: PeftModel, adapter_id: str = None) -> bool:\n        \"\"\"Set a specific adapter as active with error handling\"\"\"\n        if not isinstance(model, PeftModel):\n            logger.warning(\"Model is not a PeftModel, cannot set active adapter\")\n            return False\n            \n        available_adapters = self.loaded_adapters.get(model, [])\n        \n        if not available_adapters:\n            logger.warning(\"No adapters loaded in model\")\n            return False\n            \n        if adapter_id is None:\n            adapter_id = available_adapters[-1]\n            \n        if adapter_id in available_adapters:\n            try:\n                model.set_adapter(self._get_adapter_name(adapter_id))\n                logger.info(f\"Successfully set active adapter to: {adapter_id}\")\n                return True\n            except Exception as e:\n                logger.error(f\"Error setting adapter {adapter_id}: {str(e)}\")\n                return False\n        else:\n            logger.warning(f\"Requested adapter {adapter_id} not loaded. Available adapters: {available_adapters}\")\n            return False\n        \nclass InferencePipeline:\n    def __init__(self, model_config: ModelConfig, cache_manager, device_manager, model_manager, lora_manager):\n        self.model_config = model_config\n        self.cache_manager = cache_manager\n        self.device_manager = device_manager\n        self.model_manager = model_manager\n        self.lora_manager = lora_manager\n        self.last_used = time.time()\n        \n        try:\n            self.base_model, self.tokenizer = self.model_manager.load_base_model(\n                model_config.base_model_id,\n                quantize=model_config.quantization_bits == 4\n            )\n            \n            self.tokenizer = self.setup_tokenizer(self.tokenizer)\n            \n            if self.base_model.get_input_embeddings().num_embeddings != len(self.tokenizer):\n                self.base_model.resize_token_embeddings(len(self.tokenizer))\n            \n            self.current_model = self.base_model\n            \n            if model_config.adapter_ids:\n                for adapter_id in model_config.adapter_ids:\n                    try:\n                        self.current_model = self.lora_manager.load_adapter(\n                            self.current_model, adapter_id\n                        )\n                    except Exception as e:\n                        logger.error(f\"Error loading adapter {adapter_id}: {e}\")\n                \n                # Set active adapter and verify it's set correctly\n                if isinstance(self.current_model, PeftModel):\n                    success = self.lora_manager.set_active_adapter(self.current_model)\n                    if not success:\n                        logger.error(\"Failed to set active adapter\")\n            \n            self.dtype = self.current_model.dtype\n            self.optimal_batch_size = self._find_optimal_batch_size()\n            \n        except Exception as e:\n            logger.error(f\"Pipeline initialization error: {str(e)}\")\n            logger.error(f\"Error traceback: {traceback.format_exc()}\")\n            raise\n\n    def setup_tokenizer(self, tokenizer: AutoTokenizer) -> AutoTokenizer:\n        \"\"\"Use tokenizer with its default configuration for inference\"\"\"\n        logger.debug(\"  a. Starting tokenizer setup\")\n        \n        # Just use existing special tokens without modification\n        logger.debug(f\"  b. Using tokenizer with vocab size: {len(tokenizer)}\")\n        logger.debug(f\"  c. Special tokens: PAD={tokenizer.pad_token_id}, \"\n                    f\"EOS={tokenizer.eos_token_id}, BOS={tokenizer.bos_token_id}\")\n        \n        return tokenizer\n\n    def get_optimized_generation_config(self, generation_params: Optional[Dict[str, Any]] = None) -> Dict:\n        \"\"\"Get optimized generation config\"\"\"\n        config = {\n            \"max_new_tokens\": generation_params.get(\"max_new_tokens\", 4096),\n            \"do_sample\": generation_params.get(\"temperature\", 1.0) > 0,\n            \"temperature\": generation_params.get(\"temperature\", 1.0),\n            \"top_p\": generation_params.get(\"top_p\", 0.95),\n            \"num_return_sequences\": generation_params.get(\"num_return_sequences\", 1),\n            \"pad_token_id\": self.tokenizer.pad_token_id,\n            \"eos_token_id\": self.tokenizer.eos_token_id,\n            \"return_dict_in_generate\": True,\n            \"output_scores\": generation_params.get(\"logprobs\", False),\n            \"use_cache\": True,\n            \"return_legacy_cache\": True,  # To avoid warning\n        }\n        return config\n    \n    def generate(\n        self,\n        prompt: str,\n        generation_params: Optional[Dict[str, Any]] = None\n    ) -> Tuple[List[str], List[int]]:\n        \"\"\"Generate completions with optional logprobs\"\"\"\n        start_time = time.time()\n\n        # First: Set pad token if needed\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Tokenize with batching disabled for single prompts\n        tokenize_start = time.time()\n        inputs = self.tokenizer(\n            prompt,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n        ).to(self.current_model.device)\n        logger.info(f\"Tokenization time: {time.time() - tokenize_start:.2f}s\")\n        \n        # Extract logprobs parameters\n        calculate_logprobs = generation_params.get(\"logprobs\", False)\n        top_logprobs = generation_params.get(\"top_logprobs\", 0)\n        \n        if top_logprobs and not calculate_logprobs:\n            raise ValueError(\"logprobs must be true when top_logprobs is specified\")\n        \n        if top_logprobs and not (0 <= top_logprobs <= 20):\n            raise ValueError(\"top_logprobs must be between 0 and 20\")\n             \n        # Get optimized generation config\n        gen_config = self.get_optimized_generation_config(generation_params)\n\n        # Add optional parameters\n        if generation_params:\n            if generation_params.get(\"presence_penalty\", 0) != 0:\n                gen_config[\"presence_penalty\"] = generation_params[\"presence_penalty\"]\n            if generation_params.get(\"frequency_penalty\", 0) != 0:\n                gen_config[\"repetition_penalty\"] = 1.0 + generation_params[\"frequency_penalty\"]\n            if generation_params.get(\"stop_sequences\"):\n                gen_config[\"stopping_criteria\"] = self._create_stopping_criteria(\n                    generation_params[\"stop_sequences\"],\n                    inputs['input_ids'].shape[1]\n                )\n            if generation_params.get(\"seed\") is not None:\n                torch.manual_seed(generation_params[\"seed\"])\n                if torch.cuda.is_available():\n                    torch.cuda.manual_seed(generation_params[\"seed\"])\n\n        # Generate responses\n        generate_start = time.time()\n        with torch.inference_mode():  # Faster than no_grad\n            outputs = self.current_model.generate(\n                **inputs,\n                **gen_config\n            )\n        logger.info(f\"Generation time: {time.time() - generate_start:.2f}s\")\n        \n        generated_sequences = outputs.sequences\n        input_length = inputs['input_ids'].shape[1]\n        \n        # Process outputs\n        process_start = time.time()\n        responses = []\n        token_counts = []\n        logprobs_results = []\n        \n        # Process each generated sequence\n        for sequence in generated_sequences:\n            response_tokens = sequence[input_length:]\n            response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n            responses.append(response_text)\n            token_counts.append(len(response_tokens))\n            \n            # Calculate logprobs if requested\n            if calculate_logprobs:\n                calculator = LogProbsCalculator(self.tokenizer, self.current_model)\n                logprobs_result = calculator.calculate_logprobs(\n                    input_ids=sequence.unsqueeze(0),\n                    generated_ids=sequence.unsqueeze(0),\n                    attention_mask=torch.ones_like(sequence).unsqueeze(0),\n                    num_alternatives=top_logprobs or 5\n                )\n                logprobs_results.append({\n                    \"content\": [{\n                        \"token\": token,\n                        \"logprob\": logprob,\n                        \"bytes\": bytes_,\n                        \"top_logprobs\": top_logprobs\n                    } for token, logprob, bytes_, top_logprobs in zip(\n                        logprobs_result.tokens[input_length:],\n                        logprobs_result.token_logprobs[input_length:],\n                        logprobs_result.bytes_per_token[input_length:],\n                        logprobs_result.top_logprobs[input_length:]\n                    )]\n                })\n            else:\n                logprobs_results.append(None)\n        \n        logger.info(f\"Post-processing time: {time.time() - process_start:.2f}s\")\n        logger.info(f\"Total generation time: {time.time() - start_time:.2f}s\")\n        \n        return responses, token_counts, logprobs_results\n    \n    def setup_efficient_attention(self):\n        \"\"\"Replace standard attention with memory-efficient version\"\"\"\n        if hasattr(self.current_model, 'config') and hasattr(self.current_model.config, 'hidden_size'):\n            hidden_size = self.current_model.config.hidden_size\n            num_attention_heads = self.current_model.config.num_attention_heads\n            self.efficient_attention = MemoryEfficientAttention(hidden_size, num_attention_heads)\n            \n            # Monkey patch attention computation if possible\n            if hasattr(self.current_model, 'encoder') and hasattr(self.current_model.encoder, 'layer'):\n                for layer in self.current_model.encoder.layer:\n                    if hasattr(layer, 'attention'):\n                        layer.attention.self = self.efficient_attention\n            logger.info(\"Memory-efficient attention mechanism enabled\")\n\n    def _find_optimal_batch_size(self, initial_batch_size: int = 1, max_batch_size: int = 128) -> int:\n        \"\"\"Find optimal batch size through binary search with memory monitoring\"\"\"\n        if not torch.cuda.is_available():\n            return initial_batch_size\n\n        device = self.current_model.device\n        if 'cuda' not in str(device):\n            return initial_batch_size\n\n        left, right = initial_batch_size, max_batch_size\n        optimal_size = initial_batch_size\n        \n        sample_text = \"Sample input text for batch size optimization.\"\n        \n        while left <= right:\n            mid = (left + right) // 2\n            try:\n                torch.cuda.empty_cache()\n                \n                inputs = self.tokenizer([sample_text] * mid, \n                                     padding=True, \n                                     truncation=True,\n                                     return_tensors=\"pt\").to(device)\n                \n                with torch.amp.autocast('cuda',dtype=self.dtype):\n                    with torch.no_grad():\n                        _ = self.current_model.generate(\n                            **inputs,\n                            max_new_tokens=1,\n                            num_return_sequences=1,\n                            pad_token_id=self.tokenizer.pad_token_id\n                        )\n                \n                optimal_size = mid\n                left = mid + 1\n                \n                memory_used = torch.cuda.memory_allocated(device)\n                total_memory = torch.cuda.get_device_properties(device).total_memory\n                \n                if memory_used > 0.9 * total_memory:\n                    break\n                \n            except torch.cuda.OutOfMemoryError:\n                right = mid - 1\n                torch.cuda.empty_cache()\n        \n        return max(1, int(optimal_size * 0.9))\n\n    def optimize_generation_params(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"Optimize generation parameters based on prompt characteristics\"\"\"\n        base_params = {\n            \"max_new_tokens\": self.model_config.max_new_tokens,\n            \"do_sample\": self.model_config.do_sample,\n            \"top_p\": self.model_config.top_p,\n            \"top_k\": self.model_config.top_k,\n            \"temperature\": self.model_config.temperature,\n            \"num_return_sequences\": self.model_config.num_return_sequences,\n            \"repetition_penalty\": self.model_config.repetition_penalty,\n            \"pad_token_id\": self.model_config.pad_token_id or self.tokenizer.pad_token_id\n        }\n        \n        if self.model_config.dynamic_temperature:\n            base_params[\"temperature\"] = self.dynamic_temperature.get_optimal_temperature(\n                prompt, self.tokenizer, base_params[\"temperature\"]\n            )\n        \n        return base_params\n\n    def format_chat_prompt(self, system_prompt: str, user_prompt: str) -> str:\n        \"\"\"Format the prompt according to model's chat template\"\"\"\n        if hasattr(self.tokenizer, 'apply_chat_template'):\n            # Use the model's built-in chat template if available\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n            return self.tokenizer.apply_chat_template(messages, tokenize=False)\n        else:\n            # Fallback to a generic template\n            return f\"<|system|>{system_prompt}</s><|user|>{user_prompt}</s><|assistant|>\"\n        \n    def _create_stopping_criteria(self, stop_sequences: List[str], input_length: int):\n        \"\"\"Create stopping criteria for generation\"\"\"\n        from transformers import StoppingCriteria, StoppingCriteriaList\n        \n        class StopSequenceCriteria(StoppingCriteria):\n            def __init__(self, tokenizer, stop_sequences, input_length):\n                self.tokenizer = tokenizer\n                self.stop_ids = [\n                    self.tokenizer.encode(seq, add_special_tokens=False)\n                    for seq in stop_sequences\n                ]\n                self.input_length = input_length\n\n            def __call__(self, input_ids, scores, **kwargs):\n                for stop_ids in self.stop_ids:\n                    if input_ids[0, -len(stop_ids):].tolist() == stop_ids:\n                        return True\n                return False\n\n        return StoppingCriteriaList([\n            StopSequenceCriteria(\n                self.tokenizer,\n                stop_sequences,\n                input_length=input_length\n            )\n        ])\n    \n    def process_batch(\n        self,\n        system_prompts: List[str],\n        user_prompts: List[str],\n        generation_params: Optional[Dict[str, Any]] = None,\n        active_adapter: str = None,\n        return_token_count: bool = True\n    ) -> Tuple[List[str], List[int]]:\n        \"\"\"Process a batch of prompts with all optimizations\"\"\"\n        \n        # Set the requested adapter if specified\n        if isinstance(self.current_model, PeftModel) and active_adapter is not None:\n            self.lora_manager.set_active_adapter(self.current_model, active_adapter)\n\n        all_responses = []\n        token_counts = []\n        \n        # Format all prompts using chat template\n        formatted_prompts = [\n            self.format_chat_prompt(system_prompt, user_prompt)\n            for system_prompt, user_prompt in zip(system_prompts, user_prompts)\n        ]\n        \n        # Get number of completions requested\n        n = generation_params.get(\"num_return_sequences\", 1) if generation_params else 1\n        \n        for i in range(0, len(formatted_prompts), self.optimal_batch_size):\n            batch_prompts = formatted_prompts[i:i + self.optimal_batch_size]\n            batch_system = system_prompts[i:i + self.optimal_batch_size]\n            batch_user = user_prompts[i:i + self.optimal_batch_size]\n            \n            # Check cache first if enabled\n            if self.model_config.enable_prompt_caching:\n                cached_responses = []\n                uncached_indices = []\n                \n                for idx, prompt in enumerate(batch_prompts):\n                    temp = generation_params.get(\"temperature\", self.model_config.temperature) if generation_params else self.model_config.temperature\n                    top_p = generation_params.get(\"top_p\", self.model_config.top_p) if generation_params else self.model_config.top_p\n                    \n                    cached_response = self.cache_manager.prompt_cache.get_cached_response(\n                        prompt,\n                        temp,\n                        top_p\n                    )\n                    if cached_response is not None:\n                        # For cached responses, replicate n times if multiple completions requested\n                        cached_responses.extend([cached_response] * n)\n                    else:\n                        uncached_indices.append(idx)\n                \n                if uncached_indices:\n                    batch_prompts = [batch_prompts[i] for i in uncached_indices]\n                else:\n                    batch_prompts = []\n            \n            if batch_prompts:  # If there are any uncached prompts\n                # Configure generation parameters\n                base_params = {\n                    \"max_new_tokens\": generation_params.get(\"max_new_tokens\", 4096) if generation_params else self.model_config.max_new_tokens,\n                    \"do_sample\": generation_params.get(\"temperature\", 1.0) > 0 if generation_params else self.model_config.do_sample,\n                    \"temperature\": generation_params.get(\"temperature\", 1.0) if generation_params else self.model_config.temperature,\n                    \"top_p\": generation_params.get(\"top_p\", 1.0) if generation_params else self.model_config.top_p,\n                    \"num_return_sequences\": n,\n                    \"pad_token_id\": self.tokenizer.pad_token_id,\n                    \"eos_token_id\": self.tokenizer.eos_token_id,\n                }\n\n                # Add optional parameters if specified\n                if generation_params:\n                    if generation_params.get(\"presence_penalty\", 0) != 0:\n                        base_params[\"presence_penalty\"] = generation_params[\"presence_penalty\"]\n                    if generation_params.get(\"frequency_penalty\", 0) != 0:\n                        base_params[\"repetition_penalty\"] = 1.0 + generation_params[\"frequency_penalty\"]\n                    if generation_params.get(\"logit_bias\"):\n                        base_params[\"logit_bias\"] = generation_params[\"logit_bias\"]\n                    if generation_params.get(\"seed\") is not None:\n                        torch.manual_seed(generation_params[\"seed\"])\n                        if torch.cuda.is_available():\n                            torch.cuda.manual_seed(generation_params[\"seed\"])\n                \n                # Tokenize inputs\n                inputs = self.tokenizer(\n                    batch_prompts,\n                    padding=True,\n                    truncation=True,\n                    return_tensors=\"pt\"\n                ).to(self.current_model.device)\n\n                # Get the length of each input\n                input_lengths = inputs['input_ids'].shape[1]\n\n                # Add stopping criteria if specified\n                if generation_params and generation_params.get(\"stop_sequences\"):\n                    base_params[\"stopping_criteria\"] = self._create_stopping_criteria(\n                        generation_params[\"stop_sequences\"],\n                        input_lengths\n                    )\n\n                # Generate responses\n                with torch.amp.autocast('cuda', dtype=self.dtype):\n                    with torch.no_grad():\n                        outputs = self.current_model.generate(\n                            **inputs,\n                            **base_params\n                        )\n                \n                # Decode outputs and remove input portion\n                batch_responses = []\n                batch_token_counts = []\n                \n                # Handle multiple sequences per input\n                num_return_sequences = base_params[\"num_return_sequences\"]\n                for i in range(0, len(outputs), num_return_sequences):\n                    sequences = outputs[i:i + num_return_sequences]\n                    for seq in sequences:\n                        response_tokens = seq[input_lengths:]\n                        response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n                        batch_responses.append(response_text)\n                        batch_token_counts.append(len(response_tokens))\n                \n                # Cache new responses if enabled\n                if self.model_config.enable_prompt_caching:\n                    for prompt, response in zip(batch_prompts, batch_responses[::n]):  # Only cache first response of each input\n                        self.cache_manager.prompt_cache.add_to_cache(\n                            prompt,\n                            response,\n                            base_params[\"temperature\"],\n                            base_params[\"top_p\"]\n                        )\n                \n                # Merge cached and new responses in correct order\n                all_responses.extend(cached_responses)\n                if uncached_indices:\n                    response_idx = 0\n                    for original_idx in range(len(formatted_prompts[i:i + self.optimal_batch_size])):\n                        if original_idx in uncached_indices:\n                            # Add n responses for this uncached prompt\n                            for _ in range(n):\n                                while len(all_responses) < original_idx * n + _:\n                                    all_responses.append(\"\")\n                                if response_idx < len(batch_responses):\n                                    all_responses.append(batch_responses[response_idx])\n                                    response_idx += 1\n                \n                if return_token_count:\n                    # Count tokens for responses\n                    token_counts.extend([0] * len(cached_responses))  # 0 for cached responses\n                    token_counts.extend(batch_token_counts)\n        \n        if return_token_count:\n            return all_responses, token_counts\n        return all_responses, [0] * len(all_responses)\n    \nclass ChatCompletionMessage:\n    def __init__(self, content: str, role: str = \"assistant\", logprobs: Optional[Dict] = None):\n        self.content = content\n        self.role = role\n        self.logprobs = logprobs\n\nclass ChatCompletionChoice:\n    def __init__(\n        self,\n        index: int,\n        message: Dict[str, Any],\n        finish_reason: str = \"stop\",\n        logprobs: Optional[Dict] = None\n    ):\n        self.index = index\n        self.message = ChatCompletionMessage(**message)\n        self.finish_reason = finish_reason\n        if logprobs:\n            self.message.logprobs = logprobs\n\nclass ChatCompletionUsage:\n    def __init__(self, prompt_tokens: int, completion_tokens: int, total_tokens: int):\n        self.prompt_tokens = prompt_tokens\n        self.completion_tokens = completion_tokens\n        self.total_tokens = total_tokens\n\nclass ChatCompletion:\n    def __init__(self, response_dict: Dict):\n        self.id = response_dict[\"id\"]\n        self.object = response_dict[\"object\"]\n        self.created = response_dict[\"created\"]\n        self.model = response_dict[\"model\"]\n        self.choices = [\n            ChatCompletionChoice(\n                index=choice[\"index\"],\n                message=choice[\"message\"],\n                finish_reason=choice[\"finish_reason\"]\n            )\n            for choice in response_dict[\"choices\"]\n        ]\n        self.usage = ChatCompletionUsage(**response_dict[\"usage\"])\n    \n    def model_dump(self) -> Dict:\n        return {\n            \"id\": self.id,\n            \"object\": self.object,\n            \"created\": self.created,\n            \"model\": self.model,\n            \"choices\": [\n                {\n                    \"index\": choice.index,\n                    \"message\": {\n                        \"role\": choice.message.role,\n                        \"content\": choice.message.content,\n                        \"logprobs\": choice.message.logprobs\n                    } if choice.message.logprobs else {\n                        \"role\": choice.message.role,\n                        \"content\": choice.message.content\n                    },\n                    \"finish_reason\": choice.finish_reason\n                }\n                for choice in self.choices\n            ],\n            \"usage\": {\n                \"prompt_tokens\": self.usage.prompt_tokens,\n                \"completion_tokens\": self.usage.completion_tokens,\n                \"total_tokens\": self.usage.total_tokens\n            }\n        }\n\nclass InferenceClient:\n    \"\"\"OpenAI SDK Compatible client for local inference with dynamic model support\"\"\"\n    \n    def __init__(self):\n        self.cache_manager = CacheManager.get_instance(max_size=4)\n        self.device_manager = DeviceManager()\n        self.model_manager = ModelManager(self.cache_manager, self.device_manager)\n        self.lora_manager = LoRAManager(self.cache_manager)\n        self.mlx_manager = MLXManager(self.cache_manager)\n        self.chat = self.Chat(self)\n        self.models = self.Models()\n\n    def get_pipeline(self, model: str):\n        \"\"\"Get inference pipeline - automatically chooses MLX or PyTorch based on model\"\"\"\n        # Check if should use MLX\n        if self.mlx_manager.available and should_use_mlx(model):\n            logger.info(f\"Using MLX pipeline for model: {model}\")\n            return self.mlx_manager.create_pipeline(model)\n        else:\n            # Use existing PyTorch pipeline\n            logger.info(f\"Using PyTorch pipeline for model: {model}\")\n            model_config = parse_model_string(model)\n            return InferencePipeline(\n                model_config,\n                self.cache_manager,\n                self.device_manager,\n                self.model_manager,\n                self.lora_manager\n            )\n    \n    class Chat:\n        \"\"\"OpenAI-compatible chat interface\"\"\"\n        def __init__(self, client: 'InferenceClient'):\n            self.client = client\n            self.completions = self.Completions(client)\n\n        class Completions:\n            def __init__(self, client: 'InferenceClient'):\n                self.client = client\n\n            def create(\n                self,\n                messages: List[Dict[str, str]],\n                model: str,\n                temperature: float = 1.0,\n                top_p: float = 1.0,\n                n: int = 1,\n                stream: bool = False,\n                stop: Optional[Union[str, List[str]]] = None,\n                max_tokens: Optional[int] = None,\n                presence_penalty: float = 0,\n                frequency_penalty: float = 0,\n                logit_bias: Optional[Dict[str, float]] = None,\n                seed: Optional[int] = None,\n                logprobs: Optional[bool] = None,\n                top_logprobs: Optional[int] = None,\n                active_adapter: Optional[Dict[str, Any]] = None,\n                decoding: Optional[str] = None,\n                # CoT specific params\n                k: int = 10,\n                num_beams: int = 1,\n                length_penalty: float = 1.0,\n                no_repeat_ngram_size: int = 0,\n                early_stopping: bool = False,\n                aggregate_paths: bool = True,\n                # Entropy specific params\n                top_k: int = 27,\n                min_p: float = 0.03,\n                # Thinking specific params\n                reasoning_effort: str = \"low\",\n                thought_switch_tokens: List[str] = [],\n                min_thinking_tokens: Optional[int] = None,\n                max_thinking_tokens: Optional[int] = None,\n                max_thoughts: Optional[int] = None,\n                prefill: str = \"\",\n                start_think_token: str =\"<think>\",\n                end_think_token: str = \"</think>\",\n                **kwargs\n            ) -> ChatCompletion:\n                \"\"\"Create a chat completion with OpenAI-compatible parameters\"\"\"\n                logger.info(\"Starting chat completion creation\")\n                if stream:\n                    raise NotImplementedError(\"Streaming is not yet supported\")\n\n                logger.info(f\"Getting pipeline for model: {model}\")\n                pipeline = self.client.get_pipeline(model)\n                logger.info(\"Pipeline acquired\")\n\n                # Set active adapter if specified\n                if active_adapter is not None:\n                    logger.info(f\"Setting active adapter to: {active_adapter}\")\n                    pipeline.lora_manager.set_active_adapter(pipeline.current_model, active_adapter)\n\n                responses = []\n                logprobs_results = []\n                prompt_tokens = 0\n                completion_tokens = 0\n\n                try:\n                    # Handle specialized decoding approaches\n                    if decoding:\n                        logger.info(f\"Using specialized decoding approach: {decoding}\")\n\n                        # Ensure model is in eval mode and on correct device\n                        pipeline.current_model.eval()\n                        device = pipeline.current_model.device\n                        \n                        if decoding == \"cot_decoding\":\n                            # Use directly available parameters for CoT\n                            cot_params = {\n                                \"k\": k,\n                                \"num_beams\": num_beams,\n                                \"max_new_tokens\": max_tokens if max_tokens is not None else 512,\n                                \"temperature\": temperature,\n                                \"top_p\": top_p,\n                                \"repetition_penalty\": 1.0,\n                                \"length_penalty\": length_penalty,\n                                \"no_repeat_ngram_size\": no_repeat_ngram_size,\n                                \"early_stopping\": early_stopping,\n                                \"aggregate_paths\": aggregate_paths,\n                            }\n                            \n                            result, confidence = cot_decode(\n                                pipeline.current_model,\n                                pipeline.tokenizer,\n                                messages,\n                                **cot_params\n                            )\n                            responses = [result]\n                            logprobs_results = [{\"confidence_score\": confidence} if confidence is not None else None]\n                            completion_tokens = len(pipeline.tokenizer.encode(result))\n                            \n                        elif decoding == \"entropy_decoding\":\n\n                            # Ensure model is using full precision\n                            original_dtype = pipeline.current_model.dtype\n                            pipeline.current_model = pipeline.current_model.to(torch.float32)\n\n                            try:\n                                # Configure generator for entropy decoding\n                                generator = None\n                                if seed is not None:\n                                    generator = torch.Generator(device=device)\n                                    generator.manual_seed(seed)\n                                else:\n                                    generator = torch.Generator(device=device)\n                                    generator.manual_seed(1337)  # Default seed as in original implementation\n                                \n                                # Use directly available parameters for entropy decoding\n                                entropy_params = {\n                                    \"max_new_tokens\": max_tokens if max_tokens is not None else 4096,\n                                    \"temperature\": temperature,\n                                    \"top_p\": top_p,\n                                    \"top_k\": top_k,\n                                    \"min_p\": min_p,\n                                    \"generator\": generator\n                                }\n                                \n                                # Disable autocast and run in full precision\n                                with torch.amp.autocast('cuda', enabled=False), torch.inference_mode():\n                                    result = entropy_decode(\n                                        pipeline.current_model,\n                                        pipeline.tokenizer,\n                                        messages,\n                                        **entropy_params\n                                    )\n                                responses = [result]\n                                logprobs_results = [None]\n                                completion_tokens = len(pipeline.tokenizer.encode(result))\n                            \n                            finally:\n                                # Restore original dtype\n                                pipeline.current_model = pipeline.current_model.to(original_dtype)\n\n                        elif decoding == \"thinkdeeper\":\n                            # Get base config for reasoning effort\n                            thinkdeeper_config = get_effort_profile(reasoning_effort, max_tokens)\n                            \n                            # Override with any custom parameters\n                            custom_config = {\n                                \"min_thinking_tokens\": min_thinking_tokens if min_thinking_tokens is not None else thinkdeeper_config[\"min_thinking_tokens\"],\n                                \"max_thinking_tokens\": max_thinking_tokens if max_thinking_tokens is not None else thinkdeeper_config[\"max_thinking_tokens\"],\n                                \"max_thoughts\": max_thoughts if max_thoughts is not None else thinkdeeper_config[\"max_thoughts\"],\n                                \"thought_switch_tokens\": thought_switch_tokens if thought_switch_tokens else thinkdeeper_config[\"thought_switch_tokens\"],\n                                \"prefill\": prefill if prefill else thinkdeeper_config[\"prefill\"],\n                                \"start_think_token\": start_think_token,\n                                \"end_think_token\": end_think_token,\n                            }\n                            thinkdeeper_config.update(custom_config)\n\n                            result = thinkdeeper_decode(\n                                pipeline.current_model,\n                                pipeline.tokenizer,\n                                messages,\n                                thinkdeeper_config\n                                )\n                            responses = [result]\n                            logprobs_results = [None]\n                            completion_tokens = len(pipeline.tokenizer.encode(result))\n                        elif decoding == \"autothink\":\n                            # Get steering dataset configuration\n                            steering_dataset = kwargs.get(\"steering_dataset\", \"codelion/Qwen3-0.6B-pts-steering-vectors\")\n                            target_layer = kwargs.get(\"target_layer\", 19)\n                            \n                            # Prepare AutoThink configuration\n                            autothink_config = {\n                                \"steering_dataset\": steering_dataset,\n                                \"target_layer\": target_layer,\n                                \"pattern_strengths\": kwargs.get(\"pattern_strengths\", {\n                                    \"depth_and_thoroughness\": 2.5,\n                                    \"numerical_accuracy\": 2.0,\n                                    \"self_correction\": 3.0,\n                                    \"exploration\": 2.0,\n                                    \"organization\": 1.5\n                                })\n                            }\n                            \n                            # Process with AutoThink\n                            result = autothink_decode(\n                                pipeline.current_model,\n                                pipeline.tokenizer,\n                                messages,\n                                autothink_config\n                            )\n                            responses = [result]\n                            logprobs_results = [None]\n                            completion_tokens = len(pipeline.tokenizer.encode(result))\n                        else:\n                            raise ValueError(f\"Unknown specialized decoding approach: {decoding}\")\n                        \n                        # Calculate prompt tokens for specialized approaches\n                        prompt_text = pipeline.tokenizer.apply_chat_template(messages, tokenize=False)\n                        prompt_tokens = len(pipeline.tokenizer.encode(prompt_text))\n                        \n                    else:\n                        # Standard generation\n                        prompt = pipeline.tokenizer.apply_chat_template(\n                            messages,\n                            tokenize=False,\n                            add_generation_prompt=True\n                        )\n                        \n                        # Set generation parameters\n                        generation_params = {\n                            \"temperature\": temperature,\n                            \"top_p\": top_p,\n                            \"num_return_sequences\": n,\n                            \"max_new_tokens\": max_tokens if max_tokens is not None else 4096,\n                            \"presence_penalty\": presence_penalty,\n                            \"frequency_penalty\": frequency_penalty,\n                            \"stop_sequences\": [stop] if isinstance(stop, str) else stop,\n                            \"seed\": seed,\n                            \"logit_bias\": logit_bias,\n                            \"logprobs\": logprobs,\n                            \"top_logprobs\": top_logprobs\n                        }\n\n                        # Generate responses\n                        responses, token_counts, logprobs_results = pipeline.generate(\n                            prompt,\n                            generation_params=generation_params\n                        )\n                        \n                        prompt_tokens = len(pipeline.tokenizer.encode(prompt))\n                        completion_tokens = sum(token_counts)\n\n                    # Create OpenAI-compatible response format\n                    response_dict = {\n                        \"id\": f\"chatcmpl-{int(time.time()*1000)}\",\n                        \"object\": \"chat.completion\",\n                        \"created\": int(time.time()),\n                        \"model\": model,\n                        \"choices\": [\n                            {\n                                \"index\": idx,\n                                \"message\": {\n                                    \"role\": \"assistant\",\n                                    \"content\": response,\n                                    **({\"logprobs\": logprob_result} if logprob_result else {})\n                                },\n                                \"finish_reason\": \"stop\"\n                            }\n                            for idx, (response, logprob_result) in enumerate(zip(responses, logprobs_results))\n                        ],\n                        \"usage\": {\n                            \"prompt_tokens\": prompt_tokens,\n                            \"completion_tokens\": completion_tokens,\n                            \"total_tokens\": completion_tokens + prompt_tokens\n                        }\n                    }\n                    \n                    logger.debug(f\"Response : {response_dict}\")\n                    return ChatCompletion(response_dict)\n                    \n                except Exception as e:\n                    logger.error(f\"Error in chat completion: {str(e)}\")\n                    raise\n                \n    class Models:\n        \"\"\"OpenAI-compatible models interface\"\"\"\n        def list(self):\n            \"\"\"Return list of supported models\"\"\"\n            try:\n                import requests\n                response = requests.get(\n                    \"https://huggingface.co/api/models?sort=downloads&direction=-1&filter=text-generation&limit=20\"\n                )\n                models = response.json()\n                model_list = []\n                \n                for model in models:\n                    if 'pipeline_tag' in model and model['pipeline_tag'] == 'text-generation':\n                        model_list.append({\n                            \"id\": model['id'],\n                            \"object\": \"model\",\n                            \"created\": int(time.time()),\n                            \"owned_by\": \"huggingface\",\n                        })\n                \n                return {\"data\": model_list, \"object\": \"list\"}\n            except Exception as e:\n                logger.warning(f\"Failed to fetch models: {e}\")\n                return {\n                    \"data\": [{\n                        \"id\": \"HuggingFaceTB/SmolLM-135M-Instruct\",\n                        \"object\": \"model\",\n                        \"created\": int(time.time()),\n                        \"owned_by\": \"huggingface\",\n                    }],\n                    \"object\": \"list\"\n                }\n            \ndef create_inference_client() -> InferenceClient:\n    \"\"\"Factory function to create an inference client\"\"\"\n    return InferenceClient()\n\ndef parse_model_string(model: str) -> ModelConfig:\n    \"\"\"Parse the model string to extract base model and adapter IDs\"\"\"\n    parts = model.split('+')\n    base_model_id = parts[0]\n    adapter_ids = parts[1:] if len(parts) > 1 else None\n    \n    return ModelConfig(\n        base_model_id=base_model_id,\n        adapter_ids=adapter_ids,\n        use_memory_efficient_attention=False,\n        quantization_bits=0,\n        enable_prompt_caching=False,\n        dynamic_temperature=False,\n    )\n\ndef get_effort_profile(reasoning_effort: str, max_tokens: int = 4096) -> dict:\n    \"\"\"Get reasoning effort profile based on specified level and max tokens.\n    \n    Args:\n        reasoning_effort: 'low', 'medium', or 'high'\n        max_tokens: Maximum tokens allowed for generation, defaults to 4096\n    \n    Returns:\n        dict: Configuration for the specified reasoning effort level\n    \"\"\"\n    # Base profiles with percentages and thought counts\n    profiles = {\n        \"low\": {\n            \"min_tokens_pct\": 0.10,  \n            \"max_tokens_pct\": 0.33,  # 33% of max_tokens\n            \"max_thoughts\": 64,\n            \"thought_switch_tokens\": [\n                \"Wait,\",\n                \"Alternatively,\",\n                \"However,\",\n                \"Additionally,\",\n            ],\n            \"prefill\": \"\"\n        },\n        \"medium\": {\n            \"min_tokens_pct\": 0.10,  \n            \"max_tokens_pct\": 0.66,  # 66% of max_tokens\n            \"max_thoughts\": 256,\n            \"thought_switch_tokens\": [\n                \"Wait,\",\n                \"Alternatively,\",\n                \"However,\",\n                \"Additionally,\",\n            ],\n            \"prefill\": \"\"\n        },\n        \"high\": {\n            \"min_tokens_pct\": 0.10,  \n            \"max_tokens_pct\": 0.90,  # 90% of max_tokens\n            \"max_thoughts\": 512,\n            \"thought_switch_tokens\": [\n                \"Wait,\",\n                \"Alternatively,\",\n                \"However,\",\n                \"Additionally,\",\n            ],\n            \"prefill\": \"\"\n        }\n    }\n    \n    # Get base profile or default to medium\n    profile = profiles.get(reasoning_effort.lower(), profiles[\"low\"])\n    \n    # Calculate actual token limits based on max_tokens\n    min_thinking_tokens = int(max_tokens * profile[\"min_tokens_pct\"])\n    max_thinking_tokens = int(max_tokens * profile[\"max_tokens_pct\"])\n    \n    # Create final config\n    config = {\n        \"min_thinking_tokens\": min_thinking_tokens,\n        \"max_thinking_tokens\": max_thinking_tokens,\n        \"max_thoughts\": profile[\"max_thoughts\"],\n        \"thought_switch_tokens\": profile[\"thought_switch_tokens\"],\n        \"prefill\": profile[\"prefill\"]\n    }\n    \n    return config",
          "",
          "#!/usr/bin/env python3\nimport argparse\nimport time\nimport json\nimport os\nfrom typing import Dict, List, Any, Tuple\nimport datasets\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm import tqdm\nimport logging\nfrom datetime import datetime\nimport re\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define the approaches to test\n# Each approach is (name, description)\nAPPROACHES = [\n    (\"none\", \"Baseline without any optimization\"),\n    (\"leap\", \"LEAP Approach\"),\n    (\"rto\", \"Round Trip Optimization\"),\n    (\"cot_reflection\", \"Chain of Thought with Reflection\"),\n    (\"self_consistency\", \"Self Consistency Check\"),\n    (\"plansearch\", \"Planning with Search\"),\n    (\"re2\", \"ReRead Approach\"),\n    (\"z3\", \"Z3 Solver for Mathematical Problems\"),\n    (\"coc\", \"Chain of Code\"),\n    (\"executecode\" , \"Execute Code\"),\n    (\"spl\", \"System Prompt Learning\")\n]\n\ndef load_optillm_bench() -> datasets.Dataset:\n    \"\"\"Load the OptiLLM Bench dataset.\"\"\"\n    try:\n        dataset = load_dataset(\"codelion/optillmbench\")\n        return dataset[\"test\"]  # We use the test split for evaluation\n    except Exception as e:\n        logger.error(f\"Error loading dataset: {e}\")\n        raise\n\ndef extract_gsm8k_answer(text: str) -> float:\n    \"\"\"Extract numerical answer after ### from GSM8K responses.\"\"\"\n    match = re.search(r'###\\s*(-?\\d*\\.?\\d+)', text)\n    if match:\n        try:\n            return float(match.group(1))\n        except ValueError:\n            return None\n    return None\n\ndef remove_thinking_blocks(text: str) -> str:\n    \"\"\"\n    Remove <think>...</think> blocks from the response.\n    If there's a </think> tag, only keep the content after it.\n    \"\"\"\n    if not text:\n        return text\n        \n    # Check if there's a thinking block\n    if '</think>' in text:\n        # Get everything after the last </think> tag\n        parts = text.split('</think>')\n        return parts[-1].strip()\n    \n    # If no thinking blocks, return original text\n    return text\n\ndef extract_choice_index_from_question(question: str, answer: str) -> int:\n    \"\"\"\n    Extract the index of the correct answer from a multiple-choice question.\n    \n    Args:\n        question: The question text containing choices\n        answer: The correct answer (just the text, no index)\n    \n    Returns:\n        int: The index of the correct answer, or -1 if not found\n    \"\"\"\n    # Look for a pattern like \"N. answer\" in the question\n    answer_clean = answer.strip().lower()\n    \n    # Debug logging for critical examples\n    logger.debug(f\"Looking for answer: '{answer_clean}' in question\")\n    \n    # Check for \"Choices:\" marker in the question\n    if \"choices:\" in question.lower():\n        # Split the question by lines after \"Choices:\"\n        choices_section = question.lower().split(\"choices:\")[1].strip()\n        \n        # Log the choices section\n        logger.debug(f\"Choices section: '{choices_section}'\")\n        \n        # Try different approaches to extract choices\n        \n        # 1. If it's all on one line, use a more comprehensive regex\n        if '\\n' not in choices_section:\n            # This pattern matches \"N. text\" where N is a digit and text is any text up to the next number or end\n            all_choices = re.findall(r'(\\d+)\\s*\\.\\s*([^0-9.]+?)(?=\\s*\\d+\\s*\\.|$)', choices_section)\n            \n            logger.debug(f\"Single line choices found: {all_choices}\")\n            \n            for idx, choice_text in all_choices:\n                choice_text_clean = choice_text.strip()\n                if choice_text_clean.lower() == answer_clean:\n                    logger.debug(f\"Found match at index {idx}: '{choice_text_clean}'\")\n                    return int(idx)\n        \n        # 2. Try splitting by newlines\n        choices = choices_section.split(\"\\n\")\n        \n        for i, choice in enumerate(choices):\n            choice = choice.strip()\n            if not choice:\n                continue\n                \n            logger.debug(f\"Checking choice {i}: '{choice}'\")\n            \n            # Try to extract the index and choice text\n            match = re.match(r'\\s*(\\d+)\\s*\\.\\s*(.*)', choice)\n            if match:\n                idx = int(match.group(1))\n                choice_text = match.group(2).strip()\n                \n                logger.debug(f\"Parsed choice: index={idx}, text='{choice_text}'\")\n                \n                if choice_text.lower() == answer_clean:\n                    logger.debug(f\"Found exact match at index {idx}\")\n                    return idx\n        \n        # 3. Fallback: just look for any occurrence of the number followed by the answer\n        pattern = r'(\\d+)\\s*\\.\\s*' + re.escape(answer_clean)\n        match = re.search(pattern, choices_section)\n        if match:\n            logger.debug(f\"Fallback match found at index {match.group(1)}\")\n            return int(match.group(1))\n    \n    logger.debug(\"No match found for answer in choices\")\n    return -1\n\ndef is_numeric_only_response(response: str) -> Tuple[bool, int]:\n    \"\"\"\n    Check if the response is just a numeric value, possibly with whitespace and newlines.\n    \n    Args:\n        response: The response text to check\n        \n    Returns:\n        Tuple of (is_numeric, value)\n    \"\"\"\n    # Strip all whitespace, including newlines\n    clean_response = re.sub(r'\\s', '', response)\n    \n    # Check if it's just a number\n    if clean_response.isdigit():\n        return True, int(clean_response)\n    \n    return False, -1\n\ndef evaluate_response(response: str, ground_truth: str, category: str, question: str = None) -> bool:\n    \"\"\"\n    Evaluate if the response matches the ground truth based on category.\n    \n    Args:\n        response: Model's response\n        ground_truth: Correct answer\n        category: Problem category (gsm8k, mmlu_math, boolq, aqua_rat)\n        question: Original question text, needed for MMLU evaluation\n    \n    Returns:\n        bool: Whether the response is correct\n    \"\"\"\n    if not response or not ground_truth:\n        return False\n    \n    # First, remove any thinking blocks\n    response = remove_thinking_blocks(response)\n        \n    if category == \"gsm8k\":\n        # Extract numerical answers after ### and compare\n        response_num = extract_gsm8k_answer(response)\n        ground_truth_num = extract_gsm8k_answer(ground_truth)\n        \n        if response_num is None or ground_truth_num is None:\n            return False\n            \n        # Compare with small tolerance for floating point\n        return abs(response_num - ground_truth_num) < 1e-6\n    elif category == \"mmlu_math\":\n        # Special handling for MMLU-math multiple choice questions\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        \n        # Case 1: Exact match of answer text\n        if response_clean == ground_truth_clean:\n            logger.debug(\"Exact text match\")\n            return True\n            \n        # For other cases, we need to find what index corresponds to the ground truth\n        if question:\n            correct_index = extract_choice_index_from_question(question, ground_truth)\n            \n            if correct_index >= 0:\n                # Case 2: Check if response is just the digit (most common LLM response for indices)\n                is_numeric, value = is_numeric_only_response(response)\n                if is_numeric and value == correct_index:\n                    logger.debug(f\"Numeric match: response '{response}' -> {value} matches index {correct_index}\")\n                    return True\n                \n                # Case 3: Check if response is \"index. answer\"\n                if re.search(fr\"{correct_index}\\s*\\.\\s*{re.escape(ground_truth_clean)}\", response_clean):\n                    logger.debug(\"Pattern match for 'index. answer'\")\n                    return True\n                \n                # Case 4: Check if response contains both the index and the answer text\n                if str(correct_index) in response_clean and ground_truth_clean in response_clean:\n                    logger.debug(\"Contains both index and answer\")\n                    return True\n        \n        return False\n    else:\n        # For boolq and aqua_rat, exact match is required\n        # Clean up both strings for comparison\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        return response_clean == ground_truth_clean\n\ndef get_prompt_for_category(question: str, category: str) -> str:\n    \"\"\"\n    Generate appropriate prompt based on category.\n    \"\"\"\n    if category == \"gsm8k\":\n        return (\n            f\"Solve this math problem step by step. After solving, provide the final \"\n            f\"numerical answer after '### ' (three hash symbols and a space).\\n\\n\"\n            f\"Question: {question}\\n\\n\"\n            f\"Show your work, then give the final answer after '### '.\"\n        )\n    elif category == \"mmlu_math\":\n        return (\n            f\"Solve this math problem. Provide only the answer with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"boolq\":\n        return (\n            f\"Answer this yes/no question with only 'yes' or 'no'.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"aqua_rat\":\n        return (\n            f\"Choose the correct answer. Provide only the letter choice with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    else:\n        return f\"Question: {question}\"\n\ndef evaluate_model(\n    client: OpenAI,\n    model: str,\n    dataset: datasets.Dataset,\n    approach: str,\n    max_samples: int = None\n) -> Tuple[Dict[str, float], List[Dict[str, Any]]]:\n    \"\"\"\n    Evaluate a model on the dataset using a specific approach.\n    Returns metrics and detailed results.\n    \"\"\"\n    metrics = {\n        \"total_correct\": 0,\n        \"total_time\": 0,\n        \"samples\": 0,\n    }\n    \n    # Initialize category-specific metrics\n    category_metrics = {}\n    \n    # Detailed results for each example\n    detailed_results = []\n    \n    # Prepare the dataset\n    examples = dataset if max_samples is None else dataset.select(range(max_samples))\n    \n    # Create model name with approach\n    full_model_name = f\"{approach}-{model}\" if approach != \"none\" else model\n    \n    for example in tqdm(examples, desc=f\"Evaluating {approach}\"):\n        try:\n            # Get appropriate prompt for the category\n            prompt = get_prompt_for_category(example['question'], example['category'])\n            \n            # Record start time\n            start_time = time.time()\n            \n            # Make API call\n            response = client.chat.completions.create(\n                model=full_model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant focused on providing precise answers in the requested format.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.2,\n                max_tokens=4096,\n                extra_body= {\"spl_learning\": False},\n            )\n            \n            # Calculate time taken\n            time_taken = time.time() - start_time\n            \n            # Get the response text\n            response_text = response.choices[0].message.content\n            \n            # Also store the raw response for reference\n            raw_response = response_text\n            \n            # Process the response to remove thinking blocks\n            processed_response = remove_thinking_blocks(response_text)\n            \n            # Evaluate the processed response\n            is_correct = evaluate_response(\n                processed_response,\n                example['answer'],\n                example['category'],\n                example['question']  # Pass the question for MMLU evaluation\n            )\n            \n            # Update metrics\n            metrics[\"total_correct\"] += int(is_correct)\n            metrics[\"total_time\"] += time_taken\n            metrics[\"samples\"] += 1\n            \n            # Update category metrics\n            if example['category'] not in category_metrics:\n                category_metrics[example['category']] = {\n                    \"correct\": 0,\n                    \"total\": 0,\n                    \"time\": 0\n                }\n            category_metrics[example['category']][\"correct\"] += int(is_correct)\n            category_metrics[example['category']][\"total\"] += 1\n            category_metrics[example['category']][\"time\"] += time_taken\n            \n            # Check if thinking blocks were removed\n            has_thinking = '</think>' in raw_response\n            \n            # Record detailed result\n            detailed_results.append({\n                \"id\": example['id'],\n                \"category\": example['category'],\n                \"correct\": is_correct,\n                \"time_taken\": time_taken,\n                \"raw_response\": raw_response,\n                \"processed_response\": processed_response if has_thinking else None,\n                \"has_thinking\": has_thinking,\n                \"ground_truth\": example['answer']\n            })\n            \n        except Exception as e:\n            logger.error(f\"Error processing example {example['id']}: {e}\")\n            continue\n    \n    # Calculate final metrics\n    final_metrics = {\n        \"accuracy\": metrics[\"total_correct\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"average_time\": metrics[\"total_time\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"total_time\": metrics[\"total_time\"],\n        \"total_samples\": metrics[\"samples\"],\n    }\n    \n    # Add category-specific metrics\n    for category, cat_metrics in category_metrics.items():\n        final_metrics[f\"{category}_accuracy\"] = cat_metrics[\"correct\"] / cat_metrics[\"total\"]\n        final_metrics[f\"{category}_average_time\"] = cat_metrics[\"time\"] / cat_metrics[\"total\"]\n    \n    return final_metrics, detailed_results\n\ndef save_results(metrics: Dict[str, float], detailed_results: List[Dict[str, Any]], \n                model: str, approach: str, output_dir: str):\n    \"\"\"Save evaluation results to files.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create model-specific directory\n    model_dir = os.path.join(output_dir, model.replace('/', '_'))\n    os.makedirs(model_dir, exist_ok=True)\n    \n    base_filename = os.path.join(model_dir, f\"{approach}_{timestamp}\")\n    \n    # Save metrics\n    with open(f\"{base_filename}_metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    \n    # Save detailed results\n    with open(f\"{base_filename}_detailed.json\", \"w\") as f:\n        json.dump(detailed_results, f, indent=2)\n    \n    # Create a summary DataFrame for easier analysis\n    df = pd.DataFrame([\n        {k: v for k, v in result.items() if k != 'raw_response' and k != 'processed_response'}\n        for result in detailed_results\n    ])\n    df.to_csv(f\"{base_filename}_summary.csv\", index=False)\n    \n    logger.info(f\"Results saved to {base_filename}_*\")\n\ndef generate_report(all_metrics: Dict[str, Dict[str, float]], output_dir: str):\n    \"\"\"Generate a comprehensive report comparing all approaches.\"\"\"\n    report = []\n    \n    # Header\n    report.append(\"# OptiLLM Bench Evaluation Report\")\n    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    \n    # Overall Results Table\n    report.append(\"## Overall Results\")\n    headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\", \"Total Time (s)\"]\n    rows = []\n    \n    for approach, metrics in all_metrics.items():\n        rows.append([\n            approach,\n            f\"{metrics['accuracy']*100:.2f}%\",\n            f\"{metrics['average_time']:.2f}\",\n            f\"{metrics['total_time']:.2f}\"\n        ])\n    \n    # Convert to DataFrame for nice formatting\n    df = pd.DataFrame(rows, columns=headers)\n    report.append(df.to_markdown())\n    \n    # Category-wise Results\n    report.append(\"\\n## Results by Category\")\n    categories = [\"gsm8k\", \"mmlu_math\", \"boolq\", \"aqua_rat\"]\n    \n    for category in categories:\n        report.append(f\"\\n### {category.upper()}\")\n        headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\"]\n        rows = []\n        \n        for approach, metrics in all_metrics.items():\n            if f\"{category}_accuracy\" in metrics:\n                rows.append([\n                    approach,\n                    f\"{metrics[f'{category}_accuracy']*100:.2f}%\",\n                    f\"{metrics[f'{category}_average_time']:.2f}\"\n                ])\n        \n        df = pd.DataFrame(rows, columns=headers)\n        report.append(df.to_markdown())\n    \n    # Save report\n    report_path = f\"{output_dir}/evaluation_report.md\"\n    with open(report_path, \"w\") as f:\n        f.write(\"\\n\\n\".join(report))\n    \n    logger.info(f\"Report saved to {report_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate a model on OptiLLM Bench\")\n    parser.add_argument(\"--model\", required=True, help=\"Model identifier\")\n    parser.add_argument(\"--base-url\", default=\"http://localhost:8000/v1\", \n                        help=\"Base URL for API endpoint\")\n    parser.add_argument(\"--max-samples\", type=int, help=\"Maximum number of samples to evaluate\")\n    parser.add_argument(\"--output-dir\", default=\"results\", \n                        help=\"Directory to save results\")\n    parser.add_argument(\"--approaches\", nargs=\"+\", \n                        help=\"Specific approaches to evaluate (default: all)\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    args = parser.parse_args()\n    \n    # Set debug logging if specified\n    if args.debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    # Create output directory\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Get API key from environment\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable must be set\")\n    \n    # Initialize OpenAI client\n    client = OpenAI(\n        api_key=api_key,\n        base_url=args.base_url\n    )\n    \n    # Load dataset\n    dataset = load_optillm_bench()\n    \n    # Determine which approaches to evaluate\n    approaches_to_test = (\n        [a[0] for a in APPROACHES if a[0] in args.approaches]\n        if args.approaches\n        else [a[0] for a in APPROACHES]\n    )\n    \n    # Store all metrics for final report\n    all_metrics = {}\n    \n    # Evaluate each approach\n    for approach in approaches_to_test:\n        logger.info(f\"Evaluating approach: {approach}\")\n        \n        try:\n            metrics, detailed_results = evaluate_model(\n                client,\n                args.model,\n                dataset,\n                approach,\n                args.max_samples\n            )\n            \n            all_metrics[approach] = metrics\n            \n            # Save results for this approach\n            save_results(metrics, detailed_results, args.model, approach, \n                        args.output_dir)\n            \n            logger.info(f\"Completed evaluation for {approach}\")\n            logger.info(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n            logger.info(f\"Average time per sample: {metrics['average_time']:.2f}s\")\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating approach {approach}: {e}\")\n            continue\n    \n    # Generate final report\n    generate_report(all_metrics, args.output_dir)\n\nif __name__ == \"__main__\":\n    main()",
          "import argparse\nimport json\nimport os\nimport logging\nimport re\nimport time\nimport math\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional, Union, Counter\nfrom datetime import datetime\nfrom openai import OpenAI\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport statistics\nfrom collections import defaultdict\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize OpenAI client\n# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"), base_url=\"https://openrouter.ai/api/v1\")\n\nclient = OpenAI(api_key=\"optillm\", base_url=\"http://localhost:8000/v1\")\n\nSYSTEM_PROMPT = '''You are solving AIME (American Invitational Mathematics Examination) problems.\n\nImportant: Always end your solution with the final answer in one of these two formats:\n\n1. \\\\[\n   \\\\boxed{X}.\n   \\\\]\n\n2. $n=\\\\boxed{X}$\n\nwhere X is your integer answer between 0 and 999.'''\n\n# Define the thought transition phrases to track\nTHOUGHT_TRANSITIONS = [\n    \"Wait,\", \n    \"Alternatively,\", \n    \"However,\", \n    \"Additionally,\"\n]\n\ndef load_2024_dataset() -> list[dict]:\n    \"\"\"\n    Load the dataset of problems.\n    Returns:\n        list[dict]: The dataset of problems.\n    \"\"\"\n    dataset_original = load_dataset(\"AI-MO/aimo-validation-aime\")\n    # Filter out problems that are not from 2024\n    dataset = dataset_original[\"train\"].filter(lambda example: \"2024\" in example[\"url\"])\n    logging.debug(f\"Filtered dataset size: {len(dataset)}.\")\n    assert len(dataset) == 30, f\"Expected 30 problems after filtering by 2024, but found {len(dataset)}\"\n    return dataset\n\ndef extract_answer(response: str) -> Optional[int]:\n    \"\"\"\n    Extract the numerical answer from a math solution response.\n    Handles various formats of boxed answers and falls back to last number if needed.\n    \"\"\"\n    if not response:\n        return None\n        \n    # Clean the response\n    response = ' '.join(response.split())\n    \n    patterns = [\n        r'\\$n=\\\\boxed{(\\d+)}\\$',\n        r'\\\\\\[\\\\boxed{(\\d+)}\\\\\\]',\n        r'\\\\\\[\\\\boxed{(\\d+)}\\.\\\\\\]',\n        r'\\\\boxed{(\\d+)}',\n        r'\\$\\\\boxed{(\\d+)}\\$',\n        r'boxed{(\\d+)}',\n        r'\\\\boxed\\s*{\\s*(\\d+)\\s*}',\n        r'\\bboxed\\s*{\\s*(\\d+)\\s*}',\n        r'final answer is[^\\d]*(\\d+)',\n        r'answer is[^\\d]*(\\d+)',\n        r'answer:[^\\d]*(\\d+)',\n        r'= ?(\\d+)$'\n    ]\n    \n    for pattern in patterns:\n        matches = re.finditer(pattern, response, re.IGNORECASE)\n        last_match = None\n        for match in matches:\n            last_match = match\n            \n        if last_match:\n            try:\n                return int(last_match.group(1))\n            except (ValueError, IndexError):\n                continue\n    \n    numbers = re.findall(r'(\\d+)', response)\n    if numbers:\n        try:\n            return int(numbers[-1])\n        except ValueError:\n            pass\n            \n    return None\n\ndef analyze_thinking(response: str) -> Dict:\n    \"\"\"\n    Analyze thinking patterns in the response.\n    Extract tokens between <think> and </think> tags and count thought transitions.\n    \n    Args:\n        response (str): The model's response text\n        \n    Returns:\n        Dict: Analysis metrics including thinking tokens and thought transitions\n    \"\"\"\n    # Default result with zero values\n    result = {\n        \"has_think_tags\": False,\n        \"thinking_tokens\": 0,\n        \"thinking_tokens_text\": \"\",\n        \"total_tokens\": len(response.split()),\n        \"thought_transitions\": 0,\n        \"transition_counts\": {phrase: 0 for phrase in THOUGHT_TRANSITIONS},\n        \"transition_positions\": []\n    }\n    \n    # Extract content between <think> and </think> tags\n    think_pattern = re.compile(r'<think>(.*?)</think>', re.DOTALL)\n    think_match = think_pattern.search(response)\n    \n    if think_match:\n        thinking_text = think_match.group(1)\n        result[\"has_think_tags\"] = True\n        result[\"thinking_tokens\"] = len(thinking_text.split())\n        result[\"thinking_tokens_text\"] = thinking_text\n        \n        # Count thought transitions\n        position = 0\n        for phrase in THOUGHT_TRANSITIONS:\n            # Find all occurrences of each transition phrase\n            for match in re.finditer(r'\\b' + re.escape(phrase) + r'\\b', thinking_text):\n                result[\"transition_counts\"][phrase] += 1\n                # Record the approximate token position of the transition\n                token_position = len(thinking_text[:match.start()].split())\n                result[\"transition_positions\"].append((phrase, token_position))\n                \n        # Sort transition positions by token position\n        result[\"transition_positions\"].sort(key=lambda x: x[1])\n        \n        # Calculate total transitions\n        result[\"thought_transitions\"] = sum(result[\"transition_counts\"].values())\n    \n    return result\n\ndef analyze_logits_probs(logprobs_data: List[Dict]) -> Dict:\n    \"\"\"\n    Analyze token probability distributions and entropy patterns.\n    \n    Args:\n        logprobs_data: List of dictionaries containing token and logprob information\n        \n    Returns:\n        Dict: Analysis metrics including entropy statistics\n    \"\"\"\n    if not logprobs_data:\n        return {\n            \"entropy_stats\": None,\n            \"transition_entropy\": None,\n            \"token_count\": 0\n        }\n    \n    token_entropies = []\n    token_probs = []\n    token_texts = []\n    \n    # Process each token's logprobs\n    for token_info in logprobs_data:\n        if not token_info.get(\"top_logprobs\"):\n            continue\n        \n        # Extract probabilities from logprobs\n        probs = []\n        for token, logprob in token_info[\"top_logprobs\"].items():\n            probs.append(math.exp(logprob))\n        \n        # Normalize probabilities to sum to 1\n        total_prob = sum(probs)\n        if total_prob > 0:\n            probs = [p/total_prob for p in probs]\n        \n        # Calculate entropy: -sum(p_i * log(p_i))\n        entropy = -sum(p * math.log2(p) if p > 0 else 0 for p in probs)\n        token_entropies.append(entropy)\n        token_probs.append(probs[0] if probs else 0)  # Store top token probability\n        token_texts.append(token_info[\"token\"])\n    \n    # Analyze entropy changes around thought transitions\n    transition_entropy = {}\n    \n    for phrase in THOUGHT_TRANSITIONS:\n        # Find indices where this transition phrase begins\n        transition_indices = []\n        \n        # Simple approach: find where token texts match the start of the phrase\n        for i, token in enumerate(token_texts):\n            if phrase.startswith(token) and i < len(token_texts) - 1:\n                # Check if this could be the start of the transition phrase\n                # This is a simplification; more complex matching would require full tokenization\n                transition_indices.append(i)\n        \n        # Analyze entropy changes around transitions\n        if transition_indices:\n            before_entropy = []\n            after_entropy = []\n            \n            for idx in transition_indices:\n                # Look at 5 tokens before and after transition\n                before_window = max(0, idx-5)\n                after_window = min(len(token_entropies), idx+5)\n                \n                if idx > before_window:\n                    before_entropy.extend(token_entropies[before_window:idx])\n                if after_window > idx:\n                    after_entropy.extend(token_entropies[idx:after_window])\n            \n            transition_entropy[phrase] = {\n                \"before_mean\": statistics.mean(before_entropy) if before_entropy else 0,\n                \"after_mean\": statistics.mean(after_entropy) if after_entropy else 0,\n                \"count\": len(transition_indices)\n            }\n    \n    # Calculate overall entropy statistics\n    entropy_stats = {\n        \"mean\": statistics.mean(token_entropies) if token_entropies else 0,\n        \"median\": statistics.median(token_entropies) if token_entropies else 0,\n        \"max\": max(token_entropies) if token_entropies else 0,\n        \"min\": min(token_entropies) if token_entropies else 0,\n        \"std\": statistics.stdev(token_entropies) if len(token_entropies) > 1 else 0\n    }\n    \n    # Calculate entropy per quartile of generation\n    if token_entropies:\n        quartile_size = max(1, len(token_entropies) // 4)\n        entropy_stats[\"quartiles\"] = [\n            statistics.mean(token_entropies[i:i+quartile_size])\n            for i in range(0, len(token_entropies), quartile_size)\n            if i < len(token_entropies)\n        ]\n    else:\n        entropy_stats[\"quartiles\"] = []\n    \n    return {\n        \"entropy_stats\": entropy_stats,\n        \"transition_entropy\": transition_entropy,\n        \"token_count\": len(token_entropies)\n    }\n\ndef get_llm_response(problem: str, model: str, analyze_logits: bool = False) -> Union[str, List[Dict]]:\n    \"\"\"\n    Get response from the LLM for a given problem.\n    If multiple choices are returned, formats them as attempt dictionaries.\n    \n    Args:\n        problem (str): The problem text\n        model (str): The model identifier\n        analyze_logits (bool): Whether to request logprobs\n        \n    Returns:\n        Union[str, List[Dict]]: Either a string response or list of attempt dictionaries\n    \"\"\"\n    try:\n        # Add logprobs parameters if requested\n        kwargs = {}\n        if analyze_logits:\n            kwargs[\"logprobs\"] = True\n            kwargs[\"top_logprobs\"] = 3\n        \n        response = client.with_options(timeout=1000.0).chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"user\", \"content\": SYSTEM_PROMPT + problem}\n            ],\n            max_tokens=8192,\n            # extra_body={\n            #     \"decoding\": \"thinkdeeper\",\n            #     \"min_thinking_tokens\" : 0,\n            #     \"max_thinking_tokens\" : 8000,\n            #     \"max_thoughts\": 100,\n            # },\n            **kwargs\n        )\n        \n        # Save raw response if logprobs are requested\n        if analyze_logits:\n            raw_filename = f\"results/raw_responses_{model.replace('/', '_')}.json\"\n            problem_id = hash(problem) % 10000  # Simple hash to identify the problem\n            save_raw_response(raw_filename, problem_id, response.model_dump())\n        \n        # If there's more than one choice, format as attempts\n        if len(response.choices) > 1:\n            attempts = []\n            for i, choice in enumerate(response.choices):\n                response_text = choice.message.content.strip()\n                predicted_answer = extract_answer(response_text)\n                attempt_data = {\n                    \"attempt_number\": i + 1,\n                    \"response\": response_text,\n                    \"predicted_answer\": predicted_answer\n                }\n                \n                # Add logprobs if available\n                if analyze_logits and hasattr(choice.message, 'logprobs') and choice.message.logprobs:\n                    attempt_data[\"logprobs\"] = choice.message.logprobs\n                \n                attempts.append(attempt_data)\n            return attempts\n            \n        # If single choice, return as before\n        response_text = response.choices[0].message.content.strip()\n        \n        # If analyzing logits, return as a dictionary with logprobs\n        if analyze_logits and hasattr(response.choices[0].message, 'logprobs') and response.choices[0].message.logprobs:\n            return {\n                \"response\": response_text,\n                \"logprobs\": response.choices[0].message.logprobs\n            }\n        \n        # Otherwise return just the text\n        return response_text\n        \n    except Exception as e:\n        logger.error(f\"Error getting LLM response: {e}\")\n        return \"\"\n\ndef make_n_attempts(problem: str, model: str, n: int, analyze_thoughts: bool = False, analyze_logits: bool = False) -> List[Dict]:\n    \"\"\"\n    Make n attempts to solve a problem and return all responses and predictions.\n    \n    Args:\n        problem (str): The problem text\n        model (str): The model identifier\n        n (int): Number of attempts to make\n        analyze_thoughts (bool): Whether to analyze thinking patterns\n        analyze_logits (bool): Whether to analyze token probabilities\n        \n    Returns:\n        List[Dict]: List of dictionaries containing response and predicted answer for each attempt\n    \"\"\"\n    attempts = []\n    remaining_attempts = n\n    \n    while remaining_attempts > 0:\n        response = get_llm_response(problem, model, analyze_logits)\n        \n        # If response is already formatted as attempts\n        if isinstance(response, list):\n            for attempt in response:\n                if analyze_thoughts:\n                    attempt[\"thought_analysis\"] = analyze_thinking(attempt[\"response\"])\n                if analyze_logits and \"logprobs\" in attempt:\n                    attempt[\"logit_analysis\"] = analyze_logits_probs(attempt[\"logprobs\"][\"content\"])\n            attempts.extend(response)\n            remaining_attempts = n - len(attempts)\n        elif isinstance(response, dict) and \"response\" in response:\n            # Process dict response with logprobs\n            response_text = response[\"response\"]\n            predicted_answer = extract_answer(response_text)\n            attempt_data = {\n                \"attempt_number\": len(attempts) + 1,\n                \"response\": response_text,\n                \"predicted_answer\": predicted_answer\n            }\n            if analyze_thoughts:\n                attempt_data[\"thought_analysis\"] = analyze_thinking(response_text)\n            if analyze_logits and \"logprobs\" in response:\n                attempt_data[\"logit_analysis\"] = analyze_logits_probs(response[\"logprobs\"][\"content\"])\n            attempts.append(attempt_data)\n            remaining_attempts -= 1\n        else:\n            # Process simple string response\n            predicted_answer = extract_answer(response)\n            attempt_data = {\n                \"attempt_number\": len(attempts) + 1,\n                \"response\": response,\n                \"predicted_answer\": predicted_answer\n            }\n            if analyze_thoughts:\n                attempt_data[\"thought_analysis\"] = analyze_thinking(response)\n            attempts.append(attempt_data)\n            remaining_attempts -= 1\n    \n    return attempts\n\ndef evaluate_pass_at_n(attempts: List[Dict], correct_answer: int) -> Tuple[bool, Optional[int]]:\n    \"\"\"\n    Evaluate if any of the n attempts got the correct answer.\n    \n    Args:\n        attempts (List[Dict]): List of attempt results\n        correct_answer (int): The correct answer\n        \n    Returns:\n        Tuple[bool, Optional[int]]: (whether any attempt was correct, first correct attempt number)\n    \"\"\"\n    for attempt in attempts:\n        if attempt[\"predicted_answer\"] == correct_answer:\n            return True, attempt[\"attempt_number\"]\n    return False, None\n\ndef load_existing_results(filename: str) -> List[Dict]:\n    \"\"\"Load existing results from file if it exists.\"\"\"\n    try:\n        with open(filename, 'r') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        return []\n\ndef save_result(filename: str, result: Dict):\n    \"\"\"Save a single result to the results file.\"\"\"\n    results = load_existing_results(filename)\n    results.append(result)\n    with open(filename, 'w') as f:\n        json.dump(results, f, indent=2)\n\ndef get_last_processed_index(results: List[Dict]) -> int:\n    \"\"\"Get the index of the last processed problem.\"\"\"\n    if not results:\n        return -1\n    return max(int(r.get('index', -1)) for r in results)\n\ndef analyze_results(results: List[Dict], n: int, analyze_thoughts: bool = False, analyze_logits: bool = False):\n    \"\"\"\n    Analyze and print summary statistics of the results.\n    \n    Args:\n        results (List[Dict]): List of evaluation results\n        n (int): Number of attempts per problem\n        analyze_thoughts (bool): Whether to analyze thinking patterns\n        analyze_logits (bool): Whether to analyze token probabilities\n    \"\"\"\n    total = len(results)\n    correct = sum(1 for r in results if r['is_correct'])\n    accuracy = correct / total if total > 0 else 0\n    \n    print(\"\\n=== Results Summary ===\")\n    print(f\"Evaluation mode: pass@{n}\")\n    print(f\"Total problems: {total}\")\n    print(f\"Correct answers: {correct}\")\n    print(f\"Accuracy: {accuracy:.2%}\")\n    \n    # Calculate attempt statistics\n    successful_attempts = [r['first_correct_attempt'] for r in results if r['is_correct']]\n    if successful_attempts:\n        avg_attempts = sum(successful_attempts) / len(successful_attempts)\n        print(f\"\\nFor correct solutions:\")\n        print(f\"Average attempts needed: {avg_attempts:.2f}\")\n        print(f\"Attempt distribution:\")\n        for i in range(1, n + 1):\n            count = sum(1 for x in successful_attempts if x == i)\n            print(f\"  Attempt {i}: {count} problems\")\n    \n    if analyze_thoughts:\n        print(\"\\n=== Thinking Pattern Analysis ===\")\n        \n        # Collect metrics about thinking patterns for correct vs incorrect attempts\n        correct_attempts = []\n        incorrect_attempts = []\n        \n        for result in results:\n            for attempt in result['attempts']:\n                if 'thought_analysis' in attempt:\n                    if result['is_correct'] and attempt['predicted_answer'] == result['correct_answer']:\n                        correct_attempts.append(attempt)\n                    else:\n                        incorrect_attempts.append(attempt)\n        \n        # Function to calculate statistics for a group of attempts\n        def calc_stats(attempts):\n            if not attempts:\n                return {\n                    \"count\": 0,\n                    \"avg_thinking_tokens\": 0,\n                    \"avg_thought_transitions\": 0,\n                    \"transition_usage\": {phrase: 0 for phrase in THOUGHT_TRANSITIONS},\n                    \"has_think_tags_pct\": 0\n                }\n            \n            thinking_tokens = [a['thought_analysis']['thinking_tokens'] for a in attempts]\n            thought_transitions = [a['thought_analysis']['thought_transitions'] for a in attempts]\n            has_think_tags = sum(1 for a in attempts if a['thought_analysis']['has_think_tags'])\n            \n            # Count total transition usage\n            transition_usage = defaultdict(int)\n            for attempt in attempts:\n                for phrase, count in attempt['thought_analysis']['transition_counts'].items():\n                    transition_usage[phrase] += count\n            \n            return {\n                \"count\": len(attempts),\n                \"avg_thinking_tokens\": statistics.mean(thinking_tokens) if thinking_tokens else 0,\n                \"median_thinking_tokens\": statistics.median(thinking_tokens) if thinking_tokens else 0,\n                \"min_thinking_tokens\": min(thinking_tokens) if thinking_tokens else 0,\n                \"max_thinking_tokens\": max(thinking_tokens) if thinking_tokens else 0,\n                \"avg_thought_transitions\": statistics.mean(thought_transitions) if thought_transitions else 0,\n                \"median_thought_transitions\": statistics.median(thought_transitions) if thought_transitions else 0,\n                \"transition_usage\": dict(transition_usage),\n                \"has_think_tags_pct\": (has_think_tags / len(attempts)) * 100 if attempts else 0\n            }\n        \n        # Calculate statistics\n        correct_stats = calc_stats(correct_attempts)\n        incorrect_stats = calc_stats(incorrect_attempts)\n        all_stats = calc_stats(correct_attempts + incorrect_attempts)\n        \n        # Print statistics\n        print(f\"\\nOverall Thinking Statistics (All {all_stats['count']} Attempts):\")\n        print(f\"- Average thinking tokens: {all_stats['avg_thinking_tokens']:.2f}\")\n        print(f\"- Median thinking tokens: {all_stats['median_thinking_tokens']}\")\n        print(f\"- Range: {all_stats['min_thinking_tokens']} - {all_stats['max_thinking_tokens']} tokens\")\n        print(f\"- Average thought transitions: {all_stats['avg_thought_transitions']:.2f}\")\n        print(f\"- Median thought transitions: {all_stats['median_thought_transitions']}\")\n        print(f\"- Percentage with <think> tags: {all_stats['has_think_tags_pct']:.2f}%\")\n        print(f\"- Transition phrase usage:\")\n        for phrase, count in all_stats['transition_usage'].items():\n            print(f\"  - {phrase}: {count} occurrences\")\n        \n        print(f\"\\nCorrect Attempts ({correct_stats['count']}):\")\n        print(f\"- Average thinking tokens: {correct_stats['avg_thinking_tokens']:.2f}\")\n        print(f\"- Median thinking tokens: {correct_stats['median_thinking_tokens']}\")\n        print(f\"- Average thought transitions: {correct_stats['avg_thought_transitions']:.2f}\")\n        print(f\"- Median thought transitions: {correct_stats['median_thought_transitions']}\")\n        print(f\"- Percentage with <think> tags: {correct_stats['has_think_tags_pct']:.2f}%\")\n        print(f\"- Transition phrase usage:\")\n        for phrase, count in correct_stats['transition_usage'].items():\n            print(f\"  - {phrase}: {count} occurrences\")\n        \n        print(f\"\\nIncorrect Attempts ({incorrect_stats['count']}):\")\n        print(f\"- Average thinking tokens: {incorrect_stats['avg_thinking_tokens']:.2f}\")\n        print(f\"- Median thinking tokens: {incorrect_stats['median_thinking_tokens']}\")\n        print(f\"- Average thought transitions: {incorrect_stats['avg_thought_transitions']:.2f}\")\n        print(f\"- Median thought transitions: {incorrect_stats['median_thought_transitions']}\")\n        print(f\"- Percentage with <think> tags: {incorrect_stats['has_think_tags_pct']:.2f}%\")\n        print(f\"- Transition phrase usage:\")\n        for phrase, count in incorrect_stats['transition_usage'].items():\n            print(f\"  - {phrase}: {count} occurrences\")\n        \n        # Calculate correlation between thinking tokens and correctness\n        if correct_attempts and incorrect_attempts:\n            print(\"\\nCorrelation Analysis:\")\n            \n            # Find problems with both correct and incorrect attempts for comparison\n            problems_with_both = defaultdict(lambda: {\"correct\": [], \"incorrect\": []})\n            \n            for result in results:\n                problem_id = result['index']\n                for attempt in result['attempts']:\n                    if 'thought_analysis' in attempt:\n                        category = \"correct\" if attempt['predicted_answer'] == result['correct_answer'] else \"incorrect\"\n                        problems_with_both[problem_id][category].append(attempt)\n            \n            # Filter to problems that have both correct and incorrect attempts\n            valid_problems = {\n                k: v for k, v in problems_with_both.items() \n                if v[\"correct\"] and v[\"incorrect\"]\n            }\n            \n            if valid_problems:\n                print(f\"Found {len(valid_problems)} problems with both correct and incorrect attempts\")\n                \n                # For each problem, compare thinking patterns\n                avg_token_diff = []\n                avg_transition_diff = []\n                \n                for problem_id, attempts in valid_problems.items():\n                    correct_tokens = [a['thought_analysis']['thinking_tokens'] for a in attempts['correct']]\n                    incorrect_tokens = [a['thought_analysis']['thinking_tokens'] for a in attempts['incorrect']]\n                    \n                    correct_transitions = [a['thought_analysis']['thought_transitions'] for a in attempts['correct']]\n                    incorrect_transitions = [a['thought_analysis']['thought_transitions'] for a in attempts['incorrect']]\n                    \n                    avg_correct_tokens = statistics.mean(correct_tokens) if correct_tokens else 0\n                    avg_incorrect_tokens = statistics.mean(incorrect_tokens) if incorrect_tokens else 0\n                    \n                    avg_correct_transitions = statistics.mean(correct_transitions) if correct_transitions else 0\n                    avg_incorrect_transitions = statistics.mean(incorrect_transitions) if incorrect_transitions else 0\n                    \n                    avg_token_diff.append(avg_correct_tokens - avg_incorrect_tokens)\n                    avg_transition_diff.append(avg_correct_transitions - avg_incorrect_transitions)\n                \n                print(f\"Average token difference (correct - incorrect): {statistics.mean(avg_token_diff):.2f}\")\n                print(f\"Average transition difference (correct - incorrect): {statistics.mean(avg_transition_diff):.2f}\")\n    \n    if analyze_logits:\n        print(\"\\n=== Logit Analysis ===\")\n        \n        # Collect metrics about logit patterns for correct vs incorrect attempts\n        correct_attempts = []\n        incorrect_attempts = []\n        \n        for result in results:\n            for attempt in result['attempts']:\n                if 'logit_analysis' in attempt:\n                    if result['is_correct'] and attempt['predicted_answer'] == result['correct_answer']:\n                        correct_attempts.append(attempt)\n                    else:\n                        incorrect_attempts.append(attempt)\n        \n        # Function to calculate logit statistics for a group of attempts\n        def calc_logit_stats(attempts):\n            if not attempts:\n                return {\n                    \"count\": 0,\n                    \"entropy\": None,\n                    \"transitions\": None\n                }\n            \n            # Collect all entropy stats\n            entropy_means = []\n            entropy_stds = []\n            entropy_quartiles = []\n            transition_entropies = defaultdict(lambda: {\"before\": [], \"after\": []})\n            \n            for attempt in attempts:\n                if attempt['logit_analysis'].get('entropy_stats') and attempt['logit_analysis']['entropy_stats'].get('mean'):\n                    entropy_means.append(attempt['logit_analysis']['entropy_stats']['mean'])\n                    entropy_stds.append(attempt['logit_analysis']['entropy_stats']['std'])\n                    \n                    if attempt['logit_analysis']['entropy_stats'].get('quartiles'):\n                        entropy_quartiles.append(attempt['logit_analysis']['entropy_stats']['quartiles'])\n                    \n                    # Collect transition entropy data\n                    if attempt['logit_analysis'].get('transition_entropy'):\n                        for phrase, stats in attempt['logit_analysis']['transition_entropy'].items():\n                            if stats.get('before_mean') is not None:\n                                transition_entropies[phrase][\"before\"].append(stats['before_mean'])\n                            if stats.get('after_mean') is not None:\n                                transition_entropies[phrase][\"after\"].append(stats['after_mean'])\n            \n            # Calculate average entropy quartiles\n            avg_quartiles = []\n            if entropy_quartiles:\n                # Ensure all quartile lists have the same length\n                max_quartiles = max(len(q) for q in entropy_quartiles)\n                padded_quartiles = [q + [0] * (max_quartiles - len(q)) for q in entropy_quartiles]\n                \n                # Calculate average for each quartile position\n                for i in range(max_quartiles):\n                    quartile_values = [q[i] for q in padded_quartiles if i < len(q)]\n                    avg_quartiles.append(statistics.mean(quartile_values) if quartile_values else 0)\n            \n            # Calculate statistics for transitions\n            transition_stats = {}\n            for phrase, values in transition_entropies.items():\n                if values[\"before\"] and values[\"after\"]:\n                    before_mean = statistics.mean(values[\"before\"])\n                    after_mean = statistics.mean(values[\"after\"])\n                    transition_stats[phrase] = {\n                        \"before_mean\": before_mean,\n                        \"after_mean\": after_mean,\n                        \"entropy_change\": after_mean - before_mean,\n                        \"count\": len(values[\"before\"])\n                    }\n            \n            return {\n                \"count\": len(attempts),\n                \"entropy\": {\n                    \"mean\": statistics.mean(entropy_means) if entropy_means else 0,\n                    \"std\": statistics.mean(entropy_stds) if entropy_stds else 0,\n                    \"quartiles\": avg_quartiles\n                },\n                \"transitions\": transition_stats\n            }\n        \n        # Calculate statistics\n        correct_stats = calc_logit_stats(correct_attempts)\n        incorrect_stats = calc_logit_stats(incorrect_attempts)\n        all_stats = calc_logit_stats(correct_attempts + incorrect_attempts)\n        \n        # Print statistics\n        print(f\"\\nOverall Logit Statistics (All {all_stats['count']} Attempts):\")\n        if all_stats['entropy'] and all_stats['entropy']['mean']:\n            print(f\"- Average entropy: {all_stats['entropy']['mean']:.4f}\")\n            print(f\"- Average entropy std: {all_stats['entropy']['std']:.4f}\")\n            \n            if all_stats['entropy']['quartiles']:\n                print(f\"- Entropy by generation quartile:\")\n                for i, q in enumerate(all_stats['entropy']['quartiles']):\n                    print(f\"  - Q{i+1}: {q:.4f}\")\n            \n            if all_stats['transitions']:\n                print(f\"- Entropy around thought transitions:\")\n                for phrase, stats in all_stats['transitions'].items():\n                    change = stats['entropy_change']\n                    change_dir = \"increases\" if change > 0 else \"decreases\"\n                    print(f\"  - {phrase} (n={stats['count']}): Entropy {change_dir} by {abs(change):.4f}\")\n                    print(f\"    - Before: {stats['before_mean']:.4f}, After: {stats['after_mean']:.4f}\")\n        \n        if correct_stats['count'] > 0 and incorrect_stats['count'] > 0:\n            print(\"\\nEntropy Comparison (Correct vs Incorrect Attempts):\")\n            \n            if (correct_stats['entropy'] and correct_stats['entropy']['mean'] and \n                incorrect_stats['entropy'] and incorrect_stats['entropy']['mean']):\n                \n                correct_entropy = correct_stats['entropy']['mean']\n                incorrect_entropy = incorrect_stats['entropy']['mean']\n                diff = correct_entropy - incorrect_entropy\n                \n                print(f\"- Correct attempts avg entropy: {correct_entropy:.4f}\")\n                print(f\"- Incorrect attempts avg entropy: {incorrect_entropy:.4f}\")\n                print(f\"- Difference (correct - incorrect): {diff:.4f}\")\n                \n                # Compare entropy progression\n                if (correct_stats['entropy']['quartiles'] and incorrect_stats['entropy']['quartiles']):\n                    print(f\"- Entropy progression through generation:\")\n                    \n                    for i in range(min(len(correct_stats['entropy']['quartiles']), \n                                       len(incorrect_stats['entropy']['quartiles']))):\n                        c_q = correct_stats['entropy']['quartiles'][i]\n                        i_q = incorrect_stats['entropy']['quartiles'][i]\n                        q_diff = c_q - i_q\n                        \n                        print(f\"  - Q{i+1}: Correct: {c_q:.4f}, Incorrect: {i_q:.4f}, Diff: {q_diff:.4f}\")\n                \n                # Compare transitions\n                common_transitions = set(correct_stats['transitions'].keys()) & set(incorrect_stats['transitions'].keys())\n                \n                if common_transitions:\n                    print(f\"- Entropy changes around thought transitions:\")\n                    \n                    for phrase in common_transitions:\n                        c_stats = correct_stats['transitions'][phrase]\n                        i_stats = incorrect_stats['transitions'][phrase]\n                        \n                        c_change = c_stats['entropy_change']\n                        i_change = i_stats['entropy_change']\n                        \n                        print(f\"  - {phrase}:\")\n                        print(f\"    - Correct: {c_stats['before_mean']:.4f} â†’ {c_stats['after_mean']:.4f} (Î” {c_change:.4f})\")\n                        print(f\"    - Incorrect: {i_stats['before_mean']:.4f} â†’ {i_stats['after_mean']:.4f} (Î” {i_change:.4f})\")\n                        print(f\"    - Difference in entropy change: {c_change - i_change:.4f}\")\n    \n    print(\"\\n=== Incorrect Problems ===\")\n    for r in results:\n        if not r['is_correct']:\n            print(f\"Problem {r['index']}:\")\n            print(f\"Expected: {r['correct_answer']}\")\n            print(\"Predicted answers across attempts:\", [\n                attempt['predicted_answer'] for attempt in r['attempts']\n            ])\n            print(\"---\")\n\ndef save_raw_response(filename: str, problem_id: int, response_data: Dict):\n    \"\"\"Save raw response data (including logprobs) to a separate file.\"\"\"\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    \n    # Create a timestamped ID for this response\n    timestamp = int(time.time())\n    response_id = f\"{problem_id}_{timestamp}\"\n    \n    # Create or update the raw responses file\n    try:\n        with open(filename, 'r') as f:\n            raw_responses = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        raw_responses = {}\n    \n    # Add this response to the collection\n    raw_responses[response_id] = response_data\n    \n    # Save the updated collection\n    with open(filename, 'w') as f:\n        json.dump(raw_responses, f)\n    \n    return response_id\n\ndef main(model: str, n_attempts: int, analyze_thoughts: bool = False, analyze_logits: bool = False):\n    \"\"\"Main evaluation function that handles gaps in processed indexes.\"\"\"\n    os.makedirs(\"results\", exist_ok=True)\n    \n    # Create suffix based on analysis flags\n    suffix_parts = []\n    if analyze_thoughts:\n        suffix_parts.append(\"thought_analysis\")\n    if analyze_logits:\n        suffix_parts.append(\"logit_analysis\")\n    \n    suffix = \"_\" + \"_\".join(suffix_parts) if suffix_parts else \"\"\n    results_file = f\"results/evaluation_results_{model.replace('/', '_')}_pass_at_{n_attempts}{suffix}.json\"\n      \n    dataset = load_2024_dataset()\n    existing_results = load_existing_results(results_file)\n    \n    # Create a set of already processed indexes for efficient lookup\n    processed_indexes = {result['index'] for result in existing_results}\n    \n    for _, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        id = int(item['id'])\n        # Skip if this index has already been processed\n        if id in processed_indexes:\n            continue\n            \n        problem_text = item['problem']\n        correct_answer = int(item['answer'])\n        \n        # Make n attempts for each problem\n        attempts = make_n_attempts(problem_text, model, n_attempts, analyze_thoughts, analyze_logits)\n        is_correct, first_correct = evaluate_pass_at_n(attempts, correct_answer)\n        \n        result = {\n            \"index\": id,\n            \"problem\": problem_text,\n            \"attempts\": attempts,\n            \"correct_answer\": correct_answer,\n            \"is_correct\": is_correct,\n            \"first_correct_attempt\": first_correct\n        }\n        save_result(results_file, result)\n    \n    final_results = load_existing_results(results_file)\n    analyze_results(final_results, n_attempts, analyze_thoughts, analyze_logits)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate LLM performance on AIME 2024 problems\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"OpenAI model to use (e.g., gpt-4, gpt-3.5-turbo)\")\n    parser.add_argument(\"--n\", type=int, default=1, help=\"Number of attempts per problem (for pass@n evaluation)\")\n    parser.add_argument(\"--analyze-thoughts\", action=\"store_true\", help=\"Analyze thinking patterns in responses\")\n    parser.add_argument(\"--analyze-logits\", action=\"store_true\", help=\"Analyze token probability distributions\")\n    args = parser.parse_args()\n    \n    main(args.model, args.n, args.analyze_thoughts, args.analyze_logits)",
          "#!/usr/bin/env python3\nimport argparse\nimport time\nimport json\nimport os\nfrom typing import Dict, List, Any, Tuple\nimport datasets\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm import tqdm\nimport logging\nfrom datetime import datetime\nimport re\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define the approaches to test\n# Each approach is (name, description)\nAPPROACHES = [\n    (\"none\", \"Baseline without any optimization\"),\n    (\"leap\", \"LEAP Approach\"),\n    (\"rto\", \"Round Trip Optimization\"),\n    (\"cot_reflection\", \"Chain of Thought with Reflection\"),\n    (\"self_consistency\", \"Self Consistency Check\"),\n    (\"plansearch\", \"Planning with Search\"),\n    (\"re2\", \"ReRead Approach\"),\n    (\"z3\", \"Z3 Solver for Mathematical Problems\"),\n    (\"coc\", \"Chain of Code\"),\n    (\"executecode\" , \"Execute Code\"),\n    (\"spl\", \"System Prompt Learning\")\n]\n\ndef load_optillm_bench() -> datasets.Dataset:\n    \"\"\"Load the OptiLLM Bench dataset.\"\"\"\n    try:\n        dataset = load_dataset(\"codelion/optillmbench\")\n        return dataset[\"test\"]  # We use the test split for evaluation\n    except Exception as e:\n        logger.error(f\"Error loading dataset: {e}\")\n        raise\n\ndef extract_gsm8k_answer(text: str) -> float:\n    \"\"\"Extract numerical answer after ### from GSM8K responses.\"\"\"\n    match = re.search(r'###\\s*(-?\\d*\\.?\\d+)', text)\n    if match:\n        try:\n            return float(match.group(1))\n        except ValueError:\n            return None\n    return None\n\ndef remove_thinking_blocks(text: str) -> str:\n    \"\"\"\n    Remove <think>...</think> blocks from the response.\n    If there's a </think> tag, only keep the content after it.\n    \"\"\"\n    if not text:\n        return text\n        \n    # Check if there's a thinking block\n    if '</think>' in text:\n        # Get everything after the last </think> tag\n        parts = text.split('</think>')\n        return parts[-1].strip()\n    \n    # If no thinking blocks, return original text\n    return text\n\ndef extract_choice_index_from_question(question: str, answer: str) -> int:\n    \"\"\"\n    Extract the index of the correct answer from a multiple-choice question.\n    \n    Args:\n        question: The question text containing choices\n        answer: The correct answer (just the text, no index)\n    \n    Returns:\n        int: The index of the correct answer, or -1 if not found\n    \"\"\"\n    # Look for a pattern like \"N. answer\" in the question\n    answer_clean = answer.strip().lower()\n    \n    # Debug logging for critical examples\n    logger.debug(f\"Looking for answer: '{answer_clean}' in question\")\n    \n    # Check for \"Choices:\" marker in the question\n    if \"choices:\" in question.lower():\n        # Split the question by lines after \"Choices:\"\n        choices_section = question.lower().split(\"choices:\")[1].strip()\n        \n        # Log the choices section\n        logger.debug(f\"Choices section: '{choices_section}'\")\n        \n        # Try different approaches to extract choices\n        \n        # 1. If it's all on one line, use a more comprehensive regex\n        if '\\n' not in choices_section:\n            # This pattern matches \"N. text\" where N is a digit and text is any text up to the next number or end\n            all_choices = re.findall(r'(\\d+)\\s*\\.\\s*([^0-9.]+?)(?=\\s*\\d+\\s*\\.|$)', choices_section)\n            \n            logger.debug(f\"Single line choices found: {all_choices}\")\n            \n            for idx, choice_text in all_choices:\n                choice_text_clean = choice_text.strip()\n                if choice_text_clean.lower() == answer_clean:\n                    logger.debug(f\"Found match at index {idx}: '{choice_text_clean}'\")\n                    return int(idx)\n        \n        # 2. Try splitting by newlines\n        choices = choices_section.split(\"\\n\")\n        \n        for i, choice in enumerate(choices):\n            choice = choice.strip()\n            if not choice:\n                continue\n                \n            logger.debug(f\"Checking choice {i}: '{choice}'\")\n            \n            # Try to extract the index and choice text\n            match = re.match(r'\\s*(\\d+)\\s*\\.\\s*(.*)', choice)\n            if match:\n                idx = int(match.group(1))\n                choice_text = match.group(2).strip()\n                \n                logger.debug(f\"Parsed choice: index={idx}, text='{choice_text}'\")\n                \n                if choice_text.lower() == answer_clean:\n                    logger.debug(f\"Found exact match at index {idx}\")\n                    return idx\n        \n        # 3. Fallback: just look for any occurrence of the number followed by the answer\n        pattern = r'(\\d+)\\s*\\.\\s*' + re.escape(answer_clean)\n        match = re.search(pattern, choices_section)\n        if match:\n            logger.debug(f\"Fallback match found at index {match.group(1)}\")\n            return int(match.group(1))\n    \n    logger.debug(\"No match found for answer in choices\")\n    return -1\n\ndef is_numeric_only_response(response: str) -> Tuple[bool, int]:\n    \"\"\"\n    Check if the response is just a numeric value, possibly with whitespace and newlines.\n    \n    Args:\n        response: The response text to check\n        \n    Returns:\n        Tuple of (is_numeric, value)\n    \"\"\"\n    # Strip all whitespace, including newlines\n    clean_response = re.sub(r'\\s', '', response)\n    \n    # Check if it's just a number\n    if clean_response.isdigit():\n        return True, int(clean_response)\n    \n    return False, -1\n\ndef evaluate_response(response: str, ground_truth: str, category: str, question: str = None) -> bool:\n    \"\"\"\n    Evaluate if the response matches the ground truth based on category.\n    \n    Args:\n        response: Model's response\n        ground_truth: Correct answer\n        category: Problem category (gsm8k, mmlu_math, boolq, aqua_rat)\n        question: Original question text, needed for MMLU evaluation\n    \n    Returns:\n        bool: Whether the response is correct\n    \"\"\"\n    if not response or not ground_truth:\n        return False\n    \n    # First, remove any thinking blocks\n    response = remove_thinking_blocks(response)\n        \n    if category == \"gsm8k\":\n        # Extract numerical answers after ### and compare\n        response_num = extract_gsm8k_answer(response)\n        ground_truth_num = extract_gsm8k_answer(ground_truth)\n        \n        if response_num is None or ground_truth_num is None:\n            return False\n            \n        # Compare with small tolerance for floating point\n        return abs(response_num - ground_truth_num) < 1e-6\n    elif category == \"mmlu_math\":\n        # Special handling for MMLU-math multiple choice questions\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        \n        # Case 1: Exact match of answer text\n        if response_clean == ground_truth_clean:\n            logger.debug(\"Exact text match\")\n            return True\n            \n        # For other cases, we need to find what index corresponds to the ground truth\n        if question:\n            correct_index = extract_choice_index_from_question(question, ground_truth)\n            \n            if correct_index >= 0:\n                # Case 2: Check if response is just the digit (most common LLM response for indices)\n                is_numeric, value = is_numeric_only_response(response)\n                if is_numeric and value == correct_index:\n                    logger.debug(f\"Numeric match: response '{response}' -> {value} matches index {correct_index}\")\n                    return True\n                \n                # Case 3: Check if response is \"index. answer\"\n                if re.search(fr\"{correct_index}\\s*\\.\\s*{re.escape(ground_truth_clean)}\", response_clean):\n                    logger.debug(\"Pattern match for 'index. answer'\")\n                    return True\n                \n                # Case 4: Check if response contains both the index and the answer text\n                if str(correct_index) in response_clean and ground_truth_clean in response_clean:\n                    logger.debug(\"Contains both index and answer\")\n                    return True\n        \n        return False\n    else:\n        # For boolq and aqua_rat, exact match is required\n        # Clean up both strings for comparison\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        return response_clean == ground_truth_clean\n\ndef get_prompt_for_category(question: str, category: str) -> str:\n    \"\"\"\n    Generate appropriate prompt based on category.\n    \"\"\"\n    if category == \"gsm8k\":\n        return (\n            f\"Solve this math problem step by step. After solving, provide the final \"\n            f\"numerical answer after '### ' (three hash symbols and a space).\\n\\n\"\n            f\"Question: {question}\\n\\n\"\n            f\"Show your work, then give the final answer after '### '.\"\n        )\n    elif category == \"mmlu_math\":\n        return (\n            f\"Solve this math problem. Provide only the answer with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"boolq\":\n        return (\n            f\"Answer this yes/no question with only 'yes' or 'no'.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    elif category == \"aqua_rat\":\n        return (\n            f\"Choose the correct answer. Provide only the letter choice with no explanation.\\n\\n\"\n            f\"Question: {question}\"\n        )\n    else:\n        return f\"Question: {question}\"\n\ndef evaluate_model(\n    client: OpenAI,\n    model: str,\n    dataset: datasets.Dataset,\n    approach: str,\n    max_samples: int = None\n) -> Tuple[Dict[str, float], List[Dict[str, Any]]]:\n    \"\"\"\n    Evaluate a model on the dataset using a specific approach.\n    Returns metrics and detailed results.\n    \"\"\"\n    metrics = {\n        \"total_correct\": 0,\n        \"total_time\": 0,\n        \"samples\": 0,\n    }\n    \n    # Initialize category-specific metrics\n    category_metrics = {}\n    \n    # Detailed results for each example\n    detailed_results = []\n    \n    # Prepare the dataset\n    examples = dataset if max_samples is None else dataset.select(range(max_samples))\n    \n    # Create model name with approach\n    full_model_name = f\"{approach}-{model}\" if approach != \"none\" else model\n    \n    for example in tqdm(examples, desc=f\"Evaluating {approach}\"):\n        try:\n            # Get appropriate prompt for the category\n            prompt = get_prompt_for_category(example['question'], example['category'])\n            \n            # Record start time\n            start_time = time.time()\n            \n            # Make API call\n            response = client.chat.completions.create(\n                model=full_model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant focused on providing precise answers in the requested format.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.2,\n                max_tokens=4096,\n                extra_body= {\"spl_learning\": False},\n            )\n            \n            # Calculate time taken\n            time_taken = time.time() - start_time\n            \n            # Get the response text\n            response_text = response.choices[0].message.content\n            \n            # Also store the raw response for reference\n            raw_response = response_text\n            \n            # Process the response to remove thinking blocks\n            processed_response = remove_thinking_blocks(response_text)\n            \n            # Evaluate the processed response\n            is_correct = evaluate_response(\n                processed_response,\n                example['answer'],\n                example['category'],\n                example['question']  # Pass the question for MMLU evaluation\n            )\n            \n            # Update metrics\n            metrics[\"total_correct\"] += int(is_correct)\n            metrics[\"total_time\"] += time_taken\n            metrics[\"samples\"] += 1\n            \n            # Update category metrics\n            if example['category'] not in category_metrics:\n                category_metrics[example['category']] = {\n                    \"correct\": 0,\n                    \"total\": 0,\n                    \"time\": 0\n                }\n            category_metrics[example['category']][\"correct\"] += int(is_correct)\n            category_metrics[example['category']][\"total\"] += 1\n            category_metrics[example['category']][\"time\"] += time_taken\n            \n            # Check if thinking blocks were removed\n            has_thinking = '</think>' in raw_response\n            \n            # Record detailed result\n            detailed_results.append({\n                \"id\": example['id'],\n                \"category\": example['category'],\n                \"correct\": is_correct,\n                \"time_taken\": time_taken,\n                \"raw_response\": raw_response,\n                \"processed_response\": processed_response if has_thinking else None,\n                \"has_thinking\": has_thinking,\n                \"ground_truth\": example['answer']\n            })\n            \n        except Exception as e:\n            logger.error(f\"Error processing example {example['id']}: {e}\")\n            continue\n    \n    # Calculate final metrics\n    final_metrics = {\n        \"accuracy\": metrics[\"total_correct\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"average_time\": metrics[\"total_time\"] / metrics[\"samples\"] if metrics[\"samples\"] > 0 else 0,\n        \"total_time\": metrics[\"total_time\"],\n        \"total_samples\": metrics[\"samples\"],\n    }\n    \n    # Add category-specific metrics\n    for category, cat_metrics in category_metrics.items():\n        final_metrics[f\"{category}_accuracy\"] = cat_metrics[\"correct\"] / cat_metrics[\"total\"]\n        final_metrics[f\"{category}_average_time\"] = cat_metrics[\"time\"] / cat_metrics[\"total\"]\n    \n    return final_metrics, detailed_results\n\ndef save_results(metrics: Dict[str, float], detailed_results: List[Dict[str, Any]], \n                model: str, approach: str, output_dir: str):\n    \"\"\"Save evaluation results to files.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create model-specific directory\n    model_dir = os.path.join(output_dir, model.replace('/', '_'))\n    os.makedirs(model_dir, exist_ok=True)\n    \n    base_filename = os.path.join(model_dir, f\"{approach}_{timestamp}\")\n    \n    # Save metrics\n    with open(f\"{base_filename}_metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    \n    # Save detailed results\n    with open(f\"{base_filename}_detailed.json\", \"w\") as f:\n        json.dump(detailed_results, f, indent=2)\n    \n    # Create a summary DataFrame for easier analysis\n    df = pd.DataFrame([\n        {k: v for k, v in result.items() if k != 'raw_response' and k != 'processed_response'}\n        for result in detailed_results\n    ])\n    df.to_csv(f\"{base_filename}_summary.csv\", index=False)\n    \n    logger.info(f\"Results saved to {base_filename}_*\")\n\ndef generate_report(all_metrics: Dict[str, Dict[str, float]], output_dir: str):\n    \"\"\"Generate a comprehensive report comparing all approaches.\"\"\"\n    report = []\n    \n    # Header\n    report.append(\"# OptiLLM Bench Evaluation Report\")\n    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    \n    # Overall Results Table\n    report.append(\"## Overall Results\")\n    headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\", \"Total Time (s)\"]\n    rows = []\n    \n    for approach, metrics in all_metrics.items():\n        rows.append([\n            approach,\n            f\"{metrics['accuracy']*100:.2f}%\",\n            f\"{metrics['average_time']:.2f}\",\n            f\"{metrics['total_time']:.2f}\"\n        ])\n    \n    # Convert to DataFrame for nice formatting\n    df = pd.DataFrame(rows, columns=headers)\n    report.append(df.to_markdown())\n    \n    # Category-wise Results\n    report.append(\"\\n## Results by Category\")\n    categories = [\"gsm8k\", \"mmlu_math\", \"boolq\", \"aqua_rat\"]\n    \n    for category in categories:\n        report.append(f\"\\n### {category.upper()}\")\n        headers = [\"Approach\", \"Accuracy\", \"Avg Time (s)\"]\n        rows = []\n        \n        for approach, metrics in all_metrics.items():\n            if f\"{category}_accuracy\" in metrics:\n                rows.append([\n                    approach,\n                    f\"{metrics[f'{category}_accuracy']*100:.2f}%\",\n                    f\"{metrics[f'{category}_average_time']:.2f}\"\n                ])\n        \n        df = pd.DataFrame(rows, columns=headers)\n        report.append(df.to_markdown())\n    \n    # Save report\n    report_path = f\"{output_dir}/evaluation_report.md\"\n    with open(report_path, \"w\") as f:\n        f.write(\"\\n\\n\".join(report))\n    \n    logger.info(f\"Report saved to {report_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate a model on OptiLLM Bench\")\n    parser.add_argument(\"--model\", required=True, help=\"Model identifier\")\n    parser.add_argument(\"--base-url\", default=\"http://localhost:8000/v1\", \n                        help=\"Base URL for API endpoint\")\n    parser.add_argument(\"--max-samples\", type=int, help=\"Maximum number of samples to evaluate\")\n    parser.add_argument(\"--output-dir\", default=\"results\", \n                        help=\"Directory to save results\")\n    parser.add_argument(\"--approaches\", nargs=\"+\", \n                        help=\"Specific approaches to evaluate (default: all)\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    args = parser.parse_args()\n    \n    # Set debug logging if specified\n    if args.debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    # Create output directory\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Get API key from environment\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable must be set\")\n    \n    # Initialize OpenAI client\n    client = OpenAI(\n        api_key=api_key,\n        base_url=args.base_url\n    )\n    \n    # Load dataset\n    dataset = load_optillm_bench()\n    \n    # Determine which approaches to evaluate\n    approaches_to_test = (\n        [a[0] for a in APPROACHES if a[0] in args.approaches]\n        if args.approaches\n        else [a[0] for a in APPROACHES]\n    )\n    \n    # Store all metrics for final report\n    all_metrics = {}\n    \n    # Evaluate each approach\n    for approach in approaches_to_test:\n        logger.info(f\"Evaluating approach: {approach}\")\n        \n        try:\n            metrics, detailed_results = evaluate_model(\n                client,\n                args.model,\n                dataset,\n                approach,\n                args.max_samples\n            )\n            \n            all_metrics[approach] = metrics\n            \n            # Save results for this approach\n            save_results(metrics, detailed_results, args.model, approach, \n                        args.output_dir)\n            \n            logger.info(f\"Completed evaluation for {approach}\")\n            logger.info(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n            logger.info(f\"Average time per sample: {metrics['average_time']:.2f}s\")\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating approach {approach}: {e}\")\n            continue\n    \n    # Generate final report\n    generate_report(all_metrics, args.output_dir)\n\nif __name__ == \"__main__\":\n    main()",
          "import logging\n\nlogger = logging.getLogger(__name__)\n\ndef best_of_n_sampling(system_prompt: str, initial_query: str, client, model: str, n: int = 3) -> str:\n    bon_completion_tokens = 0\n\n    messages = [{\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": initial_query}]\n    \n    completions = []\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens=4096,\n        n=n,\n        temperature=1\n    )\n    completions = [choice.message.content for choice in response.choices]\n    logger.info(f\"Generated {len(completions)} initial completions. Tokens used: {response.usage.completion_tokens}\")\n    bon_completion_tokens += response.usage.completion_tokens\n    \n    # Rate the completions\n    rating_messages = messages.copy()\n    rating_messages.append({\"role\": \"system\", \"content\": \"Rate the following responses on a scale from 0 to 10, where 0 is poor and 10 is excellent. Consider factors such as relevance, coherence, and helpfulness. Respond with only a number.\"})\n    \n    ratings = []\n    for completion in completions:\n        rating_messages.append({\"role\": \"assistant\", \"content\": completion})\n        rating_messages.append({\"role\": \"user\", \"content\": \"Rate the above response:\"})\n        \n        rating_response = client.chat.completions.create(\n            model=model,\n            messages=rating_messages,\n            max_tokens=256,\n            n=1,\n            temperature=0.1\n        )\n        bon_completion_tokens += rating_response.usage.completion_tokens\n        try:\n            rating = float(rating_response.choices[0].message.content.strip())\n            ratings.append(rating)\n        except ValueError:\n            ratings.append(0)\n        \n        rating_messages = rating_messages[:-2]\n    \n    best_index = ratings.index(max(ratings))\n    return completions[best_index], bon_completion_tokens\n",
          "import logging\n\nlogger = logging.getLogger(__name__)\n\ndef mixture_of_agents(system_prompt: str, initial_query: str, client, model: str) -> str:\n    logger.info(f\"Starting mixture_of_agents function with model: {model}\")\n    moa_completion_tokens = 0\n    completions = []\n\n    logger.debug(f\"Generating initial completions for query: {initial_query}\")\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": initial_query}\n        ],\n        max_tokens=4096,\n        n=3,\n        temperature=1\n    )\n    completions = [choice.message.content for choice in response.choices]\n    moa_completion_tokens += response.usage.completion_tokens\n    logger.info(f\"Generated {len(completions)} initial completions. Tokens used: {response.usage.completion_tokens}\")\n    \n    logger.debug(\"Preparing critique prompt\")\n    critique_prompt = f\"\"\"\n    Original query: {initial_query}\n\n    I will present you with three candidate responses to the original query. Please analyze and critique each response, discussing their strengths and weaknesses. Provide your analysis for each candidate separately.\n\n    Candidate 1:\n    {completions[0]}\n\n    Candidate 2:\n    {completions[1]}\n\n    Candidate 3:\n    {completions[2]}\n\n    Please provide your critique for each candidate:\n    \"\"\"\n\n    logger.debug(\"Generating critiques\")\n    critique_response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": critique_prompt}\n        ],\n        max_tokens=512,\n        n=1,\n        temperature=0.1\n    )\n    critiques = critique_response.choices[0].message.content\n    moa_completion_tokens += critique_response.usage.completion_tokens\n    logger.info(f\"Generated critiques. Tokens used: {critique_response.usage.completion_tokens}\")\n    \n    logger.debug(\"Preparing final prompt\")\n    final_prompt = f\"\"\"\n    Original query: {initial_query}\n\n    Based on the following candidate responses and their critiques, generate a final response to the original query.\n\n    Candidate 1:\n    {completions[0]}\n\n    Candidate 2:\n    {completions[1]}\n\n    Candidate 3:\n    {completions[2]}\n\n    Critiques of all candidates:\n    {critiques}\n\n    Please provide a final, optimized response to the original query:\n    \"\"\"\n\n    logger.debug(\"Generating final response\")\n    final_response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": final_prompt}\n        ],\n        max_tokens=8192,\n        n=1,\n        temperature=0.1\n    )\n    moa_completion_tokens += final_response.usage.completion_tokens\n    logger.info(f\"Generated final response. Tokens used: {final_response.usage.completion_tokens}\")\n    \n    logger.info(f\"Total completion tokens used: {moa_completion_tokens}\")\n    return final_response.choices[0].message.content, moa_completion_tokens",
          "",
          "from importlib import util\nimport os\n\n# Version information\n__version__ = \"0.1.19\"\n\n# Get the path to the root optillm.py\nspec = util.spec_from_file_location(\n    \"optillm.root\",\n    os.path.join(os.path.dirname(os.path.dirname(__file__)), \"optillm.py\")\n)\nmodule = util.module_from_spec(spec)\nspec.loader.exec_module(module)\n\n# Export the main entry point\nmain = module.main\n\n# Export the core configuration and server components\nserver_config = module.server_config\napp = module.app\nknown_approaches = module.known_approaches\nplugin_approaches = module.plugin_approaches\n\n# Export utility functions\nparse_combined_approach = module.parse_combined_approach\nparse_conversation = module.parse_conversation\nextract_optillm_approach = module.extract_optillm_approach\nget_config = module.get_config\nload_plugins = module.load_plugins\n\n# Export execution functions\nexecute_single_approach = module.execute_single_approach\nexecute_combined_approaches = module.execute_combined_approaches\nexecute_parallel_approaches = module.execute_parallel_approaches\n\n# Export streaming response generation\ngenerate_streaming_response = module.generate_streaming_response\n\n# List of exported symbols\n__all__ = [\n    'main',\n    'server_config',\n    'app',\n    'known_approaches',\n    'plugin_approaches',\n    'parse_combined_approach',\n    'parse_conversation',\n    'extract_optillm_approach',\n    'get_config',\n    'load_plugins',\n    'execute_single_approach',\n    'execute_combined_approaches',\n    'execute_parallel_approaches',\n    'generate_streaming_response',\n]\n",
          "import os\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"optillm\",\n    version=\"0.1.19\",\n    packages=find_packages(include=['optillm', 'optillm.*']),  # This ensures all subpackages are included\n    py_modules=['optillm'],\n    package_data={\n        'optillm': [\n            'plugins/*.py',  # Include plugin files\n            'cepo/*.py',     # Include cepo module Python files\n            'cepo/configs/*.yaml',  # Include yaml files\n        ],\n    },\n    include_package_data=True,  # This is important\n    install_requires=[\n        \"numpy\",\n        \"networkx\",\n        \"openai\",\n        \"z3-solver\",\n        \"aiohttp\",\n        \"flask\",\n        \"torch\",\n        \"transformers\",\n        \"azure-identity\",\n        \"tiktoken\",\n        \"scikit-learn\",\n        \"litellm\",\n        \"requests\",\n        \"beautifulsoup4\",\n        \"lxml\",\n        \"presidio_analyzer\",\n        \"presidio_anonymizer\",\n        \"nbconvert\",\n        \"nbformat\",\n        \"ipython\",\n        \"ipykernel\",\n        \"peft\",\n        \"bitsandbytes\",\n        \"gradio<5.16.0\",\n        # Constrain spacy version to avoid blis build issues on ARM64\n        \"spacy<3.8.0\",\n        \"cerebras_cloud_sdk\",\n        \"outlines[transformers]\",\n        \"sentencepiece\",\n        \"mcp\",\n        \"adaptive-classifier\",\n        # MLX support for Apple Silicon optimization\n        'mlx-lm>=0.24.0; platform_machine==\"arm64\" and sys_platform==\"darwin\"',\n    ],\n    entry_points={\n        'console_scripts': [\n            'optillm=optillm:main',  # Points directly to the main function in optillm.py\n        ],\n    },\n    author=\"codelion\",\n    author_email=\"codelion@okyasoft.com\",\n    description=\"An optimizing inference proxy for LLMs.\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/codelion/optillm\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.10\",\n)\n"
        ],
        "test_patch": "",
        "patch_preview": "From 0016daaddd890b5832efd797ceba5224a8eddcc5 Mon Sep 17 00:00:00 2001\nFrom: Asankhaya Sharma <codelion@users.noreply.github.com>\nDate: Tue, 8 Jul 2025 20:27:24 +0800\nSubject: [PATCH 1/6] Add majority voting plugin for candidate selection\n\nIntroduces a plugin that generates multiple candidate solutions using the OpenAI API and selects the most frequent answer via majority voting. Includes answer extraction, normalization, and a summary of the voting process. Useful for tasks with discrete answer"
      },
      "patch": {
        "length": 84264,
        "files_changed": 13,
        "lines_added": 1105,
        "lines_deleted": 308,
        "net_change": 797,
        "changed_files": [
          {
            "file": "optillm/plugins/majority_voting_plugin.py",
            "added": 271,
            "deleted": 0
          },
          {
            "file": "optillm/plugins/majority_voting_plugin.py",
            "added": 33,
            "deleted": 27
          },
          {
            "file": "scripts/eval_optillmbench.py",
            "added": 58,
            "deleted": 15
          },
          {
            "file": "optillm/inference.py",
            "added": 129,
            "deleted": 133
          },
          {
            "file": "optillm/thinkdeeper_mlx.py",
            "added": 327,
            "deleted": 0
          },
          {
            "file": "scripts/eval_optillmbench.py",
            "added": 46,
            "deleted": 24
          },
          {
            "file": "scripts/eval_aime_benchmark.py",
            "added": 57,
            "deleted": 12
          },
          {
            "file": "scripts/eval_optillmbench.py",
            "added": 15,
            "deleted": 15
          },
          {
            "file": "optillm/bon.py",
            "added": 39,
            "deleted": 10
          },
          {
            "file": "optillm/moa.py",
            "added": 55,
            "deleted": 13
          },
          {
            "file": "optillm/plugins/majority_voting_plugin.py",
            "added": 73,
            "deleted": 57
          },
          {
            "file": "optillm/__init__.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "setup.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 93,
        "total_lines": 23321,
        "total_bytes": 1654031,
        "python_files": 68,
        "python_lines": 16735,
        "file_extensions": {
          ".png": 4,
          ".py": 68,
          ".json": 4,
          "": 2,
          ".txt": 3,
          ".proxy_only": 1,
          ".yaml": 2,
          ".in": 1,
          ".md": 6,
          ".svg": 2
        },
        "largest_files": [
          {
            "path": "optillm/inference.py",
            "size": 85746,
            "lines": 2008,
            "extension": ".py"
          },
          {
            "path": "optillm/plugins/spl/data/strategies.json",
            "size": 338692,
            "lines": 1153,
            "extension": ".json"
          },
          {
            "path": "optillm-sequence-diagram.png",
            "size": 171056,
            "lines": 1025,
            "extension": ".png"
          },
          {
            "path": "test_results.png",
            "size": 122668,
            "lines": 1008,
            "extension": ".png"
          },
          {
            "path": "moa-results.png",
            "size": 132575,
            "lines": 918,
            "extension": ".png"
          },
          {
            "path": "optillm/autothink/steering.py",
            "size": 41285,
            "lines": 907,
            "extension": ".py"
          },
          {
            "path": "optillm.py",
            "size": 36330,
            "lines": 881,
            "extension": ".py"
          },
          {
            "path": "scripts/eval_aime_benchmark.py",
            "size": 36561,
            "lines": 831,
            "extension": ".py"
          },
          {
            "path": "scripts/eval_math500_benchmark.py",
            "size": 29796,
            "lines": 789,
            "extension": ".py"
          },
          {
            "path": "optillm/plugins/mcp_plugin.py",
            "size": 31038,
            "lines": 756,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 93,
        "files_changed_count": 13,
        "files_changed_ratio": 0.13978494623655913,
        "total_lines_in_repo": 23321,
        "lines_added": 1105,
        "lines_deleted": 308,
        "net_lines_changed": 797,
        "lines_changed_ratio": 0.06058916856052485,
        "pr_body_length": 345,
        "commit_message_length": 76,
        "python_file_count": 68,
        "python_line_count": 16735
      }
    },
    {
      "tar_file_name": "codingforentrepreneurs#eCommerce#pull#15",
      "repo_name": "codingforentrepreneurs#eCommerce#pull#15",
      "success": true,
      "error": null,
      "commit": {
        "sha": "61fbb14480053272d52f048e99b568e6f4e3ea61",
        "message": "Removed archived requirements",
        "author": {
          "name": "Justin Mitchel",
          "email": "hello@teamcfe.com",
          "date": "2018-12-14T18:53:13Z"
        },
        "html_url": "https://github.com/codingforentrepreneurs/eCommerce/commit/61fbb14480053272d52f048e99b568e6f4e3ea61",
        "api_url": "https://api.github.com/repos/codingforentrepreneurs/eCommerce/commits/61fbb14480053272d52f048e99b568e6f4e3ea61"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/codingforentrepreneurs#eCommerce#pull#15",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/codingforentrepreneurs#eCommerce#pull#15.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/codingforentrepreneurs#eCommerce#pull#15/source_code"
      },
      "pr": {
        "number": 15,
        "title": "Bump django from 1.11.17 to 1.11.23 in /src",
        "body": "Bumps [django](https://github.com/django/django) from 1.11.17 to 1.11.23.\n<details>\n<summary>Commits</summary>\n\n- [`9748977`](https://github.com/django/django/commit/974897759e9afc4cc56fb87e12319fa9697e93c9) [1.11.x] Bumped version for 1.11.23 release.\n- [`869b34e`](https://github.com/django/django/commit/869b34e9b3be3a4cfcb3a145f218ffd3f5e3fd79) [1.11.x] Fixed CVE-2019-14235 -- Fixed potential memory exhaustion in django....\n- [`ed682a2`](https://github.com/django/django/commit/ed682a24fca774818542757651bfba576c3fc3ef) [1.11.x] Fixed CVE-2019-14234 -- Protected JSONField/HStoreField key and inde...\n- [`52479ac`](https://github.com/django/django/commit/52479acce792ad80bb0f915f20b835f919993c72) [1.11.x] Fixed CVE-2019-14233 -- Prevented excessive HTMLParser recursion in ...\n- [`42a66e9`](https://github.com/django/django/commit/42a66e969023c00536256469f0e8b8a099ef109d) [1.11.X] Fixed CVE-2019-14232 -- Adjusted regex to avoid backtracking issues ...\n- [`693046e`](https://github.com/django/django/commit/693046e54b9f207dece1907a2515ce555cec83be) [1.11.x] Added stub release notes for security releases.\n- [`6d054b5`](https://github.com/django/django/commit/6d054b5a8f8812169b74e4304291d94874c2b012) [1.11.x] Added CVE-2019-12781 to the security release archive.\n- [`7c849b9`](https://github.com/django/django/commit/7c849b9e3babdecfc441161847e5316c63b1ecac) [1.11.x] Post-release version bump.\n- [`480380c`](https://github.com/django/django/commit/480380c9935a2e920a41828b3a07bee66a686a67) [1.11.x] Bumped version for 1.11.22 release.\n- [`32124fc`](https://github.com/django/django/commit/32124fc41e75074141b05f10fc55a4f01ff7f050) [1.11.x] Fixed CVE-2019-12781 -- Made HttpRequest always trust SECURE_PROXY_S...\n- Additional commits viewable in [compare view](https://github.com/django/django/compare/1.11.17...1.11.23)\n</details>\n<br />\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=django&package-manager=pip&previous-version=1.11.17&new-version=1.11.23)](https://help.github.com/articles/configuring-automated-security-fixes)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/codingforentrepreneurs/eCommerce/network/alerts).\n\n</details>",
        "state": "closed",
        "created_at": "2019-10-21T17:37:35Z",
        "updated_at": "2020-02-11T23:26:45Z",
        "merged_at": null,
        "html_url": "https://github.com/codingforentrepreneurs/eCommerce/pull/15",
        "user": "dependabot[bot]",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "codingforentrepreneurs_eCommerce-15",
        "repo": "/codingforentrepreneurs/eCommerce",
        "base_commit": "61fbb14480053272d52f048e99b568e6f4e3ea61",
        "problem_statement": {},
        "edit_files": [
          "src/requirements.txt"
        ],
        "oracle_files": [
          "boto3==1.4.7\nboto==2.48.0\nbotocore==1.7.32\ncertifi==2017.7.27.1\ncfe==0.0.15\nchardet==3.0.4\ndj-database-url==0.4.2\ndjango-storages==1.6.5\ndjango==1.11.17\ndocutils==0.14\ngitdb2==2.0.3\ngitpython==2.1.7\ngunicorn==19.7.1\nidna==2.6\njmespath==0.9.3\nolefile==0.44\npillow==4.3.0\npip==9.0.1\npsycopg2==2.7.3.1\npython-dateutil==2.6.1\npytz==2017.2\nrequests>=2.20.0\ns3transfer==0.1.11\nsetuptools==36.6.0\nsix==1.11.0\nsmmap2==2.0.3\nstripe==1.68.0\nurllib3>=1.23\nwheel==0.30.0\n"
        ],
        "test_patch": "",
        "patch_preview": "From 5c574a7b93a8768fb46f7894793db1335f3d0c6c Mon Sep 17 00:00:00 2001\nFrom: \"dependabot[bot]\" <49699333+dependabot[bot]@users.noreply.github.com>\nDate: Mon, 21 Oct 2019 17:37:33 +0000\nSubject: [PATCH] Bump django from 1.11.17 to 1.11.23 in /src\n\nBumps [django](https://github.com/django/django) from 1.11.17 to 1.11.23.\n- [Release notes](https://github.com/django/django/releases)\n- [Commits](https://github.com/django/django/compare/1.11.17...1.11.23)\n\nSigned-off-by: dependabot[bot] <support@githu"
      },
      "patch": {
        "length": 904,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "src/requirements.txt",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [
        {
          "id": 584911527,
          "body": "Superseded by #17.",
          "user": "dependabot[bot]",
          "created_at": "2020-02-11T23:26:42Z",
          "html_url": "https://github.com/codingforentrepreneurs/eCommerce/pull/15#issuecomment-584911527"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 298,
        "total_lines": 39110,
        "total_bytes": 3050134,
        "python_files": 150,
        "python_lines": 5254,
        "file_extensions": {
          ".py": 150,
          ".sublime-project": 2,
          ".sublime-workspace": 1,
          ".md": 3,
          ".txt": 9,
          ".sqlite3": 2,
          "": 2,
          ".html": 49,
          ".json": 1,
          ".jpg": 3,
          ".js": 32,
          ".css": 12,
          ".png": 4,
          ".m4a": 6,
          ".svg": 19,
          ".woff": 3
        },
        "largest_files": [
          {
            "path": "static_cdn/static_root/admin/js/vendor/jquery/jquery.js",
            "size": 258648,
            "lines": 9842,
            "extension": ".js"
          },
          {
            "path": "static_cdn/static_root/admin/js/vendor/xregexp/xregexp.js",
            "size": 128820,
            "lines": 2308,
            "extension": ".js"
          },
          {
            "path": "src/db.sqlite3",
            "size": 352256,
            "lines": 1423,
            "extension": ".sqlite3"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio_uuyWQIO_soaLtH9.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio_5W1qjNh.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio_5W1qjNh_Ntvw9l5.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio_uuyWQIO.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "static_cdn/protected_media/product/my-awesome-album/basic_audio_DwLL00o.m4a",
            "size": 153001,
            "lines": 1085,
            "extension": ".m4a"
          },
          {
            "path": "ecommerce.sublime-workspace",
            "size": 18631,
            "lines": 1012,
            "extension": ".sublime-workspace"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 298,
        "files_changed_count": 1,
        "files_changed_ratio": 0.003355704697986577,
        "total_lines_in_repo": 39110,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 5.113781641523907e-05,
        "pr_body_length": 3960,
        "commit_message_length": 29,
        "python_file_count": 150,
        "python_line_count": 5254
      }
    },
    {
      "tar_file_name": "cosmic-byte#flask-restplus-boilerplate#pull#31",
      "repo_name": "cosmic-byte#flask-restplus-boilerplate#pull#31",
      "success": true,
      "error": null,
      "commit": {
        "sha": "5eb47f6f5d0b8fb5a8a10f710ca0b2a65b3355a6",
        "message": "Merge pull request #30 from cosmic-byte/dependabot/pip/flask-cors-3.0.9\n\nBump flask-cors from 3.0.3 to 3.0.9",
        "author": {
          "name": "Greg obinna",
          "email": "gregobinna@gmail.com",
          "date": "2021-05-20T12:03:16Z"
        },
        "html_url": "https://github.com/cosmic-byte/flask-restplus-boilerplate/commit/5eb47f6f5d0b8fb5a8a10f710ca0b2a65b3355a6",
        "api_url": "https://api.github.com/repos/cosmic-byte/flask-restplus-boilerplate/commits/5eb47f6f5d0b8fb5a8a10f710ca0b2a65b3355a6"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/cosmic-byte#flask-restplus-boilerplate#pull#31",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/cosmic-byte#flask-restplus-boilerplate#pull#31.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/cosmic-byte#flask-restplus-boilerplate#pull#31/source_code"
      },
      "pr": {
        "number": 31,
        "title": "Bump eventlet from 0.21.0 to 0.31.0",
        "body": "Bumps [eventlet](https://github.com/eventlet/eventlet) from 0.21.0 to 0.31.0.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/eventlet/eventlet/blob/master/NEWS\">eventlet's changelog</a>.</em></p>\n<blockquote>\n<h1>0.31.0</h1>\n<ul>\n<li>IMPORTANT: websocket: Limit maximum uncompressed frame length to 8MiB <a href=\"https://github.com/eventlet/eventlet/security/advisories/GHSA-9p9m-jm8w-94p2\">https://github.com/eventlet/eventlet/security/advisories/GHSA-9p9m-jm8w-94p2</a></li>\n</ul>\n<h1>0.30.3</h1>\n<ul>\n<li>wsgi: websocket ALREADY_HANDLED flag on corolocal</li>\n<li>green.ssl: Set suppress_ragged_eofs default based on SSLSocket defaults</li>\n<li>greenio: socket.connect_ex returned None instead of 0 on success</li>\n<li>Use _imp instead of deprecated imp</li>\n</ul>\n<h1>0.30.2</h1>\n<ul>\n<li>greendns: patch ssl to fix RecursionError on SSLContext.options.<strong>set</strong> <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/677\">eventlet/eventlet#677</a></li>\n</ul>\n<h1>0.30.1</h1>\n<ul>\n<li>patcher: built-in open() did not accept kwargs <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/683\">eventlet/eventlet#683</a></li>\n</ul>\n<h1>0.30.0</h1>\n<ul>\n<li>pyopenssl tsafe module was deprecated and removed in v20.0.0</li>\n<li>deprecate pyevent hub</li>\n<li>Deprecate CPython 2.7 and 3.4 support</li>\n<li>py39: Add _at_fork_reinit method to Semaphores</li>\n</ul>\n<h1>0.29.1</h1>\n<p>patcher: [py27] recursion error in pytest/python2.7 installing register_at_fork <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/660\">eventlet/eventlet#660</a>\npatcher: monkey_patch(builtins=True) failed on py3 because <code>file</code> class is gone <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/541\">eventlet/eventlet#541</a>\ndon't crash on PyPy 7.0.0 <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/pull/547\">eventlet/eventlet#547</a>\nOnly install monotonic on python2 <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/pull/583\">eventlet/eventlet#583</a></p>\n<h1>0.29.0</h1>\n<ul>\n<li>ssl: context wrapped listener fails accept() <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/651\">eventlet/eventlet#651</a></li>\n</ul>\n<h1>0.28.1</h1>\n<ul>\n<li>Sorry, Eventlet was broken on Windows for versions 0.27-0.28\npatcher: no os.register_at_fork on Windows (<a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/issues/654\">#654</a>)</li>\n<li>Clean up TypeError in <strong>del</strong></li>\n</ul>\n<h1>0.28.0</h1>\n<ul>\n<li>Always remove the right listener from the hub <a href=\"https://github-redirect.dependabot.com/eventlet/eventlet/pull/645\">eventlet/eventlet#645</a></li>\n</ul>\n<h1>0.27.0</h1>\n<ul>\n<li>patcher: Clean up threading book-keeping at fork when monkey-patched</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/f717f382d0bfb5cf084a9e69737fa6dfcb2eb5cf\"><code>f717f38</code></a> v0.31.0 release</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/1412f5e4125b4313f815778a1acb4d3336efcd07\"><code>1412f5e</code></a> websocket: Limit maximum uncompressed frame length to 8MiB</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/b0be94ef38a621ba5cb8ae3421d7367fc13ddae2\"><code>b0be94e</code></a> v0.30.3 release</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/df0bc00c3b758b0a632929a7ade2125d0a80c08a\"><code>df0bc00</code></a> wsgi: websocket ALREADY_HANDLED flag on corolocal</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/377b4fb39cc59273bd5ff461eb0388e3c3dffdb3\"><code>377b4fb</code></a> green.ssl: Set suppress_ragged_eofs default based on SSLSocket defaults</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/71b76bfc5166050dc333c72ead6b51a8933061e7\"><code>71b76bf</code></a> Security Policy</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/50441fc1563b85a275ea9937208909ade9907eb3\"><code>50441fc</code></a> greenio: socket.connect_ex returned None instead of 0 on success</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/e16fcab6019f97db2639ce970dc0cf9546114921\"><code>e16fcab</code></a> Use _imp instead of deprecated imp</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/83b14ed4bf714cf1464e23da34ac426474993c6b\"><code>83b14ed</code></a> v0.30.2 release</li>\n<li><a href=\"https://github.com/eventlet/eventlet/commit/79cf4ea6397db962e6f2b841f69c2d7e3b437d0f\"><code>79cf4ea</code></a> greendns: Patch ssl</li>\n<li>Additional commits viewable in <a href=\"https://github.com/eventlet/eventlet/compare/v0.21.0...v0.31.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=eventlet&package-manager=pip&previous-version=0.21.0&new-version=0.31.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/cosmic-byte/flask-restplus-boilerplate/network/alerts).\n\n</details>",
        "state": "closed",
        "created_at": "2021-05-08T16:40:02Z",
        "updated_at": "2021-05-20T12:05:12Z",
        "merged_at": "2021-05-20T12:05:04Z",
        "html_url": "https://github.com/cosmic-byte/flask-restplus-boilerplate/pull/31",
        "user": "dependabot[bot]",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "cosmic-byte_flask-restplus-boilerplate-31",
        "repo": "/cosmic-byte/flask-restplus-boilerplate",
        "base_commit": "5eb47f6f5d0b8fb5a8a10f710ca0b2a65b3355a6",
        "problem_statement": {},
        "edit_files": [],
        "oracle_files": [],
        "test_patch": "",
        "patch_preview": ""
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 25,
        "total_lines": 897,
        "total_bytes": 26938,
        "python_files": 22,
        "python_lines": 800,
        "file_extensions": {
          "": 1,
          ".txt": 1,
          ".md": 1,
          ".py": 22
        },
        "largest_files": [
          {
            "path": "app/test/test_auth.py",
            "size": 6996,
            "lines": 164,
            "extension": ".py"
          },
          {
            "path": "app/main/service/auth_helper.py",
            "size": 3062,
            "lines": 89,
            "extension": ".py"
          },
          {
            "path": "app/main/model/user.py",
            "size": 2381,
            "lines": 73,
            "extension": ".py"
          },
          {
            "path": "app/main/service/user_service.py",
            "size": 1552,
            "lines": 58,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 1286,
            "lines": 48,
            "extension": ".md"
          },
          {
            "path": "app/test/test_config.py",
            "size": 1409,
            "lines": 48,
            "extension": ".py"
          },
          {
            "path": "app/main/controller/user_controller.py",
            "size": 1219,
            "lines": 46,
            "extension": ".py"
          },
          {
            "path": "app/main/util/decorator.py",
            "size": 952,
            "lines": 44,
            "extension": ".py"
          },
          {
            "path": "app/main/config.py",
            "size": 1114,
            "lines": 42,
            "extension": ".py"
          },
          {
            "path": "manage.py",
            "size": 777,
            "lines": 38,
            "extension": ".py"
          }
        ]
      }
    },
    {
      "tar_file_name": "ddangelov#Top2Vec#pull#171",
      "repo_name": "ddangelov#Top2Vec#pull#171",
      "success": true,
      "error": null,
      "commit": {
        "sha": "e133bb18d5df2557050e1c82c0f1658c530293a9",
        "message": "gensim version fix",
        "author": {
          "name": "Dimo Angelov",
          "email": "dimo_angelov@hotmail.com",
          "date": "2021-04-01T13:13:00Z"
        },
        "html_url": "https://github.com/ddangelov/Top2Vec/commit/e133bb18d5df2557050e1c82c0f1658c530293a9",
        "api_url": "https://api.github.com/repos/ddangelov/Top2Vec/commits/e133bb18d5df2557050e1c82c0f1658c530293a9"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/ddangelov#Top2Vec#pull#171",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/ddangelov#Top2Vec#pull#171.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/ddangelov#Top2Vec#pull#171/source_code"
      },
      "pr": {
        "number": 171,
        "title": "added verbose logging",
        "body": "This type of changes would be useful regarding progress log.\r\n\r\nPlease do not consider this a \"ready\" to be merge, but as ideas.\r\n\r\nrelates to #169 ",
        "state": "closed",
        "created_at": "2021-05-04T15:33:44Z",
        "updated_at": "2021-06-06T13:02:03Z",
        "merged_at": null,
        "html_url": "https://github.com/ddangelov/Top2Vec/pull/171",
        "user": "behrica",
        "additions": 41,
        "deletions": 12,
        "changed_files": 1,
        "commits": 2
      },
      "swebench": {
        "instance_id": "ddangelov_Top2Vec-171",
        "repo": "/ddangelov/Top2Vec",
        "base_commit": "e133bb18d5df2557050e1c82c0f1658c530293a9",
        "problem_statement": {
          "title": "progress report during training ?",
          "body": "As the training of a doc2Vec takes very long time, is it possible to have a progress report while it is running ?\r\n\r\nI looked at the various options to Doc2Vec(..) , but did not find anything"
        },
        "edit_files": [
          "top2vec/Top2Vec.py",
          "top2vec/Top2Vec.py"
        ],
        "oracle_files": [
          "# Author: Dimo Angelov\n#\n# License: BSD 3 clause\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import strip_tags\nimport umap\nimport hdbscan\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom joblib import dump, load\nfrom sklearn.cluster import dbscan\nimport tempfile\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import normalize\nfrom scipy.special import softmax\n\ntry:\n    import hnswlib\n\n    _HAVE_HNSWLIB = True\nexcept ImportError:\n    _HAVE_HNSWLIB = False\n\ntry:\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import tensorflow_text\n\n    _HAVE_TENSORFLOW = True\nexcept ImportError:\n    _HAVE_TENSORFLOW = False\n\ntry:\n    from sentence_transformers import SentenceTransformer\n\n    _HAVE_TORCH = True\nexcept ImportError:\n    _HAVE_TORCH = False\n\nlogger = logging.getLogger('top2vec')\nlogger.setLevel(logging.WARNING)\nsh = logging.StreamHandler()\nsh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\nlogger.addHandler(sh)\n\n\ndef default_tokenizer(doc):\n    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n    return simple_preprocess(strip_tags(doc), deacc=True)\n\n\nclass Top2Vec:\n    \"\"\"\n    Top2Vec\n\n    Creates jointly embedded topic, document and word vectors.\n\n\n    Parameters\n    ----------\n    embedding_model: string\n        This will determine which model is used to generate the document and\n        word embeddings. The valid string options are:\n\n            * doc2vec\n            * universal-sentence-encoder\n            * universal-sentence-encoder-multilingual\n            * distiluse-base-multilingual-cased\n\n        For large data sets and data sets with very unique vocabulary doc2vec\n        could produce better results. This will train a doc2vec model from\n        scratch. This method is language agnostic. However multiple languages\n        will not be aligned.\n\n        Using the universal sentence encoder options will be much faster since\n        those are pre-trained and efficient models. The universal sentence\n        encoder options are suggested for smaller data sets. They are also\n        good options for large data sets that are in English or in languages\n        covered by the multilingual model. It is also suggested for data sets\n        that are multilingual.\n\n        For more information on universal-sentence-encoder visit:\n        https://tfhub.dev/google/universal-sentence-encoder/4\n\n        For more information on universal-sentence-encoder-multilingual visit:\n        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n\n        The distiluse-base-multilingual-cased pre-trained sentence transformer\n        is suggested for multilingual datasets and languages that are not\n        covered by the multilingual universal sentence encoder. The\n        transformer is significantly slower than the universal sentence\n        encoder options.\n\n        For more informati ond istiluse-base-multilingual-cased visit:\n        https://www.sbert.net/docs/pretrained_models.html\n\n    embedding_model_path: string (Optional)\n        Pre-trained embedding models will be downloaded automatically by\n        default. However they can also be uploaded from a file that is in the\n        location of embedding_model_path.\n\n        Warning: the model at embedding_model_path must match the\n        embedding_model parameter type.\n\n    documents: List of str\n        Input corpus, should be a list of strings.\n\n    min_count: int (Optional, default 50)\n        Ignores all words with total frequency lower than this. For smaller\n        corpora a smaller min_count will be necessary.\n\n    speed: string (Optional, default 'learn')\n\n        This parameter is only used when using doc2vec as embedding_model.\n\n        It will determine how fast the model takes to train. The\n        fast-learn option is the fastest and will generate the lowest quality\n        vectors. The learn option will learn better quality vectors but take\n        a longer time to train. The deep-learn option will learn the best\n        quality vectors but will take significant time to train. The valid\n        string speed options are:\n        \n            * fast-learn\n            * learn\n            * deep-learn\n\n    use_corpus_file: bool (Optional, default False)\n\n        This parameter is only used when using doc2vec as embedding_model.\n\n        Setting use_corpus_file to True can sometimes provide speedup for\n        large datasets when multiple worker threads are available. Documents\n        are still passed to the model as a list of str, the model will create\n        a temporary corpus file for training.\n\n    document_ids: List of str, int (Optional)\n        A unique value per document that will be used for referring to\n        documents in search results. If ids are not given to the model, the\n        index of each document in the original corpus will become the id.\n\n    keep_documents: bool (Optional, default True)\n        If set to False documents will only be used for training and not saved\n        as part of the model. This will reduce model size. When using search\n        functions only document ids will be returned, not the actual\n        documents.\n\n    workers: int (Optional)\n        The amount of worker threads to be used in training the model. Larger\n        amount will lead to faster training.\n    \n    tokenizer: callable (Optional, default None)\n        Override the default tokenization method. If None then\n        gensim.utils.simple_preprocess will be used.\n\n    use_embedding_model_tokenizer: bool (Optional, default False)\n        If using an embedding model other than doc2vec, use the model's\n        tokenizer for document embedding. If set to True the tokenizer, either\n        default or passed callable will be used to tokenize the text to\n        extract the vocabulary for word embedding.\n\n    umap_args: dict (Optional, default None)\n        Pass custom arguments to UMAP.\n\n    hdbscan_args: dict (Optional, default None)\n        Pass custom arguments to HDBSCAN.\n    \n    verbose: bool (Optional, default True)\n        Whether to print status data during training.\n    \"\"\"\n\n    def __init__(self,\n                 documents,\n                 min_count=50,\n                 embedding_model='doc2vec',\n                 embedding_model_path=None,\n                 speed='learn',\n                 use_corpus_file=False,\n                 document_ids=None,\n                 keep_documents=True,\n                 workers=None,\n                 tokenizer=None,\n                 use_embedding_model_tokenizer=False,\n                 umap_args=None,\n                 hdbscan_args=None,\n                 verbose=True\n                 ):\n\n        if verbose:\n            logger.setLevel(logging.DEBUG)\n            self.verbose = True\n        else:\n            logger.setLevel(logging.WARNING)\n            self.verbose = False\n\n        if tokenizer is None:\n            tokenizer = default_tokenizer\n\n        # validate documents\n        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n            raise ValueError(\"Documents need to be a list of strings\")\n        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n            raise ValueError(\"Documents need to be a list of strings\")\n        if keep_documents:\n            self.documents = np.array(documents, dtype=\"object\")\n        else:\n            self.documents = None\n\n        # validate document ids\n        if document_ids is not None:\n            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n                raise ValueError(\"Documents ids need to be a list of str or int\")\n\n            if len(documents) != len(document_ids):\n                raise ValueError(\"Document ids need to match number of documents\")\n            elif len(document_ids) != len(set(document_ids)):\n                raise ValueError(\"Document ids need to be unique\")\n\n            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n                self.doc_id_type = np.str_\n            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n                self.doc_id_type = np.int_\n            else:\n                raise ValueError(\"Document ids need to be str or int\")\n\n            self.document_ids_provided = True\n            self.document_ids = np.array(document_ids)\n            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n        else:\n            self.document_ids_provided = False\n            self.document_ids = np.array(range(0, len(documents)))\n            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n            self.doc_id_type = np.int_\n\n        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n                                       \"universal-sentence-encoder\",\n                                       \"distiluse-base-multilingual-cased\"]\n\n        self.embedding_model_path = embedding_model_path\n\n        if embedding_model == 'doc2vec':\n\n            # validate training inputs\n            if speed == \"fast-learn\":\n                hs = 0\n                negative = 5\n                epochs = 40\n            elif speed == \"learn\":\n                hs = 1\n                negative = 0\n                epochs = 40\n            elif speed == \"deep-learn\":\n                hs = 1\n                negative = 0\n                epochs = 400\n            elif speed == \"test-learn\":\n                hs = 0\n                negative = 5\n                epochs = 1\n            else:\n                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n\n            if workers is None:\n                pass\n            elif isinstance(workers, int):\n                pass\n            else:\n                raise ValueError(\"workers needs to be an int\")\n\n            doc2vec_args = {\"vector_size\": 300,\n                            \"min_count\": min_count,\n                            \"window\": 15,\n                            \"sample\": 1e-5,\n                            \"negative\": negative,\n                            \"hs\": hs,\n                            \"epochs\": epochs,\n                            \"dm\": 0,\n                            \"dbow_words\": 1}\n\n            if workers is not None:\n                doc2vec_args[\"workers\"] = workers\n\n            logger.info('Pre-processing documents for training')\n\n            if use_corpus_file:\n                processed = [' '.join(tokenizer(doc)) for doc in documents]\n                lines = \"\\n\".join(processed)\n                temp = tempfile.NamedTemporaryFile(mode='w+t')\n                temp.write(lines)\n                doc2vec_args[\"corpus_file\"] = temp.name\n\n            else:\n                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n                doc2vec_args[\"documents\"] = train_corpus\n\n            logger.info('Creating joint document/word embedding')\n            self.embedding_model = 'doc2vec'\n            self.model = Doc2Vec(**doc2vec_args)\n\n            if use_corpus_file:\n                temp.close()\n\n        elif embedding_model in acceptable_embedding_models:\n\n            self.embed = None\n            self.embedding_model = embedding_model\n\n            self._check_import_status()\n\n            logger.info('Pre-processing documents for training')\n\n            # preprocess documents\n            tokenized_corpus = [tokenizer(doc) for doc in documents]\n\n            def return_doc(doc):\n                return doc\n\n            # preprocess vocabulary\n            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n            words = vectorizer.get_feature_names()\n            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n            vocab_inds = np.where(word_counts > min_count)[0]\n\n            if len(vocab_inds) == 0:\n                raise ValueError(f\"A min_count of {min_count} results in \"\n                                 f\"all words being ignored, choose a lower value.\")\n            self.vocab = [words[ind] for ind in vocab_inds]\n\n            self._check_model_status()\n\n            logger.info('Creating joint document/word embedding')\n\n            # embed words\n            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n\n            # embed documents\n            if use_embedding_model_tokenizer:\n                self.document_vectors = self._embed_documents(documents)\n            else:\n                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n                self.document_vectors = self._embed_documents(train_corpus)\n\n        else:\n            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n\n        # create 5D embeddings of documents\n        logger.info('Creating lower dimension embedding of documents')\n\n        if umap_args is None:\n            umap_args = {'n_neighbors': 15,\n                         'n_components': 5,\n                         'metric': 'cosine'}\n\n        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n\n        # find dense areas of document vectors\n        logger.info('Finding dense areas of documents')\n\n        if hdbscan_args is None:\n            hdbscan_args = {'min_cluster_size': 15,\n                             'metric': 'euclidean',\n                             'cluster_selection_method': 'eom'}\n\n        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n\n        # calculate topic vectors from dense areas of documents\n        logger.info('Finding topics')\n\n        # create topic vectors\n        self._create_topic_vectors(cluster.labels_)\n\n        # deduplicate topics\n        self._deduplicate_topics()\n\n        # find topic words and scores\n        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n\n        # assign documents to topic\n        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n                                                                      self._get_document_vectors())\n\n        # calculate topic sizes\n        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n\n        # re-order topics\n        self._reorder_topics(hierarchy=False)\n\n        # initialize variables for hierarchical topic reduction\n        self.topic_vectors_reduced = None\n        self.doc_top_reduced = None\n        self.doc_dist_reduced = None\n        self.topic_sizes_reduced = None\n        self.topic_words_reduced = None\n        self.topic_word_scores_reduced = None\n        self.hierarchy = None\n\n        # initialize document indexing variables\n        self.document_index = None\n        self.serialized_document_index = None\n        self.documents_indexed = False\n        self.index_id2doc_id = None\n        self.doc_id2index_id = None\n\n        # initialize word indexing variables\n        self.word_index = None\n        self.serialized_word_index = None\n        self.words_indexed = False\n\n    def save(self, file):\n        \"\"\"\n        Saves the current model to the specified file.\n\n        Parameters\n        ----------\n        file: str\n            File where model will be saved.\n        \"\"\"\n\n        document_index_temp = None\n        word_index_temp = None\n\n        # do not save sentence encoders and sentence transformers\n        if self.embedding_model != \"doc2vec\":\n            self.embed = None\n\n        # serialize document index so that it can be saved\n        if self.documents_indexed:\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            self.document_index.save_index(temp.name)\n            self.serialized_document_index = temp.read()\n            temp.close()\n            document_index_temp = self.document_index\n            self.document_index = None\n\n        # serialize word index so that it can be saved\n        if self.words_indexed:\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            self.word_index.save_index(temp.name)\n            self.serialized_word_index = temp.read()\n            temp.close()\n            word_index_temp = self.word_index\n            self.word_index = None\n\n        dump(self, file)\n\n        self.document_index = document_index_temp\n        self.word_index = word_index_temp\n\n    @classmethod\n    def load(cls, file):\n        \"\"\"\n\n        Load a pre-trained model from the specified file.\n\n        Parameters\n        ----------\n        file: str\n            File where model will be loaded from.\n        \"\"\"\n\n        top2vec_model = load(file)\n\n        # load document index\n        if top2vec_model.documents_indexed:\n            if not _HAVE_HNSWLIB:\n                raise ImportError(f\"Cannot load document index.\\n\\n\"\n                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n                                  \"Alternatively try: pip install hnswlib\")\n\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            temp.write(top2vec_model.serialized_document_index)\n\n            if top2vec_model.embedding_model == 'doc2vec':\n                document_vectors = top2vec_model.model.docvecs.vectors_docs\n            else:\n                document_vectors = top2vec_model.document_vectors\n\n            top2vec_model.document_index = hnswlib.Index(space='ip',\n                                                         dim=document_vectors.shape[1])\n            top2vec_model.document_index.load_index(temp.name, max_elements=document_vectors.shape[0])\n            temp.close()\n            top2vec_model.serialized_document_index = None\n\n        # load word index\n        if top2vec_model.words_indexed:\n\n            if not _HAVE_HNSWLIB:\n                raise ImportError(f\"Cannot load word index.\\n\\n\"\n                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n                                  \"Alternatively try: pip install hnswlib\")\n\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            temp.write(top2vec_model.serialized_word_index)\n\n            if top2vec_model.embedding_model == 'doc2vec':\n                word_vectors = top2vec_model.model.wv.vectors\n            else:\n                word_vectors = top2vec_model.word_vectors\n\n            top2vec_model.word_index = hnswlib.Index(space='ip',\n                                                     dim=word_vectors.shape[1])\n            top2vec_model.word_index.load_index(temp.name, max_elements=word_vectors.shape[0])\n            temp.close()\n            top2vec_model.serialized_word_index = None\n\n        return top2vec_model\n\n    @staticmethod\n    def _l2_normalize(vectors):\n\n        if vectors.ndim == 2:\n            return normalize(vectors)\n        else:\n            return normalize(vectors.reshape(1, -1))[0]\n\n    def _embed_documents(self, train_corpus):\n\n        self._check_import_status()\n        self._check_model_status()\n\n        # embed documents\n        batch_size = 500\n        document_vectors = []\n\n        current = 0\n        batches = int(len(train_corpus) / batch_size)\n        extra = len(train_corpus) % batch_size\n\n        for ind in range(0, batches):\n            document_vectors.append(self.embed(train_corpus[current:current + batch_size]))\n            current += batch_size\n\n        if extra > 0:\n            document_vectors.append(self.embed(train_corpus[current:current + extra]))\n\n        document_vectors = self._l2_normalize(np.array(np.vstack(document_vectors)))\n\n        return document_vectors\n\n    def _set_document_vectors(self, document_vectors):\n        if self.embedding_model == 'doc2vec':\n            self.model.docvecs.vectors_docs = document_vectors\n        else:\n            self.document_vectors = document_vectors\n\n    def _get_document_vectors(self, norm=True):\n\n        if self.embedding_model == 'doc2vec':\n\n            if norm:\n                self.model.docvecs.init_sims()\n                return self.model.docvecs.vectors_docs_norm\n            else:\n                return self.model.docvecs.vectors_docs\n        else:\n            return self.document_vectors\n\n    def _index2word(self, index):\n        if self.embedding_model == 'doc2vec':\n            return self.model.wv.index2word[index]\n        else:\n            return self.vocab[index]\n\n    def _get_word_vectors(self):\n        if self.embedding_model == 'doc2vec':\n            self.model.wv.init_sims()\n            return self.model.wv.vectors_norm\n        else:\n            return self.word_vectors\n\n    def _create_topic_vectors(self, cluster_labels):\n\n        unique_labels = set(cluster_labels)\n        if -1 in unique_labels:\n            unique_labels.remove(-1)\n        self.topic_vectors = self._l2_normalize(\n            np.vstack([self._get_document_vectors(norm=False)[np.where(cluster_labels == label)[0]]\n                      .mean(axis=0) for label in unique_labels]))\n\n    def _deduplicate_topics(self):\n        core_samples, labels = dbscan(X=self.topic_vectors,\n                                      eps=0.1,\n                                      min_samples=2,\n                                      metric=\"cosine\")\n\n        duplicate_clusters = set(labels)\n\n        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n\n            # unique topics\n            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n\n            if -1 in duplicate_clusters:\n                duplicate_clusters.remove(-1)\n\n            # merge duplicate topics\n            for unique_label in duplicate_clusters:\n                unique_topics = np.vstack(\n                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n                                                       .mean(axis=0))])\n\n            self.topic_vectors = unique_topics\n\n    def _calculate_topic_sizes(self, hierarchy=False):\n        if hierarchy:\n            topic_sizes = pd.Series(self.doc_top_reduced).value_counts()\n        else:\n            topic_sizes = pd.Series(self.doc_top).value_counts()\n\n        return topic_sizes\n\n    def _reorder_topics(self, hierarchy=False):\n\n        if hierarchy:\n            self.topic_vectors_reduced = self.topic_vectors_reduced[self.topic_sizes_reduced.index]\n            self.topic_words_reduced = self.topic_words_reduced[self.topic_sizes_reduced.index]\n            self.topic_word_scores_reduced = self.topic_word_scores_reduced[self.topic_sizes_reduced.index]\n            old2new = dict(zip(self.topic_sizes_reduced.index, range(self.topic_sizes_reduced.index.shape[0])))\n            self.doc_top_reduced = np.array([old2new[i] for i in self.doc_top_reduced])\n            self.hierarchy = [self.hierarchy[i] for i in self.topic_sizes_reduced.index]\n            self.topic_sizes_reduced.reset_index(drop=True, inplace=True)\n        else:\n            self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n            self.topic_words = self.topic_words[self.topic_sizes.index]\n            self.topic_word_scores = self.topic_word_scores[self.topic_sizes.index]\n            old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n            self.doc_top = np.array([old2new[i] for i in self.doc_top])\n            self.topic_sizes.reset_index(drop=True, inplace=True)\n\n    @staticmethod\n    def _calculate_documents_topic(topic_vectors, document_vectors, dist=True):\n        batch_size = 10000\n        doc_top = []\n        if dist:\n            doc_dist = []\n\n        if document_vectors.shape[0] > batch_size:\n            current = 0\n            batches = int(document_vectors.shape[0] / batch_size)\n            extra = document_vectors.shape[0] % batch_size\n\n            for ind in range(0, batches):\n                res = np.inner(document_vectors[current:current + batch_size], topic_vectors)\n                doc_top.extend(np.argmax(res, axis=1))\n                if dist:\n                    doc_dist.extend(np.max(res, axis=1))\n                current += batch_size\n\n            if extra > 0:\n                res = np.inner(document_vectors[current:current + extra], topic_vectors)\n                doc_top.extend(np.argmax(res, axis=1))\n                if dist:\n                    doc_dist.extend(np.max(res, axis=1))\n            if dist:\n                doc_dist = np.array(doc_dist)\n        else:\n            res = np.inner(document_vectors, topic_vectors)\n            doc_top = np.argmax(res, axis=1)\n            if dist:\n                doc_dist = np.max(res, axis=1)\n\n        if dist:\n            return doc_top, doc_dist\n        else:\n            return doc_top\n\n    def _find_topic_words_and_scores(self, topic_vectors):\n        topic_words = []\n        topic_word_scores = []\n\n        res = np.inner(topic_vectors, self._get_word_vectors())\n        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n\n        for words, scores in zip(top_words, top_scores):\n            topic_words.append([self._index2word(i) for i in words[0:50]])\n            topic_word_scores.append(scores[0:50])\n\n        topic_words = np.array(topic_words)\n        topic_word_scores = np.array(topic_word_scores)\n\n        return topic_words, topic_word_scores\n\n    def _assign_documents_to_topic(self, document_vectors, hierarchy=False):\n\n        if hierarchy:\n            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors_reduced,\n                                                                        document_vectors,\n                                                                        dist=True)\n            self.doc_top_reduced = np.append(self.doc_top_reduced, doc_top_new)\n            self.doc_dist_reduced = np.append(self.doc_dist_reduced, doc_dist_new)\n\n            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n            for top in topic_sizes_new.index.tolist():\n                self.topic_sizes_reduced[top] += topic_sizes_new[top]\n            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n        else:\n            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors, document_vectors, dist=True)\n            self.doc_top = np.append(self.doc_top, doc_top_new)\n            self.doc_dist = np.append(self.doc_dist, doc_dist_new)\n\n            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n            for top in topic_sizes_new.index.tolist():\n                self.topic_sizes[top] += topic_sizes_new[top]\n            self.topic_sizes.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n\n    def _unassign_documents_from_topic(self, doc_indexes, hierarchy=False):\n        if hierarchy:\n            doc_top_remove = self.doc_top_reduced[doc_indexes]\n            self.doc_top_reduced = np.delete(self.doc_top_reduced, doc_indexes, 0)\n            self.doc_dist_reduced = np.delete(self.doc_dist_reduced, doc_indexes, 0)\n            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n            for top in topic_sizes_remove.index.tolist():\n                self.topic_sizes_reduced[top] -= topic_sizes_remove[top]\n            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n        else:\n            doc_top_remove = self.doc_top[doc_indexes]\n            self.doc_top = np.delete(self.doc_top, doc_indexes, 0)\n            self.doc_dist = np.delete(self.doc_dist, doc_indexes, 0)\n            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n            for top in topic_sizes_remove.index.tolist():\n                self.topic_sizes[top] -= topic_sizes_remove[top]\n            self.topic_sizes.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n\n    def _get_document_ids(self, doc_index):\n        return self.document_ids[doc_index]\n\n    def _get_document_indexes(self, doc_ids):\n        if self.document_ids is None:\n            return doc_ids\n        else:\n            return [self.doc_id2index[doc_id] for doc_id in doc_ids]\n\n    def _words2word_vectors(self, keywords):\n\n        return self._get_word_vectors()[[self._word2index(word) for word in keywords]]\n\n    def _word2index(self, word):\n        if self.embedding_model == 'doc2vec':\n            return self.model.wv.vocab[word].index\n        else:\n            return self.word_indexes[word]\n\n    def _get_combined_vec(self, vecs, vecs_neg):\n\n        combined_vector = np.zeros(self._get_document_vectors().shape[1], dtype=np.float64)\n        for vec in vecs:\n            combined_vector += vec\n        for vec in vecs_neg:\n            combined_vector -= vec\n        combined_vector /= (len(vecs) + len(vecs_neg))\n        combined_vector = self._l2_normalize(combined_vector)\n\n        return combined_vector\n\n    @staticmethod\n    def _search_vectors_by_vector(vectors, vector, num_res):\n        ranks = np.inner(vectors, vector)\n        indexes = np.flip(np.argsort(ranks)[-num_res:])\n        scores = np.array([ranks[res] for res in indexes])\n\n        return indexes, scores\n\n    @staticmethod\n    def _check_hnswlib_status():\n        if not _HAVE_HNSWLIB:\n            raise ImportError(f\"Indexing is not available.\\n\\n\"\n                              \"Try: pip install top2vec[indexing]\\n\\n\"\n                              \"Alternatively try: pip install hnswlib\")\n\n    def _check_document_index_status(self):\n        if self.document_index is None:\n            raise ImportError(\"There is no document index.\\n\\n\"\n                              \"Call index_document_vectors method before setting use_index=True.\")\n\n    def _check_word_index_status(self):\n        if self.word_index is None:\n            raise ImportError(\"There is no word index.\\n\\n\"\n                              \"Call index_word_vectors method before setting use_index=True.\")\n\n    def _check_import_status(self):\n        if self.embedding_model != 'distiluse-base-multilingual-cased':\n            if not _HAVE_TENSORFLOW:\n                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n        else:\n            if not _HAVE_TORCH:\n                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n                                  \"Alternatively try: pip install torch sentence_transformers\")\n\n    def _check_model_status(self):\n        if self.embed is None:\n            if self.verbose is False:\n                logger.setLevel(logging.DEBUG)\n\n            if self.embedding_model != \"distiluse-base-multilingual-cased\":\n                if self.embedding_model_path is None:\n                    logger.info(f'Downloading {self.embedding_model} model')\n                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n                    else:\n                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n                else:\n                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n                    module = self.embedding_model_path\n                self.embed = hub.load(module)\n\n            else:\n                if self.embedding_model_path is None:\n                    logger.info(f'Downloading {self.embedding_model} model')\n                    module = 'distiluse-base-multilingual-cased'\n                else:\n                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n                    module = self.embedding_model_path\n                model = SentenceTransformer(module)\n                self.embed = model.encode\n\n        if self.verbose is False:\n            logger.setLevel(logging.WARNING)\n\n    @staticmethod\n    def _less_than_zero(num, var_name):\n        if num < 0:\n            raise ValueError(f\"{var_name} cannot be less than 0.\")\n\n    def _validate_hierarchical_reduction(self):\n        if self.hierarchy is None:\n            raise ValueError(\"Hierarchical topic reduction has not been performed.\")\n\n    def _validate_hierarchical_reduction_num_topics(self, num_topics):\n        current_num_topics = len(self.topic_vectors)\n        if num_topics >= current_num_topics:\n            raise ValueError(f\"Number of topics must be less than {current_num_topics}.\")\n\n    def _validate_num_docs(self, num_docs):\n        self._less_than_zero(num_docs, \"num_docs\")\n        document_count = len(self.doc_top)\n        if num_docs > document_count:\n            raise ValueError(f\"num_docs cannot exceed the number of documents: {document_count}.\")\n\n    def _validate_num_topics(self, num_topics, reduced):\n        self._less_than_zero(num_topics, \"num_topics\")\n        if reduced:\n            topic_count = len(self.topic_vectors_reduced)\n            if num_topics > topic_count:\n                raise ValueError(f\"num_topics cannot exceed the number of reduced topics: {topic_count}.\")\n        else:\n            topic_count = len(self.topic_vectors)\n            if num_topics > topic_count:\n                raise ValueError(f\"num_topics cannot exceed the number of topics: {topic_count}.\")\n\n    def _validate_topic_num(self, topic_num, reduced):\n        self._less_than_zero(topic_num, \"topic_num\")\n\n        if reduced:\n            topic_count = len(self.topic_vectors_reduced) - 1\n            if topic_num > topic_count:\n                raise ValueError(f\"Invalid topic number: valid reduced topics numbers are 0 to {topic_count}.\")\n        else:\n            topic_count = len(self.topic_vectors) - 1\n            if topic_num > topic_count:\n                raise ValueError(f\"Invalid topic number: valid original topics numbers are 0 to {topic_count}.\")\n\n    def _validate_topic_search(self, topic_num, num_docs, reduced):\n        self._less_than_zero(num_docs, \"num_docs\")\n        if reduced:\n            if num_docs > self.topic_sizes_reduced[topic_num]:\n                raise ValueError(f\"Invalid number of documents: reduced topic {topic_num}\"\n                                 f\" only has {self.topic_sizes_reduced[topic_num]} documents.\")\n        else:\n            if num_docs > self.topic_sizes[topic_num]:\n                raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n                                 f\" only has {self.topic_sizes[topic_num]} documents.\")\n\n    def _validate_doc_ids(self, doc_ids, doc_ids_neg):\n\n        if not (isinstance(doc_ids, list) or isinstance(doc_ids, np.ndarray)):\n            raise ValueError(\"doc_ids must be a list of string or int.\")\n        if not (isinstance(doc_ids_neg, list) or isinstance(doc_ids_neg, np.ndarray)):\n            raise ValueError(\"doc_ids_neg must be a list of string or int.\")\n\n        if isinstance(doc_ids, np.ndarray):\n            doc_ids = list(doc_ids)\n        if isinstance(doc_ids_neg, np.ndarray):\n            doc_ids_neg = list(doc_ids_neg)\n\n        doc_ids_all = doc_ids + doc_ids_neg\n\n        if self.document_ids is not None:\n            for doc_id in doc_ids_all:\n                if doc_id not in self.doc_id2index:\n                    raise ValueError(f\"{doc_id} is not a valid document id.\")\n        elif min(doc_ids) < 0:\n            raise ValueError(f\"{min(doc_ids)} is not a valid document id.\")\n        elif max(doc_ids) > len(self.doc_top) - 1:\n            raise ValueError(f\"{max(doc_ids)} is not a valid document id.\")\n\n    def _validate_keywords(self, keywords, keywords_neg):\n        if not (isinstance(keywords, list) or isinstance(keywords, np.ndarray)):\n            raise ValueError(\"keywords must be a list of strings.\")\n\n        if not (isinstance(keywords_neg, list) or isinstance(keywords_neg, np.ndarray)):\n            raise ValueError(\"keywords_neg must be a list of strings.\")\n\n        keywords_lower = [keyword.lower() for keyword in keywords]\n        keywords_neg_lower = [keyword.lower() for keyword in keywords_neg]\n\n        if self.embedding_model == 'doc2vec':\n            vocab = self.model.wv.vocab\n        else:\n            vocab = self.vocab\n\n        for word in keywords_lower + keywords_neg_lower:\n            if word not in vocab:\n                raise ValueError(f\"'{word}' has not been learned by the model so it cannot be searched.\")\n\n        return keywords_lower, keywords_neg_lower\n\n    def _validate_document_ids_add_doc(self, documents, document_ids):\n        if document_ids is None:\n            raise ValueError(\"Document ids need to be provided.\")\n        if len(documents) != len(document_ids):\n            raise ValueError(\"Document ids need to match number of documents.\")\n        if len(document_ids) != len(set(document_ids)):\n            raise ValueError(\"Document ids need to be unique.\")\n\n        if len(set(document_ids).intersection(self.document_ids)) > 0:\n            raise ValueError(\"Some document ids already exist in model.\")\n\n        if self.doc_id_type == np.str_:\n            if not all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n                raise ValueError(\"Document ids need to be of type str.\")\n\n        if self.doc_id_type == np.int_:\n            if not all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n                raise ValueError(\"Document ids need to be of type int.\")\n\n    @staticmethod\n    def _validate_documents(documents):\n        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n            raise ValueError(\"Documents need to be a list of strings.\")\n\n    def _validate_vector(self, vector):\n        if not isinstance(vector, np.ndarray):\n            raise ValueError(\"Vector needs to be a numpy array.\")\n        vec_size = self._get_document_vectors().shape[1]\n        if not vector.shape[0] == vec_size:\n            raise ValueError(f\"Vector needs to be of {vec_size} dimensions.\")\n\n    def index_document_vectors(self, ef_construction=200, M=64):\n        \"\"\"\n        Creates an index of the document vectors using hnswlib. This will\n        lead to faster search times for models with a large number of\n        documents. \n\n        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n\n        Parameters\n        ----------\n        ef_construction: int (Optional default 200)\n            This parameter controls the trade-off between index construction\n            time and index accuracy. Larger values will lead to greater\n            accuracy but will take longer to construct.\n\n        M: int (Optional default 64)\n            This parameter controls the trade-off between both index size as\n            well as construction time and accuracy. Larger values will lead to\n            greater accuracy but will result in a larger index as well as\n            longer construction time.\n\n            For more information on the parameters see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n        \"\"\"\n\n        self._check_hnswlib_status()\n\n        document_vectors = self._get_document_vectors()\n        vec_dim = document_vectors.shape[1]\n        num_vecs = document_vectors.shape[0]\n\n        index_ids = list(range(0, len(self.document_ids)))\n\n        self.index_id2doc_id = dict(zip(index_ids, self.document_ids))\n        self.doc_id2index_id = dict(zip(self.document_ids, index_ids))\n\n        self.document_index = hnswlib.Index(space='ip', dim=vec_dim)\n        self.document_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n        self.document_index.add_items(document_vectors, index_ids)\n        self.documents_indexed = True\n\n    def index_word_vectors(self, ef_construction=200, M=64):\n        \"\"\"\n        Creates an index of the word vectors using hnswlib. This will\n        lead to faster search times for models with a large number of\n        words.\n\n        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n\n        Parameters\n        ----------\n        ef_construction: int (Optional default 200)\n            This parameter controls the trade-off between index construction\n            time and index accuracy. Larger values will lead to greater\n            accuracy but will take longer to construct.\n\n        M: int (Optional default 64)\n            This parameter controls the trade-off between both index size as\n            well as construction time and accuracy. Larger values will lead to\n            greater accuracy but will result in a larger index as well as\n            longer construction time.\n\n            For more information on the parameters see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n        \"\"\"\n        self._check_hnswlib_status()\n\n        word_vectors = self._get_word_vectors()\n        vec_dim = word_vectors.shape[1]\n        num_vecs = word_vectors.shape[0]\n\n        index_ids = list(range(0, num_vecs))\n\n        self.word_index = hnswlib.Index(space='ip', dim=vec_dim)\n        self.word_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n        self.word_index.add_items(word_vectors, index_ids)\n        self.words_indexed = True\n\n    def update_embedding_model_path(self, embedding_model_path):\n        \"\"\"\n        Update the path of the embedding model to be loaded. The model will\n        no longer be downloaded but loaded from the path location.\n\n        Warning: the model at embedding_model_path must match the\n        embedding_model parameter type.\n\n        Parameters\n        ----------\n        embedding_model_path: Str\n            Path to downloaded embedding model.\n\n        \"\"\"\n        self.embedding_model_path = embedding_model_path\n\n    def change_to_download_embedding_model(self):\n        \"\"\"\n        Use automatic download to load embedding model used for training.\n        Top2Vec will no longer try and load the embedding model from a file\n        if a embedding_model path was previously added.\n\n        \"\"\"\n        self.embedding_model_path = None\n\n    def get_documents_topics(self, doc_ids, reduced=False):\n        \"\"\"\n        Get document topics.\n\n        The topic of each document will be returned.\n\n        The corresponding original topics are returned unless reduced=True,\n        in which case the reduced topics will be returned.\n\n        Parameters\n        ----------\n        doc_ids: List of str, int\n            A unique value per document that is used for referring to documents\n            in search results. If ids were not given to the model, the index of\n            each document in the model is the id.\n\n        reduced: bool (Optional, default False)\n            Original topics are returned by default. If True the\n            reduced topics will be returned.\n\n        Returns\n        -------\n        topic_nums: array of int, shape(doc_ids)\n            The topic number of the document corresponding to each doc_id.\n\n        topic_score: array of float, shape(doc_ids)\n            Semantic similarity of document to topic. The cosine similarity of\n            the document and topic vector.\n\n        topics_words: array of shape(num_topics, 50)\n            For each topic the top 50 words are returned, in order\n            of semantic similarity to topic.\n\n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],          <Topic 4>\n            ['environment', 'warming', 'climate ... 'temperature']  <Topic 21>\n            ...]\n\n        word_scores: array of shape(num_topics, 50)\n            For each topic the cosine similarity scores of the\n            top 50 words to the topic are returned.\n\n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 4>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]  <Topic 21>\n            ...]\n\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n        # make sure documents exist\n        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n\n        # get document indexes from ids\n        doc_indexes = self._get_document_indexes(doc_ids)\n\n        if reduced:\n            doc_topics = self.doc_top_reduced[doc_indexes]\n            doc_dist = self.doc_dist_reduced[doc_indexes]\n            topic_words = self.topic_words_reduced[doc_topics]\n            topic_word_scores = self.topic_word_scores_reduced[doc_topics]\n        else:\n            doc_topics = self.doc_top[doc_indexes]\n            doc_dist = self.doc_dist[doc_indexes]\n            topic_words = self.topic_words[doc_topics]\n            topic_word_scores = self.topic_word_scores[doc_topics]\n\n        return doc_topics, doc_dist, topic_words, topic_word_scores\n\n    def add_documents(self, documents, doc_ids=None, tokenizer=None, use_embedding_model_tokenizer=False):\n        \"\"\"\n        Update the model with new documents.\n\n        The documents will be added to the current model without changing\n        existing document, word and topic vectors. Topic sizes will be updated.\n\n        If adding a large quantity of documents relative to the current model\n        size, or documents containing a largely new vocabulary, a new model\n        should be trained for best results.\n\n        Parameters\n        ----------\n        documents: List of str\n\n        doc_ids: List of str, int (Optional)\n            Only required when doc_ids were given to the original model.\n\n            A unique value per document that will be used for referring to\n            documents in search results.\n\n        tokenizer: callable (Optional, default None)\n            Override the default tokenization method. If None then\n            gensim.utils.simple_preprocess will be used.\n\n        use_embedding_model_tokenizer: bool (Optional, default False)\n            If using an embedding model other than doc2vec, use the model's\n            tokenizer for document embedding.\n        \"\"\"\n        # if tokenizer is not passed use default\n        if tokenizer is None:\n            tokenizer = default_tokenizer\n\n        # add documents\n        self._validate_documents(documents)\n        if self.documents is not None:\n            self.documents = np.append(self.documents, documents)\n\n        # add document ids\n        if self.document_ids_provided is True:\n            self._validate_document_ids_add_doc(documents, doc_ids)\n            doc_ids_len = len(self.document_ids)\n            self.document_ids = np.append(self.document_ids, doc_ids)\n            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n\n        elif doc_ids is None:\n            num_docs = len(documents)\n            start_id = max(self.document_ids) + 1\n            doc_ids = list(range(start_id, start_id + num_docs))\n            doc_ids_len = len(self.document_ids)\n            self.document_ids = np.append(self.document_ids, doc_ids)\n            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n        else:\n            raise ValueError(\"doc_ids cannot be used because they were not provided to model during training.\")\n\n        if self.embedding_model == \"doc2vec\":\n            docs_processed = [tokenizer(doc) for doc in documents]\n            document_vectors = np.vstack([self.model.infer_vector(doc_words=doc,\n                                                                  alpha=0.025,\n                                                                  min_alpha=0.01,\n                                                                  epochs=100) for doc in docs_processed])\n            num_docs = len(documents)\n            self.model.docvecs.count += num_docs\n            self.model.docvecs.max_rawint += num_docs\n            self.model.docvecs.vectors_docs_norm = None\n            self._set_document_vectors(np.vstack([self._get_document_vectors(norm=False), document_vectors]))\n            self.model.docvecs.init_sims()\n\n        else:\n            if use_embedding_model_tokenizer:\n                docs_training = documents\n            else:\n                docs_processed = [tokenizer(doc) for doc in documents]\n                docs_training = [' '.join(doc) for doc in docs_processed]\n            document_vectors = self._embed_documents(docs_training)\n            self._set_document_vectors(np.vstack([self._get_document_vectors(), document_vectors]))\n\n        # update index\n        if self.documents_indexed:\n            # update capacity of index\n            current_max = self.documents_index.get_max_elements()\n            updated_max = current_max + len(documents)\n            self.documents_index.resize_index(updated_max)\n\n            # update index_id and doc_ids\n            start_index_id = max(self.index_id2doc_id.keys()) + 1\n            new_index_ids = list(range(start_index_id, start_index_id + len(doc_ids)))\n            self.index_id2doc_id.update(dict(zip(new_index_ids, doc_ids)))\n            self.doc_id2index_id.update(dict(zip(doc_ids, new_index_ids)))\n            self.documents_index.add_items(document_vectors, new_index_ids)\n\n        # update topics\n        self._assign_documents_to_topic(document_vectors, hierarchy=False)\n\n        if self.hierarchy is not None:\n            self._assign_documents_to_topic(document_vectors, hierarchy=True)\n\n    def delete_documents(self, doc_ids):\n        \"\"\"\n        Delete documents from current model.\n\n        Warning: If document ids were not used in original model, deleting\n        documents will change the indexes and therefore doc_ids.\n\n        The documents will be deleted from the current model without changing\n        existing document, word and topic vectors. Topic sizes will be updated.\n\n        If deleting a large quantity of documents relative to the current model\n        size a new model should be trained for best results.\n\n        Parameters\n        ----------\n        doc_ids: List of str, int\n\n            A unique value per document that is used for referring to documents\n            in search results.\n        \"\"\"\n        # make sure documents exist\n        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n\n        # update index\n        if self.documents_indexed:\n            # delete doc_ids from index\n            index_ids = [self.doc_id2index_id(doc_id) for doc_id in doc_ids]\n            for index_id in index_ids:\n                self.document_index.mark_deleted(index_id)\n            # update index_id and doc_ids\n            for doc_id in doc_ids:\n                self.doc_id2index_id.pop(doc_id)\n            for index_id in index_ids:\n                self.index_id2doc_id.pop(index_id)\n\n        # get document indexes from ids\n        doc_indexes = self._get_document_indexes(doc_ids)\n\n        # delete documents\n        if self.documents is not None:\n            self.documents = np.delete(self.documents, doc_indexes, 0)\n\n        # delete document ids\n        if self.document_ids is not None:\n            for doc_id in doc_ids:\n                self.doc_id2index.pop(doc_id)\n            keys = list(self.doc_id2index.keys())\n            self.document_ids = np.array(keys)\n            values = list(range(0, len(self.doc_id2index.values())))\n            self.doc_id2index = dict(zip(keys, values))\n\n        # delete document vectors\n        self._set_document_vectors(np.delete(self._get_document_vectors(norm=False), doc_indexes, 0))\n\n        if self.embedding_model == 'doc2vec':\n            num_docs = len(doc_indexes)\n            self.model.docvecs.count -= num_docs\n            self.model.docvecs.max_rawint -= num_docs\n            self.model.docvecs.vectors_docs_norm = None\n            self.model.docvecs.init_sims()\n\n        # update topics\n        self._unassign_documents_from_topic(doc_indexes, hierarchy=False)\n\n        if self.hierarchy is not None:\n            self._unassign_documents_from_topic(doc_indexes, hierarchy=True)\n\n    def get_num_topics(self, reduced=False):\n        \"\"\"\n        Get number of topics.\n\n        This is the number of topics Top2Vec has found in the data by default.\n        If reduced is True, the number of reduced topics is returned.\n\n        Parameters\n        ----------\n        reduced: bool (Optional, default False)\n            The number of original topics will be returned by default. If True\n            will return the number of reduced topics, if hierarchical topic\n            reduction has been performed.\n\n        Returns\n        -------\n        num_topics: int\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            return len(self.topic_vectors_reduced)\n        else:\n            return len(self.topic_vectors)\n\n    def get_topic_sizes(self, reduced=False):\n        \"\"\"\n        Get topic sizes.\n\n        The number of documents most similar to each topic. Topics are\n        in increasing order of size.\n\n        The sizes of the original topics is returned unless reduced=True,\n        in which case the sizes of the reduced topics will be returned.\n\n        Parameters\n        ----------\n        reduced: bool (Optional, default False)\n            Original topic sizes are returned by default. If True the\n            reduced topic sizes will be returned.\n\n        Returns\n        -------\n        topic_sizes: array of int, shape(num_topics)\n            The number of documents most similar to the topic.\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n            return np.array(self.topic_sizes_reduced.values), np.array(self.topic_sizes_reduced.index)\n        else:\n            return np.array(self.topic_sizes.values), np.array(self.topic_sizes.index)\n\n    def get_topics(self, num_topics=None, reduced=False):\n        \"\"\"\n        Get topics, ordered by decreasing size. All topics are returned\n        if num_topics is not specified.\n\n        The original topics found are returned unless reduced=True,\n        in which case reduced topics will be returned.\n\n        Each topic will consist of the top 50 semantically similar words\n        to the topic. These are the 50 words closest to topic vector\n        along with cosine similarity of each word from vector. The\n        higher the score the more relevant the word is to the topic.\n\n        Parameters\n        ----------\n        num_topics: int, (Optional)\n            Number of topics to return.\n\n        reduced: bool (Optional, default False)\n            Original topics are returned by default. If True the\n            reduced topics will be returned.\n\n        Returns\n        -------\n        topics_words: array of shape(num_topics, 50)\n            For each topic the top 50 words are returned, in order\n            of semantic similarity to topic.\n            \n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],         <Topic 0>\n            ['environment', 'warming', 'climate ... 'temperature']  <Topic 1>\n            ...]\n\n        word_scores: array of shape(num_topics, 50)\n            For each topic the cosine similarity scores of the\n            top 50 words to the topic are returned.\n            \n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 0>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]   <Topic 1>\n            ...]\n\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n            if num_topics is None:\n                num_topics = len(self.topic_vectors_reduced)\n            else:\n                self._validate_num_topics(num_topics, reduced)\n\n            return self.topic_words_reduced[0:num_topics], self.topic_word_scores_reduced[0:num_topics], np.array(\n                range(0, num_topics))\n        else:\n\n            if num_topics is None:\n                num_topics = len(self.topic_vectors)\n            else:\n                self._validate_num_topics(num_topics, reduced)\n\n            return self.topic_words[0:num_topics], self.topic_word_scores[0:num_topics], np.array(range(0, num_topics))\n\n    def get_topic_hierarchy(self):\n        \"\"\"\n        Get the hierarchy of reduced topics. The mapping of each original topic\n        to the reduced topics is returned.\n\n        Hierarchical topic reduction must be performed before calling this\n        method.\n\n        Returns\n        -------\n        hierarchy: list of ints\n            Each index of the hierarchy corresponds to the topic number of a\n            reduced topic. For each reduced topic the topic numbers of the\n            original topics that were merged to create it are listed.\n\n            Example:\n            [[3]  <Reduced Topic 0> contains original Topic 3\n            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n            ...]\n        \"\"\"\n\n        self._validate_hierarchical_reduction()\n\n        return self.hierarchy\n\n    def hierarchical_topic_reduction(self, num_topics):\n        \"\"\"\n        Reduce the number of topics discovered by Top2Vec.\n\n        The most representative topics of the corpus will be found, by\n        iteratively merging each smallest topic to the most similar topic until\n        num_topics is reached.\n\n        Parameters\n        ----------\n        num_topics: int\n            The number of topics to reduce to.\n\n        Returns\n        -------\n        hierarchy: list of ints\n            Each index of hierarchy corresponds to the reduced topics, for each\n            reduced topic the indexes of the original topics that were merged\n            to create it are listed.\n\n            Example:\n            [[3]  <Reduced Topic 0> contains original Topic 3\n            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n            ...]\n        \"\"\"\n        self._validate_hierarchical_reduction_num_topics(num_topics)\n\n        num_topics_current = self.topic_vectors.shape[0]\n        top_vecs = self.topic_vectors\n        top_sizes = [self.topic_sizes[i] for i in range(0, len(self.topic_sizes))]\n        hierarchy = [[i] for i in range(self.topic_vectors.shape[0])]\n\n        count = 0\n        interval = max(int(self._get_document_vectors().shape[0] / 50000), 1)\n\n        while num_topics_current > num_topics:\n\n            # find smallest and most similar topics\n            smallest = np.argmin(top_sizes)\n            res = np.inner(top_vecs[smallest], top_vecs)\n            sims = np.flip(np.argsort(res))\n            most_sim = sims[1]\n            if most_sim == smallest:\n                most_sim = sims[0]\n\n            # calculate combined topic vector\n            top_vec_smallest = top_vecs[smallest]\n            smallest_size = top_sizes[smallest]\n\n            top_vec_most_sim = top_vecs[most_sim]\n            most_sim_size = top_sizes[most_sim]\n\n            combined_vec = self._l2_normalize(((top_vec_smallest * smallest_size) +\n                                               (top_vec_most_sim * most_sim_size)) / (smallest_size + most_sim_size))\n\n            # update topic vectors\n            ix_keep = list(range(len(top_vecs)))\n            ix_keep.remove(smallest)\n            ix_keep.remove(most_sim)\n            top_vecs = top_vecs[ix_keep]\n            top_vecs = np.vstack([top_vecs, combined_vec])\n            num_topics_current = top_vecs.shape[0]\n\n            # update topics sizes\n            if count % interval == 0:\n                doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n                                                          document_vectors=self._get_document_vectors(),\n                                                          dist=False)\n                topic_sizes = pd.Series(doc_top).value_counts()\n                top_sizes = [topic_sizes[i] for i in range(0, len(topic_sizes))]\n\n            else:\n                smallest_size = top_sizes.pop(smallest)\n                if most_sim < smallest:\n                    most_sim_size = top_sizes.pop(most_sim)\n                else:\n                    most_sim_size = top_sizes.pop(most_sim - 1)\n                combined_size = smallest_size + most_sim_size\n                top_sizes.append(combined_size)\n\n            count += 1\n\n            # update topic hierarchy\n            smallest_inds = hierarchy.pop(smallest)\n            if most_sim < smallest:\n                most_sim_inds = hierarchy.pop(most_sim)\n            else:\n                most_sim_inds = hierarchy.pop(most_sim - 1)\n\n            combined_inds = smallest_inds + most_sim_inds\n            hierarchy.append(combined_inds)\n\n        # re-calculate topic vectors from clusters\n        doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n                                                  document_vectors=self._get_document_vectors(),\n                                                  dist=False)\n        self.topic_vectors_reduced = self._l2_normalize(np.vstack([self._get_document_vectors()\n                                                                   [np.where(doc_top == label)[0]]\n                                                                  .mean(axis=0) for label in set(doc_top)]))\n\n        self.hierarchy = hierarchy\n\n        # assign documents to topic\n        self.doc_top_reduced, self.doc_dist_reduced = self._calculate_documents_topic(self.topic_vectors_reduced,\n                                                                                      self._get_document_vectors())\n        # find topic words and scores\n        self.topic_words_reduced, self.topic_word_scores_reduced = self._find_topic_words_and_scores(\n            topic_vectors=self.topic_vectors_reduced)\n\n        # calculate topic sizes\n        self.topic_sizes_reduced = self._calculate_topic_sizes(hierarchy=True)\n\n        # re-order topics\n        self._reorder_topics(hierarchy=True)\n\n        return self.hierarchy\n\n    def search_documents_by_vector(self, vector, num_docs, return_documents=True, use_index=False, ef=None):\n        \"\"\"\n        Semantic search of documents using a vector.\n\n        These are the documents closest to the vector. Documents are\n        ordered by proximity to the vector. Successive documents in the\n        list are less semantically similar to the vector.\n\n        Parameters\n        ----------\n        vector: array of shape(vector dimension, 1)\n            The vector dimension should be the same as the vectors in\n            the topic_vectors variable. (i.e. model.topic_vectors.shape[1])\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to vector. The cosine similarity of\n            the document and vector.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n        self._validate_vector(vector)\n        self._validate_num_docs(num_docs)\n\n        if use_index:\n            self._check_document_index_status()\n\n            if ef is not None:\n                self.document_index.set_ef(ef)\n            else:\n                self.document_index.set_ef(num_docs)\n\n            index_ids, doc_scores = self.document_index.knn_query(vector, k=num_docs)\n            index_ids = index_ids[0]\n            doc_ids = np.array([self.index_id2doc_id[index_id] for index_id in index_ids])\n            doc_scores = doc_scores[0]\n            doc_scores = np.array([1 - score for score in doc_scores])\n            doc_indexes = self._get_document_indexes(doc_ids)\n        else:\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     vector, num_docs)\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def search_words_by_vector(self, vector, num_words, use_index=False, ef=None):\n        \"\"\"\n        Semantic search of words using a vector.\n\n        These are the words closest to the vector. Words are ordered by\n        proximity to the vector. Successive words in the list are less\n        semantically similar to the vector.\n\n        Parameters\n        ----------\n        vector: array of shape(vector dimension, 1)\n            The vector dimension should be the same as the vectors in\n            the topic_vectors variable. (i.e. model.topic_vectors.shape[1])\n\n        num_words: int\n            Number of words to return.\n\n        use_index: bool (Optional default False)\n            If index_words method has been called, setting this to True will\n            speed up search for models with large number of words.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        words: array of str, shape(num_words)\n            The words in a list, the most similar are first.\n\n        word_scores: array of float, shape(num_words)\n            Semantic similarity of word to vector. The cosine similarity of\n            the word and vector.\n        \"\"\"\n\n        if use_index:\n            self._check_word_index_status()\n\n            if ef is not None:\n                self.word_index.set_ef(ef)\n            else:\n                self.word_index.set_ef(num_words)\n\n            word_indexes, word_scores = self.word_index.knn_query(vector, k=num_words)\n            word_indexes = word_indexes[0]\n            word_scores = word_scores[0]\n            word_scores = np.array([1 - score for score in word_scores])\n\n        else:\n            word_indexes, word_scores = self._search_vectors_by_vector(self._get_word_vectors(),\n                                                                       vector, num_words)\n\n        words = np.array([self._index2word(index) for index in word_indexes])\n\n        return words, word_scores\n\n    def search_documents_by_topic(self, topic_num, num_docs, return_documents=True, reduced=False):\n        \"\"\"\n        Get the most semantically similar documents to the topic.\n\n        These are the documents closest to the topic vector. Documents are\n        ordered by proximity to the topic vector. Successive documents in the\n        list are less semantically similar to the topic.\n\n        Parameters\n        ----------\n        topic_num: int\n            The topic number to search.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will not be returned.\n\n        reduced: bool (Optional, default False)\n            Original topics are used to search by default. If True the\n            reduced topics will be used.\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to topic. The cosine similarity of\n            the document and topic vector.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            self._validate_topic_num(topic_num, reduced)\n            self._validate_topic_search(topic_num, num_docs, reduced)\n\n            topic_document_indexes = np.where(self.doc_top_reduced == topic_num)[0]\n            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist_reduced[topic_document_indexes]))\n            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n            doc_scores = self.doc_dist_reduced[doc_indexes]\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        else:\n\n            self._validate_topic_num(topic_num, reduced)\n            self._validate_topic_search(topic_num, num_docs, reduced)\n\n            topic_document_indexes = np.where(self.doc_top == topic_num)[0]\n            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist[topic_document_indexes]))\n            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n            doc_scores = self.doc_dist[doc_indexes]\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def search_documents_by_keywords(self, keywords, num_docs, keywords_neg=None, return_documents=True,\n                                     use_index=False, ef=None):\n        \"\"\"\n        Semantic search of documents using keywords.\n\n        The most semantically similar documents to the combination of the\n        keywords will be returned. If negative keywords are provided, the\n        documents will be semantically dissimilar to those words. Too many\n        keywords or certain combinations of words may give strange results.\n        This method finds an average vector(negative keywords are subtracted)\n        of all the keyword vectors and returns the documents closest to the\n        resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar documents.\n\n        keywords_neg: List of str (Optional)\n            List of negative keywords being used for search of semantically\n            dissimilar documents.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will also not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to keywords. The cosine similarity\n            of the document and average of keyword vectors.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n\n        if keywords_neg is None:\n            keywords_neg = []\n\n        self._validate_num_docs(num_docs)\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n\n        if use_index:\n            self._check_document_index_status()\n            combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n            return self.search_documents_by_vector(combined_vector, num_docs, return_documents=return_documents,\n                                                   use_index=True, ef=ef)\n\n        if self.embedding_model == 'doc2vec':\n            sim_docs = self.model.docvecs.most_similar(positive=word_vecs,\n                                                       negative=neg_word_vecs,\n                                                       topn=num_docs)\n            doc_indexes = [doc[0] for doc in sim_docs]\n            doc_scores = np.array([doc[1] for doc in sim_docs])\n        else:\n            combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     combined_vector, num_docs)\n\n        doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def similar_words(self, keywords, num_words, keywords_neg=None, use_index=False, ef=None):\n        \"\"\"\n        Semantic similarity search of words.\n\n        The most semantically similar word to the combination of the keywords\n        will be returned. If negative keywords are provided, the words will be\n        semantically dissimilar to those words. Too many keywords or certain\n        combinations of words may give strange results. This method finds an\n        average vector(negative keywords are subtracted) of all the keyword\n        vectors and returns the words closest to the resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar words.\n\n        keywords_neg: List of str\n            List of negative keywords being used for search of semantically\n            dissimilar words.\n\n        num_words: int\n            Number of words to return.\n\n        use_index: bool (Optional default False)\n            If index_words method has been called, setting this to True will\n            speed up search for models with large number of words.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        words: array of str, shape(num_words)\n            The words in a list, the most similar are first.\n\n        word_scores: array of float, shape(num_words)\n            Semantic similarity of word to keywords. The cosine similarity of\n            the word and average of keyword vectors.\n        \"\"\"\n        if keywords_neg is None:\n            keywords_neg = []\n\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n        combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n\n        num_res = min(num_words + len(keywords) + len(keywords_neg), self._get_word_vectors().shape[0])\n\n        # if use_index:\n        words, word_scores = self.search_words_by_vector(vector=combined_vector,\n                                                         num_words=num_res,\n                                                         use_index=use_index,\n                                                         ef=ef)\n\n        res_indexes = [index for index, word in enumerate(words)\n                       if word not in list(keywords) + list(keywords_neg)][:num_words]\n        words = words[res_indexes]\n        word_scores = word_scores[res_indexes]\n\n        return words, word_scores\n\n    def search_topics(self, keywords, num_topics, keywords_neg=None, reduced=False):\n        \"\"\"\n        Semantic search of topics using keywords.\n\n        The most semantically similar topics to the combination of the keywords\n        will be returned. If negative keywords are provided, the topics will be\n        semantically dissimilar to those words. Topics will be ordered by\n        decreasing similarity to the keywords. Too many keywords or certain\n        combinations of words may give strange results. This method finds an\n        average vector(negative keywords are subtracted) of all the keyword\n        vectors and returns the topics closest to the resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar documents.\n\n        keywords_neg: (Optional) List of str\n            List of negative keywords being used for search of semantically\n            dissimilar documents.\n\n        num_topics: int\n            Number of documents to return.\n\n        reduced: bool (Optional, default False)\n            Original topics are searched by default. If True the\n            reduced topics will be searched.\n\n        Returns\n        -------\n        topics_words: array of shape (num_topics, 50)\n            For each topic the top 50 words are returned, in order of semantic\n            similarity to topic.\n            \n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],           <Topic 0>\n            ['environment', 'warming', 'climate ... 'temperature']    <Topic 1>\n            ...]\n\n        word_scores: array of shape (num_topics, 50)\n            For each topic the cosine similarity scores of the top 50 words\n            to the topic are returned.\n            \n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],     <Topic 0>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]     <Topic 1>\n            ...]\n\n        topic_scores: array of float, shape(num_topics)\n            For each topic the cosine similarity to the search keywords will be\n            returned.\n\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if keywords_neg is None:\n            keywords_neg = []\n\n        self._validate_num_topics(num_topics, reduced)\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n        combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors_reduced,\n                                                                      combined_vector, num_topics)\n            topic_words = [self.topic_words_reduced[topic] for topic in topic_nums]\n            word_scores = [self.topic_word_scores_reduced[topic] for topic in topic_nums]\n\n        else:\n            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors,\n                                                                      combined_vector, num_topics)\n            topic_words = [self.topic_words[topic] for topic in topic_nums]\n            word_scores = [self.topic_word_scores[topic] for topic in topic_nums]\n\n        return topic_words, word_scores, topic_scores, topic_nums\n\n    def search_documents_by_documents(self, doc_ids, num_docs, doc_ids_neg=None, return_documents=True,\n                                      use_index=False, ef=None):\n        \"\"\"\n        Semantic similarity search of documents.\n\n        The most semantically similar documents to the semantic combination of\n        document ids provided will be returned. If negative document ids are\n        provided, the documents will be semantically dissimilar to those\n        document ids. Documents will be ordered by decreasing similarity. This\n        method finds the closest document vectors to the provided documents\n        averaged.\n\n        Parameters\n        ----------\n        doc_ids: List of int, str\n            Unique ids of document. If ids were not given, the index of\n            document in the original corpus.\n\n        doc_ids_neg: (Optional) List of int, str\n            Unique ids of document. If ids were not given, the index of\n            document in the original corpus.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will also not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to keywords. The cosine similarity\n            of the document and average of keyword vectors.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n        if doc_ids_neg is None:\n            doc_ids_neg = []\n\n        self._validate_num_docs(num_docs)\n        self._validate_doc_ids(doc_ids, doc_ids_neg)\n\n        doc_indexes = self._get_document_indexes(doc_ids)\n        doc_indexes_neg = self._get_document_indexes(doc_ids_neg)\n\n        if use_index:\n            self._check_document_index_status()\n            document_vectors = self._get_document_vectors()\n            doc_vecs = [document_vectors[ind] for ind in doc_indexes]\n            doc_vecs_neg = [document_vectors[ind] for ind in doc_indexes_neg]\n            combined_vector = self._get_combined_vec(doc_vecs, doc_vecs_neg)\n            return self.search_documents_by_vector(combined_vector, num_docs, return_documents=return_documents,\n                                                   use_index=True, ef=ef)\n\n        if self.embedding_model == 'doc2vec':\n            sim_docs = self.model.docvecs.most_similar(positive=doc_indexes,\n                                                       negative=doc_indexes_neg,\n                                                       topn=num_docs)\n            doc_indexes = [doc[0] for doc in sim_docs]\n            doc_scores = np.array([doc[1] for doc in sim_docs])\n        else:\n            doc_vecs = [self.document_vectors[ind] for ind in doc_indexes]\n            doc_vecs_neg = [self.document_vectors[ind] for ind in doc_indexes_neg]\n            combined_vector = self._get_combined_vec(doc_vecs, doc_vecs_neg)\n\n            num_res = min(num_docs + len(doc_indexes) + len(doc_indexes_neg),\n                          self._get_document_vectors().shape[0])\n\n            # don't return documents that were searched\n            search_doc_indexes = list(doc_indexes) + list(doc_indexes_neg)\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     combined_vector, num_res)\n            res_indexes = [index for index, doc_ind in enumerate(doc_indexes)\n                           if doc_ind not in search_doc_indexes][:num_docs]\n            doc_indexes = doc_indexes[res_indexes]\n            doc_scores = doc_scores[res_indexes]\n\n        doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def generate_topic_wordcloud(self, topic_num, background_color=\"black\", reduced=False):\n        \"\"\"\n        Create a word cloud for a topic.\n\n        A word cloud will be generated and displayed. The most semantically\n        similar words to the topic will have the largest size, less similar\n        words will be smaller. The size is determined using the cosine distance\n        of the word vectors from the topic vector.\n\n        Parameters\n        ----------\n        topic_num: int\n            The topic number to search.\n\n        background_color : str (Optional, default='white')\n            Background color for the word cloud image. Suggested options are:\n                * white\n                * black\n\n        reduced: bool (Optional, default False)\n            Original topics are used by default. If True the\n            reduced topics will be used.\n\n        Returns\n        -------\n        A matplotlib plot of the word cloud with the topic number will be\n        displayed.\n\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            self._validate_topic_num(topic_num, reduced)\n            word_score_dict = dict(zip(self.topic_words_reduced[topic_num],\n                                       softmax(self.topic_word_scores_reduced[topic_num])))\n        else:\n            self._validate_topic_num(topic_num, reduced)\n            word_score_dict = dict(zip(self.topic_words[topic_num],\n                                       softmax(self.topic_word_scores[topic_num])))\n\n        plt.figure(figsize=(16, 4),\n                   dpi=200)\n        plt.axis(\"off\")\n        plt.imshow(\n            WordCloud(width=1600,\n                      height=400,\n                      background_color=background_color).generate_from_frequencies(word_score_dict))\n        plt.title(\"Topic \" + str(topic_num), loc='left', fontsize=25, pad=20)\n",
          "# Author: Dimo Angelov\n#\n# License: BSD 3 clause\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import strip_tags\nimport umap\nimport hdbscan\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom joblib import dump, load\nfrom sklearn.cluster import dbscan\nimport tempfile\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import normalize\nfrom scipy.special import softmax\n\ntry:\n    import hnswlib\n\n    _HAVE_HNSWLIB = True\nexcept ImportError:\n    _HAVE_HNSWLIB = False\n\ntry:\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import tensorflow_text\n\n    _HAVE_TENSORFLOW = True\nexcept ImportError:\n    _HAVE_TENSORFLOW = False\n\ntry:\n    from sentence_transformers import SentenceTransformer\n\n    _HAVE_TORCH = True\nexcept ImportError:\n    _HAVE_TORCH = False\n\nlogger = logging.getLogger('top2vec')\nlogger.setLevel(logging.WARNING)\nsh = logging.StreamHandler()\nsh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\nlogger.addHandler(sh)\n\n\ndef default_tokenizer(doc):\n    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n    return simple_preprocess(strip_tags(doc), deacc=True)\n\n\nclass Top2Vec:\n    \"\"\"\n    Top2Vec\n\n    Creates jointly embedded topic, document and word vectors.\n\n\n    Parameters\n    ----------\n    embedding_model: string\n        This will determine which model is used to generate the document and\n        word embeddings. The valid string options are:\n\n            * doc2vec\n            * universal-sentence-encoder\n            * universal-sentence-encoder-multilingual\n            * distiluse-base-multilingual-cased\n\n        For large data sets and data sets with very unique vocabulary doc2vec\n        could produce better results. This will train a doc2vec model from\n        scratch. This method is language agnostic. However multiple languages\n        will not be aligned.\n\n        Using the universal sentence encoder options will be much faster since\n        those are pre-trained and efficient models. The universal sentence\n        encoder options are suggested for smaller data sets. They are also\n        good options for large data sets that are in English or in languages\n        covered by the multilingual model. It is also suggested for data sets\n        that are multilingual.\n\n        For more information on universal-sentence-encoder visit:\n        https://tfhub.dev/google/universal-sentence-encoder/4\n\n        For more information on universal-sentence-encoder-multilingual visit:\n        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n\n        The distiluse-base-multilingual-cased pre-trained sentence transformer\n        is suggested for multilingual datasets and languages that are not\n        covered by the multilingual universal sentence encoder. The\n        transformer is significantly slower than the universal sentence\n        encoder options.\n\n        For more informati ond istiluse-base-multilingual-cased visit:\n        https://www.sbert.net/docs/pretrained_models.html\n\n    embedding_model_path: string (Optional)\n        Pre-trained embedding models will be downloaded automatically by\n        default. However they can also be uploaded from a file that is in the\n        location of embedding_model_path.\n\n        Warning: the model at embedding_model_path must match the\n        embedding_model parameter type.\n\n    documents: List of str\n        Input corpus, should be a list of strings.\n\n    min_count: int (Optional, default 50)\n        Ignores all words with total frequency lower than this. For smaller\n        corpora a smaller min_count will be necessary.\n\n    speed: string (Optional, default 'learn')\n\n        This parameter is only used when using doc2vec as embedding_model.\n\n        It will determine how fast the model takes to train. The\n        fast-learn option is the fastest and will generate the lowest quality\n        vectors. The learn option will learn better quality vectors but take\n        a longer time to train. The deep-learn option will learn the best\n        quality vectors but will take significant time to train. The valid\n        string speed options are:\n        \n            * fast-learn\n            * learn\n            * deep-learn\n\n    use_corpus_file: bool (Optional, default False)\n\n        This parameter is only used when using doc2vec as embedding_model.\n\n        Setting use_corpus_file to True can sometimes provide speedup for\n        large datasets when multiple worker threads are available. Documents\n        are still passed to the model as a list of str, the model will create\n        a temporary corpus file for training.\n\n    document_ids: List of str, int (Optional)\n        A unique value per document that will be used for referring to\n        documents in search results. If ids are not given to the model, the\n        index of each document in the original corpus will become the id.\n\n    keep_documents: bool (Optional, default True)\n        If set to False documents will only be used for training and not saved\n        as part of the model. This will reduce model size. When using search\n        functions only document ids will be returned, not the actual\n        documents.\n\n    workers: int (Optional)\n        The amount of worker threads to be used in training the model. Larger\n        amount will lead to faster training.\n    \n    tokenizer: callable (Optional, default None)\n        Override the default tokenization method. If None then\n        gensim.utils.simple_preprocess will be used.\n\n    use_embedding_model_tokenizer: bool (Optional, default False)\n        If using an embedding model other than doc2vec, use the model's\n        tokenizer for document embedding. If set to True the tokenizer, either\n        default or passed callable will be used to tokenize the text to\n        extract the vocabulary for word embedding.\n\n    umap_args: dict (Optional, default None)\n        Pass custom arguments to UMAP.\n\n    hdbscan_args: dict (Optional, default None)\n        Pass custom arguments to HDBSCAN.\n    \n    verbose: bool (Optional, default True)\n        Whether to print status data during training.\n    \"\"\"\n\n    def __init__(self,\n                 documents,\n                 min_count=50,\n                 embedding_model='doc2vec',\n                 embedding_model_path=None,\n                 speed='learn',\n                 use_corpus_file=False,\n                 document_ids=None,\n                 keep_documents=True,\n                 workers=None,\n                 tokenizer=None,\n                 use_embedding_model_tokenizer=False,\n                 umap_args=None,\n                 hdbscan_args=None,\n                 verbose=True\n                 ):\n\n        if verbose:\n            logger.setLevel(logging.DEBUG)\n            self.verbose = True\n        else:\n            logger.setLevel(logging.WARNING)\n            self.verbose = False\n\n        if tokenizer is None:\n            tokenizer = default_tokenizer\n\n        # validate documents\n        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n            raise ValueError(\"Documents need to be a list of strings\")\n        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n            raise ValueError(\"Documents need to be a list of strings\")\n        if keep_documents:\n            self.documents = np.array(documents, dtype=\"object\")\n        else:\n            self.documents = None\n\n        # validate document ids\n        if document_ids is not None:\n            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n                raise ValueError(\"Documents ids need to be a list of str or int\")\n\n            if len(documents) != len(document_ids):\n                raise ValueError(\"Document ids need to match number of documents\")\n            elif len(document_ids) != len(set(document_ids)):\n                raise ValueError(\"Document ids need to be unique\")\n\n            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n                self.doc_id_type = np.str_\n            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n                self.doc_id_type = np.int_\n            else:\n                raise ValueError(\"Document ids need to be str or int\")\n\n            self.document_ids_provided = True\n            self.document_ids = np.array(document_ids)\n            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n        else:\n            self.document_ids_provided = False\n            self.document_ids = np.array(range(0, len(documents)))\n            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n            self.doc_id_type = np.int_\n\n        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n                                       \"universal-sentence-encoder\",\n                                       \"distiluse-base-multilingual-cased\"]\n\n        self.embedding_model_path = embedding_model_path\n\n        if embedding_model == 'doc2vec':\n\n            # validate training inputs\n            if speed == \"fast-learn\":\n                hs = 0\n                negative = 5\n                epochs = 40\n            elif speed == \"learn\":\n                hs = 1\n                negative = 0\n                epochs = 40\n            elif speed == \"deep-learn\":\n                hs = 1\n                negative = 0\n                epochs = 400\n            elif speed == \"test-learn\":\n                hs = 0\n                negative = 5\n                epochs = 1\n            else:\n                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n\n            if workers is None:\n                pass\n            elif isinstance(workers, int):\n                pass\n            else:\n                raise ValueError(\"workers needs to be an int\")\n\n            doc2vec_args = {\"vector_size\": 300,\n                            \"min_count\": min_count,\n                            \"window\": 15,\n                            \"sample\": 1e-5,\n                            \"negative\": negative,\n                            \"hs\": hs,\n                            \"epochs\": epochs,\n                            \"dm\": 0,\n                            \"dbow_words\": 1}\n\n            if workers is not None:\n                doc2vec_args[\"workers\"] = workers\n\n            logger.info('Pre-processing documents for training')\n\n            if use_corpus_file:\n                processed = [' '.join(tokenizer(doc)) for doc in documents]\n                lines = \"\\n\".join(processed)\n                temp = tempfile.NamedTemporaryFile(mode='w+t')\n                temp.write(lines)\n                doc2vec_args[\"corpus_file\"] = temp.name\n\n            else:\n                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n                doc2vec_args[\"documents\"] = train_corpus\n\n            logger.info('Creating joint document/word embedding')\n            self.embedding_model = 'doc2vec'\n            self.model = Doc2Vec(**doc2vec_args)\n\n            if use_corpus_file:\n                temp.close()\n\n        elif embedding_model in acceptable_embedding_models:\n\n            self.embed = None\n            self.embedding_model = embedding_model\n\n            self._check_import_status()\n\n            logger.info('Pre-processing documents for training')\n\n            # preprocess documents\n            tokenized_corpus = [tokenizer(doc) for doc in documents]\n\n            def return_doc(doc):\n                return doc\n\n            # preprocess vocabulary\n            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n            words = vectorizer.get_feature_names()\n            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n            vocab_inds = np.where(word_counts > min_count)[0]\n\n            if len(vocab_inds) == 0:\n                raise ValueError(f\"A min_count of {min_count} results in \"\n                                 f\"all words being ignored, choose a lower value.\")\n            self.vocab = [words[ind] for ind in vocab_inds]\n\n            self._check_model_status()\n\n            logger.info('Creating joint document/word embedding')\n\n            # embed words\n            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n\n            # embed documents\n            if use_embedding_model_tokenizer:\n                self.document_vectors = self._embed_documents(documents)\n            else:\n                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n                self.document_vectors = self._embed_documents(train_corpus)\n\n        else:\n            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n\n        # create 5D embeddings of documents\n        logger.info('Creating lower dimension embedding of documents')\n\n        if umap_args is None:\n            umap_args = {'n_neighbors': 15,\n                         'n_components': 5,\n                         'metric': 'cosine'}\n\n        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n\n        # find dense areas of document vectors\n        logger.info('Finding dense areas of documents')\n\n        if hdbscan_args is None:\n            hdbscan_args = {'min_cluster_size': 15,\n                             'metric': 'euclidean',\n                             'cluster_selection_method': 'eom'}\n\n        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n\n        # calculate topic vectors from dense areas of documents\n        logger.info('Finding topics')\n\n        # create topic vectors\n        self._create_topic_vectors(cluster.labels_)\n\n        # deduplicate topics\n        self._deduplicate_topics()\n\n        # find topic words and scores\n        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n\n        # assign documents to topic\n        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n                                                                      self._get_document_vectors())\n\n        # calculate topic sizes\n        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n\n        # re-order topics\n        self._reorder_topics(hierarchy=False)\n\n        # initialize variables for hierarchical topic reduction\n        self.topic_vectors_reduced = None\n        self.doc_top_reduced = None\n        self.doc_dist_reduced = None\n        self.topic_sizes_reduced = None\n        self.topic_words_reduced = None\n        self.topic_word_scores_reduced = None\n        self.hierarchy = None\n\n        # initialize document indexing variables\n        self.document_index = None\n        self.serialized_document_index = None\n        self.documents_indexed = False\n        self.index_id2doc_id = None\n        self.doc_id2index_id = None\n\n        # initialize word indexing variables\n        self.word_index = None\n        self.serialized_word_index = None\n        self.words_indexed = False\n\n    def save(self, file):\n        \"\"\"\n        Saves the current model to the specified file.\n\n        Parameters\n        ----------\n        file: str\n            File where model will be saved.\n        \"\"\"\n\n        document_index_temp = None\n        word_index_temp = None\n\n        # do not save sentence encoders and sentence transformers\n        if self.embedding_model != \"doc2vec\":\n            self.embed = None\n\n        # serialize document index so that it can be saved\n        if self.documents_indexed:\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            self.document_index.save_index(temp.name)\n            self.serialized_document_index = temp.read()\n            temp.close()\n            document_index_temp = self.document_index\n            self.document_index = None\n\n        # serialize word index so that it can be saved\n        if self.words_indexed:\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            self.word_index.save_index(temp.name)\n            self.serialized_word_index = temp.read()\n            temp.close()\n            word_index_temp = self.word_index\n            self.word_index = None\n\n        dump(self, file)\n\n        self.document_index = document_index_temp\n        self.word_index = word_index_temp\n\n    @classmethod\n    def load(cls, file):\n        \"\"\"\n\n        Load a pre-trained model from the specified file.\n\n        Parameters\n        ----------\n        file: str\n            File where model will be loaded from.\n        \"\"\"\n\n        top2vec_model = load(file)\n\n        # load document index\n        if top2vec_model.documents_indexed:\n            if not _HAVE_HNSWLIB:\n                raise ImportError(f\"Cannot load document index.\\n\\n\"\n                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n                                  \"Alternatively try: pip install hnswlib\")\n\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            temp.write(top2vec_model.serialized_document_index)\n\n            if top2vec_model.embedding_model == 'doc2vec':\n                document_vectors = top2vec_model.model.docvecs.vectors_docs\n            else:\n                document_vectors = top2vec_model.document_vectors\n\n            top2vec_model.document_index = hnswlib.Index(space='ip',\n                                                         dim=document_vectors.shape[1])\n            top2vec_model.document_index.load_index(temp.name, max_elements=document_vectors.shape[0])\n            temp.close()\n            top2vec_model.serialized_document_index = None\n\n        # load word index\n        if top2vec_model.words_indexed:\n\n            if not _HAVE_HNSWLIB:\n                raise ImportError(f\"Cannot load word index.\\n\\n\"\n                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n                                  \"Alternatively try: pip install hnswlib\")\n\n            temp = tempfile.NamedTemporaryFile(mode='w+b')\n            temp.write(top2vec_model.serialized_word_index)\n\n            if top2vec_model.embedding_model == 'doc2vec':\n                word_vectors = top2vec_model.model.wv.vectors\n            else:\n                word_vectors = top2vec_model.word_vectors\n\n            top2vec_model.word_index = hnswlib.Index(space='ip',\n                                                     dim=word_vectors.shape[1])\n            top2vec_model.word_index.load_index(temp.name, max_elements=word_vectors.shape[0])\n            temp.close()\n            top2vec_model.serialized_word_index = None\n\n        return top2vec_model\n\n    @staticmethod\n    def _l2_normalize(vectors):\n\n        if vectors.ndim == 2:\n            return normalize(vectors)\n        else:\n            return normalize(vectors.reshape(1, -1))[0]\n\n    def _embed_documents(self, train_corpus):\n\n        self._check_import_status()\n        self._check_model_status()\n\n        # embed documents\n        batch_size = 500\n        document_vectors = []\n\n        current = 0\n        batches = int(len(train_corpus) / batch_size)\n        extra = len(train_corpus) % batch_size\n\n        for ind in range(0, batches):\n            document_vectors.append(self.embed(train_corpus[current:current + batch_size]))\n            current += batch_size\n\n        if extra > 0:\n            document_vectors.append(self.embed(train_corpus[current:current + extra]))\n\n        document_vectors = self._l2_normalize(np.array(np.vstack(document_vectors)))\n\n        return document_vectors\n\n    def _set_document_vectors(self, document_vectors):\n        if self.embedding_model == 'doc2vec':\n            self.model.docvecs.vectors_docs = document_vectors\n        else:\n            self.document_vectors = document_vectors\n\n    def _get_document_vectors(self, norm=True):\n\n        if self.embedding_model == 'doc2vec':\n\n            if norm:\n                self.model.docvecs.init_sims()\n                return self.model.docvecs.vectors_docs_norm\n            else:\n                return self.model.docvecs.vectors_docs\n        else:\n            return self.document_vectors\n\n    def _index2word(self, index):\n        if self.embedding_model == 'doc2vec':\n            return self.model.wv.index2word[index]\n        else:\n            return self.vocab[index]\n\n    def _get_word_vectors(self):\n        if self.embedding_model == 'doc2vec':\n            self.model.wv.init_sims()\n            return self.model.wv.vectors_norm\n        else:\n            return self.word_vectors\n\n    def _create_topic_vectors(self, cluster_labels):\n\n        unique_labels = set(cluster_labels)\n        if -1 in unique_labels:\n            unique_labels.remove(-1)\n        self.topic_vectors = self._l2_normalize(\n            np.vstack([self._get_document_vectors(norm=False)[np.where(cluster_labels == label)[0]]\n                      .mean(axis=0) for label in unique_labels]))\n\n    def _deduplicate_topics(self):\n        core_samples, labels = dbscan(X=self.topic_vectors,\n                                      eps=0.1,\n                                      min_samples=2,\n                                      metric=\"cosine\")\n\n        duplicate_clusters = set(labels)\n\n        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n\n            # unique topics\n            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n\n            if -1 in duplicate_clusters:\n                duplicate_clusters.remove(-1)\n\n            # merge duplicate topics\n            for unique_label in duplicate_clusters:\n                unique_topics = np.vstack(\n                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n                                                       .mean(axis=0))])\n\n            self.topic_vectors = unique_topics\n\n    def _calculate_topic_sizes(self, hierarchy=False):\n        if hierarchy:\n            topic_sizes = pd.Series(self.doc_top_reduced).value_counts()\n        else:\n            topic_sizes = pd.Series(self.doc_top).value_counts()\n\n        return topic_sizes\n\n    def _reorder_topics(self, hierarchy=False):\n\n        if hierarchy:\n            self.topic_vectors_reduced = self.topic_vectors_reduced[self.topic_sizes_reduced.index]\n            self.topic_words_reduced = self.topic_words_reduced[self.topic_sizes_reduced.index]\n            self.topic_word_scores_reduced = self.topic_word_scores_reduced[self.topic_sizes_reduced.index]\n            old2new = dict(zip(self.topic_sizes_reduced.index, range(self.topic_sizes_reduced.index.shape[0])))\n            self.doc_top_reduced = np.array([old2new[i] for i in self.doc_top_reduced])\n            self.hierarchy = [self.hierarchy[i] for i in self.topic_sizes_reduced.index]\n            self.topic_sizes_reduced.reset_index(drop=True, inplace=True)\n        else:\n            self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n            self.topic_words = self.topic_words[self.topic_sizes.index]\n            self.topic_word_scores = self.topic_word_scores[self.topic_sizes.index]\n            old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n            self.doc_top = np.array([old2new[i] for i in self.doc_top])\n            self.topic_sizes.reset_index(drop=True, inplace=True)\n\n    @staticmethod\n    def _calculate_documents_topic(topic_vectors, document_vectors, dist=True):\n        batch_size = 10000\n        doc_top = []\n        if dist:\n            doc_dist = []\n\n        if document_vectors.shape[0] > batch_size:\n            current = 0\n            batches = int(document_vectors.shape[0] / batch_size)\n            extra = document_vectors.shape[0] % batch_size\n\n            for ind in range(0, batches):\n                res = np.inner(document_vectors[current:current + batch_size], topic_vectors)\n                doc_top.extend(np.argmax(res, axis=1))\n                if dist:\n                    doc_dist.extend(np.max(res, axis=1))\n                current += batch_size\n\n            if extra > 0:\n                res = np.inner(document_vectors[current:current + extra], topic_vectors)\n                doc_top.extend(np.argmax(res, axis=1))\n                if dist:\n                    doc_dist.extend(np.max(res, axis=1))\n            if dist:\n                doc_dist = np.array(doc_dist)\n        else:\n            res = np.inner(document_vectors, topic_vectors)\n            doc_top = np.argmax(res, axis=1)\n            if dist:\n                doc_dist = np.max(res, axis=1)\n\n        if dist:\n            return doc_top, doc_dist\n        else:\n            return doc_top\n\n    def _find_topic_words_and_scores(self, topic_vectors):\n        topic_words = []\n        topic_word_scores = []\n\n        res = np.inner(topic_vectors, self._get_word_vectors())\n        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n\n        for words, scores in zip(top_words, top_scores):\n            topic_words.append([self._index2word(i) for i in words[0:50]])\n            topic_word_scores.append(scores[0:50])\n\n        topic_words = np.array(topic_words)\n        topic_word_scores = np.array(topic_word_scores)\n\n        return topic_words, topic_word_scores\n\n    def _assign_documents_to_topic(self, document_vectors, hierarchy=False):\n\n        if hierarchy:\n            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors_reduced,\n                                                                        document_vectors,\n                                                                        dist=True)\n            self.doc_top_reduced = np.append(self.doc_top_reduced, doc_top_new)\n            self.doc_dist_reduced = np.append(self.doc_dist_reduced, doc_dist_new)\n\n            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n            for top in topic_sizes_new.index.tolist():\n                self.topic_sizes_reduced[top] += topic_sizes_new[top]\n            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n        else:\n            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors, document_vectors, dist=True)\n            self.doc_top = np.append(self.doc_top, doc_top_new)\n            self.doc_dist = np.append(self.doc_dist, doc_dist_new)\n\n            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n            for top in topic_sizes_new.index.tolist():\n                self.topic_sizes[top] += topic_sizes_new[top]\n            self.topic_sizes.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n\n    def _unassign_documents_from_topic(self, doc_indexes, hierarchy=False):\n        if hierarchy:\n            doc_top_remove = self.doc_top_reduced[doc_indexes]\n            self.doc_top_reduced = np.delete(self.doc_top_reduced, doc_indexes, 0)\n            self.doc_dist_reduced = np.delete(self.doc_dist_reduced, doc_indexes, 0)\n            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n            for top in topic_sizes_remove.index.tolist():\n                self.topic_sizes_reduced[top] -= topic_sizes_remove[top]\n            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n        else:\n            doc_top_remove = self.doc_top[doc_indexes]\n            self.doc_top = np.delete(self.doc_top, doc_indexes, 0)\n            self.doc_dist = np.delete(self.doc_dist, doc_indexes, 0)\n            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n            for top in topic_sizes_remove.index.tolist():\n                self.topic_sizes[top] -= topic_sizes_remove[top]\n            self.topic_sizes.sort_values(ascending=False, inplace=True)\n            self._reorder_topics(hierarchy)\n\n    def _get_document_ids(self, doc_index):\n        return self.document_ids[doc_index]\n\n    def _get_document_indexes(self, doc_ids):\n        if self.document_ids is None:\n            return doc_ids\n        else:\n            return [self.doc_id2index[doc_id] for doc_id in doc_ids]\n\n    def _words2word_vectors(self, keywords):\n\n        return self._get_word_vectors()[[self._word2index(word) for word in keywords]]\n\n    def _word2index(self, word):\n        if self.embedding_model == 'doc2vec':\n            return self.model.wv.vocab[word].index\n        else:\n            return self.word_indexes[word]\n\n    def _get_combined_vec(self, vecs, vecs_neg):\n\n        combined_vector = np.zeros(self._get_document_vectors().shape[1], dtype=np.float64)\n        for vec in vecs:\n            combined_vector += vec\n        for vec in vecs_neg:\n            combined_vector -= vec\n        combined_vector /= (len(vecs) + len(vecs_neg))\n        combined_vector = self._l2_normalize(combined_vector)\n\n        return combined_vector\n\n    @staticmethod\n    def _search_vectors_by_vector(vectors, vector, num_res):\n        ranks = np.inner(vectors, vector)\n        indexes = np.flip(np.argsort(ranks)[-num_res:])\n        scores = np.array([ranks[res] for res in indexes])\n\n        return indexes, scores\n\n    @staticmethod\n    def _check_hnswlib_status():\n        if not _HAVE_HNSWLIB:\n            raise ImportError(f\"Indexing is not available.\\n\\n\"\n                              \"Try: pip install top2vec[indexing]\\n\\n\"\n                              \"Alternatively try: pip install hnswlib\")\n\n    def _check_document_index_status(self):\n        if self.document_index is None:\n            raise ImportError(\"There is no document index.\\n\\n\"\n                              \"Call index_document_vectors method before setting use_index=True.\")\n\n    def _check_word_index_status(self):\n        if self.word_index is None:\n            raise ImportError(\"There is no word index.\\n\\n\"\n                              \"Call index_word_vectors method before setting use_index=True.\")\n\n    def _check_import_status(self):\n        if self.embedding_model != 'distiluse-base-multilingual-cased':\n            if not _HAVE_TENSORFLOW:\n                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n        else:\n            if not _HAVE_TORCH:\n                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n                                  \"Alternatively try: pip install torch sentence_transformers\")\n\n    def _check_model_status(self):\n        if self.embed is None:\n            if self.verbose is False:\n                logger.setLevel(logging.DEBUG)\n\n            if self.embedding_model != \"distiluse-base-multilingual-cased\":\n                if self.embedding_model_path is None:\n                    logger.info(f'Downloading {self.embedding_model} model')\n                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n                    else:\n                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n                else:\n                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n                    module = self.embedding_model_path\n                self.embed = hub.load(module)\n\n            else:\n                if self.embedding_model_path is None:\n                    logger.info(f'Downloading {self.embedding_model} model')\n                    module = 'distiluse-base-multilingual-cased'\n                else:\n                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n                    module = self.embedding_model_path\n                model = SentenceTransformer(module)\n                self.embed = model.encode\n\n        if self.verbose is False:\n            logger.setLevel(logging.WARNING)\n\n    @staticmethod\n    def _less_than_zero(num, var_name):\n        if num < 0:\n            raise ValueError(f\"{var_name} cannot be less than 0.\")\n\n    def _validate_hierarchical_reduction(self):\n        if self.hierarchy is None:\n            raise ValueError(\"Hierarchical topic reduction has not been performed.\")\n\n    def _validate_hierarchical_reduction_num_topics(self, num_topics):\n        current_num_topics = len(self.topic_vectors)\n        if num_topics >= current_num_topics:\n            raise ValueError(f\"Number of topics must be less than {current_num_topics}.\")\n\n    def _validate_num_docs(self, num_docs):\n        self._less_than_zero(num_docs, \"num_docs\")\n        document_count = len(self.doc_top)\n        if num_docs > document_count:\n            raise ValueError(f\"num_docs cannot exceed the number of documents: {document_count}.\")\n\n    def _validate_num_topics(self, num_topics, reduced):\n        self._less_than_zero(num_topics, \"num_topics\")\n        if reduced:\n            topic_count = len(self.topic_vectors_reduced)\n            if num_topics > topic_count:\n                raise ValueError(f\"num_topics cannot exceed the number of reduced topics: {topic_count}.\")\n        else:\n            topic_count = len(self.topic_vectors)\n            if num_topics > topic_count:\n                raise ValueError(f\"num_topics cannot exceed the number of topics: {topic_count}.\")\n\n    def _validate_topic_num(self, topic_num, reduced):\n        self._less_than_zero(topic_num, \"topic_num\")\n\n        if reduced:\n            topic_count = len(self.topic_vectors_reduced) - 1\n            if topic_num > topic_count:\n                raise ValueError(f\"Invalid topic number: valid reduced topics numbers are 0 to {topic_count}.\")\n        else:\n            topic_count = len(self.topic_vectors) - 1\n            if topic_num > topic_count:\n                raise ValueError(f\"Invalid topic number: valid original topics numbers are 0 to {topic_count}.\")\n\n    def _validate_topic_search(self, topic_num, num_docs, reduced):\n        self._less_than_zero(num_docs, \"num_docs\")\n        if reduced:\n            if num_docs > self.topic_sizes_reduced[topic_num]:\n                raise ValueError(f\"Invalid number of documents: reduced topic {topic_num}\"\n                                 f\" only has {self.topic_sizes_reduced[topic_num]} documents.\")\n        else:\n            if num_docs > self.topic_sizes[topic_num]:\n                raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n                                 f\" only has {self.topic_sizes[topic_num]} documents.\")\n\n    def _validate_doc_ids(self, doc_ids, doc_ids_neg):\n\n        if not (isinstance(doc_ids, list) or isinstance(doc_ids, np.ndarray)):\n            raise ValueError(\"doc_ids must be a list of string or int.\")\n        if not (isinstance(doc_ids_neg, list) or isinstance(doc_ids_neg, np.ndarray)):\n            raise ValueError(\"doc_ids_neg must be a list of string or int.\")\n\n        if isinstance(doc_ids, np.ndarray):\n            doc_ids = list(doc_ids)\n        if isinstance(doc_ids_neg, np.ndarray):\n            doc_ids_neg = list(doc_ids_neg)\n\n        doc_ids_all = doc_ids + doc_ids_neg\n\n        if self.document_ids is not None:\n            for doc_id in doc_ids_all:\n                if doc_id not in self.doc_id2index:\n                    raise ValueError(f\"{doc_id} is not a valid document id.\")\n        elif min(doc_ids) < 0:\n            raise ValueError(f\"{min(doc_ids)} is not a valid document id.\")\n        elif max(doc_ids) > len(self.doc_top) - 1:\n            raise ValueError(f\"{max(doc_ids)} is not a valid document id.\")\n\n    def _validate_keywords(self, keywords, keywords_neg):\n        if not (isinstance(keywords, list) or isinstance(keywords, np.ndarray)):\n            raise ValueError(\"keywords must be a list of strings.\")\n\n        if not (isinstance(keywords_neg, list) or isinstance(keywords_neg, np.ndarray)):\n            raise ValueError(\"keywords_neg must be a list of strings.\")\n\n        keywords_lower = [keyword.lower() for keyword in keywords]\n        keywords_neg_lower = [keyword.lower() for keyword in keywords_neg]\n\n        if self.embedding_model == 'doc2vec':\n            vocab = self.model.wv.vocab\n        else:\n            vocab = self.vocab\n\n        for word in keywords_lower + keywords_neg_lower:\n            if word not in vocab:\n                raise ValueError(f\"'{word}' has not been learned by the model so it cannot be searched.\")\n\n        return keywords_lower, keywords_neg_lower\n\n    def _validate_document_ids_add_doc(self, documents, document_ids):\n        if document_ids is None:\n            raise ValueError(\"Document ids need to be provided.\")\n        if len(documents) != len(document_ids):\n            raise ValueError(\"Document ids need to match number of documents.\")\n        if len(document_ids) != len(set(document_ids)):\n            raise ValueError(\"Document ids need to be unique.\")\n\n        if len(set(document_ids).intersection(self.document_ids)) > 0:\n            raise ValueError(\"Some document ids already exist in model.\")\n\n        if self.doc_id_type == np.str_:\n            if not all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n                raise ValueError(\"Document ids need to be of type str.\")\n\n        if self.doc_id_type == np.int_:\n            if not all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n                raise ValueError(\"Document ids need to be of type int.\")\n\n    @staticmethod\n    def _validate_documents(documents):\n        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n            raise ValueError(\"Documents need to be a list of strings.\")\n\n    def _validate_vector(self, vector):\n        if not isinstance(vector, np.ndarray):\n            raise ValueError(\"Vector needs to be a numpy array.\")\n        vec_size = self._get_document_vectors().shape[1]\n        if not vector.shape[0] == vec_size:\n            raise ValueError(f\"Vector needs to be of {vec_size} dimensions.\")\n\n    def index_document_vectors(self, ef_construction=200, M=64):\n        \"\"\"\n        Creates an index of the document vectors using hnswlib. This will\n        lead to faster search times for models with a large number of\n        documents. \n\n        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n\n        Parameters\n        ----------\n        ef_construction: int (Optional default 200)\n            This parameter controls the trade-off between index construction\n            time and index accuracy. Larger values will lead to greater\n            accuracy but will take longer to construct.\n\n        M: int (Optional default 64)\n            This parameter controls the trade-off between both index size as\n            well as construction time and accuracy. Larger values will lead to\n            greater accuracy but will result in a larger index as well as\n            longer construction time.\n\n            For more information on the parameters see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n        \"\"\"\n\n        self._check_hnswlib_status()\n\n        document_vectors = self._get_document_vectors()\n        vec_dim = document_vectors.shape[1]\n        num_vecs = document_vectors.shape[0]\n\n        index_ids = list(range(0, len(self.document_ids)))\n\n        self.index_id2doc_id = dict(zip(index_ids, self.document_ids))\n        self.doc_id2index_id = dict(zip(self.document_ids, index_ids))\n\n        self.document_index = hnswlib.Index(space='ip', dim=vec_dim)\n        self.document_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n        self.document_index.add_items(document_vectors, index_ids)\n        self.documents_indexed = True\n\n    def index_word_vectors(self, ef_construction=200, M=64):\n        \"\"\"\n        Creates an index of the word vectors using hnswlib. This will\n        lead to faster search times for models with a large number of\n        words.\n\n        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n\n        Parameters\n        ----------\n        ef_construction: int (Optional default 200)\n            This parameter controls the trade-off between index construction\n            time and index accuracy. Larger values will lead to greater\n            accuracy but will take longer to construct.\n\n        M: int (Optional default 64)\n            This parameter controls the trade-off between both index size as\n            well as construction time and accuracy. Larger values will lead to\n            greater accuracy but will result in a larger index as well as\n            longer construction time.\n\n            For more information on the parameters see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n        \"\"\"\n        self._check_hnswlib_status()\n\n        word_vectors = self._get_word_vectors()\n        vec_dim = word_vectors.shape[1]\n        num_vecs = word_vectors.shape[0]\n\n        index_ids = list(range(0, num_vecs))\n\n        self.word_index = hnswlib.Index(space='ip', dim=vec_dim)\n        self.word_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n        self.word_index.add_items(word_vectors, index_ids)\n        self.words_indexed = True\n\n    def update_embedding_model_path(self, embedding_model_path):\n        \"\"\"\n        Update the path of the embedding model to be loaded. The model will\n        no longer be downloaded but loaded from the path location.\n\n        Warning: the model at embedding_model_path must match the\n        embedding_model parameter type.\n\n        Parameters\n        ----------\n        embedding_model_path: Str\n            Path to downloaded embedding model.\n\n        \"\"\"\n        self.embedding_model_path = embedding_model_path\n\n    def change_to_download_embedding_model(self):\n        \"\"\"\n        Use automatic download to load embedding model used for training.\n        Top2Vec will no longer try and load the embedding model from a file\n        if a embedding_model path was previously added.\n\n        \"\"\"\n        self.embedding_model_path = None\n\n    def get_documents_topics(self, doc_ids, reduced=False):\n        \"\"\"\n        Get document topics.\n\n        The topic of each document will be returned.\n\n        The corresponding original topics are returned unless reduced=True,\n        in which case the reduced topics will be returned.\n\n        Parameters\n        ----------\n        doc_ids: List of str, int\n            A unique value per document that is used for referring to documents\n            in search results. If ids were not given to the model, the index of\n            each document in the model is the id.\n\n        reduced: bool (Optional, default False)\n            Original topics are returned by default. If True the\n            reduced topics will be returned.\n\n        Returns\n        -------\n        topic_nums: array of int, shape(doc_ids)\n            The topic number of the document corresponding to each doc_id.\n\n        topic_score: array of float, shape(doc_ids)\n            Semantic similarity of document to topic. The cosine similarity of\n            the document and topic vector.\n\n        topics_words: array of shape(num_topics, 50)\n            For each topic the top 50 words are returned, in order\n            of semantic similarity to topic.\n\n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],          <Topic 4>\n            ['environment', 'warming', 'climate ... 'temperature']  <Topic 21>\n            ...]\n\n        word_scores: array of shape(num_topics, 50)\n            For each topic the cosine similarity scores of the\n            top 50 words to the topic are returned.\n\n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 4>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]  <Topic 21>\n            ...]\n\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n        # make sure documents exist\n        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n\n        # get document indexes from ids\n        doc_indexes = self._get_document_indexes(doc_ids)\n\n        if reduced:\n            doc_topics = self.doc_top_reduced[doc_indexes]\n            doc_dist = self.doc_dist_reduced[doc_indexes]\n            topic_words = self.topic_words_reduced[doc_topics]\n            topic_word_scores = self.topic_word_scores_reduced[doc_topics]\n        else:\n            doc_topics = self.doc_top[doc_indexes]\n            doc_dist = self.doc_dist[doc_indexes]\n            topic_words = self.topic_words[doc_topics]\n            topic_word_scores = self.topic_word_scores[doc_topics]\n\n        return doc_topics, doc_dist, topic_words, topic_word_scores\n\n    def add_documents(self, documents, doc_ids=None, tokenizer=None, use_embedding_model_tokenizer=False):\n        \"\"\"\n        Update the model with new documents.\n\n        The documents will be added to the current model without changing\n        existing document, word and topic vectors. Topic sizes will be updated.\n\n        If adding a large quantity of documents relative to the current model\n        size, or documents containing a largely new vocabulary, a new model\n        should be trained for best results.\n\n        Parameters\n        ----------\n        documents: List of str\n\n        doc_ids: List of str, int (Optional)\n            Only required when doc_ids were given to the original model.\n\n            A unique value per document that will be used for referring to\n            documents in search results.\n\n        tokenizer: callable (Optional, default None)\n            Override the default tokenization method. If None then\n            gensim.utils.simple_preprocess will be used.\n\n        use_embedding_model_tokenizer: bool (Optional, default False)\n            If using an embedding model other than doc2vec, use the model's\n            tokenizer for document embedding.\n        \"\"\"\n        # if tokenizer is not passed use default\n        if tokenizer is None:\n            tokenizer = default_tokenizer\n\n        # add documents\n        self._validate_documents(documents)\n        if self.documents is not None:\n            self.documents = np.append(self.documents, documents)\n\n        # add document ids\n        if self.document_ids_provided is True:\n            self._validate_document_ids_add_doc(documents, doc_ids)\n            doc_ids_len = len(self.document_ids)\n            self.document_ids = np.append(self.document_ids, doc_ids)\n            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n\n        elif doc_ids is None:\n            num_docs = len(documents)\n            start_id = max(self.document_ids) + 1\n            doc_ids = list(range(start_id, start_id + num_docs))\n            doc_ids_len = len(self.document_ids)\n            self.document_ids = np.append(self.document_ids, doc_ids)\n            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n        else:\n            raise ValueError(\"doc_ids cannot be used because they were not provided to model during training.\")\n\n        if self.embedding_model == \"doc2vec\":\n            docs_processed = [tokenizer(doc) for doc in documents]\n            document_vectors = np.vstack([self.model.infer_vector(doc_words=doc,\n                                                                  alpha=0.025,\n                                                                  min_alpha=0.01,\n                                                                  epochs=100) for doc in docs_processed])\n            num_docs = len(documents)\n            self.model.docvecs.count += num_docs\n            self.model.docvecs.max_rawint += num_docs\n            self.model.docvecs.vectors_docs_norm = None\n            self._set_document_vectors(np.vstack([self._get_document_vectors(norm=False), document_vectors]))\n            self.model.docvecs.init_sims()\n\n        else:\n            if use_embedding_model_tokenizer:\n                docs_training = documents\n            else:\n                docs_processed = [tokenizer(doc) for doc in documents]\n                docs_training = [' '.join(doc) for doc in docs_processed]\n            document_vectors = self._embed_documents(docs_training)\n            self._set_document_vectors(np.vstack([self._get_document_vectors(), document_vectors]))\n\n        # update index\n        if self.documents_indexed:\n            # update capacity of index\n            current_max = self.documents_index.get_max_elements()\n            updated_max = current_max + len(documents)\n            self.documents_index.resize_index(updated_max)\n\n            # update index_id and doc_ids\n            start_index_id = max(self.index_id2doc_id.keys()) + 1\n            new_index_ids = list(range(start_index_id, start_index_id + len(doc_ids)))\n            self.index_id2doc_id.update(dict(zip(new_index_ids, doc_ids)))\n            self.doc_id2index_id.update(dict(zip(doc_ids, new_index_ids)))\n            self.documents_index.add_items(document_vectors, new_index_ids)\n\n        # update topics\n        self._assign_documents_to_topic(document_vectors, hierarchy=False)\n\n        if self.hierarchy is not None:\n            self._assign_documents_to_topic(document_vectors, hierarchy=True)\n\n    def delete_documents(self, doc_ids):\n        \"\"\"\n        Delete documents from current model.\n\n        Warning: If document ids were not used in original model, deleting\n        documents will change the indexes and therefore doc_ids.\n\n        The documents will be deleted from the current model without changing\n        existing document, word and topic vectors. Topic sizes will be updated.\n\n        If deleting a large quantity of documents relative to the current model\n        size a new model should be trained for best results.\n\n        Parameters\n        ----------\n        doc_ids: List of str, int\n\n            A unique value per document that is used for referring to documents\n            in search results.\n        \"\"\"\n        # make sure documents exist\n        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n\n        # update index\n        if self.documents_indexed:\n            # delete doc_ids from index\n            index_ids = [self.doc_id2index_id(doc_id) for doc_id in doc_ids]\n            for index_id in index_ids:\n                self.document_index.mark_deleted(index_id)\n            # update index_id and doc_ids\n            for doc_id in doc_ids:\n                self.doc_id2index_id.pop(doc_id)\n            for index_id in index_ids:\n                self.index_id2doc_id.pop(index_id)\n\n        # get document indexes from ids\n        doc_indexes = self._get_document_indexes(doc_ids)\n\n        # delete documents\n        if self.documents is not None:\n            self.documents = np.delete(self.documents, doc_indexes, 0)\n\n        # delete document ids\n        if self.document_ids is not None:\n            for doc_id in doc_ids:\n                self.doc_id2index.pop(doc_id)\n            keys = list(self.doc_id2index.keys())\n            self.document_ids = np.array(keys)\n            values = list(range(0, len(self.doc_id2index.values())))\n            self.doc_id2index = dict(zip(keys, values))\n\n        # delete document vectors\n        self._set_document_vectors(np.delete(self._get_document_vectors(norm=False), doc_indexes, 0))\n\n        if self.embedding_model == 'doc2vec':\n            num_docs = len(doc_indexes)\n            self.model.docvecs.count -= num_docs\n            self.model.docvecs.max_rawint -= num_docs\n            self.model.docvecs.vectors_docs_norm = None\n            self.model.docvecs.init_sims()\n\n        # update topics\n        self._unassign_documents_from_topic(doc_indexes, hierarchy=False)\n\n        if self.hierarchy is not None:\n            self._unassign_documents_from_topic(doc_indexes, hierarchy=True)\n\n    def get_num_topics(self, reduced=False):\n        \"\"\"\n        Get number of topics.\n\n        This is the number of topics Top2Vec has found in the data by default.\n        If reduced is True, the number of reduced topics is returned.\n\n        Parameters\n        ----------\n        reduced: bool (Optional, default False)\n            The number of original topics will be returned by default. If True\n            will return the number of reduced topics, if hierarchical topic\n            reduction has been performed.\n\n        Returns\n        -------\n        num_topics: int\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            return len(self.topic_vectors_reduced)\n        else:\n            return len(self.topic_vectors)\n\n    def get_topic_sizes(self, reduced=False):\n        \"\"\"\n        Get topic sizes.\n\n        The number of documents most similar to each topic. Topics are\n        in increasing order of size.\n\n        The sizes of the original topics is returned unless reduced=True,\n        in which case the sizes of the reduced topics will be returned.\n\n        Parameters\n        ----------\n        reduced: bool (Optional, default False)\n            Original topic sizes are returned by default. If True the\n            reduced topic sizes will be returned.\n\n        Returns\n        -------\n        topic_sizes: array of int, shape(num_topics)\n            The number of documents most similar to the topic.\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n            return np.array(self.topic_sizes_reduced.values), np.array(self.topic_sizes_reduced.index)\n        else:\n            return np.array(self.topic_sizes.values), np.array(self.topic_sizes.index)\n\n    def get_topics(self, num_topics=None, reduced=False):\n        \"\"\"\n        Get topics, ordered by decreasing size. All topics are returned\n        if num_topics is not specified.\n\n        The original topics found are returned unless reduced=True,\n        in which case reduced topics will be returned.\n\n        Each topic will consist of the top 50 semantically similar words\n        to the topic. These are the 50 words closest to topic vector\n        along with cosine similarity of each word from vector. The\n        higher the score the more relevant the word is to the topic.\n\n        Parameters\n        ----------\n        num_topics: int, (Optional)\n            Number of topics to return.\n\n        reduced: bool (Optional, default False)\n            Original topics are returned by default. If True the\n            reduced topics will be returned.\n\n        Returns\n        -------\n        topics_words: array of shape(num_topics, 50)\n            For each topic the top 50 words are returned, in order\n            of semantic similarity to topic.\n            \n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],         <Topic 0>\n            ['environment', 'warming', 'climate ... 'temperature']  <Topic 1>\n            ...]\n\n        word_scores: array of shape(num_topics, 50)\n            For each topic the cosine similarity scores of the\n            top 50 words to the topic are returned.\n            \n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 0>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]   <Topic 1>\n            ...]\n\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n            if num_topics is None:\n                num_topics = len(self.topic_vectors_reduced)\n            else:\n                self._validate_num_topics(num_topics, reduced)\n\n            return self.topic_words_reduced[0:num_topics], self.topic_word_scores_reduced[0:num_topics], np.array(\n                range(0, num_topics))\n        else:\n\n            if num_topics is None:\n                num_topics = len(self.topic_vectors)\n            else:\n                self._validate_num_topics(num_topics, reduced)\n\n            return self.topic_words[0:num_topics], self.topic_word_scores[0:num_topics], np.array(range(0, num_topics))\n\n    def get_topic_hierarchy(self):\n        \"\"\"\n        Get the hierarchy of reduced topics. The mapping of each original topic\n        to the reduced topics is returned.\n\n        Hierarchical topic reduction must be performed before calling this\n        method.\n\n        Returns\n        -------\n        hierarchy: list of ints\n            Each index of the hierarchy corresponds to the topic number of a\n            reduced topic. For each reduced topic the topic numbers of the\n            original topics that were merged to create it are listed.\n\n            Example:\n            [[3]  <Reduced Topic 0> contains original Topic 3\n            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n            ...]\n        \"\"\"\n\n        self._validate_hierarchical_reduction()\n\n        return self.hierarchy\n\n    def hierarchical_topic_reduction(self, num_topics):\n        \"\"\"\n        Reduce the number of topics discovered by Top2Vec.\n\n        The most representative topics of the corpus will be found, by\n        iteratively merging each smallest topic to the most similar topic until\n        num_topics is reached.\n\n        Parameters\n        ----------\n        num_topics: int\n            The number of topics to reduce to.\n\n        Returns\n        -------\n        hierarchy: list of ints\n            Each index of hierarchy corresponds to the reduced topics, for each\n            reduced topic the indexes of the original topics that were merged\n            to create it are listed.\n\n            Example:\n            [[3]  <Reduced Topic 0> contains original Topic 3\n            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n            ...]\n        \"\"\"\n        self._validate_hierarchical_reduction_num_topics(num_topics)\n\n        num_topics_current = self.topic_vectors.shape[0]\n        top_vecs = self.topic_vectors\n        top_sizes = [self.topic_sizes[i] for i in range(0, len(self.topic_sizes))]\n        hierarchy = [[i] for i in range(self.topic_vectors.shape[0])]\n\n        count = 0\n        interval = max(int(self._get_document_vectors().shape[0] / 50000), 1)\n\n        while num_topics_current > num_topics:\n\n            # find smallest and most similar topics\n            smallest = np.argmin(top_sizes)\n            res = np.inner(top_vecs[smallest], top_vecs)\n            sims = np.flip(np.argsort(res))\n            most_sim = sims[1]\n            if most_sim == smallest:\n                most_sim = sims[0]\n\n            # calculate combined topic vector\n            top_vec_smallest = top_vecs[smallest]\n            smallest_size = top_sizes[smallest]\n\n            top_vec_most_sim = top_vecs[most_sim]\n            most_sim_size = top_sizes[most_sim]\n\n            combined_vec = self._l2_normalize(((top_vec_smallest * smallest_size) +\n                                               (top_vec_most_sim * most_sim_size)) / (smallest_size + most_sim_size))\n\n            # update topic vectors\n            ix_keep = list(range(len(top_vecs)))\n            ix_keep.remove(smallest)\n            ix_keep.remove(most_sim)\n            top_vecs = top_vecs[ix_keep]\n            top_vecs = np.vstack([top_vecs, combined_vec])\n            num_topics_current = top_vecs.shape[0]\n\n            # update topics sizes\n            if count % interval == 0:\n                doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n                                                          document_vectors=self._get_document_vectors(),\n                                                          dist=False)\n                topic_sizes = pd.Series(doc_top).value_counts()\n                top_sizes = [topic_sizes[i] for i in range(0, len(topic_sizes))]\n\n            else:\n                smallest_size = top_sizes.pop(smallest)\n                if most_sim < smallest:\n                    most_sim_size = top_sizes.pop(most_sim)\n                else:\n                    most_sim_size = top_sizes.pop(most_sim - 1)\n                combined_size = smallest_size + most_sim_size\n                top_sizes.append(combined_size)\n\n            count += 1\n\n            # update topic hierarchy\n            smallest_inds = hierarchy.pop(smallest)\n            if most_sim < smallest:\n                most_sim_inds = hierarchy.pop(most_sim)\n            else:\n                most_sim_inds = hierarchy.pop(most_sim - 1)\n\n            combined_inds = smallest_inds + most_sim_inds\n            hierarchy.append(combined_inds)\n\n        # re-calculate topic vectors from clusters\n        doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n                                                  document_vectors=self._get_document_vectors(),\n                                                  dist=False)\n        self.topic_vectors_reduced = self._l2_normalize(np.vstack([self._get_document_vectors()\n                                                                   [np.where(doc_top == label)[0]]\n                                                                  .mean(axis=0) for label in set(doc_top)]))\n\n        self.hierarchy = hierarchy\n\n        # assign documents to topic\n        self.doc_top_reduced, self.doc_dist_reduced = self._calculate_documents_topic(self.topic_vectors_reduced,\n                                                                                      self._get_document_vectors())\n        # find topic words and scores\n        self.topic_words_reduced, self.topic_word_scores_reduced = self._find_topic_words_and_scores(\n            topic_vectors=self.topic_vectors_reduced)\n\n        # calculate topic sizes\n        self.topic_sizes_reduced = self._calculate_topic_sizes(hierarchy=True)\n\n        # re-order topics\n        self._reorder_topics(hierarchy=True)\n\n        return self.hierarchy\n\n    def search_documents_by_vector(self, vector, num_docs, return_documents=True, use_index=False, ef=None):\n        \"\"\"\n        Semantic search of documents using a vector.\n\n        These are the documents closest to the vector. Documents are\n        ordered by proximity to the vector. Successive documents in the\n        list are less semantically similar to the vector.\n\n        Parameters\n        ----------\n        vector: array of shape(vector dimension, 1)\n            The vector dimension should be the same as the vectors in\n            the topic_vectors variable. (i.e. model.topic_vectors.shape[1])\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to vector. The cosine similarity of\n            the document and vector.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n        self._validate_vector(vector)\n        self._validate_num_docs(num_docs)\n\n        if use_index:\n            self._check_document_index_status()\n\n            if ef is not None:\n                self.document_index.set_ef(ef)\n            else:\n                self.document_index.set_ef(num_docs)\n\n            index_ids, doc_scores = self.document_index.knn_query(vector, k=num_docs)\n            index_ids = index_ids[0]\n            doc_ids = np.array([self.index_id2doc_id[index_id] for index_id in index_ids])\n            doc_scores = doc_scores[0]\n            doc_scores = np.array([1 - score for score in doc_scores])\n            doc_indexes = self._get_document_indexes(doc_ids)\n        else:\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     vector, num_docs)\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def search_words_by_vector(self, vector, num_words, use_index=False, ef=None):\n        \"\"\"\n        Semantic search of words using a vector.\n\n        These are the words closest to the vector. Words are ordered by\n        proximity to the vector. Successive words in the list are less\n        semantically similar to the vector.\n\n        Parameters\n        ----------\n        vector: array of shape(vector dimension, 1)\n            The vector dimension should be the same as the vectors in\n            the topic_vectors variable. (i.e. model.topic_vectors.shape[1])\n\n        num_words: int\n            Number of words to return.\n\n        use_index: bool (Optional default False)\n            If index_words method has been called, setting this to True will\n            speed up search for models with large number of words.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        words: array of str, shape(num_words)\n            The words in a list, the most similar are first.\n\n        word_scores: array of float, shape(num_words)\n            Semantic similarity of word to vector. The cosine similarity of\n            the word and vector.\n        \"\"\"\n\n        if use_index:\n            self._check_word_index_status()\n\n            if ef is not None:\n                self.word_index.set_ef(ef)\n            else:\n                self.word_index.set_ef(num_words)\n\n            word_indexes, word_scores = self.word_index.knn_query(vector, k=num_words)\n            word_indexes = word_indexes[0]\n            word_scores = word_scores[0]\n            word_scores = np.array([1 - score for score in word_scores])\n\n        else:\n            word_indexes, word_scores = self._search_vectors_by_vector(self._get_word_vectors(),\n                                                                       vector, num_words)\n\n        words = np.array([self._index2word(index) for index in word_indexes])\n\n        return words, word_scores\n\n    def search_documents_by_topic(self, topic_num, num_docs, return_documents=True, reduced=False):\n        \"\"\"\n        Get the most semantically similar documents to the topic.\n\n        These are the documents closest to the topic vector. Documents are\n        ordered by proximity to the topic vector. Successive documents in the\n        list are less semantically similar to the topic.\n\n        Parameters\n        ----------\n        topic_num: int\n            The topic number to search.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will not be returned.\n\n        reduced: bool (Optional, default False)\n            Original topics are used to search by default. If True the\n            reduced topics will be used.\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to topic. The cosine similarity of\n            the document and topic vector.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            self._validate_topic_num(topic_num, reduced)\n            self._validate_topic_search(topic_num, num_docs, reduced)\n\n            topic_document_indexes = np.where(self.doc_top_reduced == topic_num)[0]\n            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist_reduced[topic_document_indexes]))\n            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n            doc_scores = self.doc_dist_reduced[doc_indexes]\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        else:\n\n            self._validate_topic_num(topic_num, reduced)\n            self._validate_topic_search(topic_num, num_docs, reduced)\n\n            topic_document_indexes = np.where(self.doc_top == topic_num)[0]\n            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist[topic_document_indexes]))\n            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n            doc_scores = self.doc_dist[doc_indexes]\n            doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def search_documents_by_keywords(self, keywords, num_docs, keywords_neg=None, return_documents=True,\n                                     use_index=False, ef=None):\n        \"\"\"\n        Semantic search of documents using keywords.\n\n        The most semantically similar documents to the combination of the\n        keywords will be returned. If negative keywords are provided, the\n        documents will be semantically dissimilar to those words. Too many\n        keywords or certain combinations of words may give strange results.\n        This method finds an average vector(negative keywords are subtracted)\n        of all the keyword vectors and returns the documents closest to the\n        resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar documents.\n\n        keywords_neg: List of str (Optional)\n            List of negative keywords being used for search of semantically\n            dissimilar documents.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will also not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to keywords. The cosine similarity\n            of the document and average of keyword vectors.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n\n        if keywords_neg is None:\n            keywords_neg = []\n\n        self._validate_num_docs(num_docs)\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n\n        if use_index:\n            self._check_document_index_status()\n            combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n            return self.search_documents_by_vector(combined_vector, num_docs, return_documents=return_documents,\n                                                   use_index=True, ef=ef)\n\n        if self.embedding_model == 'doc2vec':\n            sim_docs = self.model.docvecs.most_similar(positive=word_vecs,\n                                                       negative=neg_word_vecs,\n                                                       topn=num_docs)\n            doc_indexes = [doc[0] for doc in sim_docs]\n            doc_scores = np.array([doc[1] for doc in sim_docs])\n        else:\n            combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     combined_vector, num_docs)\n\n        doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def similar_words(self, keywords, num_words, keywords_neg=None, use_index=False, ef=None):\n        \"\"\"\n        Semantic similarity search of words.\n\n        The most semantically similar word to the combination of the keywords\n        will be returned. If negative keywords are provided, the words will be\n        semantically dissimilar to those words. Too many keywords or certain\n        combinations of words may give strange results. This method finds an\n        average vector(negative keywords are subtracted) of all the keyword\n        vectors and returns the words closest to the resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar words.\n\n        keywords_neg: List of str\n            List of negative keywords being used for search of semantically\n            dissimilar words.\n\n        num_words: int\n            Number of words to return.\n\n        use_index: bool (Optional default False)\n            If index_words method has been called, setting this to True will\n            speed up search for models with large number of words.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        words: array of str, shape(num_words)\n            The words in a list, the most similar are first.\n\n        word_scores: array of float, shape(num_words)\n            Semantic similarity of word to keywords. The cosine similarity of\n            the word and average of keyword vectors.\n        \"\"\"\n        if keywords_neg is None:\n            keywords_neg = []\n\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n        combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n\n        num_res = min(num_words + len(keywords) + len(keywords_neg), self._get_word_vectors().shape[0])\n\n        # if use_index:\n        words, word_scores = self.search_words_by_vector(vector=combined_vector,\n                                                         num_words=num_res,\n                                                         use_index=use_index,\n                                                         ef=ef)\n\n        res_indexes = [index for index, word in enumerate(words)\n                       if word not in list(keywords) + list(keywords_neg)][:num_words]\n        words = words[res_indexes]\n        word_scores = word_scores[res_indexes]\n\n        return words, word_scores\n\n    def search_topics(self, keywords, num_topics, keywords_neg=None, reduced=False):\n        \"\"\"\n        Semantic search of topics using keywords.\n\n        The most semantically similar topics to the combination of the keywords\n        will be returned. If negative keywords are provided, the topics will be\n        semantically dissimilar to those words. Topics will be ordered by\n        decreasing similarity to the keywords. Too many keywords or certain\n        combinations of words may give strange results. This method finds an\n        average vector(negative keywords are subtracted) of all the keyword\n        vectors and returns the topics closest to the resulting vector.\n\n        Parameters\n        ----------\n        keywords: List of str\n            List of positive keywords being used for search of semantically\n            similar documents.\n\n        keywords_neg: (Optional) List of str\n            List of negative keywords being used for search of semantically\n            dissimilar documents.\n\n        num_topics: int\n            Number of documents to return.\n\n        reduced: bool (Optional, default False)\n            Original topics are searched by default. If True the\n            reduced topics will be searched.\n\n        Returns\n        -------\n        topics_words: array of shape (num_topics, 50)\n            For each topic the top 50 words are returned, in order of semantic\n            similarity to topic.\n            \n            Example:\n            [['data', 'deep', 'learning' ... 'artificial'],           <Topic 0>\n            ['environment', 'warming', 'climate ... 'temperature']    <Topic 1>\n            ...]\n\n        word_scores: array of shape (num_topics, 50)\n            For each topic the cosine similarity scores of the top 50 words\n            to the topic are returned.\n            \n            Example:\n            [[0.7132, 0.6473, 0.5700 ... 0.3455],     <Topic 0>\n            [0.7818', 0.7671, 0.7603 ... 0.6769]     <Topic 1>\n            ...]\n\n        topic_scores: array of float, shape(num_topics)\n            For each topic the cosine similarity to the search keywords will be\n            returned.\n\n        topic_nums: array of int, shape(num_topics)\n            The unique number of every topic will be returned.\n        \"\"\"\n        if keywords_neg is None:\n            keywords_neg = []\n\n        self._validate_num_topics(num_topics, reduced)\n        keywords, keywords_neg = self._validate_keywords(keywords, keywords_neg)\n        word_vecs = self._words2word_vectors(keywords)\n        neg_word_vecs = self._words2word_vectors(keywords_neg)\n        combined_vector = self._get_combined_vec(word_vecs, neg_word_vecs)\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n\n            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors_reduced,\n                                                                      combined_vector, num_topics)\n            topic_words = [self.topic_words_reduced[topic] for topic in topic_nums]\n            word_scores = [self.topic_word_scores_reduced[topic] for topic in topic_nums]\n\n        else:\n            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors,\n                                                                      combined_vector, num_topics)\n            topic_words = [self.topic_words[topic] for topic in topic_nums]\n            word_scores = [self.topic_word_scores[topic] for topic in topic_nums]\n\n        return topic_words, word_scores, topic_scores, topic_nums\n\n    def search_documents_by_documents(self, doc_ids, num_docs, doc_ids_neg=None, return_documents=True,\n                                      use_index=False, ef=None):\n        \"\"\"\n        Semantic similarity search of documents.\n\n        The most semantically similar documents to the semantic combination of\n        document ids provided will be returned. If negative document ids are\n        provided, the documents will be semantically dissimilar to those\n        document ids. Documents will be ordered by decreasing similarity. This\n        method finds the closest document vectors to the provided documents\n        averaged.\n\n        Parameters\n        ----------\n        doc_ids: List of int, str\n            Unique ids of document. If ids were not given, the index of\n            document in the original corpus.\n\n        doc_ids_neg: (Optional) List of int, str\n            Unique ids of document. If ids were not given, the index of\n            document in the original corpus.\n\n        num_docs: int\n            Number of documents to return.\n\n        return_documents: bool (Optional default True)\n            Determines if the documents will be returned. If they were not\n            saved in the model they will also not be returned.\n\n        use_index: bool (Optional default False)\n            If index_documents method has been called, setting this to True\n            will speed up search for models with large number of documents.\n\n        ef: int (Optional default None)\n            Higher ef leads to more accurate but slower search. This value\n            must be higher than num_docs.\n\n            For more information see:\n            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n\n        Returns\n        -------\n        documents: (Optional) array of str, shape(num_docs)\n            The documents in a list, the most similar are first.\n\n            Will only be returned if the documents were saved and if\n            return_documents is set to True.\n\n        doc_scores: array of float, shape(num_docs)\n            Semantic similarity of document to keywords. The cosine similarity\n            of the document and average of keyword vectors.\n\n        doc_ids: array of int, shape(num_docs)\n            Unique ids of documents. If ids were not given to the model, the\n            index of the document in the model will be returned.\n        \"\"\"\n        if doc_ids_neg is None:\n            doc_ids_neg = []\n\n        self._validate_num_docs(num_docs)\n        self._validate_doc_ids(doc_ids, doc_ids_neg)\n\n        doc_indexes = self._get_document_indexes(doc_ids)\n        doc_indexes_neg = self._get_document_indexes(doc_ids_neg)\n\n        if use_index:\n            self._check_document_index_status()\n            document_vectors = self._get_document_vectors()\n            doc_vecs = [document_vectors[ind] for ind in doc_indexes]\n            doc_vecs_neg = [document_vectors[ind] for ind in doc_indexes_neg]\n            combined_vector = self._get_combined_vec(doc_vecs, doc_vecs_neg)\n            return self.search_documents_by_vector(combined_vector, num_docs, return_documents=return_documents,\n                                                   use_index=True, ef=ef)\n\n        if self.embedding_model == 'doc2vec':\n            sim_docs = self.model.docvecs.most_similar(positive=doc_indexes,\n                                                       negative=doc_indexes_neg,\n                                                       topn=num_docs)\n            doc_indexes = [doc[0] for doc in sim_docs]\n            doc_scores = np.array([doc[1] for doc in sim_docs])\n        else:\n            doc_vecs = [self.document_vectors[ind] for ind in doc_indexes]\n            doc_vecs_neg = [self.document_vectors[ind] for ind in doc_indexes_neg]\n            combined_vector = self._get_combined_vec(doc_vecs, doc_vecs_neg)\n\n            num_res = min(num_docs + len(doc_indexes) + len(doc_indexes_neg),\n                          self._get_document_vectors().shape[0])\n\n            # don't return documents that were searched\n            search_doc_indexes = list(doc_indexes) + list(doc_indexes_neg)\n            doc_indexes, doc_scores = self._search_vectors_by_vector(self._get_document_vectors(),\n                                                                     combined_vector, num_res)\n            res_indexes = [index for index, doc_ind in enumerate(doc_indexes)\n                           if doc_ind not in search_doc_indexes][:num_docs]\n            doc_indexes = doc_indexes[res_indexes]\n            doc_scores = doc_scores[res_indexes]\n\n        doc_ids = self._get_document_ids(doc_indexes)\n\n        if self.documents is not None and return_documents:\n            documents = self.documents[doc_indexes]\n            return documents, doc_scores, doc_ids\n        else:\n            return doc_scores, doc_ids\n\n    def generate_topic_wordcloud(self, topic_num, background_color=\"black\", reduced=False):\n        \"\"\"\n        Create a word cloud for a topic.\n\n        A word cloud will be generated and displayed. The most semantically\n        similar words to the topic will have the largest size, less similar\n        words will be smaller. The size is determined using the cosine distance\n        of the word vectors from the topic vector.\n\n        Parameters\n        ----------\n        topic_num: int\n            The topic number to search.\n\n        background_color : str (Optional, default='white')\n            Background color for the word cloud image. Suggested options are:\n                * white\n                * black\n\n        reduced: bool (Optional, default False)\n            Original topics are used by default. If True the\n            reduced topics will be used.\n\n        Returns\n        -------\n        A matplotlib plot of the word cloud with the topic number will be\n        displayed.\n\n        \"\"\"\n\n        if reduced:\n            self._validate_hierarchical_reduction()\n            self._validate_topic_num(topic_num, reduced)\n            word_score_dict = dict(zip(self.topic_words_reduced[topic_num],\n                                       softmax(self.topic_word_scores_reduced[topic_num])))\n        else:\n            self._validate_topic_num(topic_num, reduced)\n            word_score_dict = dict(zip(self.topic_words[topic_num],\n                                       softmax(self.topic_word_scores[topic_num])))\n\n        plt.figure(figsize=(16, 4),\n                   dpi=200)\n        plt.axis(\"off\")\n        plt.imshow(\n            WordCloud(width=1600,\n                      height=400,\n                      background_color=background_color).generate_from_frequencies(word_score_dict))\n        plt.title(\"Topic \" + str(topic_num), loc='left', fontsize=25, pad=20)\n"
        ],
        "test_patch": "",
        "patch_preview": "From 027502dc6d8b0694a7121533b739586ebcebc164 Mon Sep 17 00:00:00 2001\nFrom: Carsten Behring <carsten.behring@efsa.europa.eu>\nDate: Thu, 29 Apr 2021 22:12:19 +0200\nSubject: [PATCH 1/2] added verbose logging\n\n---\n top2vec/Top2Vec.py | 55 +++++++++++++++++++++++++++++++++++-----------\n 1 file changed, 42 insertions(+), 13 deletions(-)\n\ndiff --git a/top2vec/Top2Vec.py b/top2vec/Top2Vec.py\nindex 6c25c66..b18e8bb 100644\n--- a/top2vec/Top2Vec.py\n+++ b/top2vec/Top2Vec.py\n@@ -17,6 +17,10 @@\n from sklear"
      },
      "patch": {
        "length": 8400,
        "files_changed": 2,
        "lines_added": 43,
        "lines_deleted": 14,
        "net_change": 29,
        "changed_files": [
          {
            "file": "top2vec/Top2Vec.py",
            "added": 42,
            "deleted": 13
          },
          {
            "file": "top2vec/Top2Vec.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 23,
        "total_lines": 50340,
        "total_bytes": 7529742,
        "python_files": 5,
        "python_lines": 2763,
        "file_extensions": {
          "": 1,
          ".txt": 1,
          ".py": 5,
          ".md": 2,
          ".ipynb": 1,
          ".rst": 2,
          ".png": 8,
          ".svg": 3
        },
        "largest_files": [
          {
            "path": "images/hdbscan_docs.png",
            "size": 2216276,
            "lines": 16222,
            "extension": ".png"
          },
          {
            "path": "images/umap_docs.png",
            "size": 1829900,
            "lines": 11823,
            "extension": ".png"
          },
          {
            "path": "images/topic48.png",
            "size": 574108,
            "lines": 3825,
            "extension": ".png"
          },
          {
            "path": "images/topic61.png",
            "size": 553297,
            "lines": 3669,
            "extension": ".png"
          },
          {
            "path": "images/topic9.png",
            "size": 524354,
            "lines": 3504,
            "extension": ".png"
          },
          {
            "path": "images/topic29.png",
            "size": 525545,
            "lines": 3389,
            "extension": ".png"
          },
          {
            "path": "images/topic21.png",
            "size": 506434,
            "lines": 3164,
            "extension": ".png"
          },
          {
            "path": "top2vec/Top2Vec.py",
            "size": 88062,
            "lines": 2156,
            "extension": ".py"
          },
          {
            "path": "images/restful-top2vec.png",
            "size": 262082,
            "lines": 1485,
            "extension": ".png"
          },
          {
            "path": "top2vec/tests/test_top2vec.py",
            "size": 21871,
            "lines": 479,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 23,
        "files_changed_count": 2,
        "files_changed_ratio": 0.08695652173913043,
        "total_lines_in_repo": 50340,
        "lines_added": 43,
        "lines_deleted": 14,
        "net_lines_changed": 29,
        "lines_changed_ratio": 0.001132300357568534,
        "pr_body_length": 148,
        "commit_message_length": 18,
        "python_file_count": 5,
        "python_line_count": 2763
      }
    },
    {
      "tar_file_name": "epinna#weevely3#pull#168",
      "repo_name": "epinna#weevely3#pull#168",
      "success": true,
      "error": null,
      "commit": {
        "sha": "49639d4f8e2ac1811d878bcd473df826d6cd1ae4",
        "message": "add tests",
        "author": {
          "name": "Lucien A",
          "email": "lu.aubert84@gmail.com",
          "date": "2023-06-14T00:18:53Z"
        },
        "html_url": "https://github.com/epinna/weevely3/commit/49639d4f8e2ac1811d878bcd473df826d6cd1ae4",
        "api_url": "https://api.github.com/repos/epinna/weevely3/commits/49639d4f8e2ac1811d878bcd473df826d6cd1ae4"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/epinna#weevely3#pull#168",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/epinna#weevely3#pull#168.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/epinna#weevely3#pull#168/source_code"
      },
      "pr": {
        "number": 168,
        "title": "Pass unit tests on both PHP 7 and 8",
        "body": "- [x] Update Docker container\r\n    - [x] Install Sury's PHP repositories\r\n    - [x] Install both PHP 7.4 and 8.2 (FPM)\r\n    - [x] Add Apache config to handle `*.phar` files with PHP 8.2\r\n    - [x] Update `entrypoint.sh` to generate `agent.php` (for 7.4) and `agent.phar` (for 8.2)\r\n    - [x] Update `run.sh` to process all tests for the two generated agents \r\n- [x] Fix `file_tar` module\r\n    - [x] Remove `EasyTar.class`\r\n    - [x] Reimplement module using native `Phar` object (available since 5.3)\r\n- [x] Fix `sql_console` module\r\n- [x] Add printed info in `unittest` base class to show what PHP version a failing test was running on",
        "state": "closed",
        "created_at": "2023-06-15T15:11:54Z",
        "updated_at": "2023-06-16T20:58:50Z",
        "merged_at": "2023-06-15T16:08:18Z",
        "html_url": "https://github.com/epinna/weevely3/pull/168",
        "user": "ZanyMonk",
        "additions": 142,
        "deletions": 276,
        "changed_files": 13,
        "commits": 1
      },
      "swebench": {
        "instance_id": "epinna_weevely3-168",
        "repo": "/epinna/weevely3",
        "base_commit": "49639d4f8e2ac1811d878bcd473df826d6cd1ae4",
        "problem_statement": {},
        "edit_files": [
          "modules/file/_tar/EasyTar.class.php",
          "modules/file/_tar/php_tar.tpl",
          "modules/file/tar.py",
          "modules/sql/console.py",
          "modules/system/info.py",
          "tests/base_test.py",
          "tests/docker/000-default.conf",
          "tests/docker/Dockerfile",
          "tests/docker/entrypoint.sh",
          "tests/run.sh",
          "tests/test_file_tar.py",
          "tests/test_sql_console.py",
          "tests/test_system_info.py"
        ],
        "oracle_files": [
          "/**-------------------------------------------------\n | EasyTar.class V0.8 -  by Alban LOPEZ\n | Copyright (c) 2007 Alban LOPEZ\n | Email bugs/suggestions to alban.lopez+easytar@gmail.com\n +--------------------------------------------------\n | This file is part of EasyArchive.class V0.9.\n | EasyArchive is free software: you can redistribute it and/or modify\n | it under the terms of the GNU General Public License as published by\n | the Free Software Foundation, either version 3 of the License, or\n | (at your option) any later version.\n | EasyArchive is distributed in the hope that it will be useful,\n | but WITHOUT ANY WARRANTY; without even the implied warranty of\n | MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n | See the GNU General Public License for more details on http://www.gnu.org/licenses/\n +--------------------------------------------------\n http://www.phpclasses.org/browse/package/4239.html **/\nclass tar\n{ /* http://www.mkssoftware.com/docs/man4/tar.4.asp */\n/**\n// You can use this class like that.\n$test = new tar;\n$test->makeTar('./','./toto.Tar');\nvar_export($test->infosTar('./toto.Tar'));\n$test->extractTar('./toto.Tar', './new/');\n**/\n\tfunction infosTar ($src, $data=true)\n\t{\n\t\tif ($this->is_tar($src))\n\t\t{\n\t\t\tdie('pwd is '.getcwd());\n\t\t\tfile_put_contents ($tmp=TMP_CACHE_LOCATION.'/~tmp('.microtime().').tar', $src);\n\t\t\t$src = $tmp;\n\t\t}\n\t\t$ptr = fopen($src, 'r');\n\t\twhile (!feof($ptr))\n\t\t{\n\t\t\t$infos = $this->readTarHeader ($ptr);\n\t\t\tif ($infos['name'])\n\t\t\t{\n\t\t\t\tif (!$data) unset($infos['data']);\n\t\t\t\t$result[$infos['name']]=$infos;\n\t\t\t}\n\t\t}\n\t\tif (is_file($tmp)) unlink($tmp);\n\t\treturn $result;\n\t}\n\tfunction makeTar($src, $dest=false)\n\t{\n\t\t$src = is_array($src) ? $src : array($src);\n\t\t$src = array_map('realpath', $src);\n\t\tforeach ($src as $item) {\n          // @weevely3\n          // Skip empty file to avoid creating empty archvies\n          if($item)\n\t\t\t  $Tar .= $this->addTarItem($item.((is_dir($item) && substr($item, -1)!='/')?'/':''), dirname($item).'/');\n        }\n\n        // @weevely3\n        // When empty, returns\n        if (empty($Tar)) return $Tar;\n\t\t$Tar = str_pad($Tar, floor((strlen($Tar) + 10240 - 1) / 10240) * 10240, \"\\0\");\n\t\tif (empty($dest)) return $Tar;\n\t\telseif (file_put_contents($dest, $Tar)) return $dest;\n\t\telse false;\n\t}\n\tfunction extractTar ($src, $dest)\n\t{\n\t\t$ptr = fopen($src, 'r');\n\t\twhile (!feof($ptr))\n\t\t{\n            $infos = $this->readTarHeader ($ptr);\n\n            // @weevely3\n            // Sanitize name field from unprintable char, and join name and dest folder properly\n            $infos['name'] = ltrim(preg_replace('/[\\x00-\\x1F\\x80-\\xFF]/', '', $infos['name']), DIRECTORY_SEPARATOR);\n            $dest = trim($dest, DIRECTORY_SEPARATOR) . DIRECTORY_SEPARATOR;\n\n\t\t\tif ($infos['type']=='5' && @mkdir($dest.$infos['name'], 0775, true))\n\t\t\t  $result[]=$dest.$infos['name'];\n\t\t\telseif (($infos['type']=='0' || $infos['type']==chr(0)) && file_put_contents($dest.$infos['name'], $infos['data'])) {\n\t\t\t  $result[]=$dest.$infos['name'];\n\t\t\t// @weevely3 \n\t\t\t// Better to not set 0775 on already existant folders \n\t\t\t//if ($infos)\n\t\t\t  chmod($dest.$infos['name'], 0775);\n\t\t\t}\n\t\t}\n\t\treturn $result;\n\t}\n\tfunction is_tar($str)\n\t{\n\t\t$block = substr($str,0, 512);\n\t\tif (strlen($block)!=512) return false;\n\t\t$realchecksum = octdec(substr($str,148,8));\n\t\t$checksum = 0;\n\t\t$block = substr_replace($block, '        ', 148, 8);\n\t\tfor ($i = 0; $i < 512; $i++)\n\t\t\t$checksum += ord(substr($block, $i, 1));\n\t\tif ($realchecksum==$checksum) return true;\n\t\treturn false;\n\t}\n\tfunction tarHeader512($infos)\n\t{ /* http://www.mkssoftware.com/docs/man4/tar.4.asp */\n\t\t$bigheader = $header = '';\n\t\tif (strlen($infos['name100'])>100)\n\t\t{\n\t\t\t$bigheader = pack(\"a100a8a8a8a12a12a8a1a100a6a2a32a32a8a8a155a12\",\n\t\t\t\t'././@LongLink','0000000','0000000','0000000',\n\t\t\t\tsprintf(\"%011o\", strlen($infos['name100'])),'00000000000',\n\t\t\t\t'        ', 'L', '', 'ustar ', '0',\n\t\t\t\t$infos['userName32'],\n\t\t\t\t$infos['groupName32'],'','','','');\n\n\t\t\t$bigheader .= str_pad($infos['name100'], floor((strlen($infos['name100']) + 512 - 1) / 512) * 512, \"\\0\");\n\n\t\t\t$checksum = 0;\n\t\t\tfor ($i = 0; $i < 512; $i++)\n\t\t\t\t$checksum += ord(substr($bigheader, $i, 1));\n\t\t\t$bigheader = substr_replace($bigheader, sprintf(\"%06o\", $checksum).\"\\0 \", 148, 8);\n\t\t}\n\t\t$header = pack(\"a100a8a8a8a12a12a8a1a100a6a2a32a32a8a8a155a12\", // book the memorie area\n\t\t\tsubstr($infos['name100'],0,100),\t\t//  0 \t100 \tFile name\n\t\t\tstr_pad(substr(sprintf(\"%07o\",$infos['mode8']),-4), 7, '0', STR_PAD_LEFT),\t\t// 100 \t8 \t\tFile mode\n\t\t\tsprintf(\"%07o\", $infos['uid8']),\t\t// 108 \t8 \t\tOwner user ID\n\t\t\tsprintf(\"%07o\", $infos['gid8']),\t\t// 116 \t8 \t\tGroup user ID\n\t\t\tsprintf(\"%011o\", $infos['size12']),\t\t// 124 \t12 \t\tFile size in bytes\n\t\t\tsprintf(\"%011o\", $infos['mtime12']),\t// 136 \t12 \t\tLast modification time\n\t\t\t'        ',\t\t\t\t\t\t\t\t// 148 \t8 \t\tCheck sum for header block\n\t\t\t$infos['link1'],\t\t\t\t\t\t// 156 \t1 \t\tLink indicator / ustar Type flag\n\t\t\t$infos['link100'],\t\t\t\t\t\t// 157 \t100 \tName of linked file\n\t\t\t'ustar ',\t\t\t\t\t\t\t\t// 257 \t6 \t\tUSTAR indicator \"ustar\"\n\t\t\t' ',\t\t\t\t\t\t\t\t\t// 263 \t2 \t\tUSTAR version \"00\"\n\t\t\t$infos['userName32'],\t\t\t\t// 265 \t32 \t\tOwner user name\n\t\t\t$infos['groupName32'],\t\t\t\t// 297 \t32 \t\tOwner group name\n\t\t\t'',\t\t\t\t\t\t\t\t\t// 329 \t8 \t\tDevice major number\n\t\t\t'',\t\t\t\t\t\t\t\t\t// 337 \t8 \t\tDevice minor number\n\t\t\t$infos['prefix155'],\t\t\t\t\t// 345 \t155 \tFilename prefix\n\t\t\t'');\t\t\t\t\t\t\t\t\t// 500 \t12 \t\t??\n\n\t\t$checksum = 0;\n\t\tfor ($i = 0; $i < 512; $i++)\n\t\t\t$checksum += ord(substr($header, $i, 1));\n\t\t$header = substr_replace($header, sprintf(\"%06o\", $checksum).\"\\0 \", 148, 8);\n\n\t\treturn $bigheader.$header;\n\t}\n\tfunction addTarItem ($item, $racine)\n\t{\n\t\t$infos['name100'] = str_replace($racine, '', $item);\n\t\tlist (, , $infos['mode8'], , $infos['uid8'], $infos['gid8'], , , , $infos['mtime12'] ) = stat($item);\n\t\t$infos['size12'] = is_dir($item) ? 0 : filesize($item);\n\t\t$infos['link1'] = is_link($item) ? 2 : is_dir ($item) ? 5 : 0;\n\t\t$infos['link100'] == 2 ? readlink($item) : \"\";\n\n\t\t\t$a=function_exists('posix_getpwuid')?posix_getpwuid (fileowner($item)):array('name'=>'Unknown');\n\t\t$infos['userName32'] = $a['name'];\n\n\t\t\t$a=function_exists('posix_getgrgid')?posix_getgrgid (filegroup($item)):array('name'=>'Unknown');\n\t\t$infos['groupName32'] = $a['name'];\n\t\t$infos['prefix155'] = '';\n\n\t\t$header = $this->tarHeader512($infos);\n\t\t$data = str_pad(file_get_contents($item), floor(($infos['size12'] + 512 - 1) / 512) * 512, \"\\0\");\n\t\tif (is_dir($item))\n\t\t{\n\t\t\t$lst = scandir($item);\n\t\t\tarray_shift($lst); // remove  ./  of $lst\n\t\t\tarray_shift($lst); // remove ../  of $lst\n\t\t\tforeach ($lst as $subitem)\n\t\t\t\t$sub .= $this->addTarItem($item.$subitem.(is_dir($item.$subitem)?'/':''), $racine);\n\t\t}\n\t\treturn $header.$data.$sub;\n\t}\n\tfunction readTarHeader ($ptr)\n\t{\n\t\t$block = fread($ptr, 512);\n\t\tif (strlen($block)!=512) return false;\n\t\t$hdr = unpack (\"a100name/a8mode/a8uid/a8gid/a12size/a12mtime/a8checksum/a1type/a100symlink/a6magic/a2version/a32uname/a32gname/a8devmajor/a8devminor/a155prefix/a12temp\", $block);\n\t\t\t$hdr['mode']=$hdr['mode']+0;\n\t\t\t$hdr['uid']=octdec($hdr['uid']);\n\t\t\t$hdr['gid']=octdec($hdr['gid']);\n\t\t\t$hdr['size']=octdec($hdr['size']);\n\t\t\t$hdr['mtime']=octdec($hdr['mtime']);\n\t\t\t$hdr['checksum']=octdec($hdr['checksum']);\n\t\t$checksum = 0;\n\t\t$block = substr_replace($block, '        ', 148, 8);\n\t\tfor ($i = 0; $i < 512; $i++)\n\t\t\t$checksum += ord(substr($block, $i, 1));\n\t\tif (isset($hdr['name']) && $hdr['checksum']==$checksum)\n\t\t{\n\t\t\tif ($hdr['name']=='././@LongLink' && $hdr['type']=='L')\n\t\t\t{\n\t\t\t\t$realName = substr(fread($ptr, floor(($hdr['size'] + 512 - 1) / 512) * 512), 0, $hdr['size']-1);\n\t\t\t\t$hdr2 = $this->readTarHeader ($ptr);\n\t\t\t\t$hdr2['name'] = $realName;\n\t\t\t\treturn $hdr2;\n\t\t\t}\n\t\t\telseif (strtolower(substr($hdr['magic'], 0, 5) == 'ustar'))\n\t\t\t{\n\t\t\t\tif ($hdr['size']>0)\n\t\t\t\t\t$hdr['data'] = substr(fread($ptr, floor(($hdr['size'] + 512 - 1) / 512) * 512), 0, $hdr['size']);\n\t\t\t\telse $hdr['data'] = '';\n\t\t\t\treturn $hdr;\n\t\t\t}\n\t\t\telse return false;\n\t\t}\n\t\telse return false;\n\t}\n}\n",
          "<%include file=\"EasyTar.class.php\"/>\n\n$f='set_time_limit'&&is_callable($f)&&$f(0);\n$f='ini_set'&&is_callable($f)&&$f('max_execution_time', 0);\n$a = new tar;\n\n$z = '${ rtar }';\n\n$fs=array (\n% for f in rfiles:\n        '${ f }',\n% endfor\n);\n\n## Here decompress\n% if decompress:\n\nif(!file_exists($z) || !is_readable($z)) {\n    print(\"Skipping file '$z', check existance and permission\");\n}\nelse {\n        $a->extractTar($z, '${ rfiles[0] if rfiles and rfiles[0] else '.' }');\n}\n\n## Here compress\n% else:\n\nif(file_exists($z)) {\n    print(\"File '$z' already exists, skipping compressing\");\n}\nelse {\n    $a->makeTar($fs, $z);\n}\n\n## Since makeTar does not complain for missing $z, just double\n## check the existance of the zipped file and print generic error message\nif(!file_exists($z)) {\n    print(\"File '$z' not created, check existance and permission\");\n}\n\n% endif\n",
          "from core.vectors import PhpFile, ModuleExec\nfrom core.module import Module\nfrom core import messages\nfrom core import modules\nfrom core.loggers import log\nimport os\n\nclass Tar(Module):\n\n    \"\"\"Compress or expand tar archives.\"\"\"\n\n    aliases = [ 'tar' ]\n\n    def init(self):\n\n        self.register_info(\n            {\n                'author': [\n                    'Emilio Pinna'\n                ],\n                'license': 'GPLv3'\n            }\n        )\n\n        self.register_vectors(\n            [\n            PhpFile(\n              payload_path = os.path.join(self.folder, 'php_tar.tpl'),\n              name = 'php_tar',\n            )\n            ]\n        )\n\n        self.register_arguments([\n          { 'name' : 'rtar', 'help' : 'Remote Tar file' },\n          { 'name' : 'rfiles', 'help' : 'Remote files to compress. If decompressing, set destination folder.', 'nargs' : '+' },\n          { 'name' : '--decompress', 'action' : 'store_true', 'default' : False, 'help' : 'Simulate tar -x' },\n          { 'name' : '-z', 'action' : 'store_true', 'default' : False, 'help' : 'Simulate tar -xz for gzip compressed archives' },\n          { 'name' : '-j', 'action' : 'store_true', 'default' : False, 'help' : 'Simulate tar -xj for bzip2 compressed archives' },\n        ])\n\n    def run(self):\n\n        if self.args.get('z'):\n            ModuleExec('file_gzip', [ '--keep', '--decompress', self.args['rtar'] ]).run()\n            self.args['rtar'] = '.'.join(self.args['rtar'].split('.')[:-1])\n        elif self.args.get('j'):\n            ModuleExec('file_bzip2', [ '--keep', '--decompress', self.args['rtar'] ]).run()\n            self.args['rtar'] = '.'.join(self.args['rtar'].split('.')[:-1])\n\n        # The correct execution returns something only on errors\n        result_err = self.vectors.get_result(\n            name = 'php_tar',\n            format_args = self.args,\n        )\n\n        if result_err:\n            log.warn(result_err)\n            return\n\n        return True\n",
          "import utils\nfrom core import messages\nfrom core.loggers import log\nfrom core.module import Module\nfrom core.vectors import PhpCode\nimport re\n\nclass Console(Module):\n    \"\"\"Execute SQL query or run console.\"\"\"\n\n    def init(self):\n\n        self.register_info(\n            {\n                'author': [\n                    'Emilio Pinna'\n                ],\n                'license': 'GPLv3'\n            }\n        )\n\n        self.register_vectors(\n            [\n                PhpCode(\n                    \"\"\"if($s=mysqli_connect('${host}','${user}','${passwd}')){$r=mysqli_query($s,'${query}');if($r){$f=mysqli_fetch_fields($r);foreach($f as $v){echo $v->name.'${linsep}';};echo '${colsep}';while($c=mysqli_fetch_row($r)){echo implode('${linsep}',$c);echo '${linsep}${colsep}';}};}echo '${errsep}'.@mysqli_connect_error().' '.@mysqli_error($s);@mysqli_close($s);\"\"\",\n                    name='mysql',\n                ),\n                PhpCode(\n                    \"\"\"if($s=mysqli_connect('${host}','${user}','${passwd}','${database}')){$r=mysqli_query($s,'${query}');if($r){$f=mysqli_fetch_fields($r);foreach($f as $v){echo $v->name.'${linsep}';};echo '${colsep}';while($c=mysqli_fetch_row($r)){echo implode('${linsep}',$c);echo '${linsep}${colsep}';}};}echo '${errsep}'.@mysqli_connect_error().' '.@mysqli_error($s);@mysqli_close($s);\"\"\",\n                    name='mysql_database',\n                ),\n                PhpCode(\n                    \"\"\"$r=mysqli_query('${query}');if($r){while($c=mysqli_fetch_row($r)){foreach($c as $key=>$value){echo $value.'${linsep}';}echo '${colsep}';}};mysqli_close();echo '${errsep}'.@mysqli_connect_error().' '.@mysqli_error();\"\"\",\n                    name=\"mysql_fallback\"\n                ),\n                PhpCode(\n                    \"\"\"if(pg_connect('host=${host} user=${user} password=${passwd}')){$r=pg_query('${query}');if($r){while($c=pg_fetch_row($r)){foreach($c as $key=>$value){echo $value.'${linsep}';}echo '${colsep}';}};pg_close();}echo '${errsep}'.@pg_last_error();\"\"\",\n                    name=\"pgsql\"\n                ),\n                PhpCode(\n                    \"\"\"if(pg_connect('host=${host} user=${user} dbname=${database} password=${passwd}')){$r=pg_query('${query}');if($r){while($c=pg_fetch_row($r)){foreach($c as $key=>$value){echo $value.'${linsep}';}echo '${colsep}';}};pg_close();}echo '${errsep}'.@pg_last_error();\"\"\",\n                    name=\"pgsql_database\"\n                ),\n                PhpCode(\n                    \"\"\"$r=pg_query('${query}');if($r){while($c=pg_fetch_row($r)){foreach($c as $key=>$value){echo $value.'${linsep}';} echo '${colsep}';}};pg_close();echo '${errsep}'.@pg_last_error();\"\"\",\n                    name=\"pgsql_fallback\"\n                ),\n            ]\n        )\n\n        self.register_arguments([\n            {'name': '-user', 'help': 'SQL username'},\n            {'name': '-passwd', 'help': 'SQL password'},\n            {'name': '-host', 'help': 'Db host (default: localhost)', 'nargs': '?', 'default': 'localhost'},\n            {'name': '-dbms', 'help': 'Db type', 'choices': ('mysql', 'pgsql'), 'default': 'mysql'},\n            {'name': '-database', 'help': 'Database name'},\n            {'name': '-query', 'help': 'Execute a single query'},\n            {'name': '-encoding', 'help': 'Db text encoding', 'default': 'utf-8'},\n        ])\n\n    def _query(self, vector, args):\n\n        # Randomly generate separators\n        colsep = '----%s' % utils.strings.randstr(6).decode('utf-8')\n        linsep = '----%s' % utils.strings.randstr(6).decode('utf-8')\n        errsep = '----%s' % utils.strings.randstr(6).decode('utf-8')\n\n        args.update(\n            {'colsep': colsep, 'linsep': linsep, 'errsep': errsep}\n        )\n\n        # Escape ' in query strings\n        self.args['query'] = self.args['query'].replace('\\\\', '\\\\\\\\').replace('\\'', '\\\\\\'')\n\n        result = self.vectors.get_result(vector, args)\n\n        # we wan't the result to be unicode, but depending on the source\n        # of the data, it could be encoded differently\n        try:\n            result = str(result)\n        except UnicodeError:\n            result = str(result.decode(args.get('encoding')))\n        # If there is not errstr, something gone really bad (e.g. functions not callable)\n        if errsep not in result:\n            return {\n                'error': messages.module_sql_console.unexpected_response,\n                'result': []\n            }\n        else:\n\n            # Split result by errsep\n            result, error = result.split(errsep)\n\n            return {\n                'error': error,\n                'result': [\n                    line.split(linsep) for line\n                    in result.replace(linsep + colsep, colsep).split(colsep) if line\n                ]\n            }\n\n    def run(self):\n\n        # The vector name is given by the db type\n        vector = self.args.get('dbms')\n        encoding = self.args.get('encoding')\n        database = self.args.get('database')\n\n        # Check if PostgreSQL and database is given\n        if database:\n            vector += '_database'\n        else:\n            # And by the user and password presence\n            vector += (\n                '' if self.args.get('user') and self.args.get('passwd')\n                else '_fallback'\n            )\n\n        # If the query is set, just execute it\n        if self.args.get('query'):\n            return self._query(vector, self.args)\n\n        # Else, start the console.\n        # Check credentials\n        self.args['query'] = (\n            'SELECT USER;' if vector.startswith('pgsql')\n            else 'SELECT USER();'\n        )\n\n        result = self._query(vector, self.args)\n        if not result['result']:\n            return result\n\n        if result['result'][0]:\n            user = result['result'][1][0]\n\n        # Console loop\n        while True:\n\n            query = input('{}:{} SQL> '.format(user, database)).strip()\n\n            if not query:\n                continue\n            if query in ['quit', '\\q', 'exit']:\n                return {\"result\": \"sql_console exited.\", \"error\": False}\n            m = re.findall(\"^use\\s+([\\w_]+);?$\", query, re.IGNORECASE)\n            if len(m):\n                database = m[0]\n                self.args.update({\"database\": database})\n                print(\"databse changed to {}.\".format(database))\n                continue\n            self.args['query'] = query\n            result = self._query(vector, self.args)\n            self.print_result(result)\n\n    def print_result(self, result):\n\n        if result['error']:\n            log.info(result['error'])\n\n        if result['result']:\n            if type(result['result']) is str:\n                log.info(result[\"result\"])\n            else:\n                Module.print_result(self, result['result'], header=True)\n\n        elif not result['error']:\n\n            log.warn('%s %s' % (\n                messages.module_sql_console.no_data,\n                messages.module_sql_console.check_credentials)\n                     )\n\n            command_last_chars = utils.prettify.shorten(\n                self.args['query'].rstrip(),\n                keep_trailer=10\n            )\n\n            if (command_last_chars and command_last_chars[-1] != ';'):\n                log.warn(messages.module_sql_console.missing_sql_trailer_s % command_last_chars)\n",
          "from core.vectors import PhpCode\nfrom core.module import Module\nfrom core import messages\nfrom core.loggers import log\nfrom core import modules\nimport utils\n\n\nclass Info(Module):\n\n    \"\"\"Collect system information.\"\"\"\n\n    aliases = [\n        'whoami',\n        'hostname',\n        'pwd',\n        'uname'\n    ]\n\n    default_provider = 'http://ifconfig.me/'\n\n    extended_vectors = [\n        'server_soft',\n        'server_ip',\n        'ini_path',\n        'tmp_path',\n        'free_space',\n        'dir_sep'\n    ]\n\n    def init(self):\n\n        self.register_info(\n            {\n                'author': [\n                    'Emilio Pinna'\n                ],\n                'license': 'GPLv3'\n            }\n        )\n\n        self.register_vectors(\n            [\n                PhpCode(\"print(@$_SERVER['DOCUMENT_ROOT']);\", 'document_root'),\n                PhpCode(\"@print(getcwd());\", 'pwd'),\n                PhpCode(\"print(dirname(__FILE__));\", 'script_folder'),\n                PhpCode(\"print(@$_SERVER['SCRIPT_NAME']);\", 'script'),\n                PhpCode(\"print(@$_SERVER['PHP_SELF']);\", 'php_self'),\n                PhpCode(\"\"\"\n                    if(is_callable('posix_getpwuid')&&is_callable('posix_geteuid')) {\n                        $u=@posix_getpwuid(@posix_geteuid());\n                        if($u){\n                            $u=$u['name'];\n                        } else {\n                            $u=getenv('username');\n                        }\n                        print($u);\n                    }\n                \"\"\", 'whoami'),\n                PhpCode(\"print(@gethostname());\", 'hostname'),\n                PhpCode(\"$v=@ini_get('open_basedir'); if($v) print($v);\", 'open_basedir'),\n                PhpCode(\"print(@ini_get('disable_functions'));\", 'disable_functions'),\n                PhpCode(\"print(@php_ini_loaded_file());\", 'ini_path'),\n                PhpCode(\"print(@sys_get_temp_dir());\", 'tmp_path'),\n                PhpCode(\"print(@disk_free_space(__DIR__));\", 'free_space',\n                        postprocess=lambda x: utils.prettify.format_size(int(x))),\n                PhpCode(\"print(@ini_get('safe_mode') ? 1 : 0);\", 'safe_mode',\n                        postprocess=lambda x: True if x == '1' else False),\n                PhpCode(\"print(@$_SERVER['SERVER_SOFTWARE']);\", 'server_soft'),\n                PhpCode(\"print(@php_uname());\", 'uname'),\n                PhpCode(\"print(@php_uname('s') . ' ' . @php_uname('m'));\", 'os'),\n                PhpCode(\"print(@$_SERVER['REMOTE_ADDR']);\", 'client_ip'),\n                PhpCode(\"print(@file_get_contents('${provider}'));\", 'server_ip'),\n                PhpCode(\"print(@$_SERVER['SERVER_NAME']);\", 'server_name'),\n                PhpCode(\"print(@ini_get('max_execution_time'));\", 'max_execution_time',\n                        postprocess=lambda x: int(x) if x and x.isdigit() else False),\n                PhpCode(\"@print(DIRECTORY_SEPARATOR);\", 'dir_sep'),\n                PhpCode(\"\"\"\n                    $v='';\n                    if(function_exists('phpversion')) {\n                        $v=phpversion();\n                    } elseif(defined('PHP_VERSION')) {\n                        $v=PHP_VERSION;\n                    } elseif(defined('PHP_VERSION_ID')) {\n                        $v=PHP_VERSION_ID;\n                    }\n                    print($v);\n                \"\"\", 'php_version')\n            ]\n        )\n\n        self.register_arguments([\n            {'name': '-info',\n             'help': 'Select information (possible values are: %s)' % (', '.join(self.vectors.get_names())),\n             'choices': self.vectors.get_names(),\n             'default': [],\n             'nargs': '+',\n             'metavar': 'arg'},\n            {'name': '-extended',\n             'help': 'Get more info. Slower. (extended info: %s)' % (', '.join(self.extended_vectors)),\n             'action': 'store_true',\n             'default': False},\n            {'name': '-provider',\n             'help': 'The URL to get server_ip from (default: %s)' % self.default_provider,\n             'metavar': 'http://...',\n             'default': self.default_provider}\n        ])\n\n    def run(self):\n\n        vectors = self.args.get('info')\n\n        if not vectors and not self.args.get('extended'):\n            vectors = [i for i in self.vectors.get_names() if i not in self.extended_vectors]\n\n        result = self.vectors.get_results(\n            names=vectors,\n            results_to_store=(\n                'whoami',\n                'hostname',\n                'dir_sep',\n                'os',\n                'script_folder',\n                'server_ip'\n            ),\n            format_args={\n                'provider': self.args.get('provider')\n            }\n        )\n\n        # Returns a string when a single information is requested,\n        # else returns a dictionary containing all the results.\n        info = self.args.get('info')\n        if info and len(info) == 1:\n            return result[info[0]]\n        else:\n            return result\n\n    def run_alias(self, args, cmd):\n\n        if self.session['default_shell'] != 'shell_sh':\n            log.debug(messages.module.running_the_alias_s % self.name)\n            return self.run_cmdline('-info %s' % cmd)\n        else:\n            modules.loaded['shell_sh'].run_cmdline(\n                '%s -- %s' % (cmd, args)\n            )\n",
          "from unittest import TestCase\nfrom . import config\n\nclass BaseTest(TestCase):\n    \n    url = config.url\n    password = config.password\n    path = config.agent",
          "",
          "FROM python:3\n\nWORKDIR /app\n\nRUN apt-get update\nRUN apt-get -y install apache2 php libapache2-mod-php expect php-curl php-gd php-mysql zip unzip php-zip php-bz2 vim\n\nRUN bash -c \"debconf-set-selections <<< 'mysql-server mysql-server/root_password password root'\"\nRUN bash -c \"debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password root'\"\nRUN apt-get -y install default-mysql-server\n\nCOPY requirements.txt /app/\nRUN pip install -r /app/requirements.txt\nRUN pip install testfixtures coverage # Additional library for testing\n\n# Add unprivileged testuser:testuser user\nRUN echo 'testuser:$1$xyz$iqgi.17OXQwhicZgFC1OZ.:1001:1002:,,,:/home/testuser:/bin/bash' >> /etc/passwd\n\nENTRYPOINT \"/app/tests/docker/entrypoint.sh\"\n",
          "#!/bin/bash\nset -e -x\n\nBASE_FOLDER=\"`python -c 'from tests import config;print(config.base_folder)'`\"\nAGENT=\"`python -c 'from tests import config;print(config.agent)'`\"\nURL=\"`python -c 'from tests import config;print(config.url)'`\"\nPWD=\"`python -c 'from tests import config;print(config.password)'`\"\n\n# Generic environment setting install\nmkdir -p \"$BASE_FOLDER\"\nfind -type f -name '*.pyc' -exec rm -f {} \\;\npython ./weevely.py generate \"$PWD\" \"$AGENT\"\n\nservice apache2 start\nservice mariadb start\n\n# Grant root user to connect from network socket\nmysql -u root --password=root -e \"grant all privileges on *.* to 'root'@'localhost' identified by 'root'; flush privileges;\"\n\nsleep 10000\n",
          "#!/bin/bash\nset -e\n \n# Load docker defaults and check conf\ndocker info\n\n# Change folder to the root folder\nPARENTDIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\"/../ && pwd )\"\ncd $PARENTDIR\n\n# Delete any instance if previously existent\ndocker rm -f httpbin-inst || echo ''\ndocker rm -f weevely-inst || echo ''\ndocker network rm weevely-testnet || echo ''\n\n# Create the network\ndocker network create weevely-testnet\n\n# Run httpbin container for local testing\ndocker pull kennethreitz/httpbin\ndocker run -p 8888:80 --net=weevely-testnet --rm --name httpbin-inst -d kennethreitz/httpbin\n\n# Wait until the http server is serving\nuntil $(curl --output /dev/null --silent --head http://localhost:8888/); do\n    sleep 1\ndone\n\n# Build weevely container\ndocker build -f tests/docker/Dockerfile . -t weevely\ndocker run  --rm --net=weevely-testnet --name weevely-inst -v `pwd`:/app/ -p 80:80 -d weevely \n\n# Wait until the http server is serving\nuntil $(curl --output /dev/null --silent --head http://localhost/); do\n    sleep 1\ndone\n\nif [ -z \"$1\" ]\n  then\n    docker exec -it weevely-inst python -m unittest discover ./tests/ \"test_*.py\"\nelif [ \"$1\" = \"bash\" ]\n  then\n    docker exec -it weevely-inst /bin/bash\nelse\n    docker exec -it weevely-inst python -m unittest discover ./tests/ \"test_$1.py\"\nfi\n\ndocker rm -f weevely-inst \ndocker rm -f httpbin-inst\ndocker network rm weevely-testnet",
          "from testfixtures import log_capture\nfrom tests.base_test import BaseTest\nfrom tests import config\nfrom core.sessions import SessionURL\nfrom core import modules\nimport utils\nfrom core import messages\nimport subprocess\nimport os\n\ndef setUpModule():\n    subprocess.check_output(\"\"\"\nBASE_FOLDER=\"{config.base_folder}/test_file_tar/\"\nrm -rf \"$BASE_FOLDER\"\n\nmkdir -p \"$BASE_FOLDER/dir1/dir2/dir3/dir4\"\n\necho -n 1 > \"$BASE_FOLDER/dir1/f1\"\necho -n 1 > \"$BASE_FOLDER/dir1/dir2/f2\"\necho -n 1 > \"$BASE_FOLDER/dir1/dir2/dir3/f3\"\necho -n 1 > \"$BASE_FOLDER/dir1/dir2/dir3/dir4/f4\"\n\ntar cf \"$BASE_FOLDER/test_0.tar\" -C \"$BASE_FOLDER\" \"dir1/\"\n\necho -n 1 > \"$BASE_FOLDER/f5\"\n\nrm -rf \"$BASE_FOLDER/dir1\"\n\nchown www-data: -R \"$BASE_FOLDER/\"\n\n\"\"\".format(\nconfig = config\n), shell=True)\n\nclass FileTar(BaseTest):\n\n    folders_rel = [\n        'test_file_tar/dir1',\n        'test_file_tar/dir1/dir2',\n        'test_file_tar/dir1/dir2/dir3',\n        'test_file_tar/dir1/dir2/dir3/dir4',\n    ]\n    folders_abs = [ \n        os.path.join(config.base_folder, f) \n        for f in folders_rel \n    ]\n    \n    files_rel = [\n        'test_file_tar/dir1/f1',\n        'test_file_tar/dir1/dir2/f2',\n        'test_file_tar/dir1/dir2/dir3/f3',\n        'test_file_tar/dir1/dir2/dir3/dir4/f4',\n    ]\n    files_abs = [ \n        os.path.join(config.base_folder, f) \n        for f in files_rel \n    ]\n        \n    tars_rel = [\n        'test_file_tar/test_0.tar'\n    ]\n    tars_abs = [ \n        os.path.join(config.base_folder, f) \n        for f in tars_rel \n    ]\n    \n    other_file_rel = 'test_file_tar/f5'\n    other_file_abs = os.path.join(config.base_folder, other_file_rel) \n    \n    def setUp(self):\n        self.session = SessionURL(\n                    self.url,\n                    self.password,\n                    volatile = True\n                    )\n\n        modules.load_modules(self.session)\n\n        self.run_argv = modules.loaded['file_tar'].run_argv\n\n    def test_compress_decompress(self):\n\n        # Uncompress test.tar\n        self.assertTrue(self.run_argv([\"--decompress\", self.tars_rel[0], 'test_file_tar/' ]));\n        for file in self.files_abs:\n            self.assertEqual(subprocess.check_output(\"cat %s\" % file, shell=True), b'1')\n        for folder in self.folders_abs:\n            subprocess.check_call('stat -c %%a \"%s\"' % folder, shell=True)\n\n        # Compress it again giving starting folder\n        self.assertTrue(self.run_argv(['test_file_tar/test_1.tar', self.folders_rel[0]]));\n        self.tars_rel.append('test_file_tar/test_1.tar')\n        self.tars_abs.append(os.path.join(config.base_folder, self.tars_rel[-1]))\n\n        # Uncompress the new archive and recheck\n        self.assertTrue(self.run_argv([\"--decompress\", 'test_file_tar/test_1.tar', 'test_file_tar/']));\n        for file in self.files_abs:\n            self.assertEqual(subprocess.check_output(\"cat %s\" % file, shell=True), b'1')\n        for folder in self.folders_abs:\n            subprocess.check_call('stat -c %%a \"%s\"' % folder, shell=True)\n\n    def test_compress_multiple(self):\n    \n        # Uncompress test.tar\n        self.assertTrue(self.run_argv([\"--decompress\", self.tars_rel[0], 'test_file_tar/' ]));\n        for file in self.files_abs:\n            self.assertEqual(subprocess.check_output(\"cat %s\" % file, shell=True), b'1')\n        for folder in self.folders_abs:\n            subprocess.check_call('stat -c %%a \"%s\"' % folder, shell=True)\n    \n        # Create a new tar adding also other_file\n        self.assertTrue(self.run_argv(['test_file_tar/test_2.tar', self.folders_rel[0], self.other_file_rel]));\n        self.tars_rel.append('test_file_tar/test_2.tar')\n        self.tars_abs.append(os.path.join(config.base_folder, self.tars_rel[-1]))\n    \n        # Remove all the files\n        subprocess.check_output(\"rm -rf %s\" % self.folders_abs[0], shell=True)\n        subprocess.check_output(\"rm %s\" % self.other_file_abs, shell=True)\n\n        # Uncompress the new archive and recheck\n        self.assertTrue(self.run_argv([\"--decompress\", 'test_file_tar/test_2.tar', 'test_file_tar/']));\n        for file in self.files_abs:\n            self.assertEqual(subprocess.check_output(\"cat %s\" % file, shell=True), b'1')\n        for folder in self.folders_abs:\n            subprocess.check_call('stat -c %%a \"%s\"' % folder, shell=True)\n\n        self.assertEqual(subprocess.check_output(\"cat %s\" % self.other_file_abs, shell=True), b'1')\n\n    @log_capture()\n    def test_already_exists(self, log_captured):\n    \n            # Create a new tar with other_file, with the name test_0.tar\n        self.assertIsNone(self.run_argv(['test_file_tar/test_0.tar', self.other_file_rel]));\n        self.assertEqual(log_captured.records[-1].msg,\n                         \"File 'test_file_tar/test_0.tar' already exists, skipping compressing\")\n    \n\n    @log_capture()\n    def test_unexistant_decompress(self, log_captured):\n    \n        self.assertIsNone(self.run_argv([\"--decompress\", 'bogus', '.']));\n        self.assertEqual(log_captured.records[-1].msg,\n                         \"Skipping file 'bogus', check existance and permission\")\n    \n    \n    @log_capture()\n    def test_unexistant_compress(self, log_captured):\n    \n        self.assertIsNone(self.run_argv(['bogus.tar', 'bogus']));\n        self.assertEqual(log_captured.records[-1].msg,\n                         \"File 'bogus.tar' not created, check existance and permission\")\n    \n",
          "from tests.base_test import BaseTest\nfrom testfixtures import log_capture\nfrom core import modules\nfrom core.sessions import SessionURL\nfrom core import messages\nfrom tests import config\nimport unittest\nimport subprocess\nimport os\n\ndef setUpModule():\n    try:\n        # This workaround fixes https://github.com/docker/for-linux/issues/72\n        subprocess.check_output(\"\"\"find /var/lib/mysql -type f -exec touch {} \\; && service mariadb start\"\"\", shell=True)\n    except Exception as e:\n        print('[!] Failed mysql')\n        print(subprocess.check_output(\"\"\"grep \"\" /var/log/mysql/*\"\"\", shell=True))\n        raise\n\nclass MySQLConsole(BaseTest):\n\n    def setUp(self):\n        self.session = SessionURL(self.url, self.password, volatile = True)\n        modules.load_modules(self.session)\n\n        self.run_argv = modules.loaded['sql_console'].run_argv\n        self.run_cmdline = modules.loaded['sql_console'].run_cmdline\n\n    @unittest.skipIf(not config.sql_autologin,\n                    \"Autologin is not set\")\n    def test_autologin(self):\n        self.assertEqual(self.run_argv(['-query', \"select 'A';\"]), { 'error' : '', 'result' : [[\"A\"], [\"A\"]] })\n        self.assertEqual(self.run_argv(['-query', 'select @@hostname;'])['error'], '')\n        self.assertEqual(self.run_argv(['-query', 'show databases;'])['error'], '')\n\n    @log_capture()\n    @unittest.skipIf(not config.sql_autologin,\n                    \"Autologin is not set\")\n    def test_wrongcommand(self, log_captured):\n        # Wrong command\n        self.assertEqual(self.run_cmdline('-query bogus')['result'], [])\n\n        # Checking if the error message start about the missing comma is ok\n        self.assertEqual('%s %s' % (messages.module_sql_console.no_data,\n                                    messages.module_sql_console.check_credentials),\n                         log_captured.records[-2].msg)\n\n\n    def test_wronglogin(self):\n\n        wrong_login = '-user bogus -passwd bogus -query \"select \\'A\\';\"'\n\n        # Using run_cmdline to test the outputs\n        self.assertIn('Access denied for user', self.run_cmdline(wrong_login)['error'])\n\n    def test_login(self):\n\n        login = ['-user', config.sql_user, '-passwd', config.sql_passwd ]\n\n        self.assertEqual(self.run_argv(login + [ '-query', \"select 'A';\"]), { 'error' : ' ', 'result' :  [['A'], ['A']] })\n        self.assertEqual(self.run_argv(login + ['-query', 'select @@hostname;'])['error'], ' ')\n        self.assertEqual(self.run_argv(login + ['-query', 'show databases;'])['error'], ' ')\n\n        # The user is returned in the form `[[ user@host ]]`\n        self.assertEqual(\n            self.run_argv(login + ['-query', 'SELECT USER();'])['result'][1][0][:len(config.sql_user)],\n            config.sql_user\n        )\n        self.assertEqual(\n            self.run_argv(login + ['-query', 'SELECT CURRENT_USER();'])['result'][1][0][:len(config.sql_user)],\n            config.sql_user\n        )\n",
          "from tests.base_test import BaseTest\nfrom testfixtures import log_capture\nfrom core.weexceptions import ArgparseError\nfrom core import modules\nfrom core.sessions import SessionURL\nfrom core import messages\nimport logging\nimport os\n\nclass SystemInfo(BaseTest):\n\n    def setUp(self):\n        session = SessionURL(self.url, self.password, volatile = True)\n        modules.load_modules(session)\n\n        self.run_argv = modules.loaded['system_info'].run_argv\n\n    @log_capture()\n    def test_commands(self, log_captured):\n\n        # Get all infos, returns a dict\n        vectors_names = [v.name for v in modules.loaded['system_info'].vectors ]\n        self.assertEqual(set(self.run_argv([ '-extended' ]).keys()), set(vectors_names));\n\n        # Get just one info, returns a string\n        self.assertEqual(\n                      os.path.split(self.run_argv([\"-info\", \"script\"]))[1],\n                      os.path.split(self.path)[1]\n        );\n\n        # Pass unexistant info\n        self.assertRaises(ArgparseError, self.run_argv, [\"-info\", \"BOGUS\"]);\n"
        ],
        "test_patch": "",
        "patch_preview": "From 06c14960e3b2522f4a2aadc73b3e5b3399da9990 Mon Sep 17 00:00:00 2001\nFrom: Lucien A <lu.aubert84@gmail.com>\nDate: Thu, 15 Jun 2023 12:58:56 +0200\nSubject: [PATCH] pass unit tests on both versions of php\n\n---\n modules/file/_tar/EasyTar.class.php | 209 ----------------------------\n modules/file/_tar/php_tar.tpl       |  65 +++++----\n modules/file/tar.py                 |   5 +-\n modules/sql/console.py              |   6 +-\n modules/system/info.py              |   7 +-\n tests/base_test.py        "
      },
      "patch": {
        "length": 28561,
        "files_changed": 13,
        "lines_added": 142,
        "lines_deleted": 276,
        "net_change": -134,
        "changed_files": [
          {
            "file": "modules/file/_tar/EasyTar.class.php",
            "added": 0,
            "deleted": 209
          },
          {
            "file": "modules/file/_tar/php_tar.tpl",
            "added": 39,
            "deleted": 26
          },
          {
            "file": "modules/file/tar.py",
            "added": 2,
            "deleted": 3
          },
          {
            "file": "modules/sql/console.py",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "modules/system/info.py",
            "added": 4,
            "deleted": 3
          },
          {
            "file": "tests/base_test.py",
            "added": 10,
            "deleted": 3
          },
          {
            "file": "tests/docker/000-default.conf",
            "added": 38,
            "deleted": 0
          },
          {
            "file": "tests/docker/Dockerfile",
            "added": 11,
            "deleted": 1
          },
          {
            "file": "tests/docker/entrypoint.sh",
            "added": 7,
            "deleted": 0
          },
          {
            "file": "tests/run.sh",
            "added": 9,
            "deleted": 7
          },
          {
            "file": "tests/test_file_tar.py",
            "added": 16,
            "deleted": 16
          },
          {
            "file": "tests/test_sql_console.py",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "tests/test_system_info.py",
            "added": 0,
            "deleted": 2
          }
        ]
      },
      "issue_comments": [
        {
          "id": 1593370653,
          "body": "Fixes #167 ",
          "user": "ZanyMonk",
          "created_at": "2023-06-15T16:21:31Z",
          "html_url": "https://github.com/epinna/weevely3/pull/168#issuecomment-1593370653"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 137,
        "total_lines": 21035,
        "total_bytes": 955104,
        "python_files": 106,
        "python_lines": 11562,
        "file_extensions": {
          ".md": 2,
          ".1": 1,
          ".py": 106,
          "": 2,
          ".txt": 2,
          ".sh": 3,
          ".tpl": 16,
          ".php": 5
        },
        "largest_files": [
          {
            "path": "utils/_http/user-agents.txt",
            "size": 393450,
            "lines": 4226,
            "extension": ".txt"
          },
          {
            "path": "utils/ipaddr.py",
            "size": 56500,
            "lines": 1820,
            "extension": ".py"
          },
          {
            "path": "modules/sql/_dump/mysqldump.tpl",
            "size": 42335,
            "lines": 1489,
            "extension": ".tpl"
          },
          {
            "path": "modules/net/_phpproxy/poxy.php",
            "size": 52503,
            "lines": 1425,
            "extension": ".php"
          },
          {
            "path": "LICENSE",
            "size": 35147,
            "lines": 674,
            "extension": ""
          },
          {
            "path": "core/module.py",
            "size": 12128,
            "lines": 398,
            "extension": ".py"
          },
          {
            "path": "modules/net/proxy.py",
            "size": 12369,
            "lines": 369,
            "extension": ".py"
          },
          {
            "path": "core/terminal.py",
            "size": 11683,
            "lines": 356,
            "extension": ".py"
          },
          {
            "path": "core/vectors.py",
            "size": 9286,
            "lines": 305,
            "extension": ".py"
          },
          {
            "path": "weevely.1",
            "size": 4291,
            "lines": 288,
            "extension": ".1"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 137,
        "files_changed_count": 13,
        "files_changed_ratio": 0.0948905109489051,
        "total_lines_in_repo": 21035,
        "lines_added": 142,
        "lines_deleted": 276,
        "net_lines_changed": -134,
        "lines_changed_ratio": 0.01987164250059425,
        "pr_body_length": 636,
        "commit_message_length": 9,
        "python_file_count": 106,
        "python_line_count": 11562
      }
    },
    {
      "tar_file_name": "erikrose#blessings#pull#103",
      "repo_name": "erikrose#blessings#pull#103",
      "success": true,
      "error": null,
      "commit": {
        "sha": "086f2366dcffc840e9c8d10525a936ea41d7fbfa",
        "message": "Merge pull request #102 from erikrose/blessed-integration-remove-interruptable\n\nremove interruptable=True, following python3.5",
        "author": {
          "name": "Jeff Quast",
          "email": "contact@jeffquast.com",
          "date": "2015-04-15T07:33:29Z"
        },
        "html_url": "https://github.com/erikrose/blessings/commit/086f2366dcffc840e9c8d10525a936ea41d7fbfa",
        "api_url": "https://api.github.com/repos/erikrose/blessings/commits/086f2366dcffc840e9c8d10525a936ea41d7fbfa"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/erikrose#blessings#pull#103",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/erikrose#blessings#pull#103.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/erikrose#blessings#pull#103/source_code"
      },
      "pr": {
        "number": 103,
        "title": "blessed-integration: update bin/*.py examples and add examples.rst",
        "body": "Fix all `bin/*.py` example programs (they used blessed's, not the `keystroke*` renames used in blessings), and add docs/examples.rst describing and linking to each.\n",
        "state": "closed",
        "created_at": "2015-04-15T07:34:42Z",
        "updated_at": "2015-04-15T08:37:32Z",
        "merged_at": "2015-04-15T08:37:30Z",
        "html_url": "https://github.com/erikrose/blessings/pull/103",
        "user": "jquast",
        "additions": 498,
        "deletions": 330,
        "changed_files": 10,
        "commits": 2
      },
      "swebench": {
        "instance_id": "erikrose_blessings-103",
        "repo": "/erikrose/blessings",
        "base_commit": "086f2366dcffc840e9c8d10525a936ea41d7fbfa",
        "problem_statement": {},
        "edit_files": [
          "docs/further.rst",
          ".prospector.yaml",
          "bin/editor.py",
          "bin/keymatrix.py",
          "bin/on_resize.py",
          "bin/progress_bar.py",
          "bin/tprint.py",
          "bin/worms.py",
          "docs/examples.rst",
          "docs/index.rst"
        ],
        "oracle_files": [
          "Further Reading\n===============\n\nAs a developer's API, blessings is often bundled with frameworks and toolsets\nthat dive deeper into Terminal I/O programming than :class:`~.Terminal` offers.\nHere are some recommended readings to help you along:\n\n- `terminfo(5)\n  <http://invisible-island.net/ncurses/man/terminfo.5.html>`_\n  manpage of your preferred posix-like operating system. The capabilities\n  available as attributes of :class:`~.Terminal` are directly mapped to those\n  listed in the column **Cap-name**.\n\n- `termios(4)\n  <http://www.openbsd.org/cgi-bin/man.cgi?query=termios&apropos=0&sektion=4>`_\n  of your preferred posix-like operating system.\n\n- `The TTY demystified\n  <http://www.linusakesson.net/programming/tty/index.php>`_\n  by Linus Ã…kesson.\n\n- `A Brief Introduction to Termios\n  <https://blog.nelhage.com/2009/12/a-brief-introduction-to-termios/>`_ by\n  Nelson Elhage.\n\n- Richard Steven's `Advance Unix Programming\n  <http://www.amazon.com/exec/obidos/ISBN=0201563177/wrichardstevensA/>`_\n  (\"AUP\") provides two very good chapters, \"Terminal I/O\" and\n  \"Pseudo Terminals\".\n\n- GNU's `The Termcap Manual\n  <https://www.gnu.org/software/termutils/manual/termcap-1.3/html_mono/termcap.html>`_\n  by Richard M. Stallman.\n\n- `Chapter 4 <http://compsci.hunter.cuny.edu/~sweiss/course_materials/unix_lecture_notes/chapter_04.pdf>`_\n  of CUNY's course material for *Introduction to System Programming*, by\n  `Stewart Weiss <http://compsci.hunter.cuny.edu/~sweiss/>`_\n\n- `Chapter 11\n  <http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap11.html>`_\n  of the IEEE Open Group Base Specifications Issue 7, \"General Terminal\n  Interface\"\n\n- The GNU C Library documentation, section `Low-Level Terminal Interface\n  <http://www.gnu.org/software/libc/manual/html_mono/libc.html#toc-Low_002dLevel-Terminal-Interface-1>`_\n\n- The source code of many popular terminal emulators.  If there is ever any\n  question of \"the meaning of a terminal capability\", or whether or not your\n  preferred terminal emulator actually handles them, read the source!\n\n  These are often written in the C language, and directly map the\n  \"Control Sequence Inducers\" (CSI, literally ``\\x1b[`` for most modern\n  terminal types) emitted by most terminal capabilities to an action in a\n  series of ``case`` switch statements.\n\n  - Many modern libraries are now based on `libvte\n    <https://github.com/GNOME/vte>`_ (or just 'vte'): Gnome Terminal,\n    sakura, Terminator, Lilyterm, ROXTerm, evilvte, Termit, Termite, Tilda,\n    tinyterm, lxterminal.\n  - `Thomas E. Dickey <http://invisible-island.net/>`_ has been maintaining\n    `xterm <http://invisible-island.net/xterm/xterm.html>`_, as well as a\n    primary maintainer of many related packages such as `ncurses\n    <http://invisible-island.net/ncurses/ncurses.html>`_ for quite a long\n    while.  There is often speculation and misinformation among developers of\n    terminal emulators and programs that interact with them.  Thomas Dickey's\n    analysis is always thorough and complete.\n  - xterm, urxvt, SyncTerm, and EtherTerm.\n  - There are far too many to name, Chose one you like!\n\n\n- The source code of the tty(4), pty(4), and the given \"console driver\" for\n  any posix-like operating system.  If you search thoroughly enough, you will\n  eventually discover a terminal sequence decoder, usually a ``case`` switch\n  that translates ``\\x1b[0m`` into a \"reset color\" action towards the video\n  driver.  Though ``tty.c`` is linked here (the only kernel file common among\n  them), it is probably not the most interesting, but it can get you started:\n\n     - `FreeBSD <https://github.com/freebsd/freebsd/blob/master/sys/kern/tty.c>`_\n     - `OpenBSD <http://cvsweb.openbsd.org/cgi-bin/cvsweb/~checkout~/src/sys/kern/tty.c?content-type=text/plain>`_\n     - `Illumos (Solaris) <https://github.com/illumos/illumos-gate/blob/master/usr/src/uts/common/io/tty_common.c>`_\n     - `Minix <https://github.com/minix3/minix/blob/master/minix/drivers/tty/tty/tty.c>`_\n     - `Linux <https://github.com/torvalds/linux/blob/master/drivers/tty/n_tty.c>`_\n\n  The TTY driver is a great introduction to Kernel and Systems programming,\n  because familiar components may be discovered and experimented with.  It is\n  available on all operating systems (except windows), and because of its\n  critical nature, examples of efficient file I/O, character buffers (often\n  implemented as \"ring buffers\") and even fine-grained kernel locking can be\n  found.\n\n- `termcap & terminfo (O'Reilly Nutshell)\n  <http://www.amazon.com/termcap-terminfo-OReilly-Nutshell-Linda/dp/0937175226>`_\n  by Linda Mui, Tim O'Reilly, and John Strang.\n\n- Note that System-V systems, also known as `Unix98\n  <http://en.wikipedia.org/wiki/Single_UNIX_Specification>`_ (SunOS, HP-UX,\n  AIX and others) use a `Streams <http://en.wikipedia.org/wiki/STREAMS>`_\n  interface.  On these systems, the `ioctl(2)\n  <http://pubs.opengroup.org/onlinepubs/009695399/functions/ioctl.html>`_\n  interface provides the ``PUSH`` and ``POP`` parameters to communicate with\n  a Streams device driver, which differs significantly from Linux and BSD.\n\n  Many of these systems provide compatible interfaces for Linux, but they may\n  not always be as complete as the counterpart they emulate, most especially\n  in regards to managing pseudo-terminals.\n\n\n",
          "inherits:\n    - strictness_veryhigh\n\nignore-patterns:\n  - (^|/)\\..+\n  - ^docs/\n  - ^build/\n  # ignore these, their quality does not so much matter.\n  - ^bin/\n  - ^blessings/tests/\n  - ^tools/\n  # not maintained\n  - ^fabfile.py\n\ntest-warnings: true\n\noutput-format: grouped\n\ndodgy:\n    # Looks at Python code to search for things which look \"dodgy\"\n    # such as passwords or git conflict artifacts\n    run: true\n\nfrosted:\n    # static analysis\n    run: true\n\nmccabe:\n    # complexity checking.\n    run: true\n    disable:\n        # Terminal.__init__ is too complex (14)\n        - MC0001\n\npep257:\n    # docstring checking\n    run: true\n\npep8:\n    # style checking\n    run: true\n\npyflakes:\n    # preferring 'frosted' instead (a fork of)\n    run: false\n\npylint:\n    # static analysis and then some\n    run: true\n    options:\n        # pytest module has dynamically assigned functions,\n        # raising errors such as: E1101: Module 'pytest' has\n        # no 'mark' member\n        ignored-classes: pytest\n    disable:\n        # Access to a protected member _sugar of a client class\n        - protected-access\n        # blessings.Terminal: Too many instance attributes (12/7)\n        - too-many-instance-attributes\n        # blessings.Terminal: Too many public methods (25/20)\n        - too-many-public-methods\n        # blessings.Terminal: Too many branches (13/12)\n        - too-many-branches\n        # blessings.sequences.get_wontmove_sequence_patterns:\n        #   Used builtin function 'map'\n        - bad-builtin\n\n\npyroma:\n    # checks setup.py\n    run: true\n\nvulture:\n    # this tool does a good job of finding unused code, which isn't terribly\n    # useful from an API perspective, disable.\n    run: false\n\n# vim: noai:ts=4:sw=4\n",
          "#!/usr/bin/env python3\n# Dumb full-screen editor. It doesn't save anything but to the screen.\n#\n# \"Why wont python let me read memory\n#  from screen like assembler? That's dumb.\" -hellbeard\n#\n# This program makes example how to deal with a keypad for directional\n# movement, with both numlock on and off.\nfrom __future__ import division, print_function\nimport collections\nimport functools\n\nfrom blessings import Terminal\n\n\necho = lambda text: (\n    functools.partial(print, end='', flush=True)(text))\n\necho_yx = lambda cursor, text: (\n    echo(cursor.term.move(cursor.y, cursor.x) + text))\n\nCursor = collections.namedtuple('Point', ('y', 'x', 'term'))\n\nabove = lambda csr, n: (\n    Cursor(y=max(0, csr.y - n),\n           x=csr.x,\n           term=csr.term))\n\nbelow = lambda csr, n: (\n    Cursor(y=min(csr.term.height - 1, csr.y + n),\n           x=csr.x,\n           term=csr.term))\n\nright_of = lambda csr, n: (\n    Cursor(y=csr.y,\n           x=min(csr.term.width - 1, csr.x + n),\n           term=csr.term))\n\nleft_of = lambda csr, n: (\n    Cursor(y=csr.y,\n           x=max(0, csr.x - n),\n           term=csr.term))\n\nhome = lambda csr: (\n    Cursor(y=csr.y,\n           x=0,\n           term=csr.term))\n\nend = lambda csr: (\n    Cursor(y=csr.y,\n           x=csr.term.width - 1,\n           term=csr.term))\n\nbottom = lambda csr: (\n    Cursor(y=csr.term.height - 1,\n           x=csr.x,\n           term=csr.term))\n\ntop = lambda csr: (\n    Cursor(y=1,\n           x=csr.x,\n           term=csr.term))\n\ncenter = lambda csr: Cursor(\n    csr.term.height // 2,\n    csr.term.width // 2,\n    csr.term)\n\n\nlookup_move = lambda inp_code, csr, term: {\n    # arrows, including angled directionals\n    csr.term.KEY_END: below(left_of(csr, 1), 1),\n    csr.term.KEY_KP_1: below(left_of(csr, 1), 1),\n\n    csr.term.KEY_DOWN: below(csr, 1),\n    csr.term.KEY_KP_2: below(csr, 1),\n\n    csr.term.KEY_PGDOWN: below(right_of(csr, 1), 1),\n    csr.term.KEY_LR: below(right_of(csr, 1), 1),\n    csr.term.KEY_KP_3: below(right_of(csr, 1), 1),\n\n    csr.term.KEY_LEFT: left_of(csr, 1),\n    csr.term.KEY_KP_4: left_of(csr, 1),\n\n    csr.term.KEY_CENTER: center(csr),\n    csr.term.KEY_KP_5: center(csr),\n\n    csr.term.KEY_RIGHT: right_of(csr, 1),\n    csr.term.KEY_KP_6: right_of(csr, 1),\n\n    csr.term.KEY_HOME: above(left_of(csr, 1), 1),\n    csr.term.KEY_KP_7: above(left_of(csr, 1), 1),\n\n    csr.term.KEY_UP: above(csr, 1),\n    csr.term.KEY_KP_8: above(csr, 1),\n\n    csr.term.KEY_PGUP: above(right_of(csr, 1), 1),\n    csr.term.KEY_KP_9: above(right_of(csr, 1), 1),\n\n    # shift + arrows\n    csr.term.KEY_SLEFT: left_of(csr, 10),\n    csr.term.KEY_SRIGHT: right_of(csr, 10),\n    csr.term.KEY_SDOWN: below(csr, 10),\n    csr.term.KEY_SUP: above(csr, 10),\n\n    # carriage return\n    csr.term.KEY_ENTER: home(below(csr, 1)),\n}.get(inp_code, csr)\n\n\ndef readline(term, width=20):\n    # a rudimentary readline function\n    string = u''\n    while True:\n        inp = term.inkey()\n        if inp.code == term.KEY_ENTER:\n            break\n        elif inp.code == term.KEY_ESCAPE or inp == chr(3):\n            string = None\n            break\n        elif not inp.is_sequence and len(string) < width:\n            string += inp\n            echo(inp)\n        elif inp.code in (term.KEY_BACKSPACE, term.KEY_DELETE):\n            string = string[:-1]\n            echo('\\b \\b')\n    return string\n\n\ndef save(screen, fname):\n    if not fname:\n        return\n    with open(fname, 'w') as fp:\n        cur_row = cur_col = 0\n        for (row, col) in sorted(screen):\n            char = screen[(row, col)]\n            while row != cur_row:\n                cur_row += 1\n                cur_col = 0\n                fp.write(u'\\n')\n            while col > cur_col:\n                cur_col += 1\n                fp.write(u' ')\n            fp.write(char)\n            cur_col += 1\n        fp.write(u'\\n')\n\n\ndef redraw(term, screen, start=None, end=None):\n    if start is None and end is None:\n        echo(term.clear)\n        start, end = (Cursor(y=min([y for (y, x) in screen or [(0, 0)]]),\n                             x=min([x for (y, x) in screen or [(0, 0)]]),\n                             term=term),\n                      Cursor(y=max([y for (y, x) in screen or [(0, 0)]]),\n                             x=max([x for (y, x) in screen or [(0, 0)]]),\n                             term=term))\n    lastcol, lastrow = -1, -1\n    for row, col in sorted(screen):\n        if (row >= start.y and row <= end.y and\n                col >= start.x and col <= end.x):\n            if col >= term.width or row >= term.height:\n                # out of bounds\n                continue\n            if not (row == lastrow and col == lastcol + 1):\n                # use cursor movement\n                echo_yx(Cursor(row, col, term), screen[row, col])\n            else:\n                # just write past last one\n                echo(screen[row, col])\n\n\ndef main():\n    term = Terminal()\n    csr = Cursor(0, 0, term)\n    screen = {}\n    with term.hidden_cursor(), \\\n            term.raw(), \\\n            term.location(), \\\n            term.fullscreen(), \\\n            term.keypad():\n        inp = None\n        while True:\n            echo_yx(csr, term.reverse(screen.get((csr.y, csr.x), u' ')))\n            inp = term.inkey()\n\n            if inp == chr(3):\n                # ^c exits\n                break\n\n            elif inp == chr(19):\n                # ^s saves\n                echo_yx(home(bottom(csr)),\n                        term.ljust(term.bold_white('Filename: ')))\n                echo_yx(right_of(home(bottom(csr)), len('Filename: ')), u'')\n                save(screen, readline(term))\n                echo_yx(home(bottom(csr)), term.clear_eol)\n                redraw(term=term, screen=screen,\n                       start=home(bottom(csr)),\n                       end=end(bottom(csr)))\n                continue\n\n            elif inp == chr(12):\n                # ^l refreshes\n                redraw(term=term, screen=screen)\n\n            n_csr = lookup_move(inp.code, csr, term)\n            if n_csr != csr:\n                # erase old cursor,\n                echo_yx(csr, screen.get((csr.y, csr.x), u' '))\n                csr = n_csr\n\n            elif not inp.is_sequence and inp.isprintable():\n                echo_yx(csr, inp)\n                screen[(csr.y, csr.x)] = inp.__str__()\n                n_csr = right_of(csr, 1)\n                if n_csr == csr:\n                    # wrap around margin\n                    n_csr = home(below(csr, 1))\n                csr = n_csr\n\nif __name__ == '__main__':\n    main()\n",
          "#!/usr/bin/env python\nfrom __future__ import division\nimport sys\n\nfrom blessings import Terminal\n\n\ndef main():\n    \"\"\"\n    Displays all known key capabilities that may match the terminal.\n    As each key is pressed on input, it is lit up and points are scored.\n    \"\"\"\n    term = Terminal()\n    score = level = hit_highbit = hit_unicode = 0\n    dirty = True\n\n    def refresh(term, board, level, score, inp):\n        sys.stdout.write(term.home + term.clear)\n        level_color = level % 7\n        if level_color == 0:\n            level_color = 4\n        bottom = 0\n        for keycode, attr in board.items():\n            sys.stdout.write(u''.join((\n                term.move(attr['row'], attr['column']),\n                term.color(level_color),\n                (term.reverse if attr['hit'] else term.bold),\n                keycode,\n                term.normal)))\n            bottom = max(bottom, attr['row'])\n        sys.stdout.write(term.move(term.height, 0)\n                         + 'level: %s score: %s' % (level, score,))\n        sys.stdout.flush()\n        if bottom >= (term.height - 5):\n            sys.stderr.write(\n                ('\\n' * (term.height // 2)) +\n                term.center(term.red_underline('cheater!')) + '\\n')\n            sys.stderr.write(\n                term.center(\"(use a larger screen)\") +\n                ('\\n' * (term.height // 2)))\n            sys.exit(1)\n        for row, inp in enumerate(inps[(term.height - (bottom + 2)) * -1:]):\n            sys.stdout.write(term.move(bottom + row+1))\n            sys.stdout.write('%r, %s, %s' % (inp.__str__() if inp.is_sequence\n                                             else inp, inp.code, inp.name, ))\n            sys.stdout.flush()\n\n    def build_gameboard(term):\n        column, row = 0, 0\n        board = dict()\n        spacing = 2\n        for keycode in sorted(term._keycodes.values()):\n            if (keycode.startswith('KEY_F')\n                    and keycode[-1].isdigit()\n                    and int(keycode[len('KEY_F'):]) > 24):\n                continue\n            if column + len(keycode) + (spacing * 2) >= term.width:\n                column = 0\n                row += 1\n            board[keycode] = {'column': column,\n                              'row': row,\n                              'hit': 0,\n                              }\n            column += len(keycode) + (spacing * 2)\n        return board\n\n    def add_score(score, pts, level):\n        lvl_multiplier = 10\n        score += pts\n        if 0 == (score % (pts * lvl_multiplier)):\n            level += 1\n        return score, level\n\n    gb = build_gameboard(term)\n    inps = []\n\n    with term.raw(), term.keypad(), term.location():\n        inp = term.inkey(timeout=0)\n        while inp != chr(3):\n            if dirty:\n                refresh(term, gb, level, score, inps)\n                dirty = False\n            inp = term.inkey(timeout=5.0)\n            dirty = True\n            if (inp.is_sequence and\n                    inp.name in gb and\n                    0 == gb[inp.name]['hit']):\n                gb[inp.name]['hit'] = 1\n                score, level = add_score(score, 100, level)\n            elif inp and not inp.is_sequence and 128 <= ord(inp) <= 255:\n                hit_highbit += 1\n                if hit_highbit < 5:\n                    score, level = add_score(score, 100, level)\n            elif inp and not inp.is_sequence and ord(inp) > 256:\n                hit_unicode += 1\n                if hit_unicode < 5:\n                    score, level = add_score(score, 100, level)\n            inps.append(inp)\n\n    with term.cbreak():\n        sys.stdout.write(term.move(term.height))\n        sys.stdout.write(\n            u'{term.clear_eol}Your final score was {score} '\n            u'at level {level}{term.clear_eol}\\n'\n            u'{term.clear_eol}\\n'\n            u'{term.clear_eol}You hit {hit_highbit} '\n            u' 8-bit characters\\n{term.clear_eol}\\n'\n            u'{term.clear_eol}You hit {hit_unicode} '\n            u' unicode characters.\\n{term.clear_eol}\\n'\n            u'{term.clear_eol}press any key\\n'.format(\n                term=term,\n                score=score, level=level,\n                hit_highbit=hit_highbit,\n                hit_unicode=hit_unicode)\n        )\n        term.inkey()\n\nif __name__ == '__main__':\n    main()\n",
          "#!/usr/bin/env python\n\"\"\"\nThis is an example application for the 'blessings' Terminal library for python.\n\nWindow size changes are caught by the 'on_resize' function using a traditional\nsignal handler.  Meanwhile, blocking keyboard input is displayed to stdout.\nIf a resize event is discovered, an empty string is returned by\nterm.keystroke() when interruptable is False, as it is here.\n\"\"\"\nimport signal\n\nfrom blessings import Terminal\n\n\nterm = Terminal()\n\n\ndef on_resize(sig, action):\n    # Its generally not a good idea to put blocking functions (such as print)\n    # within a signal handler -- if another SIGWINCH is recieved while this\n    # function blocks, an error will occur. In most programs, you'll want to\n    # set some kind of 'dirty' flag, perhaps by a Semaphore or global variable.\n    print('height={t.height}, width={t.width}\\r'.format(t=term))\n\nsignal.signal(signal.SIGWINCH, on_resize)\n\n# note that, a terminal driver actually writes '\\r\\n' when '\\n' is found, but\n# in raw mode, we are allowed to write directly to the terminal without the\n# interference of such driver -- so we must write \\r\\n ourselves; as python\n# will append '\\n' to our print statements, we simply end our statements with\n# \\r.\nwith term.raw():\n    print(\"press 'X' to stop.\\r\")\n    inp = None\n    while inp != 'X':\n        inp = term.inkey(interruptable=False)\n        print(repr(inp) + u'\\r')\n",
          "#!/usr/bin/env python\n\"\"\"\nThis is an example application for the 'blessings' Terminal library for python.\n\nThis isn't a real progress bar, just a sample \"animated prompt\" of sorts\nthat demonstrates the separate move_x() and move_y() functions, made\nmainly to test the `hpa' compatibility for 'screen' terminal type which\nfails to provide one, but blessings recognizes that it actually does, and\nprovides a proxy.\n\"\"\"\nfrom __future__ import print_function\nimport sys\n\nfrom blessings import Terminal\n\n\ndef main():\n    term = Terminal()\n    assert term.hpa(1) != u'', (\n        'Terminal does not support hpa (Horizontal position absolute)')\n\n    col, offset = 1, 1\n    with term.cbreak():\n        inp = None\n        print(\"press 'X' to stop.\")\n        sys.stderr.write(term.move(term.height, 0) + u'[')\n        sys.stderr.write(term.move_x(term.width) + u']' + term.move_x(1))\n        while inp != 'X':\n            if col >= (term.width - 2):\n                offset = -1\n            elif col <= 1:\n                offset = 1\n            sys.stderr.write(term.move_x(col) + u'.' if offset == -1 else '=')\n            col += offset\n            sys.stderr.write(term.move_x(col) + u'|\\b')\n            sys.stderr.flush()\n            inp = term.inkey(0.04)\n    print()\n\nif __name__ == '__main__':\n    main()\n",
          "#!/usr/bin/env python\n\nimport argparse\n\nfrom blessings import Terminal\n\n\nparser = argparse.ArgumentParser(\n    description='displays argument as specified style')\n\nparser.add_argument('style', type=str, help='style formatter')\nparser.add_argument('text', type=str, nargs='+')\n\n\nterm = Terminal()\nargs = parser.parse_args()\n\nstyle = getattr(term, args.style)\n\nprint(style(' '.join(args.text)))\n",
          "#!/usr/bin/env python\n\"\"\"\nThis is an example application for the 'blessings' Terminal library for python.\n\nIt is also an experiment in functional programming.\n\"\"\"\n\nfrom __future__ import division, print_function\nfrom collections import namedtuple\nfrom functools import partial\nfrom random import randrange\n\nfrom blessings import Terminal\n\n\n# python 2/3 compatibility, provide 'echo' function as an\n# alias for \"print without newline and flush\"\ntry:\n    echo = partial(print, end='', flush=True)\n    echo('begin.')\nexcept TypeError:\n    # TypeError: 'flush' is an invalid keyword argument for this function\n    import sys\n\n    def echo(object):\n        sys.stdout.write(u'{}'.format(object))\n        sys.stdout.flush()\n\n# a worm is a list of (y, x) segments Locations\nLocation = namedtuple('Point', ('y', 'x',))\n\n# a nibble is a (x,y) Location and value\nNibble = namedtuple('Nibble', ('location', 'value'))\n\n# A direction is a bearing, fe.\n# y=0, x=-1 = move right\n# y=1, x=0 = move down\nDirection = namedtuple('Direction', ('y', 'x',))\n\n# these functions return a new Location instance, given\n# the direction indicated by their name.\nLEFT = (0, -1)\nleft_of = lambda segment, term: Location(\n    y=segment.y,\n    x=max(0, segment.x - 1))\n\nRIGHT = (0, 1)\nright_of = lambda segment, term: Location(\n    y=segment.y,\n    x=min(term.width - 1, segment.x + 1))\n\nUP = (-1, 0)\nabove = lambda segment, term: Location(\n    y=max(0, segment.y - 1),\n    x=segment.x)\n\nDOWN = (1, 0)\nbelow = lambda segment, term: Location(\n    y=min(term.height - 1, segment.y + 1),\n    x=segment.x)\n\n# return a direction function that defines the new bearing for any matching\n# keyboard code of inp_code; otherwise, the function for the current bearing.\nnext_bearing = lambda term, inp_code, bearing: {\n    term.KEY_LEFT: left_of,\n    term.KEY_RIGHT: right_of,\n    term.KEY_UP: above,\n    term.KEY_DOWN: below,\n}.get(inp_code,\n      # direction function given the current bearing\n      {LEFT: left_of,\n       RIGHT: right_of,\n       UP: above,\n       DOWN: below}[(bearing.y, bearing.x)])\n\n\n# return new bearing given the movement f(x).\nchange_bearing = lambda f_mov, segment, term: Direction(\n    f_mov(segment, term).y - segment.y,\n    f_mov(segment, term).x - segment.x)\n\n# direction-flipped check, reject traveling in opposite direction.\nbearing_flipped = lambda dir1, dir2: (\n    (0, 0) == (dir1.y + dir2.y, dir1.x + dir2.x)\n)\n\n# returns True if `loc' matches any (y, x) coordinates,\n# within list `segments' -- such as a list composing a worm.\nhit_any = lambda loc, segments: loc in segments\n\n# same as above, but `locations' is also an array of (y, x) coordinates.\nhit_vany = lambda locations, segments: any(\n    hit_any(loc, segments) for loc in locations)\n\n# returns True if segments are same position (hit detection)\nhit = lambda src, dst: src.x == dst.x and src.y == dst.y\n\n# returns new worm_length if current nibble is hit,\nnext_wormlength = lambda nibble, head, worm_length: (\n    worm_length + nibble.value if hit(head, nibble.location)\n    else worm_length)\n\n# returns new speed if current nibble is hit,\nnext_speed = lambda nibble, head, speed, modifier: (\n    speed * modifier if hit(head, nibble.location)\n    else speed)\n\n# when displaying worm head, show a different glyph for horizontal/vertical\nhead_glyph = lambda direction: (u':' if direction in (left_of, right_of)\n                                else u'\"')\n\n\n# provide the next nibble -- continuously generate a random new nibble so\n# long as the current nibble hits any location of the worm, otherwise\n# return a nibble of the same location and value as provided.\ndef next_nibble(term, nibble, head, worm):\n    l, v = nibble.location, nibble.value\n    while hit_vany([head] + worm, nibble_locations(l, v)):\n        l = Location(x=randrange(1, term.width - 1),\n                     y=randrange(1, term.height - 1))\n        v = nibble.value + 1\n    return Nibble(l, v)\n\n\n# generate an array of locations for the current nibble's location -- a digit\n# such as '123' may be hit at 3 different (y, x) coordinates.\ndef nibble_locations(nibble_location, nibble_value):\n    return [Location(x=nibble_location.x + offset,\n                     y=nibble_location.y)\n            for offset in range(0, 1 + len('{}'.format(nibble_value)) - 1)]\n\n\ndef main():\n    term = Terminal()\n    worm = [Location(x=term.width // 2, y=term.height // 2)]\n    worm_length = 2\n    bearing = Direction(*LEFT)\n    direction = left_of\n    nibble = Nibble(location=worm[0], value=0)\n    color_nibble = term.black_on_green\n    color_worm = term.yellow_reverse\n    color_head = term.red_reverse\n    color_bg = term.on_blue\n    echo(term.move(1, 1))\n    echo(color_bg(term.clear))\n\n    # speed is actually a measure of time; the shorter, the faster.\n    speed = 0.1\n    modifier = 0.93\n    inp = None\n\n    with term.hidden_cursor(), term.raw():\n        while inp not in (u'q', u'Q'):\n\n            # delete the tail of the worm at worm_length\n            if len(worm) > worm_length:\n                echo(term.move(*worm.pop(0)))\n                echo(color_bg(u' '))\n\n            # compute head location\n            head = worm.pop()\n\n            # check for hit against self; hitting a wall results in the (y, x)\n            # location being clipped, -- and death by hitting self (not wall).\n            if hit_any(head, worm):\n                break\n\n            # get the next nibble, which may be equal to ours unless this\n            # nibble has been struck by any portion of our worm body.\n            n_nibble = next_nibble(term, nibble, head, worm)\n\n            # get the next worm_length and speed, unless unchanged.\n            worm_length = next_wormlength(nibble, head, worm_length)\n            speed = next_speed(nibble, head, speed, modifier)\n\n            if n_nibble != nibble:\n                # erase the old one, careful to redraw the nibble contents\n                # with a worm color for those portions that overlay.\n                for (y, x) in nibble_locations(*nibble):\n                    echo(term.move(y, x) + (color_worm if (y, x) == head\n                                            else color_bg)(u' '))\n                    echo(term.normal)\n                # and draw the new,\n                echo(term.move(*n_nibble.location) + (\n                    color_nibble('{}'.format(n_nibble.value))))\n\n            # display new worm head\n            echo(term.move(*head) + color_head(head_glyph(direction)))\n\n            # and its old head (now, a body piece)\n            if worm:\n                echo(term.move(*(worm[-1])))\n                echo(color_worm(u' '))\n            echo(term.move(*head))\n\n            # wait for keyboard input, which may indicate\n            # a new direction (up/down/left/right)\n            inp = term.inkey(speed)\n\n            # discover new direction, given keyboard input and/or bearing.\n            nxt_direction = next_bearing(term, inp.code, bearing)\n\n            # discover new bearing, given new direction compared to prev\n            nxt_bearing = change_bearing(nxt_direction, head, term)\n\n            # disallow new bearing/direction when flipped (running into\n            # oneself, fe. travelling left while traveling right)\n            if not bearing_flipped(bearing, nxt_bearing):\n                direction = nxt_direction\n                bearing = nxt_bearing\n\n            # append the prior `head' onto the worm, then\n            # a new `head' for the given direction.\n            worm.extend([head, direction(head, term)])\n\n            # re-assign new nibble,\n            nibble = n_nibble\n\n    echo(term.normal)\n    score = (worm_length - 1) * 100\n    echo(u''.join((term.move(term.height - 1, 1), term.normal)))\n    echo(u''.join((u'\\r\\n', u'score: {}'.format(score), u'\\r\\n')))\n\nif __name__ == '__main__':\n    main()\n",
          "",
          "===================================\nWelcome to Blessings documentation!\n===================================\n\nContents:\n\n.. toctree::\n   :maxdepth: 3\n   :glob:\n\n   intro\n   overview\n   further\n   pains\n   api\n   history\n\n=======\nIndexes\n=======\n\n* :ref:`genindex`\n* :ref:`modindex`\n"
        ],
        "test_patch": "",
        "patch_preview": "From 0ba17756a2292efde716e60255fea23e1d45a4d0 Mon Sep 17 00:00:00 2001\nFrom: Jeff Quast <jquast@io.com>\nDate: Wed, 15 Apr 2015 00:32:37 -0700\nSubject: [PATCH 1/2] move Thomas E Dickey's link out of 'source code' section\n\n---\n docs/further.rst | 14 +++++++-------\n 1 file changed, 7 insertions(+), 7 deletions(-)\n\ndiff --git a/docs/further.rst b/docs/further.rst\nindex ed13d813..33cd8fc3 100644\n--- a/docs/further.rst\n+++ b/docs/further.rst\n@@ -57,13 +57,6 @@ Here are some recommended readings to hel"
      },
      "patch": {
        "length": 38987,
        "files_changed": 10,
        "lines_added": 498,
        "lines_deleted": 330,
        "net_change": 168,
        "changed_files": [
          {
            "file": "docs/further.rst",
            "added": 7,
            "deleted": 7
          },
          {
            "file": ".prospector.yaml",
            "added": 0,
            "deleted": 1
          },
          {
            "file": "bin/editor.py",
            "added": 135,
            "deleted": 121
          },
          {
            "file": "bin/keymatrix.py",
            "added": 99,
            "deleted": 74
          },
          {
            "file": "bin/on_resize.py",
            "added": 35,
            "deleted": 25
          },
          {
            "file": "bin/progress_bar.py",
            "added": 10,
            "deleted": 4
          },
          {
            "file": "bin/tprint.py",
            "added": 15,
            "deleted": 10
          },
          {
            "file": "bin/worms.py",
            "added": 128,
            "deleted": 88
          },
          {
            "file": "docs/examples.rst",
            "added": 68,
            "deleted": 0
          },
          {
            "file": "docs/index.rst",
            "added": 1,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 43,
        "total_lines": 9710,
        "total_bytes": 326540,
        "python_files": 25,
        "python_lines": 7762,
        "file_extensions": {
          ".ini": 1,
          "": 2,
          ".txt": 1,
          ".cfg": 1,
          ".in": 1,
          ".py": 25,
          ".rst": 8,
          ".bat": 1,
          ".sh": 2,
          ".ans": 1
        },
        "largest_files": [
          {
            "path": "blessings/terminal.py",
            "size": 36649,
            "lines": 966,
            "extension": ".py"
          },
          {
            "path": "blessings/_binterms.py",
            "size": 9163,
            "lines": 878,
            "extension": ".py"
          },
          {
            "path": "blessings/sequences.py",
            "size": 30871,
            "lines": 812,
            "extension": ".py"
          },
          {
            "path": "blessings/tests/test_keyboard.py",
            "size": 27895,
            "lines": 808,
            "extension": ".py"
          },
          {
            "path": "docs/overview.rst",
            "size": 18886,
            "lines": 591,
            "extension": ".rst"
          },
          {
            "path": "blessings/tests/test_sequences.py",
            "size": 17470,
            "lines": 551,
            "extension": ".py"
          },
          {
            "path": "blessings/tests/test_core.py",
            "size": 13961,
            "lines": 453,
            "extension": ".py"
          },
          {
            "path": "blessings/formatters.py",
            "size": 16076,
            "lines": 411,
            "extension": ".py"
          },
          {
            "path": "blessings/tests/test_formatters.py",
            "size": 13835,
            "lines": 403,
            "extension": ".py"
          },
          {
            "path": "docs/pains.rst",
            "size": 17455,
            "lines": 380,
            "extension": ".rst"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 43,
        "files_changed_count": 10,
        "files_changed_ratio": 0.23255813953488372,
        "total_lines_in_repo": 9710,
        "lines_added": 498,
        "lines_deleted": 330,
        "net_lines_changed": 168,
        "lines_changed_ratio": 0.08527291452111226,
        "pr_body_length": 165,
        "commit_message_length": 126,
        "python_file_count": 25,
        "python_line_count": 7762
      }
    },
    {
      "tar_file_name": "euclio#vim-markdown-composer#pull#16",
      "repo_name": "euclio#vim-markdown-composer#pull#16",
      "success": true,
      "error": null,
      "commit": {
        "sha": "d7909e58be559c5e0792f72935a01f8b73dbedb0",
        "message": "serve static content from current file's directory\n\nCloses #3.",
        "author": {
          "name": "Andy Russell",
          "email": "arussell123@gmail.com",
          "date": "2016-01-02T02:09:16Z"
        },
        "html_url": "https://github.com/euclio/vim-markdown-composer/commit/d7909e58be559c5e0792f72935a01f8b73dbedb0",
        "api_url": "https://api.github.com/repos/euclio/vim-markdown-composer/commits/d7909e58be559c5e0792f72935a01f8b73dbedb0"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/euclio#vim-markdown-composer#pull#16",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/euclio#vim-markdown-composer#pull#16.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/euclio#vim-markdown-composer#pull#16/source_code"
      },
      "pr": {
        "number": 16,
        "title": "add support for aurelius v0.1.9 and therefore KaTeX rendering",
        "body": "I really like your Plugin, but I need rendering formulas for my lecture notes. Since you already use aurelius and its newest version supports KaTeX for LaTeX formulas, I changed it to use the newest version. \nI had to change the dependency from cargo.io to the GitHub repo directly, since in cargo, it is not with the current version yet.\n\nCheers!\n",
        "state": "closed",
        "created_at": "2016-07-07T18:23:21Z",
        "updated_at": "2016-07-08T01:43:31Z",
        "merged_at": null,
        "html_url": "https://github.com/euclio/vim-markdown-composer/pull/16",
        "user": "PaulDebus",
        "additions": 215,
        "deletions": 199,
        "changed_files": 3,
        "commits": 2
      },
      "swebench": {
        "instance_id": "euclio_vim-markdown-composer-16",
        "repo": "/euclio/vim-markdown-composer",
        "base_commit": "d7909e58be559c5e0792f72935a01f8b73dbedb0",
        "problem_statement": {},
        "edit_files": [
          "Cargo.toml",
          "src/main.rs",
          "Cargo.lock"
        ],
        "oracle_files": [
          "[package]\nname = \"markdown-composer\"\nversion = \"0.1.0\"\nauthors = [\"Andy Russell <arussell123@gmail.com>\"]\ndescription = \"A complete solution for previewing markdown.\"\n\n[dependencies]\naurelius = \"0.1.5\"\ndocopt = \"0.6\"\nlog = \"0.3.1\"\nlog4rs = \"0.3.3\"\nrmp = \"0.7.3\"\nrmp-serialize = \"0.7.0\"\nrustc-serialize = \"0.3\"\n",
          "//! A simple client that listens for msgpack-serialized strings on a port and renders them as\n//! markdown.\n//!\n//! The markdown is rendered on an arbitrary port on localhost, which is then automatically opened\n//! in a browser. As new messages are received on the input port, the markdown is asynchonously\n//! rendered in the browser (no refresh is required).\n\n#[macro_use]\nextern crate log;\nextern crate aurelius;\nextern crate docopt;\nextern crate log4rs;\nextern crate rmp as msgpack;\nextern crate rmp_serialize;\nextern crate rustc_serialize;\n\nuse std::default::Default;\nuse std::env;\nuse std::io::BufReader;\nuse std::net::TcpStream;\nuse std::path::PathBuf;\n\nuse aurelius::Server;\nuse aurelius::browser;\nuse docopt::Docopt;\nuse msgpack::decode::ReadError::UnexpectedEOF;\nuse rmp_serialize::Decoder;\nuse rmp_serialize::decode::Error;\nuse rustc_serialize::Decodable;\n\n#[cfg_attr(rustfmt, rustfmt_skip)]\nstatic USAGE: &'static str = \"\nUsage: markdown_composer [options] <nvim-port> [<initial-markdown>]\n       markdown_composer --help\n\nOptions:\n    -h, --help                  Show this message.\n\n    --no-browser                Don't open the web browser automatically.\n\n    --browser=<executable>      Specify a browser that the program should open. If not supplied,\n                                the program will determine the user's default browser.\n\n    --highlight-theme=<theme>   The theme to use for syntax highlighting. All highlight.js themes\n                                are supported. If no theme is supplied, the 'github' theme is used.\n\n    --working-directory=<dir>   The directory that static files should be served out of. Useful for\n                                static content linked in the markdown. Can be changed at runtime\n                                with the 'chdir' command.\n\";\n\n#[derive(RustcDecodable, Debug)]\nstruct Args {\n    arg_nvim_port: u16,\n    arg_initial_markdown: Option<String>,\n    flag_no_browser: bool,\n    flag_browser: Option<String>,\n    flag_highlight_theme: Option<String>,\n    flag_working_directory: Option<String>,\n}\n\nfn open_browser(server: &Server, browser: Option<String>) {\n    let url = format!(\"http://{}\", server.http_addr().unwrap());\n\n    if let Some(ref browser) = browser {\n        let split_cmd = browser.split_whitespace().collect::<Vec<_>>();\n        let (cmd, args) = split_cmd.split_first().unwrap();\n        browser::open_specific(&url, cmd, args).unwrap();\n    } else {\n        browser::open(&url).unwrap();\n    }\n}\n\nfn main() {\n    log4rs::init_file(\"config/log.toml\", Default::default()).unwrap();\n\n    let args: Args = Docopt::new(USAGE)\n                         .and_then(|d| d.decode())\n                         .unwrap_or_else(|e| e.exit());\n\n    let mut server = Server::new_with_config(aurelius::Config {\n        initial_markdown: args.arg_initial_markdown.unwrap_or_default(),\n        highlight_theme: args.flag_highlight_theme.unwrap_or(\"github\".to_owned()),\n        working_directory: args.flag_working_directory.map_or(env::current_dir().unwrap().to_owned(), |path| PathBuf::from(path)),\n    });\n    let sender = server.start();\n\n    if !args.flag_no_browser {\n        open_browser(&server, args.flag_browser.clone());\n    }\n\n    let nvim_port = args.arg_nvim_port;\n    let stream = TcpStream::connect((\"localhost\", nvim_port))\n                     .ok()\n                     .expect(&format!(\"no listener on port {}\", nvim_port));\n\n    let mut decoder = Decoder::new(BufReader::new(stream));\n    loop {\n        let msg = <Vec<String> as Decodable>::decode(&mut decoder);\n        match msg {\n            Ok(msg) => {\n                let cmd = &msg.first().unwrap()[..];\n                let params = &msg[1..];\n                match cmd {\n                    \"send_data\" => sender.send(params[0].to_owned()).unwrap(),\n                    \"open_browser\" => open_browser(&server, args.flag_browser.clone()),\n                    \"chdir\" => server.change_working_directory(params[0].to_owned()),\n                    _ => panic!(\"Received unknown command: {}\", cmd),\n                }\n            }\n            Err(Error::InvalidMarkerRead(UnexpectedEOF)) => {\n                // In this case, the remote client probably just hung up.\n                break;\n            }\n            Err(err) => panic!(err),\n        }\n    }\n}\n",
          "[root]\nname = \"markdown-composer\"\nversion = \"0.1.0\"\ndependencies = [\n \"aurelius 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"docopt 0.6.78 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log4rs 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rmp 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rmp-serialize 0.7.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"advapi32-sys\"\nversion = \"0.1.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"aho-corasick\"\nversion = \"0.4.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"memchr 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"aurelius\"\nversion = \"0.1.8\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"chan 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"hoedown 3.0.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"nickel 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"porthole 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"websocket 0.15.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"bitflags\"\nversion = \"0.3.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"byteorder\"\nversion = \"0.3.13\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"byteorder\"\nversion = \"0.4.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"cfg-if\"\nversion = \"0.1.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"chan\"\nversion = \"0.1.14\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rand 0.3.12 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"cookie\"\nversion = \"0.1.21\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.2.38 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"cookie\"\nversion = \"0.2.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"openssl 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"docopt\"\nversion = \"0.6.78\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"regex 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"strsim 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"gcc\"\nversion = \"0.3.21\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"advapi32-sys 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"gdi32-sys\"\nversion = \"0.1.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"groupable\"\nversion = \"0.2.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"hoedown\"\nversion = \"3.0.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"bitflags 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"gcc 0.3.21 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"hpack\"\nversion = \"0.2.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"httparse\"\nversion = \"1.1.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"hyper\"\nversion = \"0.6.16\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"cookie 0.1.21 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"httparse 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"language-tags 0.0.7 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"mime 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"num_cpus 0.2.10 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"solicit 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"traitobject 0.0.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"typeable 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"unicase 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.2.38 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"hyper\"\nversion = \"0.7.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"cookie 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"httparse 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"language-tags 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"mime 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"num_cpus 0.2.10 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"openssl 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"solicit 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"traitobject 0.0.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"typeable 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"unicase 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"kernel32-sys\"\nversion = \"0.2.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"language-tags\"\nversion = \"0.0.7\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"language-tags\"\nversion = \"0.2.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"lazy_static\"\nversion = \"0.1.15\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"libc\"\nversion = \"0.2.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"libressl-pnacl-sys\"\nversion = \"2.1.6\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"pnacl-build-helper 1.4.10 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"log\"\nversion = \"0.3.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"log4rs\"\nversion = \"0.3.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"toml 0.1.25 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"matches\"\nversion = \"0.1.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"memchr\"\nversion = \"0.1.7\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"mime\"\nversion = \"0.1.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"serde 0.6.7 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"modifier\"\nversion = \"0.1.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"mustache\"\nversion = \"0.6.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"net2\"\nversion = \"0.2.20\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"cfg-if 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"kernel32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"nickel\"\nversion = \"0.7.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"groupable 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"hyper 0.6.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"lazy_static 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"modifier 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"mustache 0.6.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"plugin 0.2.6 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"regex 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"time 0.1.34 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"typemap 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.2.38 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"num\"\nversion = \"0.1.29\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rand 0.3.12 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"num_cpus\"\nversion = \"0.2.10\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"kernel32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"openssl\"\nversion = \"0.7.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"bitflags 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"gcc 0.3.21 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"lazy_static 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"openssl-sys 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"openssl-sys-extras 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"openssl-sys\"\nversion = \"0.7.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"gdi32-sys 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libressl-pnacl-sys 2.1.6 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"pkg-config 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"user32-sys 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"openssl-sys-extras\"\nversion = \"0.7.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"gcc 0.3.21 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"openssl-sys 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"pkg-config\"\nversion = \"0.3.6\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"plugin\"\nversion = \"0.2.6\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"typemap 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"pnacl-build-helper\"\nversion = \"1.4.10\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"tempdir 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"porthole\"\nversion = \"0.1.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"rand\"\nversion = \"0.3.12\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"advapi32-sys 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"regex\"\nversion = \"0.1.44\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"aho-corasick 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"memchr 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"regex-syntax 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"regex-syntax\"\nversion = \"0.2.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"rmp\"\nversion = \"0.7.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"byteorder 0.3.13 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"rmp-serialize\"\nversion = \"0.7.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rmp 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"rustc-serialize\"\nversion = \"0.3.16\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"rustc_version\"\nversion = \"0.1.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"semver 0.1.20 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"semver\"\nversion = \"0.1.20\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"serde\"\nversion = \"0.6.7\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"num 0.1.29 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"solicit\"\nversion = \"0.4.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"hpack 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"log 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"strsim\"\nversion = \"0.3.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"tempdir\"\nversion = \"0.3.4\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rand 0.3.12 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"time\"\nversion = \"0.1.34\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"kernel32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"libc 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"toml\"\nversion = \"0.1.25\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"traitobject\"\nversion = \"0.0.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"traitobject\"\nversion = \"0.0.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"typeable\"\nversion = \"0.1.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"typemap\"\nversion = \"0.3.3\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"unsafe-any 0.4.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"unicase\"\nversion = \"1.1.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rustc_version 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"unsafe-any\"\nversion = \"0.4.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"traitobject 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"url\"\nversion = \"0.2.38\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"matches 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"uuid 0.1.18 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"url\"\nversion = \"0.5.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"matches 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"uuid 0.1.18 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"user32-sys\"\nversion = \"0.1.2\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"uuid\"\nversion = \"0.1.18\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"rand 0.3.12 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"websocket\"\nversion = \"0.15.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"bitflags 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"byteorder 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"hyper 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"net2 0.2.20 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"openssl 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rand 0.3.12 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"rustc-serialize 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"unicase 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"url 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n[[package]]\nname = \"winapi\"\nversion = \"0.2.5\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"winapi-build\"\nversion = \"0.1.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\n\n[[package]]\nname = \"ws2_32-sys\"\nversion = \"0.2.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\ndependencies = [\n \"winapi 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)\",\n \"winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)\",\n]\n\n"
        ],
        "test_patch": "",
        "patch_preview": "From f9b559c9b00fcfd8cc17a9d4dab68f4d418d1f09 Mon Sep 17 00:00:00 2001\nFrom: Paul Debus <debuspaul@gmail.com>\nDate: Thu, 7 Jul 2016 20:19:15 +0200\nSubject: [PATCH 1/2] add support for aurelius v0.1.9 and therefore KaTeX\n rendering\n\n---\n Cargo.toml  | 2 +-\n src/main.rs | 1 +\n 2 files changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 307f59f..35b715e 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -5,7 +5,7 @@ authors = [\"Andy Russell <arussell123@gmail.com>\"]\n descri"
      },
      "patch": {
        "length": 32039,
        "files_changed": 3,
        "lines_added": 215,
        "lines_deleted": 199,
        "net_change": 16,
        "changed_files": [
          {
            "file": "Cargo.toml",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "src/main.rs",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "Cargo.lock",
            "added": 213,
            "deleted": 198
          }
        ]
      },
      "issue_comments": [
        {
          "id": 231228583,
          "body": "HI @PaulDebus, thanks for the PR! You should be able to use `0.1.9` from crates.io instead of the git version.\n",
          "user": "euclio",
          "created_at": "2016-07-07T22:37:34Z",
          "html_url": "https://github.com/euclio/vim-markdown-composer/pull/16#issuecomment-231228583"
        },
        {
          "id": 231255394,
          "body": "Strange. crates.io definitely says that 0.1.9 is already published, but I was running into the same problem you were. I ended up just publishing a new version.\n",
          "user": "euclio",
          "created_at": "2016-07-08T01:43:31Z",
          "html_url": "https://github.com/euclio/vim-markdown-composer/pull/16#issuecomment-231255394"
        },
        {
          "id": 231240821,
          "body": "Hey,\nI tried many times, but It did always say, that the newest available version was 0.1.8 on crate.io or rather [https://github.com/rust-lang/crates.io-index/blob/master/au/re/aurelius]. I do not know any rust and even less about those crates, so I found it easier to fix it this way. If you can correct this, I would be happy. \n\nCheers!\n",
          "user": "PaulDebus",
          "created_at": "2016-07-07T23:52:01Z",
          "html_url": "https://github.com/euclio/vim-markdown-composer/pull/16#issuecomment-231240821"
        }
      ],
      "issue_comments_count": 3,
      "code_statistics": {
        "total_files": 7,
        "total_lines": 1034,
        "total_bytes": 35053,
        "python_files": 2,
        "python_lines": 147,
        "file_extensions": {
          ".lock": 1,
          ".toml": 1,
          ".md": 1,
          ".py": 2,
          ".rs": 1,
          ".txt": 1
        },
        "largest_files": [
          {
            "path": "Cargo.lock",
            "size": 20544,
            "lines": 610,
            "extension": ".lock"
          },
          {
            "path": "rplugin/python3/markdown_composer/__init__.py",
            "size": 4633,
            "lines": 147,
            "extension": ".py"
          },
          {
            "path": "src/main.rs",
            "size": 4310,
            "lines": 118,
            "extension": ".rs"
          },
          {
            "path": "README.md",
            "size": 2518,
            "lines": 76,
            "extension": ".md"
          },
          {
            "path": "doc/markdown-composer.txt",
            "size": 2738,
            "lines": 69,
            "extension": ".txt"
          },
          {
            "path": "Cargo.toml",
            "size": 310,
            "lines": 14,
            "extension": ".toml"
          },
          {
            "path": "rplugin/python3/markdown_composer.py",
            "size": 0,
            "lines": 0,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 7,
        "files_changed_count": 3,
        "files_changed_ratio": 0.42857142857142855,
        "total_lines_in_repo": 1034,
        "lines_added": 215,
        "lines_deleted": 199,
        "net_lines_changed": 16,
        "lines_changed_ratio": 0.40038684719535783,
        "pr_body_length": 348,
        "commit_message_length": 62,
        "python_file_count": 2,
        "python_line_count": 147
      }
    },
    {
      "tar_file_name": "euske#pdfminer#pull#186",
      "repo_name": "euske#pdfminer#pull#186",
      "success": true,
      "error": null,
      "commit": {
        "sha": "8150458718e9024c80b00e74965510b20206e588",
        "message": "Added: a simpler ordering mode when 1<F.",
        "author": {
          "name": "Yusuke Shinyama",
          "email": "yusuke@shinyama.jp",
          "date": "2016-09-26T09:06:34Z"
        },
        "html_url": "https://github.com/euske/pdfminer/commit/8150458718e9024c80b00e74965510b20206e588",
        "api_url": "https://api.github.com/repos/euske/pdfminer/commits/8150458718e9024c80b00e74965510b20206e588"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/euske#pdfminer#pull#186",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/euske#pdfminer#pull#186.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/euske#pdfminer#pull#186/source_code"
      },
      "pr": {
        "number": 186,
        "title": "fixes two issues found in malformed PDFs",
        "body": "i use python to analyze malicious PDFs so i trip over stuff at times. this PR fixes two issues\r\n\r\n- #56  - but specifically for the MediaBox attribute mentioned there\r\n- #175 - applies the suggested patch (albeit a bit differently, same effect)\r\n\r\ntested locally OK with malicious PDFs and benign PDFs",
        "state": "open",
        "created_at": "2017-08-03T23:11:38Z",
        "updated_at": "2017-08-03T23:11:38Z",
        "merged_at": null,
        "html_url": "https://github.com/euske/pdfminer/pull/186",
        "user": "paralax",
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "commits": 2
      },
      "swebench": {
        "instance_id": "euske_pdfminer-186",
        "repo": "/euske/pdfminer",
        "base_commit": "8150458718e9024c80b00e74965510b20206e588",
        "problem_statement": {
          "title": "KeyError: 'Resources' on some files",
          "body": "You can find the file [here](https://www.dropbox.com/s/zllzxr5dngs0hsc/impius%20-%20orumPatres1.pdf)\n\n``` python\nfrom pdfminer.pdfparser import PDFParser\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfpage import PDFTextExtractionNotAllowed\nfrom pdfminer.pdfinterp import PDFResourceManager\nfrom pdfminer.pdfinterp import PDFPageInterpreter\nfrom pdfminer.pdfdevice import PDFDevice\n\nfp = open('impius - orumPatres1.pdf', 'rb')\n\nparser = PDFParser(fp)\ndocument = PDFDocument(parser, password)\nif not document.is_extractable:\n    raise PDFTextExtractionNotAllowed\nrsrcmgr = PDFResourceManager()\ndevice = PDFDevice(rsrcmgr)\ninterpreter = PDFPageInterpreter(rsrcmgr, device)\nfor page in PDFPage.create_pages(document):\n    interpreter.process_page(page)\n```\n\nResult : \n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/jc/.virtualenvs/pdfquery/lib/python2.7/site-packages/pdfminer/pdfpage.py\", line 100, in create_pages\n    yield klass(document, objid, tree)\n  File \"/Users/jc/.virtualenvs/pdfquery/lib/python2.7/site-packages/pdfminer/pdfpage.py\", line 52, in __init__\n    self.resources = resolve1(self.attrs['Resources'])\nKeyError: 'Resources'\n```\n"
        },
        "edit_files": [
          "pdfminer/pdftypes.py",
          "pdfminer/pdfpage.py"
        ],
        "oracle_files": [
          "#!/usr/bin/env python\nimport zlib\nfrom .lzw import lzwdecode\nfrom .ascii85 import ascii85decode\nfrom .ascii85 import asciihexdecode\nfrom .runlength import rldecode\nfrom .ccitt import ccittfaxdecode\nfrom .psparser import PSException\nfrom .psparser import PSObject\nfrom .psparser import LIT\nfrom .psparser import STRICT\nfrom .utils import apply_png_predictor\nfrom .utils import isnumber\n\n\nLITERAL_CRYPT = LIT('Crypt')\n\n# Abbreviation of Filter names in PDF 4.8.6. \"Inline Images\"\nLITERALS_FLATE_DECODE = (LIT('FlateDecode'), LIT('Fl'))\nLITERALS_LZW_DECODE = (LIT('LZWDecode'), LIT('LZW'))\nLITERALS_ASCII85_DECODE = (LIT('ASCII85Decode'), LIT('A85'))\nLITERALS_ASCIIHEX_DECODE = (LIT('ASCIIHexDecode'), LIT('AHx'))\nLITERALS_RUNLENGTH_DECODE = (LIT('RunLengthDecode'), LIT('RL'))\nLITERALS_CCITTFAX_DECODE = (LIT('CCITTFaxDecode'), LIT('CCF'))\nLITERALS_DCT_DECODE = (LIT('DCTDecode'), LIT('DCT'))\n\n\n##  PDF Objects\n##\nclass PDFObject(PSObject):\n    pass\n\nclass PDFException(PSException):\n    pass\n\nclass PDFTypeError(PDFException):\n    pass\n\nclass PDFValueError(PDFException):\n    pass\n\nclass PDFObjectNotFound(PDFException):\n    pass\n\nclass PDFNotImplementedError(PDFException):\n    pass\n\n\n##  PDFObjRef\n##\nclass PDFObjRef(PDFObject):\n\n    def __init__(self, doc, objid, _):\n        if objid == 0:\n            if STRICT:\n                raise PDFValueError('PDF object id cannot be 0.')\n        self.doc = doc\n        self.objid = objid\n        #self.genno = genno  # Never used.\n        return\n\n    def __repr__(self):\n        return '<PDFObjRef:%d>' % (self.objid)\n\n    def resolve(self, default=None):\n        try:\n            return self.doc.getobj(self.objid)\n        except PDFObjectNotFound:\n            return default\n\n\n# resolve\ndef resolve1(x, default=None):\n    \"\"\"Resolves an object.\n\n    If this is an array or dictionary, it may still contains\n    some indirect objects inside.\n    \"\"\"\n    while isinstance(x, PDFObjRef):\n        x = x.resolve(default=default)\n    return x\n\n\ndef resolve_all(x, default=None):\n    \"\"\"Recursively resolves the given object and all the internals.\n\n    Make sure there is no indirect reference within the nested object.\n    This procedure might be slow.\n    \"\"\"\n    while isinstance(x, PDFObjRef):\n        x = x.resolve(default=default)\n    if isinstance(x, list):\n        x = [resolve_all(v, default=default) for v in x]\n    elif isinstance(x, dict):\n        for (k, v) in x.iteritems():\n            x[k] = resolve_all(v, default=default)\n    return x\n\n\ndef decipher_all(decipher, objid, genno, x):\n    \"\"\"Recursively deciphers the given object.\n    \"\"\"\n    if isinstance(x, str):\n        return decipher(objid, genno, x)\n    if isinstance(x, list):\n        x = [decipher_all(decipher, objid, genno, v) for v in x]\n    elif isinstance(x, dict):\n        for (k, v) in x.iteritems():\n            x[k] = decipher_all(decipher, objid, genno, v)\n    return x\n\n\n# Type checking\ndef int_value(x):\n    x = resolve1(x)\n    if not isinstance(x, int):\n        if STRICT:\n            raise PDFTypeError('Integer required: %r' % x)\n        return 0\n    return x\n\n\ndef float_value(x):\n    x = resolve1(x)\n    if not isinstance(x, float):\n        if STRICT:\n            raise PDFTypeError('Float required: %r' % x)\n        return 0.0\n    return x\n\n\ndef num_value(x):\n    x = resolve1(x)\n    if not isnumber(x):\n        if STRICT:\n            raise PDFTypeError('Int or Float required: %r' % x)\n        return 0\n    return x\n\n\ndef str_value(x):\n    x = resolve1(x)\n    if not isinstance(x, str):\n        if STRICT:\n            raise PDFTypeError('String required: %r' % x)\n        return ''\n    return x\n\n\ndef list_value(x):\n    x = resolve1(x)\n    if not isinstance(x, (list, tuple)):\n        if STRICT:\n            raise PDFTypeError('List required: %r' % x)\n        return []\n    return x\n\n\ndef dict_value(x):\n    x = resolve1(x)\n    if not isinstance(x, dict):\n        if STRICT:\n            raise PDFTypeError('Dict required: %r' % x)\n        return {}\n    return x\n\n\ndef stream_value(x):\n    x = resolve1(x)\n    if not isinstance(x, PDFStream):\n        if STRICT:\n            raise PDFTypeError('PDFStream required: %r' % x)\n        return PDFStream({}, '')\n    return x\n\n\n##  PDFStream type\n##\nclass PDFStream(PDFObject):\n\n    def __init__(self, attrs, rawdata, decipher=None):\n        assert isinstance(attrs, dict)\n        self.attrs = attrs\n        self.rawdata = rawdata\n        self.decipher = decipher\n        self.data = None\n        self.objid = None\n        self.genno = None\n        return\n\n    def set_objid(self, objid, genno):\n        self.objid = objid\n        self.genno = genno\n        return\n\n    def __repr__(self):\n        if self.data is None:\n            assert self.rawdata is not None\n            return '<PDFStream(%r): raw=%d, %r>' % (self.objid, len(self.rawdata), self.attrs)\n        else:\n            assert self.data is not None\n            return '<PDFStream(%r): len=%d, %r>' % (self.objid, len(self.data), self.attrs)\n\n    def __contains__(self, name):\n        return name in self.attrs\n\n    def __getitem__(self, name):\n        return self.attrs[name]\n\n    def get(self, name, default=None):\n        return self.attrs.get(name, default)\n\n    def get_any(self, names, default=None):\n        for name in names:\n            if name in self.attrs:\n                return self.attrs[name]\n        return default\n\n    def get_filters(self):\n        filters = self.get_any(('F', 'Filter'))\n        params = self.get_any(('DP', 'DecodeParms', 'FDecodeParms'), {})\n        if not filters:\n            return []\n        if not isinstance(filters, list):\n            filters = [filters]\n        if not isinstance(params, list):\n            # Make sure the parameters list is the same as filters.\n            params = [params]*len(filters)\n        if STRICT and len(params) != len(filters):\n            raise PDFException(\"Parameters len filter mismatch\")\n        return zip(filters, params)\n\n    def decode(self):\n        assert self.data is None and self.rawdata is not None\n        data = self.rawdata\n        if self.decipher:\n            # Handle encryption\n            data = self.decipher(self.objid, self.genno, data, self.attrs)\n        filters = self.get_filters()\n        if not filters:\n            self.data = data\n            self.rawdata = None\n            return\n        for (f,params) in filters:\n            if f in LITERALS_FLATE_DECODE:\n                # will get errors if the document is encrypted.\n                try:\n                    data = zlib.decompress(data)\n                except zlib.error as e:\n                    if STRICT:\n                        raise PDFException('Invalid zlib bytes: %r, %r' % (e, data))\n                    data = b''\n            elif f in LITERALS_LZW_DECODE:\n                data = lzwdecode(data)\n            elif f in LITERALS_ASCII85_DECODE:\n                data = ascii85decode(data)\n            elif f in LITERALS_ASCIIHEX_DECODE:\n                data = asciihexdecode(data)\n            elif f in LITERALS_RUNLENGTH_DECODE:\n                data = rldecode(data)\n            elif f in LITERALS_CCITTFAX_DECODE:\n                data = ccittfaxdecode(data, params)\n            elif f in LITERALS_DCT_DECODE:\n                # This is probably a JPG stream - it does not need to be decoded twice.\n                # Just return the stream to the user.\n                pass\n            elif f == LITERAL_CRYPT:\n                # not yet..\n                raise PDFNotImplementedError('/Crypt filter is unsupported')\n            else:\n                raise PDFNotImplementedError('Unsupported filter: %r' % f)\n            # apply predictors\n            if 'Predictor' in params:\n                pred = int_value(params['Predictor'])\n                if pred == 1:\n                    # no predictor\n                    pass\n                elif 10 <= pred:\n                    # PNG predictor\n                    colors = int_value(params.get('Colors', 1))\n                    columns = int_value(params.get('Columns', 1))\n                    bitspercomponent = int_value(params.get('BitsPerComponent', 8))\n                    data = apply_png_predictor(pred, colors, columns, bitspercomponent, data)\n                else:\n                    raise PDFNotImplementedError('Unsupported predictor: %r' % pred)\n        self.data = data\n        self.rawdata = None\n        return\n\n    def get_data(self):\n        if self.data is None:\n            self.decode()\n        return self.data\n\n    def get_rawdata(self):\n        return self.rawdata\n",
          "#!/usr/bin/env python\nimport logging\nfrom .psparser import LIT\nfrom .pdftypes import PDFObjectNotFound\nfrom .pdftypes import resolve1\nfrom .pdftypes import int_value\nfrom .pdftypes import list_value\nfrom .pdftypes import dict_value\nfrom .pdfparser import PDFParser\nfrom .pdfdocument import PDFDocument\nfrom .pdfdocument import PDFTextExtractionNotAllowed\n\n# some predefined literals and keywords.\nLITERAL_PAGE = LIT('Page')\nLITERAL_PAGES = LIT('Pages')\n\n\n##  PDFPage\n##\nclass PDFPage(object):\n\n    \"\"\"An object that holds the information about a page.\n\n    A PDFPage object is merely a convenience class that has a set\n    of keys and values, which describe the properties of a page\n    and point to its contents.\n\n    Attributes:\n      doc: a PDFDocument object.\n      pageid: any Python object that can uniquely identify the page.\n      attrs: a dictionary of page attributes.\n      contents: a list of PDFStream objects that represents the page content.\n      lastmod: the last modified time of the page.\n      resources: a list of resources used by the page.\n      mediabox: the physical size of the page.\n      cropbox: the crop rectangle of the page.\n      rotate: the page rotation (in degree).\n      annots: the page annotations.\n      beads: a chain that represents natural reading order.\n    \"\"\"\n\n    debug = False\n\n    def __init__(self, doc, pageid, attrs):\n        \"\"\"Initialize a page object.\n\n        doc: a PDFDocument object.\n        pageid: any Python object that can uniquely identify the page.\n        attrs: a dictionary of page attributes.\n        \"\"\"\n        self.doc = doc\n        self.pageid = pageid\n        self.attrs = dict_value(attrs)\n        self.lastmod = resolve1(self.attrs.get('LastModified'))\n        self.resources = resolve1(self.attrs.get('Resources', dict()))\n        self.mediabox = resolve1(self.attrs['MediaBox'])\n        if 'CropBox' in self.attrs:\n            self.cropbox = resolve1(self.attrs['CropBox'])\n        else:\n            self.cropbox = self.mediabox\n        self.rotate = (int_value(self.attrs.get('Rotate', 0))+360) % 360\n        self.annots = self.attrs.get('Annots')\n        self.beads = self.attrs.get('B')\n        if 'Contents' in self.attrs:\n            contents = resolve1(self.attrs['Contents'])\n        else:\n            contents = []\n        if not isinstance(contents, list):\n            contents = [contents]\n        self.contents = contents\n        return\n\n    def __repr__(self):\n        return '<PDFPage: Resources=%r, MediaBox=%r>' % (self.resources, self.mediabox)\n\n    INHERITABLE_ATTRS = set(['Resources', 'MediaBox', 'CropBox', 'Rotate'])\n\n    @classmethod\n    def create_pages(klass, document):\n        def search(obj, parent):\n            if isinstance(obj, int):\n                objid = obj\n                tree = dict_value(document.getobj(objid)).copy()\n            else:\n                objid = obj.objid\n                tree = dict_value(obj).copy()\n            for (k, v) in parent.iteritems():\n                if k in klass.INHERITABLE_ATTRS and k not in tree:\n                    tree[k] = v\n            if tree.get('Type') is LITERAL_PAGES and 'Kids' in tree:\n                if klass.debug: logging.info('Pages: Kids=%r' % tree['Kids'])\n                for c in list_value(tree['Kids']):\n                    for x in search(c, tree):\n                        yield x\n            elif tree.get('Type') is LITERAL_PAGE:\n                if klass.debug: logging.info('Page: %r' % tree)\n                yield (objid, tree)\n        pages = False\n        if 'Pages' in document.catalog:\n            for (objid, tree) in search(document.catalog['Pages'], document.catalog):\n                yield klass(document, objid, tree)\n                pages = True\n        if not pages:\n            # fallback when /Pages is missing.\n            for xref in document.xrefs:\n                for objid in xref.get_objids():\n                    try:\n                        obj = document.getobj(objid)\n                        if isinstance(obj, dict) and obj.get('Type') is LITERAL_PAGE:\n                            yield klass(document, objid, obj)\n                    except PDFObjectNotFound:\n                        pass\n        return\n\n    @classmethod\n    def get_pages(klass, fp,\n                  pagenos=None, maxpages=0, password=b'',\n                  caching=True, check_extractable=True):\n        # Create a PDF parser object associated with the file object.\n        parser = PDFParser(fp)\n        # Create a PDF document object that stores the document structure.\n        doc = PDFDocument(parser, password=password, caching=caching)\n        # Check if the document allows text extraction. If not, abort.\n        if check_extractable and not doc.is_extractable:\n            raise PDFTextExtractionNotAllowed('Text extraction is not allowed: %r' % fp)\n        # Process each page contained in the document.\n        for (pageno, page) in enumerate(klass.create_pages(doc)):\n            if pagenos and (pageno not in pagenos):\n                continue\n            yield page\n            if maxpages and maxpages <= pageno+1:\n                break\n        return\n"
        ],
        "test_patch": "",
        "patch_preview": "From b784e5a53146808f6c1b7ee9c30cbdd68c11799c Mon Sep 17 00:00:00 2001\nFrom: jose nazario <jose.monkey.org@gmail.com>\nDate: Thu, 3 Aug 2017 16:08:54 -0700\nSubject: [PATCH 1/2] fix #175 - params may be None, deal with it\n\n---\n pdfminer/pdftypes.py | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a/pdfminer/pdftypes.py b/pdfminer/pdftypes.py\nindex 20d981dd..d1eb0df0 100644\n--- a/pdfminer/pdftypes.py\n+++ b/pdfminer/pdftypes.py\n@@ -270,6 +270,8 @@ def decode(self):\n             else:\n            "
      },
      "patch": {
        "length": 1697,
        "files_changed": 2,
        "lines_added": 3,
        "lines_deleted": 1,
        "net_change": 2,
        "changed_files": [
          {
            "file": "pdfminer/pdftypes.py",
            "added": 2,
            "deleted": 0
          },
          {
            "file": "pdfminer/pdfpage.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 107,
        "total_lines": 163241,
        "total_bytes": 12654532,
        "python_files": 34,
        "python_lines": 13899,
        "file_extensions": {
          "": 7,
          ".in": 1,
          ".py": 34,
          ".md": 1,
          ".pdf": 17,
          ".ref": 30,
          ".tex": 1,
          ".xml": 1,
          ".txt": 5,
          ".html": 2,
          ".png": 3,
          ".obj": 3,
          ".css": 1,
          ".cgi": 1
        },
        "largest_files": [
          {
            "path": "cmaprsrc/cid2code_Adobe_GB1.txt",
            "size": 1900416,
            "lines": 30369,
            "extension": ".txt"
          },
          {
            "path": "cmaprsrc/cid2code_Adobe_Japan1.txt",
            "size": 2681742,
            "lines": 23510,
            "extension": ".txt"
          },
          {
            "path": "cmaprsrc/cid2code_Adobe_CNS1.txt",
            "size": 2046762,
            "lines": 19266,
            "extension": ".txt"
          },
          {
            "path": "cmaprsrc/cid2code_Adobe_Korea1.txt",
            "size": 1028252,
            "lines": 18528,
            "extension": ".txt"
          },
          {
            "path": "samples/nonfree/f1040nr.pdf",
            "size": 655458,
            "lines": 10170,
            "extension": ".pdf"
          },
          {
            "path": "samples/nonfree/nlp2004slides.pdf",
            "size": 812685,
            "lines": 7678,
            "extension": ".pdf"
          },
          {
            "path": "samples/nonfree/i1040nr.xml.ref",
            "size": 536015,
            "lines": 6424,
            "extension": ".ref"
          },
          {
            "path": "samples/nonfree/f1040nr.xml.ref",
            "size": 435421,
            "lines": 6026,
            "extension": ".ref"
          },
          {
            "path": "samples/nonfree/i1040nr.pdf",
            "size": 553306,
            "lines": 5737,
            "extension": ".pdf"
          },
          {
            "path": "pdfminer/glyphlist.py",
            "size": 121510,
            "lines": 4339,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 107,
        "files_changed_count": 2,
        "files_changed_ratio": 0.018691588785046728,
        "total_lines_in_repo": 163241,
        "lines_added": 3,
        "lines_deleted": 1,
        "net_lines_changed": 2,
        "lines_changed_ratio": 2.450364798059311e-05,
        "pr_body_length": 301,
        "commit_message_length": 40,
        "python_file_count": 34,
        "python_line_count": 13899
      }
    },
    {
      "tar_file_name": "flosell#trailscraper#pull#718",
      "repo_name": "flosell#trailscraper#pull#718",
      "success": true,
      "error": null,
      "commit": {
        "sha": "8a770beab5aaca938d36d09e526480d358bb75d1",
        "message": "Bump boto3 from 1.35.97 to 1.36.2\n\nBumps [boto3](https://github.com/boto/boto3) from 1.35.97 to 1.36.2.\n- [Release notes](https://github.com/boto/boto3/releases)\n- [Commits](https://github.com/boto/boto3/compare/1.35.97...1.36.2)\n\n---\nupdated-dependencies:\n- dependency-name: boto3\n  dependency-type: direct:production\n  update-type: version-update:semver-minor\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "author": {
          "name": "dependabot[bot]",
          "email": "49699333+dependabot[bot]@users.noreply.github.com",
          "date": "2025-01-20T01:35:11Z"
        },
        "html_url": "https://github.com/flosell/trailscraper/commit/8a770beab5aaca938d36d09e526480d358bb75d1",
        "api_url": "https://api.github.com/repos/flosell/trailscraper/commits/8a770beab5aaca938d36d09e526480d358bb75d1"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/flosell#trailscraper#pull#718",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/flosell#trailscraper#pull#718.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/flosell#trailscraper#pull#718/source_code"
      },
      "pr": {
        "number": 718,
        "title": "Bump twine from 6.0.1 to 6.1.0",
        "body": "Bumps [twine](https://github.com/pypa/twine) from 6.0.1 to 6.1.0.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/twine/blob/main/docs/changelog.rst\">twine's changelog</a>.</em></p>\n<blockquote>\n<h2>Twine 6.1.0 (2025-01-17)</h2>\n<p>Features\n^^^^^^^^</p>\n<ul>\n<li>Twine now has preliminary built-in support for\n<code>Trusted Publishing &lt;https://docs.pypi.org/trusted-publishers/&gt;</code>_ as an\nauthentication mechanism. (<code>[#1194](https://github.com/pypa/twine/issues/1194) &lt;https://github.com/pypa/twine/pull/1194&gt;</code>_)</li>\n</ul>\n<p>Deprecations and Removals\n^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<ul>\n<li>\n<p>Remove support for <code>egg</code> and <code>wininst</code> distribution types. These are not\naccepted by PyPI and not produced by any modern build-backends.\n(<code>[#1195](https://github.com/pypa/twine/issues/1195) &lt;https://github.com/pypa/twine/issues/1195&gt;</code>_)</p>\n</li>\n<li>\n<p>Twine no longer supports <code>.tar.bz2</code> source distributions.\n(<code>[#1200](https://github.com/pypa/twine/issues/1200) &lt;https://github.com/pypa/twine/pull/1200&gt;</code>_)</p>\n</li>\n</ul>\n<p>Misc\n^^^^</p>\n<ul>\n<li>\n<p><code>packaging</code> is used instead of <code>pkginfo</code> for parsing and validating\nmetadata. This aligns metadata validation to the one performed by PyPI.\n<code>packaging</code> version 24.0 or later is required. Support for metadata\nversion 2.4 requires <code>packaging</code> 24.2 or later. <code>pkginfo</code> is not a\ndependency anymore. (<code>[#1180](https://github.com/pypa/twine/issues/1180) &lt;https://github.com/pypa/twine/issues/1180&gt;</code>_)</p>\n</li>\n<li>\n<p>Use <code>&quot;source&quot;</code> instead of <code>None</code> as <code>pyversion</code> for <code>sdist</code>\nuploads. This is what PyPI (and most likely other package indexes)\nexpects. (<code>[#1191](https://github.com/pypa/twine/issues/1191) &lt;https://github.com/pypa/twine/issues/1191&gt;</code>_)</p>\n</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/twine/commit/aa3a910cdef8e0a3cb4e893f4c371b58015f52e0\"><code>aa3a910</code></a> Update changelog for 6.1.0 (<a href=\"https://redirect.github.com/pypa/twine/issues/1214\">#1214</a>)</li>\n<li><a href=\"https://github.com/pypa/twine/commit/440603423ac579946aec0c15b280c6ef44477400\"><code>4406034</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/twine/issues/1208\">#1208</a> from dnicolodi/rm-setuptools</li>\n<li><a href=\"https://github.com/pypa/twine/commit/2ca55db34c537bbcb00e157e407320c1e5f8f08b\"><code>2ca55db</code></a> Simplify generation of test packages used in test_check</li>\n<li><a href=\"https://github.com/pypa/twine/commit/bffd2963bbc9c321670eea659d30178000a7bae7\"><code>bffd296</code></a> Move build_archive() from test_sdist to common helpers module</li>\n<li><a href=\"https://github.com/pypa/twine/commit/fd0646e12e25752d136f9520d7af0d108bc1f29e\"><code>fd0646e</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/twine/issues/1206\">#1206</a> from dnicolodi/rm-binary-blobs-part1</li>\n<li><a href=\"https://github.com/pypa/twine/commit/ab4ec8cc0f926a935070731246905f3985ff735d\"><code>ab4ec8c</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/twine/issues/1211\">#1211</a> from pypa/dependabot/github_actions/actions/upload-a...</li>\n<li><a href=\"https://github.com/pypa/twine/commit/b562f7422403b0cadff694d2e81b98cf2e28894f\"><code>b562f74</code></a> build(deps): bump actions/upload-artifact from 4.5.0 to 4.6.0</li>\n<li><a href=\"https://github.com/pypa/twine/commit/b2832de88421edd0d11bfe2ceb53470e12f18bb2\"><code>b2832de</code></a> Remove tests/fixtures/twine-1.5.0.zip</li>\n<li><a href=\"https://github.com/pypa/twine/commit/970851d9b188dc916e6d95083b1797bd6c277ce5\"><code>970851d</code></a> Remove tests/alt-fixtures/twine-1.5.0-py2.py3-none-any.whl</li>\n<li><a href=\"https://github.com/pypa/twine/commit/2386ca5300cd7bde59432834d362c07de61e9a53\"><code>2386ca5</code></a> build(deps): bump actions/upload-artifact from 4.4.3 to 4.5.0 (<a href=\"https://redirect.github.com/pypa/twine/issues/1205\">#1205</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/pypa/twine/compare/6.0.1...6.1.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=twine&package-manager=pip&previous-version=6.0.1&new-version=6.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "state": "closed",
        "created_at": "2025-01-27T01:24:24Z",
        "updated_at": "2025-01-27T01:27:14Z",
        "merged_at": "2025-01-27T01:27:07Z",
        "html_url": "https://github.com/flosell/trailscraper/pull/718",
        "user": "dependabot[bot]",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "flosell_trailscraper-718",
        "repo": "/flosell/trailscraper",
        "base_commit": "8a770beab5aaca938d36d09e526480d358bb75d1",
        "problem_statement": {},
        "edit_files": [
          "requirements-dev.txt"
        ],
        "oracle_files": [
          "bumpversion==0.6.0\npylint==3.3.3\npip==24.3.1\npytest==8.3.4\npytest-runner==6.0.1\nsetuptools==75.8.0\nfreezegun==1.5.1\nmoto==4.2.14\nwheel==0.45.1\ntwine==6.0.1"
        ],
        "test_patch": "",
        "patch_preview": "From 821ec50f74b5a330e23c4ad064c8a7b59cc949ee Mon Sep 17 00:00:00 2001\nFrom: \"dependabot[bot]\" <49699333+dependabot[bot]@users.noreply.github.com>\nDate: Mon, 27 Jan 2025 01:24:23 +0000\nSubject: [PATCH] Bump twine from 6.0.1 to 6.1.0\n\nBumps [twine](https://github.com/pypa/twine) from 6.0.1 to 6.1.0.\n- [Release notes](https://github.com/pypa/twine/releases)\n- [Changelog](https://github.com/pypa/twine/blob/main/docs/changelog.rst)\n- [Commits](https://github.com/pypa/twine/compare/6.0.1...6.1.0)\n\n--"
      },
      "patch": {
        "length": 1077,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "requirements-dev.txt",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 64,
        "total_lines": 9545,
        "total_bytes": 310388,
        "python_files": 47,
        "python_lines": 3599,
        "file_extensions": {
          ".txt": 4,
          ".md": 4,
          ".ini": 1,
          "": 3,
          ".tpl": 2,
          ".cfg": 1,
          ".py": 47,
          ".gz": 1,
          ".json": 1
        },
        "largest_files": [
          {
            "path": "trailscraper/known-iam-actions.txt",
            "size": 111228,
            "lines": 3936,
            "extension": ".txt"
          },
          {
            "path": "tests/cloudtrail/map_to_iam_sanity_test.py",
            "size": 42735,
            "lines": 1193,
            "extension": ".py"
          },
          {
            "path": "unknown_actions.txt",
            "size": 20770,
            "lines": 681,
            "extension": ".txt"
          },
          {
            "path": "go",
            "size": 9391,
            "lines": 353,
            "extension": ""
          },
          {
            "path": "README.md",
            "size": 8160,
            "lines": 283,
            "extension": ".md"
          },
          {
            "path": "trailscraper/cloudtrail.py",
            "size": 11189,
            "lines": 266,
            "extension": ".py"
          },
          {
            "path": "CHANGELOG.md",
            "size": 5760,
            "lines": 210,
            "extension": ".md"
          },
          {
            "path": "LICENSE",
            "size": 11356,
            "lines": 201,
            "extension": ""
          },
          {
            "path": "trailscraper/iam.py",
            "size": 6054,
            "lines": 186,
            "extension": ".py"
          },
          {
            "path": "tests/policy_generator/generate_policy_test.py",
            "size": 5602,
            "lines": 173,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 64,
        "files_changed_count": 1,
        "files_changed_ratio": 0.015625,
        "total_lines_in_repo": 9545,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.00020953378732320587,
        "pr_body_length": 6262,
        "commit_message_length": 418,
        "python_file_count": 47,
        "python_line_count": 3599
      }
    },
    {
      "tar_file_name": "fra31#auto-attack#pull#24",
      "repo_name": "fra31#auto-attack#pull#24",
      "success": true,
      "error": null,
      "commit": {
        "sha": "69051e7d4f9293bf36f0eb824a27d5efd2a2f6fa",
        "message": "update lists",
        "author": {
          "name": "fra31",
          "email": "francesco91.croce@gmail.com",
          "date": "2020-10-01T14:24:20Z"
        },
        "html_url": "https://github.com/fra31/auto-attack/commit/69051e7d4f9293bf36f0eb824a27d5efd2a2f6fa",
        "api_url": "https://api.github.com/repos/fra31/auto-attack/commits/69051e7d4f9293bf36f0eb824a27d5efd2a2f6fa"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/fra31#auto-attack#pull#24",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/fra31#auto-attack#pull#24.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/fra31#auto-attack#pull#24/source_code"
      },
      "pr": {
        "number": 24,
        "title": "Correct BibTeX typo",
        "body": "Thank you for the interesting work! I tried to cite your paper but noticed your BibTeX is slightly wrongÃ¢â‚¬â€it should say `author` instead of `authors`. This PR corrects that in case anybody else has the same issue.",
        "state": "closed",
        "created_at": "2020-10-01T16:00:08Z",
        "updated_at": "2020-10-01T16:37:48Z",
        "merged_at": "2020-10-01T16:37:43Z",
        "html_url": "https://github.com/fra31/auto-attack/pull/24",
        "user": "cassidylaidlaw",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "fra31_auto-attack-24",
        "repo": "/fra31/auto-attack",
        "base_commit": "69051e7d4f9293bf36f0eb824a27d5efd2a2f6fa",
        "problem_statement": {},
        "edit_files": [
          "README.md"
        ],
        "oracle_files": [
          "# AutoAttack\n\n\"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\"\\\n*Francesco Croce*, *Matthias Hein*\\\nICML 2020\\\n[https://arxiv.org/abs/2003.01690](https://arxiv.org/abs/2003.01690)\n\n\nWe propose to use an ensemble of four diverse attacks to reliably evaluate robustness:\n+ **APGD-CE**, our new step size-free version of PGD on the cross-entropy,\n+ **APGD-DLR**, our new step size-free version of PGD on the new DLR loss,\n+ **FAB**, which minimizes the norm of the adversarial perturbations [(Croce & Hein, 2019)](https://arxiv.org/abs/1907.02044),\n+ **Square Attack**, a query-efficient black-box attack [(Andriushchenko et al, 2019)](https://arxiv.org/abs/1912.00049).\n\n**Note**: we fix all the hyperparameters of the attacks, so no tuning is required to test every new classifier.\n\n## News\n+ [Aug 2020]\n\t+ **Updated version**: in order to *i)* scale AutoAttack (AA) to datasets with many classes and *ii)* have a faster and more accurate evaluation, we use APGD-DLR and FAB with their *targeted* versions.\n\t+ We add the evaluation of models on CIFAR-100 wrt Linf and CIFAR-10 wrt L2.\n+ [Jul 2020] A short version of the paper is accepted at [ICML'20 UDL workshop](https://sites.google.com/view/udlworkshop2020/) for a spotlight presentation!\n+ [Jun 2020] The paper is accepted at ICML 2020!\n\n# Adversarial Defenses Evaluation\nWe here list adversarial defenses, for many threat models, recently proposed and evaluated with the standard version of\n**AutoAttack (AA)**, including\n+ *untargeted APGD-CE* (no restarts),\n+ *targeted APGD-DLR* (9 target classes),\n+ *targeted FAB* (9 target classes),\n+ *Square Attack* (5000 queries).\n\nSee below for the more expensive AutoAttack+ (AA+) and more options.\n\nWe report the source of the model, i.e. if it is publicly *available*, if we received it from the *authors* or if we *retrained* it, the architecture, the clean accuracy and the reported robust accuracy (note that might be calculated on a subset of the test set or on different models trained with the same defense). The robust accuracy for AA is on the full test set.\n\nWe plan to add new models as they appear and are made available. Feel free to suggest new defenses to test!\n\n**To have a model added**: please check [here](https://github.com/fra31/auto-attack/issues/new/choose).\n\n## CIFAR-10 - Linf\nThe robust accuracy is evaluated at `eps = 8/255`, except for those marked with * for which `eps = 0.031`, where `eps` is the maximal Linf-norm allowed for the adversarial perturbations. The `eps` used is the same set in the original papers.\\\n**Note**: â€¡ indicates models which exploit additional data for training (e.g. unlabeled data, pre-training).\n\n|#    |paper           |model     |architecture |clean         |report. |AA  |\n|:---:|---|:---:|:---:|---:|---:|---:|\n|**1**| [(Carmon et al., 2019)](https://arxiv.org/abs/1905.13736)â€¡| *available*| WRN-28-10| 89.69| 62.5| 59.53|\n|**2**| [(Sehwag et al., 2020)](https://github.com/fra31/auto-attack/issues/7)â€¡| *available*| WRN-28-10| 88.98| -| 57.14|\n|**3**| [(Wang et al., 2020)](https://openreview.net/forum?id=rklOg6EFwS)â€¡| *available*| WRN-28-10| 87.50| 65.04| 56.29|\n|**4**| [(Alayrac et al., 2019)](https://arxiv.org/abs/1905.13725)â€¡| *available*| WRN-106-8| 86.46| 56.30| 56.03|\n|**5**| [(Hendrycks et al., 2019)](https://arxiv.org/abs/1901.09960)â€¡| *available*| WRN-28-10| 87.11| 57.4| 54.92|\n|**6**| [(Pang et al., 2020b)](https://arxiv.org/abs/2002.08619)| *available*| WRN-34-20| 85.14| -| 53.74|\n|**7**| [(Zhang et al., 2020b)](https://arxiv.org/abs/2002.11242)| *available*| WRN-34-10| 84.52| 54.36| 53.51|\n|**8**| [(Rice et al., 2020)](https://arxiv.org/abs/2002.11569)| *available*| WRN-34-20| 85.34| 58| 53.42|\n|**9**| [(Huang et al., 2020)](https://arxiv.org/abs/2002.10319)\\*| *available*| WRN-34-10| 83.48| 58.03| 53.34|\n|**10**| [(Zhang et al., 2019b)](https://arxiv.org/abs/1901.08573)\\*| *available*| WRN-34-10| 84.92| 56.43| 53.08|\n|**11**| [(Qin et al., 2019)](https://arxiv.org/abs/1907.02610v2)| *available*| WRN-40-8| 86.28| 52.81| 52.84|\n|**12**| [(Chen et al., 2020)](https://arxiv.org/abs/2003.12862)| *available*| RN-50 (x3)| 86.04| 54.64| 51.56|\n|**13**| [(Sitawarin et al., 2020)](https://github.com/fra31/auto-attack/issues/23)| *available*| WRN-34-10| 86.84| 50.72| 50.72|\n|**14**| [(Engstrom et al., 2019)](https://github.com/MadryLab/robustness)| *available*| RN-50| 87.03| 53.29| 49.25|\n|**15**| [(Kumari et al., 2019)](https://arxiv.org/abs/1905.05186)| *available*| WRN-34-10| 87.80| 53.04| 49.12|\n|**16**| [(Mao et al., 2019)](http://papers.nips.cc/paper/8339-metric-learning-for-adversarial-robustness)| *available*| WRN-34-10| 86.21| 50.03| 47.41|\n|**17**| [(Zhang et al., 2019a)](https://arxiv.org/abs/1905.00877)| *retrained*| WRN-34-10| 87.20| 47.98| 44.83|\n|**18**| [(Madry et al., 2018)](https://arxiv.org/abs/1706.06083)| *available*| WRN-34-10| 87.14| 47.04| 44.04|\n|**19**| [(Pang et al., 2020)](https://arxiv.org/abs/1905.10626)| *available*| RN-32| 80.89| 55.0| 43.48|\n|**20**| [(Wong et al., 2020)](https://arxiv.org/abs/2001.03994)| *available*| RN-18| 83.34| 46.06| 43.21|\n|**21**| [(Shafahi et al., 2019)](https://arxiv.org/abs/1904.12843)| *available*| WRN-34-10| 86.11| 46.19| 41.47|\n|**22**| [(Ding et al., 2020)](https://openreview.net/forum?id=HkeryxBtPB)| *available*| WRN-28-4| 84.36| 47.18| 41.44|\n|**23**| [(Atzmon et al., 2019)](https://arxiv.org/abs/1905.11911)\\*| *available*| RN-18| 81.30| 43.17| 40.22|\n|**24**| [(Moosavi-Dezfooli et al., 2019)](http://openaccess.thecvf.com/content_CVPR_2019/html/Moosavi-Dezfooli_Robustness_via_Curvature_Regularization_and_Vice_Versa_CVPR_2019_paper)| *authors*| WRN-28-10| 83.11| 41.4| 38.50|\n|**25**| [(Zhang & Wang, 2019)](http://papers.nips.cc/paper/8459-defense-against-adversarial-attacks-using-feature-scattering-based-adversarial-training)| *available*| WRN-28-10| 89.98| 60.6| 36.64|\n|**26**| [(Zhang & Xu, 2020)](https://openreview.net/forum?id=Syejj0NYvr&noteId=Syejj0NYvr)| *available*| WRN-28-10| 90.25| 68.7| 36.45|\n|**27**| [(Jang et al., 2019)](http://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html)| *available*| RN-20| 78.91| 37.40| 34.95|\n|**28**| [(Kim & Wang, 2020)](https://openreview.net/forum?id=rJlf_RVKwr)| *available*| WRN-34-10| 91.51| 57.23| 34.22|\n|**29**| [(Wang & Zhang, 2019)](http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html)| *available*| WRN-28-10| 92.80| 58.6| 29.35|\n|**30**| [(Xiao et al., 2020)](https://arxiv.org/abs/1905.10510)\\*| *available*| DenseNet-121| 79.28| 52.4| 18.50|\n|**31**| [(Jin & Rinard, 2020)](https://arxiv.org/abs/2003.04286)| *available*| RN-18| 90.84| 71.22| 1.35|\n|**32**| [(Mustafa et al., 2019)](https://arxiv.org/abs/1904.00887)| *available*| RN-110| 89.16| 32.32| 0.28|\n|**33**| [(Chan et al., 2020)](https://arxiv.org/abs/1912.10185)| *retrained*| WRN-34-10| 93.79| 15.5| 0.26|\n\n## CIFAR-100 - Linf\nThe robust accuracy is computed at `eps = 8/255` in the Linf-norm.\\\n**Note**: â€¡ indicates models which exploit additional data for training (e.g. unlabeled data, pre-training).\n\n|#    |paper           |model     |architecture |clean         |report. |AA  |\n|:---:|---|:---:|:---:|---:|---:|---:|\n|**1**| [(Hendrycks et al., 2019)](https://arxiv.org/abs/1901.09960)â€¡| *available*| WRN-28-10| 59.23| 33.5| 28.42|\n|**2**| [(Sitawarin et al., 2020)](https://github.com/fra31/auto-attack/issues/22)| *available*| WRN-34-10| 62.82| 24.57| 24.57|\n|**3**| [(Rice et al., 2020)](https://arxiv.org/abs/2002.11569)| *available*| RN-18| 53.83| 28.1| 18.95|\n\n## MNIST - Linf\nThe robust accuracy is computed at `eps = 0.3` in the Linf-norm.\n\n|#    |paper           |model     |clean         |report. |AA  |\n|:---:|---|:---:|---:|---:|---:|\n|**1**| [(Zhang et al., 2020a)](https://arxiv.org/abs/1906.06316)| *available*| 98.38| 96.38| 93.96|\n|**2**| [(Gowal et al., 2019)](https://arxiv.org/abs/1810.12715)| *available*| 98.34| 93.78| 92.83|\n|**3**| [(Zhang et al., 2019b)](https://arxiv.org/abs/1901.08573)| *available*| 99.48| 95.60| 92.81|\n|**4**| [(Ding et al., 2020)](https://openreview.net/forum?id=HkeryxBtPB)| *available*| 98.95| 92.59| 91.40|\n|**5**| [(Atzmon et al., 2019)](https://arxiv.org/abs/1905.11911)| *available*| 99.35| 97.35| 90.85|\n|**6**| [(Madry et al., 2018)](https://arxiv.org/abs/1706.06083)| *available*| 98.53| 89.62| 88.50|\n|**7**| [(Jang et al., 2019)](http://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html)| *available*| 98.47| 94.61| 87.99|\n|**8**| [(Wong et al., 2020)](https://arxiv.org/abs/2001.03994)| *available*| 98.50| 88.77| 82.93|\n|**9**| [(Taghanaki et al., 2019)](http://openaccess.thecvf.com/content_CVPR_2019/html/Taghanaki_A_Kernelized_Manifold_Mapping_to_Diminish_the_Effect_of_Adversarial_CVPR_2019_paper.html)| *retrained*| 98.86| 64.25| 0.00|\n\n## CIFAR-10 - L2\nThe robust accuracy is computed at `eps = 0.5` in the L2-norm.\\\n**Note**: â€¡ indicates models which exploit additional data for training (e.g. unlabeled data, pre-training).\n\n|#    |paper           |model     |architecture |clean         |report. |AA  |\n|:---:|---|:---:|:---:|---:|---:|---:|\n|**1**| [(Augustin et al., 2020)](https://arxiv.org/abs/2003.09461)â€¡| *authors*| RN-50| 91.08| 73.27| 72.91|\n|**2**| [(Engstrom et al., 2019)](https://github.com/MadryLab/robustness)| *available*| RN-50| 90.83| 70.11| 69.24|\n|**3**| [(Rice et al., 2020)](https://arxiv.org/abs/2002.11569)| *available*| RN-18| 88.67| 71.6| 67.68|\n|**4**| [(Rony et al., 2019)](https://arxiv.org/abs/1811.09600)| *available*| WRN-28-10| 89.05| 67.6| 66.44|\n|**5**| [(Ding et al., 2020)](https://openreview.net/forum?id=HkeryxBtPB)| *available*| WRN-28-4| 88.02| 66.18| 66.09|\n\n# How to use AutoAttack\n\n### Installation\n\n```\npip install git+https://github.com/fra31/auto-attack\n```\n\n### PyTorch models\nImport and initialize AutoAttack with\n\n```python\nfrom autoattack import AutoAttack\nadversary = AutoAttack(forward_pass, norm='Linf', eps=epsilon, version='standard')\n```\n\nwhere:\n+ `forward_pass` returns the logits and takes input with components in [0, 1] (NCHW format expected),\n+ `norm = ['Linf' | 'L2']` is the norm of the threat model,\n+ `eps` is the bound on the norm of the adversarial perturbations,\n+ `version = 'standard'` uses the standard version of AA.\n\nTo apply the standard evaluation, where the attacks are run sequentially on batches of size `bs` of `images`, use\n\n```python\nx_adv = adversary.run_standard_evaluation(images, labels, bs=batch_size)\n```\n\nTo run the attacks individually, use\n\n```python\ndict_adv = adversary.run_standard_evaluation_individual(images, labels, bs=batch_size)\n```\n\nwhich returns a dictionary with the adversarial examples found by each attack.\n\nTo specify a subset of attacks add e.g. `adversary.attacks_to_run = ['apgd-ce']`.\n\n### TensorFlow models\nTo evaluate models implemented in TensorFlow 1.X, use\n\n```python\nimport utils_tf\nmodel_adapted = utils_tf.ModelAdapter(logits, x_input, y_input, sess)\n\nfrom autoattack import AutoAttack\nadversary = AutoAttack(model_adapted, norm='Linf', eps=epsilon, version='standard', is_tf_model=True)\n```\n\nwhere:\n+ `logits` is the tensor with the logits given by the model,\n+ `x_input` is a placeholder for the input for the classifier (NHWC format expected),\n+ `y_input` is a placeholder for the correct labels,\n+ `sess` is a TF session.\n\nIf TensorFlow's version is 2.X, use\n\n```python\nimport utils_tf2\nmodel_adapted = utils_tf2.ModelAdapter(tf_model)\n\nfrom autoattack import AutoAttack\nadversary = AutoAttack(model_adapted, norm='Linf', eps=epsilon, version='standard', is_tf_model=True)\n```\n\nwhere:\n+ `tf_model` is tf.keras model without activation function 'softmax'\n\nThe evaluation can be run in the same way as done with PT models.\n\n### Examples\nExamples of how to use AutoAttack can be found in `examples/`. To run the standard evaluation on a pretrained\nPyTorch model on CIFAR-10 use\n```\npython eval.py [--individual] --version=['standard' | 'plus']\n```\nwhere the optional flags activate respectively the *individual* evaluations (all the attacks are run on the full test set) and the *version* of AA to use (see below).\n\n## Other versions\n### AutoAttack+\nA more expensive evaluation can be used specifying `version='plus'` when initializing AutoAttack. This includes\n+ *untargeted APGD-CE* (5 restarts),\n+ *untargeted APGD-DLR* (5 restarts),\n+ *untargeted FAB* (5 restarts),\n+ *Square Attack* (5000 queries),\n+ *targeted APGD-DLR* (9 target classes),\n+ *targeted FAB* (9 target classes).\n\n### Randomized defenses\nIn case of classifiers with stochastic components one can combine AA with Expectation over Transformation (EoT) as in [(Athalye et al., 2018)](https://arxiv.org/abs/1802.00420) specifying `version='rand'` when initializing AutoAttack.\nThis runs\n+ *untargeted APGD-CE* (no restarts, 20 iterations for EoT),\n+ *untargeted APGD-DLR* (no restarts, 20 iterations for EoT).\n\n### Custom version\nIt is possible to customize the attacks to run specifying `version='custom'` when initializing the attack and then, for example,\n```python\nif args.version == 'custom':\n\tadversary.attacks_to_run = ['apgd-ce', 'fab']\n        adversary.apgd.n_restarts = 2\n        adversary.fab.n_restarts = 2\n```\n\n## Other options\n### Random seed\nIt is possible to fix the random seed used for the attacks with, e.g., `adversary.seed = 0`. In this case the same seed is used for all the attacks used, otherwise a different random seed is picked for each attack.\n\n### Log results\nTo log the intermediate results of the evaluation specify `log_path=/path/to/logfile.txt` when initializing the attack.\n\n## Citation\n```\n@inproceedings{croce2020reliable,\n    title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},\n    authors = {Francesco Croce and Matthias Hein},\n    booktitle = {ICML},\n    year = {2020}\n}\n```\n"
        ],
        "test_patch": "",
        "patch_preview": "From dbc76316f8b5772d0e376f848384b7384d591b4d Mon Sep 17 00:00:00 2001\nFrom: Cassidy Laidlaw <cassidy256@gmail.com>\nDate: Thu, 1 Oct 2020 11:59:00 -0400\nSubject: [PATCH] Correct BibTeX typo\n\n---\n README.md | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/README.md b/README.md\nindex 76a6fcf..a3ac429 100644\n--- a/README.md\n+++ b/README.md\n@@ -228,7 +228,7 @@ To log the intermediate results of the evaluation specify `log_path=/path/to/log\n ```\n @inproceedings{croce2020reliable,\n "
      },
      "patch": {
        "length": 762,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "README.md",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [
        {
          "id": 702257490,
          "body": "Hi,\r\n\r\nthanks a lot for noticing and fixing it! And sorry for the inconvenience.",
          "user": "fra31",
          "created_at": "2020-10-01T16:37:48Z",
          "html_url": "https://github.com/fra31/auto-attack/pull/24#issuecomment-702257490"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 20,
        "total_lines": 298325,
        "total_bytes": 45816030,
        "python_files": 15,
        "python_lines": 3846,
        "file_extensions": {
          "": 1,
          ".py": 15,
          ".md": 1,
          ".pt": 1,
          ".h5": 1,
          ".pyc": 1
        },
        "largest_files": [
          {
            "path": "autoattack/examples/model_test.pt",
            "size": 44754497,
            "lines": 288668,
            "extension": ".pt"
          },
          {
            "path": "autoattack/examples/tf_model.weight.h5",
            "size": 875888,
            "lines": 5450,
            "extension": ".h5"
          },
          {
            "path": "autoattack/fab_pt.py",
            "size": 32148,
            "lines": 748,
            "extension": ".py"
          },
          {
            "path": "autoattack/fab_tf.py",
            "size": 32066,
            "lines": 745,
            "extension": ".py"
          },
          {
            "path": "autoattack/square.py",
            "size": 17594,
            "lines": 441,
            "extension": ".py"
          },
          {
            "path": "autoattack/autopgd_pt.py",
            "size": 20742,
            "lines": 431,
            "extension": ".py"
          },
          {
            "path": "autoattack/autopgd_tf.py",
            "size": 18674,
            "lines": 389,
            "extension": ".py"
          },
          {
            "path": "autoattack/autoattack.py",
            "size": 11640,
            "lines": 252,
            "extension": ".py"
          },
          {
            "path": "README.md",
            "size": 14078,
            "lines": 235,
            "extension": ".md"
          },
          {
            "path": "autoattack/utils_tf2.py",
            "size": 8038,
            "lines": 210,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 20,
        "files_changed_count": 1,
        "files_changed_ratio": 0.05,
        "total_lines_in_repo": 298325,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 6.704097879829046e-06,
        "pr_body_length": 214,
        "commit_message_length": 12,
        "python_file_count": 15,
        "python_line_count": 3846
      }
    },
    {
      "tar_file_name": "gbeced#pyalgotrade#pull#114",
      "repo_name": "gbeced#pyalgotrade#pull#114",
      "success": true,
      "error": null,
      "commit": {
        "sha": "2c78a21cc3f93f6457c20541c23c74d00e1b1533",
        "message": "Removed broken badges.",
        "author": {
          "name": "Gabriel Becedillas",
          "email": "gabriel.becedillas@gmail.com",
          "date": "2016-08-18T01:36:11Z"
        },
        "html_url": "https://github.com/gbeced/pyalgotrade/commit/2c78a21cc3f93f6457c20541c23c74d00e1b1533",
        "api_url": "https://api.github.com/repos/gbeced/pyalgotrade/commits/2c78a21cc3f93f6457c20541c23c74d00e1b1533"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/gbeced#pyalgotrade#pull#114",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/gbeced#pyalgotrade#pull#114.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/gbeced#pyalgotrade#pull#114/source_code"
      },
      "pr": {
        "number": 114,
        "title": "Upgrade to python 3.6",
        "body": "",
        "state": "closed",
        "created_at": "2017-10-12T00:50:41Z",
        "updated_at": "2018-07-23T03:45:10Z",
        "merged_at": null,
        "html_url": "https://github.com/gbeced/pyalgotrade/pull/114",
        "user": "snow-flake",
        "additions": 841,
        "deletions": 876,
        "changed_files": 120,
        "commits": 8
      },
      "swebench": {
        "instance_id": "gbeced_pyalgotrade-114",
        "repo": "/gbeced/pyalgotrade",
        "base_commit": "2c78a21cc3f93f6457c20541c23c74d00e1b1533",
        "problem_statement": {},
        "edit_files": [
          ".gitignore",
          "doc/conf.py",
          "pyalgotrade/bar.py",
          "pyalgotrade/barfeed/csvfeed.py",
          "pyalgotrade/barfeed/membf.py",
          "pyalgotrade/barfeed/resampled.py",
          "pyalgotrade/bitstamp/httpclient.py",
          "pyalgotrade/bitstamp/livebroker.py",
          "pyalgotrade/bitstamp/livefeed.py",
          "pyalgotrade/bitstamp/wsclient.py",
          "pyalgotrade/broker/__init__.py",
          "pyalgotrade/broker/backtesting.py",
          "pyalgotrade/broker/fillstrategy.py",
          "pyalgotrade/broker/slippage.py",
          "pyalgotrade/dataseries/__init__.py",
          "pyalgotrade/dataseries/bards.py",
          "pyalgotrade/dataseries/resampled.py",
          "pyalgotrade/eventprofiler.py",
          "pyalgotrade/feed/__init__.py",
          "pyalgotrade/feed/csvfeed.py",
          "pyalgotrade/feed/memfeed.py",
          "pyalgotrade/observer.py",
          "pyalgotrade/optimizer/base.py",
          "pyalgotrade/optimizer/local.py",
          "pyalgotrade/optimizer/worker.py",
          "pyalgotrade/optimizer/xmlrpcserver.py",
          "pyalgotrade/plotter.py",
          "pyalgotrade/resamplebase.py",
          "pyalgotrade/strategy/__init__.py",
          "pyalgotrade/strategy/position.py",
          "pyalgotrade/technical/cross.py",
          "pyalgotrade/technical/linreg.py",
          "pyalgotrade/technical/rsi.py",
          "pyalgotrade/tools/googlefinance.py",
          "pyalgotrade/tools/quandl.py",
          "pyalgotrade/tools/yahoofinance.py",
          "pyalgotrade/twitter/feed.py",
          "pyalgotrade/utils/csvutils.py",
          "pyalgotrade/websocket/client.py",
          "pyalgotrade/websocket/pusher.py",
          "samples/bbands.py",
          "samples/bccharts_example_2.py",
          "samples/compinv-1.py",
          "samples/compinv-3.py",
          "samples/csvfeed_1.py",
          "samples/eventstudy.py",
          "samples/market_timing.py",
          "samples/rsi2_sample.py",
          "samples/sample-strategy-analyzer.py",
          "samples/sma_crossover_sample.py",
          "samples/statarb_erniechan.py",
          "samples/technical-1.py",
          "samples/tutorial-4.py",
          "samples/tutorial-optimizer-local.py",
          "samples/tutorial-optimizer-server.py",
          "samples/vwap_momentum.py",
          "testcases/bar_test.py",
          "testcases/barfeed_test.py",
          "testcases/bitstamp_test.py",
          "testcases/broker_backtesting_test.py",
          "testcases/broker_test.py",
          "testcases/btcharts_test.py",
          "testcases/common.py",
          "testcases/csvfeed_test.py",
          "testcases/dataseries_test.py",
          "testcases/dbfeed_test.py",
          "testcases/drawdown_analyzer_test.py",
          "testcases/eventprofiler_test.py",
          "testcases/fill_strategy_test.py",
          "testcases/google_test.py",
          "testcases/http_server.py",
          "testcases/memfeed_test.py",
          "testcases/multi_instrument_strategy_test.py",
          "testcases/ninjatraderfeed_test.py",
          "testcases/observer_test.py",
          "testcases/optimizer_testcase.py",
          "testcases/plotter_test.py",
          "testcases/position_test.py",
          "testcases/quandl_test.py",
          "testcases/resample_test.py",
          "testcases/returns_analyzer_test.py",
          "testcases/sharpe_analyzer_test.py",
          "testcases/slippage_model_test.py",
          "testcases/smacrossover_strategy_test.py",
          "testcases/strategy_test.py",
          "testcases/talib_test.py",
          "testcases/technical_atr_test.py",
          "testcases/technical_bollinger_test.py",
          "testcases/technical_cross_test.py",
          "testcases/technical_cumret_test.py",
          "testcases/technical_highlow_test.py",
          "testcases/technical_hurst_test.py",
          "testcases/technical_linebreak_test.py",
          "testcases/technical_linreg_test.py",
          "testcases/technical_ma_test.py",
          "testcases/technical_macd_test.py",
          "testcases/technical_ratio_test.py",
          "testcases/technical_roc_test.py",
          "testcases/technical_rsi_test.py",
          "testcases/technical_stats_test.py",
          "testcases/technical_stoch_test.py",
          "testcases/technical_test.py",
          "testcases/technical_trend_test.py",
          "testcases/technical_vwap_test.py",
          "testcases/trades_analyzer_test.py",
          "testcases/twitter_test.py",
          "testcases/utils_test.py",
          "testcases/yahoo_test.py",
          "testcases/yahoofeed_test.py",
          "tools/symbols/get_merval_symbols.py",
          "tools/symbols/get_nasdaq_symbols.py",
          "tools/symbols/get_nyse_symbols.py",
          "tools/symbols/get_sp500_symbols.py",
          "tools/yahoodbfeed/analyze_gaps.py",
          "tools/yahoodbfeed/download_data.py",
          ".travis.yml",
          "travis/Dockerfile",
          ".travis.yml",
          "docker/Dockerfile",
          "travis/Dockerfile",
          "docker/Dockerfile",
          ".dockerignore",
          ".travis.yml",
          "docker/Dockerfile",
          "travis/Dockerfile",
          ".gitignore",
          "pyalgotrade/barfeed/membf.py",
          "pyalgotrade/feed/memfeed.py",
          "travis/Dockerfile",
          "travis/run_tests.sh",
          ".dockerignore",
          ".gitignore",
          "travis/run_tests.sh"
        ],
        "oracle_files": [
          "*pyc\nMANIFEST\ndoc/_build\n.idea\n.coverage\n.ipynb_checkpoints\n.ropeproject\n\ndata\ndist\nbuild\nPyAlgoTrade.egg-info\n\ntestcases/twitter_credentials.py\n",
          "# -*- coding: utf-8 -*-\n#\n# PyAlgoTrade documentation build configuration file, created by\n# sphinx-quickstart on Fri Nov  4 21:48:31 2011.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('..'))\n\nautoclass_content=\"both\"\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'PyAlgoTrade'\ncopyright = u'2011-2014, Gabriel MartÃ­n Becedillas Ruiz'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '0.18'\n# The full version, including alpha/beta/rc tags.\nrelease = '0.18'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'default'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'PyAlgoTradedoc'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'PyAlgoTrade.tex', u'PyAlgoTrade Documentation',\n   u'Gabriel MartÃ­n Becedillas Ruiz', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'pyalgotrade', u'PyAlgoTrade Documentation',\n     [u'Gabriel MartÃ­n Becedillas Ruiz'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'PyAlgoTrade', u'PyAlgoTrade Documentation',\n   u'Gabriel MartÃ­n Becedillas Ruiz', 'PyAlgoTrade', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\n\nclass Frequency(object):\n\n    \"\"\"Enum like class for bar frequencies. Valid values are:\n\n    * **Frequency.TRADE**: The bar represents a single trade.\n    * **Frequency.SECOND**: The bar summarizes the trading activity during 1 second.\n    * **Frequency.MINUTE**: The bar summarizes the trading activity during 1 minute.\n    * **Frequency.HOUR**: The bar summarizes the trading activity during 1 hour.\n    * **Frequency.DAY**: The bar summarizes the trading activity during 1 day.\n    * **Frequency.WEEK**: The bar summarizes the trading activity during 1 week.\n    * **Frequency.MONTH**: The bar summarizes the trading activity during 1 month.\n    \"\"\"\n\n    # It is important for frequency values to get bigger for bigger windows.\n    TRADE = -1\n    SECOND = 1\n    MINUTE = 60\n    HOUR = 60*60\n    DAY = 24*60*60\n    WEEK = 24*60*60*7\n    MONTH = 24*60*60*31\n\n\nclass Bar(object):\n\n    \"\"\"A Bar is a summary of the trading activity for a security in a given period.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def setUseAdjustedValue(self, useAdjusted):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getUseAdjValue(self):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getDateTime(self):\n        \"\"\"Returns the :class:`datetime.datetime`.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getOpen(self, adjusted=False):\n        \"\"\"Returns the opening price.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getHigh(self, adjusted=False):\n        \"\"\"Returns the highest price.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getLow(self, adjusted=False):\n        \"\"\"Returns the lowest price.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getClose(self, adjusted=False):\n        \"\"\"Returns the closing price.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getVolume(self):\n        \"\"\"Returns the volume.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getAdjClose(self):\n        \"\"\"Returns the adjusted closing price.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getFrequency(self):\n        \"\"\"The bar's period.\"\"\"\n        raise NotImplementedError()\n\n    def getTypicalPrice(self):\n        \"\"\"Returns the typical price.\"\"\"\n        return (self.getHigh() + self.getLow() + self.getClose()) / 3.0\n\n    @abc.abstractmethod\n    def getPrice(self):\n        \"\"\"Returns the closing or adjusted closing price.\"\"\"\n        raise NotImplementedError()\n\n    def getExtraColumns(self):\n        return {}\n\n\nclass BasicBar(Bar):\n    # Optimization to reduce memory footprint.\n    __slots__ = (\n        '__dateTime',\n        '__open',\n        '__close',\n        '__high',\n        '__low',\n        '__volume',\n        '__adjClose',\n        '__frequency',\n        '__useAdjustedValue',\n        '__extra',\n    )\n\n    def __init__(self, dateTime, open_, high, low, close, volume, adjClose, frequency, extra={}):\n        if high < low:\n            raise Exception(\"high < low on %s\" % (dateTime))\n        elif high < open_:\n            raise Exception(\"high < open on %s\" % (dateTime))\n        elif high < close:\n            raise Exception(\"high < close on %s\" % (dateTime))\n        elif low > open_:\n            raise Exception(\"low > open on %s\" % (dateTime))\n        elif low > close:\n            raise Exception(\"low > close on %s\" % (dateTime))\n\n        self.__dateTime = dateTime\n        self.__open = open_\n        self.__close = close\n        self.__high = high\n        self.__low = low\n        self.__volume = volume\n        self.__adjClose = adjClose\n        self.__frequency = frequency\n        self.__useAdjustedValue = False\n        self.__extra = extra\n\n    def __setstate__(self, state):\n        (self.__dateTime,\n            self.__open,\n            self.__close,\n            self.__high,\n            self.__low,\n            self.__volume,\n            self.__adjClose,\n            self.__frequency,\n            self.__useAdjustedValue,\n            self.__extra) = state\n\n    def __getstate__(self):\n        return (\n            self.__dateTime,\n            self.__open,\n            self.__close,\n            self.__high,\n            self.__low,\n            self.__volume,\n            self.__adjClose,\n            self.__frequency,\n            self.__useAdjustedValue,\n            self.__extra\n        )\n\n    def setUseAdjustedValue(self, useAdjusted):\n        if useAdjusted and self.__adjClose is None:\n            raise Exception(\"Adjusted close is not available\")\n        self.__useAdjustedValue = useAdjusted\n\n    def getUseAdjValue(self):\n        return self.__useAdjustedValue\n\n    def getDateTime(self):\n        return self.__dateTime\n\n    def getOpen(self, adjusted=False):\n        if adjusted:\n            if self.__adjClose is None:\n                raise Exception(\"Adjusted close is missing\")\n            return self.__adjClose * self.__open / float(self.__close)\n        else:\n            return self.__open\n\n    def getHigh(self, adjusted=False):\n        if adjusted:\n            if self.__adjClose is None:\n                raise Exception(\"Adjusted close is missing\")\n            return self.__adjClose * self.__high / float(self.__close)\n        else:\n            return self.__high\n\n    def getLow(self, adjusted=False):\n        if adjusted:\n            if self.__adjClose is None:\n                raise Exception(\"Adjusted close is missing\")\n            return self.__adjClose * self.__low / float(self.__close)\n        else:\n            return self.__low\n\n    def getClose(self, adjusted=False):\n        if adjusted:\n            if self.__adjClose is None:\n                raise Exception(\"Adjusted close is missing\")\n            return self.__adjClose\n        else:\n            return self.__close\n\n    def getVolume(self):\n        return self.__volume\n\n    def getAdjClose(self):\n        return self.__adjClose\n\n    def getFrequency(self):\n        return self.__frequency\n\n    def getPrice(self):\n        if self.__useAdjustedValue:\n            return self.__adjClose\n        else:\n            return self.__close\n\n    def getExtraColumns(self):\n        return self.__extra\n\n\nclass Bars(object):\n\n    \"\"\"A group of :class:`Bar` objects.\n\n    :param barDict: A map of instrument to :class:`Bar` objects.\n    :type barDict: map.\n\n    .. note::\n        All bars must have the same datetime.\n    \"\"\"\n\n    def __init__(self, barDict):\n        if len(barDict) == 0:\n            raise Exception(\"No bars supplied\")\n\n        # Check that bar datetimes are in sync\n        firstDateTime = None\n        firstInstrument = None\n        for instrument, currentBar in barDict.iteritems():\n            if firstDateTime is None:\n                firstDateTime = currentBar.getDateTime()\n                firstInstrument = instrument\n            elif currentBar.getDateTime() != firstDateTime:\n                raise Exception(\"Bar data times are not in sync. %s %s != %s %s\" % (\n                    instrument,\n                    currentBar.getDateTime(),\n                    firstInstrument,\n                    firstDateTime\n                ))\n\n        self.__barDict = barDict\n        self.__dateTime = firstDateTime\n\n    def __getitem__(self, instrument):\n        \"\"\"Returns the :class:`pyalgotrade.bar.Bar` for the given instrument.\n        If the instrument is not found an exception is raised.\"\"\"\n        return self.__barDict[instrument]\n\n    def __contains__(self, instrument):\n        \"\"\"Returns True if a :class:`pyalgotrade.bar.Bar` for the given instrument is available.\"\"\"\n        return instrument in self.__barDict\n\n    def items(self):\n        return self.__barDict.items()\n\n    def keys(self):\n        return self.__barDict.keys()\n\n    def getInstruments(self):\n        \"\"\"Returns the instrument symbols.\"\"\"\n        return self.__barDict.keys()\n\n    def getDateTime(self):\n        \"\"\"Returns the :class:`datetime.datetime` for this set of bars.\"\"\"\n        return self.__dateTime\n\n    def getBar(self, instrument):\n        \"\"\"Returns the :class:`pyalgotrade.bar.Bar` for the given instrument or None if the instrument is not found.\"\"\"\n        return self.__barDict.get(instrument, None)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.utils import csvutils\nfrom pyalgotrade.barfeed import membf\nfrom pyalgotrade import bar\n\nimport datetime\nimport pytz\n\n\n# Interface for csv row parsers.\nclass RowParser(object):\n    def parseBar(self, csvRowDict):\n        raise NotImplementedError()\n\n    def getFieldNames(self):\n        raise NotImplementedError()\n\n    def getDelimiter(self):\n        raise NotImplementedError()\n\n\n# Interface for bar filters.\nclass BarFilter(object):\n    def includeBar(self, bar_):\n        raise NotImplementedError()\n\n\nclass DateRangeFilter(BarFilter):\n    def __init__(self, fromDate=None, toDate=None):\n        self.__fromDate = fromDate\n        self.__toDate = toDate\n\n    def includeBar(self, bar_):\n        if self.__toDate and bar_.getDateTime() > self.__toDate:\n            return False\n        if self.__fromDate and bar_.getDateTime() < self.__fromDate:\n            return False\n        return True\n\n\n# US Equities Regular Trading Hours filter\n# Monday ~ Friday\n# 9:30 ~ 16 (GMT-5)\nclass USEquitiesRTH(DateRangeFilter):\n    timezone = pytz.timezone(\"US/Eastern\")\n\n    def __init__(self, fromDate=None, toDate=None):\n        super(USEquitiesRTH, self).__init__(fromDate, toDate)\n\n        self.__fromTime = datetime.time(9, 30, 0)\n        self.__toTime = datetime.time(16, 0, 0)\n\n    def includeBar(self, bar_):\n        ret = super(USEquitiesRTH, self).includeBar(bar_)\n        if ret:\n            # Check day of week\n            barDay = bar_.getDateTime().weekday()\n            if barDay > 4:\n                return False\n\n            # Check time\n            barTime = dt.localize(bar_.getDateTime(), USEquitiesRTH.timezone).time()\n            if barTime < self.__fromTime:\n                return False\n            if barTime > self.__toTime:\n                return False\n        return ret\n\n\nclass BarFeed(membf.BarFeed):\n    \"\"\"Base class for CSV file based :class:`pyalgotrade.barfeed.BarFeed`.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, frequency, maxLen=None):\n        super(BarFeed, self).__init__(frequency, maxLen)\n\n        self.__barFilter = None\n        self.__dailyTime = datetime.time(0, 0, 0)\n\n    def getDailyBarTime(self):\n        return self.__dailyTime\n\n    def setDailyBarTime(self, time):\n        self.__dailyTime = time\n\n    def getBarFilter(self):\n        return self.__barFilter\n\n    def setBarFilter(self, barFilter):\n        self.__barFilter = barFilter\n\n    def addBarsFromCSV(self, instrument, path, rowParser):\n        # Load the csv file\n        loadedBars = []\n        reader = csvutils.FastDictReader(open(path, \"r\"), fieldnames=rowParser.getFieldNames(), delimiter=rowParser.getDelimiter())\n        for row in reader:\n            bar_ = rowParser.parseBar(row)\n            if bar_ is not None and (self.__barFilter is None or self.__barFilter.includeBar(bar_)):\n                loadedBars.append(bar_)\n\n        self.addBarsFromSequence(instrument, loadedBars)\n\n\nclass GenericRowParser(RowParser):\n    def __init__(self, columnNames, dateTimeFormat, dailyBarTime, frequency, timezone, barClass=bar.BasicBar):\n        self.__dateTimeFormat = dateTimeFormat\n        self.__dailyBarTime = dailyBarTime\n        self.__frequency = frequency\n        self.__timezone = timezone\n        self.__haveAdjClose = False\n        self.__barClass = barClass\n        # Column names.\n        self.__dateTimeColName = columnNames[\"datetime\"]\n        self.__openColName = columnNames[\"open\"]\n        self.__highColName = columnNames[\"high\"]\n        self.__lowColName = columnNames[\"low\"]\n        self.__closeColName = columnNames[\"close\"]\n        self.__volumeColName = columnNames[\"volume\"]\n        self.__adjCloseColName = columnNames[\"adj_close\"]\n        self.__columnNames = columnNames\n\n    def _parseDate(self, dateString):\n        ret = datetime.datetime.strptime(dateString, self.__dateTimeFormat)\n\n        if self.__dailyBarTime is not None:\n            ret = datetime.datetime.combine(ret, self.__dailyBarTime)\n        # Localize the datetime if a timezone was given.\n        if self.__timezone:\n            ret = dt.localize(ret, self.__timezone)\n        return ret\n\n    def barsHaveAdjClose(self):\n        return self.__haveAdjClose\n\n    def getFieldNames(self):\n        # It is expected for the first row to have the field names.\n        return None\n\n    def getDelimiter(self):\n        return \",\"\n\n    def parseBar(self, csvRowDict):\n        dateTime = self._parseDate(csvRowDict[self.__dateTimeColName])\n        open_ = float(csvRowDict[self.__openColName])\n        high = float(csvRowDict[self.__highColName])\n        low = float(csvRowDict[self.__lowColName])\n        close = float(csvRowDict[self.__closeColName])\n        volume = float(csvRowDict[self.__volumeColName])\n        adjClose = None\n        if self.__adjCloseColName is not None:\n            adjCloseValue = csvRowDict.get(self.__adjCloseColName, \"\")\n            if len(adjCloseValue) > 0:\n                adjClose = float(adjCloseValue)\n                self.__haveAdjClose = True\n\n        # Process extra columns.\n        extra = {}\n        for k, v in csvRowDict.iteritems():\n            if k not in self.__columnNames:\n                extra[k] = csvutils.float_or_string(v)\n\n        return self.__barClass(\n            dateTime, open_, high, low, close, volume, adjClose, self.__frequency, extra=extra\n        )\n\n\nclass GenericBarFeed(BarFeed):\n    \"\"\"A BarFeed that loads bars from CSV files that have the following format:\n    ::\n\n        Date Time,Open,High,Low,Close,Volume,Adj Close\n        2013-01-01 13:59:00,13.51001,13.56,13.51,13.56,273.88014126,13.51001\n\n    :param frequency: The frequency of the bars. Check :class:`pyalgotrade.bar.Frequency`.\n    :param timezone: The default timezone to use to localize bars. Check :mod:`pyalgotrade.marketsession`.\n    :type timezone: A pytz timezone.\n    :param maxLen: The maximum number of values that the :class:`pyalgotrade.dataseries.bards.BarDataSeries` will hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n\n    .. note::\n        * The CSV file **must** have the column names in the first row.\n        * It is ok if the **Adj Close** column is empty.\n        * When working with multiple instruments:\n\n         * If all the instruments loaded are in the same timezone, then the timezone parameter may not be specified.\n         * If any of the instruments loaded are in different timezones, then the timezone parameter should be set.\n    \"\"\"\n\n    def __init__(self, frequency, timezone=None, maxLen=None):\n        super(GenericBarFeed, self).__init__(frequency, maxLen)\n\n        self.__timezone = timezone\n        # Assume bars don't have adjusted close. This will be set to True after\n        # loading the first file if the adj_close column is there.\n        self.__haveAdjClose = False\n\n        self.__barClass = bar.BasicBar\n\n        self.__dateTimeFormat = \"%Y-%m-%d %H:%M:%S\"\n        self.__columnNames = {\n            \"datetime\": \"Date Time\",\n            \"open\": \"Open\",\n            \"high\": \"High\",\n            \"low\": \"Low\",\n            \"close\": \"Close\",\n            \"volume\": \"Volume\",\n            \"adj_close\": \"Adj Close\",\n        }\n        # self.__dateTimeFormat expects time to be set so there is no need to\n        # fix time.\n        self.setDailyBarTime(None)\n\n    def barsHaveAdjClose(self):\n        return self.__haveAdjClose\n\n    def setNoAdjClose(self):\n        self.__columnNames[\"adj_close\"] = None\n        self.__haveAdjClose = False\n\n    def setColumnName(self, col, name):\n        self.__columnNames[col] = name\n\n    def setDateTimeFormat(self, dateTimeFormat):\n        self.__dateTimeFormat = dateTimeFormat\n\n    def setBarClass(self, barClass):\n        self.__barClass = barClass\n\n    def addBarsFromCSV(self, instrument, path, timezone=None):\n        \"\"\"Loads bars for a given instrument from a CSV formatted file.\n        The instrument gets registered in the bar feed.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param path: The path to the CSV file.\n        :type path: string.\n        :param timezone: The timezone to use to localize bars. Check :mod:`pyalgotrade.marketsession`.\n        :type timezone: A pytz timezone.\n        \"\"\"\n\n        if timezone is None:\n            timezone = self.__timezone\n\n        rowParser = GenericRowParser(\n            self.__columnNames, self.__dateTimeFormat, self.getDailyBarTime(), self.getFrequency(),\n            timezone, self.__barClass\n        )\n\n        super(GenericBarFeed, self).addBarsFromCSV(instrument, path, rowParser)\n\n        if rowParser.barsHaveAdjClose():\n            self.__haveAdjClose = True\n        elif self.__haveAdjClose:\n            raise Exception(\"Previous bars had adjusted close and these ones don't have.\")\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import barfeed\nfrom pyalgotrade import bar\nfrom pyalgotrade import utils\n\n\n# A non real-time BarFeed responsible for:\n# - Holding bars in memory.\n# - Aligning them with respect to time.\n#\n# Subclasses should:\n# - Forward the call to start() if they override it.\n\nclass BarFeed(barfeed.BaseBarFeed):\n    def __init__(self, frequency, maxLen=None):\n        super(BarFeed, self).__init__(frequency, maxLen)\n\n        self.__bars = {}\n        self.__nextPos = {}\n        self.__started = False\n        self.__currDateTime = None\n\n    def reset(self):\n        self.__nextPos = {}\n        for instrument in self.__bars.keys():\n            self.__nextPos.setdefault(instrument, 0)\n        self.__currDateTime = None\n        super(BarFeed, self).reset()\n\n    def getCurrentDateTime(self):\n        return self.__currDateTime\n\n    def start(self):\n        super(BarFeed, self).start()\n        self.__started = True\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def addBarsFromSequence(self, instrument, bars):\n        if self.__started:\n            raise Exception(\"Can't add more bars once you started consuming bars\")\n\n        self.__bars.setdefault(instrument, [])\n        self.__nextPos.setdefault(instrument, 0)\n\n        # Add and sort the bars\n        self.__bars[instrument].extend(bars)\n        barCmp = lambda x, y: cmp(x.getDateTime(), y.getDateTime())\n        self.__bars[instrument].sort(barCmp)\n\n        self.registerInstrument(instrument)\n\n    def eof(self):\n        ret = True\n        # Check if there is at least one more bar to return.\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars):\n                ret = False\n                break\n        return ret\n\n    def peekDateTime(self):\n        ret = None\n\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars):\n                ret = utils.safe_min(ret, bars[nextPos].getDateTime())\n        return ret\n\n    def getNextBars(self):\n        # All bars must have the same datetime. We will return all the ones with the smallest datetime.\n        smallestDateTime = self.peekDateTime()\n\n        if smallestDateTime is None:\n            return None\n\n        # Make a second pass to get all the bars that had the smallest datetime.\n        ret = {}\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars) and bars[nextPos].getDateTime() == smallestDateTime:\n                ret[instrument] = bars[nextPos]\n                self.__nextPos[instrument] += 1\n\n        if self.__currDateTime == smallestDateTime:\n            raise Exception(\"Duplicate bars found for %s on %s\" % (ret.keys(), smallestDateTime))\n\n        self.__currDateTime = smallestDateTime\n        return bar.Bars(ret)\n\n    def loadAll(self):\n        for dateTime, bars in self:\n            pass\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom pyalgotrade import barfeed\nfrom pyalgotrade.dataseries import resampled\nfrom pyalgotrade import resamplebase\nfrom pyalgotrade import bar\n\n\nclass BarsGrouper(resamplebase.Grouper):\n    def __init__(self, groupDateTime, bars, frequency):\n        resamplebase.Grouper.__init__(self, groupDateTime)\n        self.__barGroupers = {}\n        self.__frequency = frequency\n\n        # Initialize BarGrouper instances for each instrument.\n        for instrument, bar_ in bars.items():\n            barGrouper = resampled.BarGrouper(groupDateTime, bar_, frequency)\n            self.__barGroupers[instrument] = barGrouper\n\n    def addValue(self, value):\n        # Update or initialize BarGrouper instances for each instrument.\n        for instrument, bar_ in value.items():\n            barGrouper = self.__barGroupers.get(instrument)\n            if barGrouper:\n                barGrouper.addValue(bar_)\n            else:\n                barGrouper = resampled.BarGrouper(self.getDateTime(), bar_, self.__frequency)\n                self.__barGroupers[instrument] = barGrouper\n\n    def getGrouped(self):\n        bar_dict = {}\n        for instrument, grouper in self.__barGroupers.items():\n            bar_dict[instrument] = grouper.getGrouped()\n        return bar.Bars(bar_dict)\n\n\nclass ResampledBarFeed(barfeed.BaseBarFeed):\n\n    def __init__(self, barFeed, frequency, maxLen=None):\n        super(ResampledBarFeed, self).__init__(frequency, maxLen)\n\n        if not isinstance(barFeed, barfeed.BaseBarFeed):\n            raise Exception(\"barFeed must be a barfeed.BaseBarFeed instance\")\n\n        if not resamplebase.is_valid_frequency(frequency):\n            raise Exception(\"Unsupported frequency\")\n\n        # Register the same instruments as in the underlying barfeed.\n        for instrument in barFeed.getRegisteredInstruments():\n            self.registerInstrument(instrument)\n\n        self.__values = []\n        self.__barFeed = barFeed\n        self.__grouper = None\n        self.__range = None\n\n        barFeed.getNewValuesEvent().subscribe(self.__onNewValues)\n\n    def __onNewValues(self, dateTime, value):\n        if self.__range is None:\n            self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n            self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n        elif self.__range.belongs(dateTime):\n            self.__grouper.addValue(value)\n        else:\n            self.__values.append(self.__grouper.getGrouped())\n            self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n            self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n\n    def getCurrentDateTime(self):\n        return self.__barFeed.getCurrentDateTime()\n\n    def barsHaveAdjClose(self):\n        return self.__barFeed.barsHaveAdjClose()\n\n    def getNextBars(self):\n        ret = None\n        if len(self.__values):\n            ret = self.__values.pop(0)\n        return ret\n\n    def eof(self):\n        return len(self.__values) == 0\n\n    def join(self):\n        pass\n\n    def peekDateTime(self):\n        # We can't determine when the next event will be generated since it'll\n        # depend on the values generated by the barfeed being wrapped.\n        return None\n\n    def start(self):\n        super(ResampledBarFeed, self).start()\n\n    def stop(self):\n        pass\n\n    def checkNow(self, dateTime):\n        if self.__range is not None and not self.__range.belongs(dateTime):\n            self.__values.append(self.__grouper.getGrouped())\n            self.__grouper = None\n            self.__range = None\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport time\nimport datetime\nimport hmac\nimport hashlib\nimport requests\nimport threading\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.bitstamp import common\n\nimport logging\nlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\n\ndef parse_datetime(dateTime):\n    try:\n        ret = datetime.datetime.strptime(dateTime, \"%Y-%m-%d %H:%M:%S\")\n    except ValueError:\n        ret = datetime.datetime.strptime(dateTime, \"%Y-%m-%d %H:%M:%S.%f\")\n    return dt.as_utc(ret)\n\n\nclass AccountBalance(object):\n    def __init__(self, jsonDict):\n        self.__jsonDict = jsonDict\n\n    def getDict(self):\n        return self.__jsonDict\n\n    def getUSDAvailable(self):\n        return float(self.__jsonDict[\"usd_available\"])\n\n    def getBTCAvailable(self):\n        return float(self.__jsonDict[\"btc_available\"])\n\n\nclass Order(object):\n    def __init__(self, jsonDict):\n        self.__jsonDict = jsonDict\n\n    def getDict(self):\n        return self.__jsonDict\n\n    def getId(self):\n        return int(self.__jsonDict[\"id\"])\n\n    def isBuy(self):\n        return self.__jsonDict[\"type\"] == 0\n\n    def isSell(self):\n        return self.__jsonDict[\"type\"] == 1\n\n    def getPrice(self):\n        return float(self.__jsonDict[\"price\"])\n\n    def getAmount(self):\n        return float(self.__jsonDict[\"amount\"])\n\n    def getDateTime(self):\n        return parse_datetime(self.__jsonDict[\"datetime\"])\n\n\nclass UserTransaction(object):\n    def __init__(self, jsonDict):\n        self.__jsonDict = jsonDict\n\n    def getDict(self):\n        return self.__jsonDict\n\n    def getBTC(self):\n        return float(self.__jsonDict[\"btc\"])\n\n    def getBTCUSD(self):\n        return float(self.__jsonDict[\"btc_usd\"])\n\n    def getDateTime(self):\n        return parse_datetime(self.__jsonDict[\"datetime\"])\n\n    def getFee(self):\n        return float(self.__jsonDict[\"fee\"])\n\n    def getId(self):\n        return int(self.__jsonDict[\"id\"])\n\n    def getOrderId(self):\n        return int(self.__jsonDict[\"order_id\"])\n\n    def getUSD(self):\n        return float(self.__jsonDict[\"usd\"])\n\n\nclass HTTPClient(object):\n    USER_AGENT = \"PyAlgoTrade\"\n    REQUEST_TIMEOUT = 30\n\n    class UserTransactionType:\n        MARKET_TRADE = 2\n\n    def __init__(self, clientId, key, secret):\n        self.__clientId = clientId\n        self.__key = key\n        self.__secret = secret\n        self.__prevNonce = None\n        self.__lock = threading.Lock()\n\n    def _getNonce(self):\n        ret = int(time.time())\n        if ret == self.__prevNonce:\n            ret += 1\n        self.__prevNonce = ret\n        return ret\n\n    def _buildQuery(self, params):\n        # Build the signature.\n        nonce = self._getNonce()\n        message = \"%d%s%s\" % (nonce, self.__clientId, self.__key)\n        signature = hmac.new(self.__secret, msg=message, digestmod=hashlib.sha256).hexdigest().upper()\n\n        # Headers\n        headers = {}\n        headers[\"User-Agent\"] = HTTPClient.USER_AGENT\n\n        # POST data.\n        data = {}\n        data.update(params)\n        data[\"key\"] = self.__key\n        data[\"signature\"] = signature\n        data[\"nonce\"] = nonce\n\n        return (data, headers)\n\n    def _post(self, url, params):\n        common.logger.debug(\"POST to %s with params %s\" % (url, str(params)))\n\n        # Serialize access to nonce generation and http requests to avoid\n        # sending them in the wrong order.\n        with self.__lock:\n            data, headers = self._buildQuery(params)\n            response = requests.post(url, headers=headers, data=data, timeout=HTTPClient.REQUEST_TIMEOUT)\n            response.raise_for_status()\n\n        jsonResponse = response.json()\n\n        # Check for errors.\n        if isinstance(jsonResponse, dict):\n            error = jsonResponse.get(\"error\")\n            if error is not None:\n                raise Exception(error)\n\n        return jsonResponse\n\n    def getAccountBalance(self):\n        url = \"https://www.bitstamp.net/api/balance/\"\n        jsonResponse = self._post(url, {})\n        return AccountBalance(jsonResponse)\n\n    def getOpenOrders(self):\n        url = \"https://www.bitstamp.net/api/open_orders/\"\n        jsonResponse = self._post(url, {})\n        return [Order(json_open_order) for json_open_order in jsonResponse]\n\n    def cancelOrder(self, orderId):\n        url = \"https://www.bitstamp.net/api/cancel_order/\"\n        params = {\"id\": orderId}\n        jsonResponse = self._post(url, params)\n        if jsonResponse != True:\n            raise Exception(\"Failed to cancel order\")\n\n    def buyLimit(self, limitPrice, quantity):\n        url = \"https://www.bitstamp.net/api/buy/\"\n\n        # Rounding price to avoid 'Ensure that there are no more than 2 decimal places'\n        # error.\n        price = round(limitPrice, 2)\n        # Rounding amount to avoid 'Ensure that there are no more than 8 decimal places'\n        # error.\n        amount = round(quantity, 8)\n\n        params = {\n            \"price\": price,\n            \"amount\": amount\n        }\n        jsonResponse = self._post(url, params)\n        return Order(jsonResponse)\n\n    def sellLimit(self, limitPrice, quantity):\n        url = \"https://www.bitstamp.net/api/sell/\"\n\n        # Rounding price to avoid 'Ensure that there are no more than 2 decimal places'\n        # error.\n        price = round(limitPrice, 2)\n        # Rounding amount to avoid 'Ensure that there are no more than 8 decimal places'\n        # error.\n        amount = round(quantity, 8)\n\n        params = {\n            \"price\": price,\n            \"amount\": amount\n        }\n        jsonResponse = self._post(url, params)\n        return Order(jsonResponse)\n\n    def getUserTransactions(self, transactionType=None):\n        url = \"https://www.bitstamp.net/api/user_transactions/\"\n        jsonResponse = self._post(url, {})\n        if transactionType is not None:\n            jsonUserTransactions = filter(\n                lambda jsonUserTransaction: jsonUserTransaction[\"type\"] == transactionType, jsonResponse\n            )\n        else:\n            jsonUserTransactions = jsonResponse\n        return [UserTransaction(jsonUserTransaction) for jsonUserTransaction in jsonUserTransactions]\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport threading\nimport time\nimport Queue\n\nfrom pyalgotrade import broker\nfrom pyalgotrade.bitstamp import httpclient\nfrom pyalgotrade.bitstamp import common\n\n\ndef build_order_from_open_order(openOrder, instrumentTraits):\n    if openOrder.isBuy():\n        action = broker.Order.Action.BUY\n    elif openOrder.isSell():\n        action = broker.Order.Action.SELL\n    else:\n        raise Exception(\"Invalid order type\")\n\n    ret = broker.LimitOrder(action, common.btc_symbol, openOrder.getPrice(), openOrder.getAmount(), instrumentTraits)\n    ret.setSubmitted(openOrder.getId(), openOrder.getDateTime())\n    ret.setState(broker.Order.State.ACCEPTED)\n    return ret\n\n\nclass TradeMonitor(threading.Thread):\n    POLL_FREQUENCY = 2\n\n    # Events\n    ON_USER_TRADE = 1\n\n    def __init__(self, httpClient):\n        super(TradeMonitor, self).__init__()\n        self.__lastTradeId = -1\n        self.__httpClient = httpClient\n        self.__queue = Queue.Queue()\n        self.__stop = False\n\n    def _getNewTrades(self):\n        userTrades = self.__httpClient.getUserTransactions(httpclient.HTTPClient.UserTransactionType.MARKET_TRADE)\n\n        # Get the new trades only.\n        ret = []\n        for userTrade in userTrades:\n            if userTrade.getId() > self.__lastTradeId:\n                ret.append(userTrade)\n            else:\n                break\n        # Older trades first.\n        ret.reverse()\n        return ret\n\n    def getQueue(self):\n        return self.__queue\n\n    def start(self):\n        trades = self._getNewTrades()\n        # Store the last trade id since we'll start processing new ones only.\n        if len(trades):\n            self.__lastTradeId = trades[-1].getId()\n            common.logger.info(\"Last trade found: %d\" % (self.__lastTradeId))\n\n        super(TradeMonitor, self).start()\n\n    def run(self):\n        while not self.__stop:\n            try:\n                trades = self._getNewTrades()\n                if len(trades):\n                    self.__lastTradeId = trades[-1].getId()\n                    common.logger.info(\"%d new trade/s found\" % (len(trades)))\n                    self.__queue.put((TradeMonitor.ON_USER_TRADE, trades))\n            except Exception, e:\n                common.logger.critical(\"Error retrieving user transactions\", exc_info=e)\n\n            time.sleep(TradeMonitor.POLL_FREQUENCY)\n\n    def stop(self):\n        self.__stop = True\n\n\nclass LiveBroker(broker.Broker):\n    \"\"\"A Bitstamp live broker.\n\n    :param clientId: Client id.\n    :type clientId: string.\n    :param key: API key.\n    :type key: string.\n    :param secret: API secret.\n    :type secret: string.\n\n\n    .. note::\n        * Only limit orders are supported.\n        * Orders are automatically set as **goodTillCanceled=True** and  **allOrNone=False**.\n        * BUY_TO_COVER orders are mapped to BUY orders.\n        * SELL_SHORT orders are mapped to SELL orders.\n        * API access permissions should include:\n\n          * Account balance\n          * Open orders\n          * Buy limit order\n          * User transactions\n          * Cancel order\n          * Sell limit order\n    \"\"\"\n\n    QUEUE_TIMEOUT = 0.01\n\n    def __init__(self, clientId, key, secret):\n        super(LiveBroker, self).__init__()\n        self.__stop = False\n        self.__httpClient = self.buildHTTPClient(clientId, key, secret)\n        self.__tradeMonitor = TradeMonitor(self.__httpClient)\n        self.__cash = 0\n        self.__shares = {}\n        self.__activeOrders = {}\n\n    def _registerOrder(self, order):\n        assert(order.getId() not in self.__activeOrders)\n        assert(order.getId() is not None)\n        self.__activeOrders[order.getId()] = order\n\n    def _unregisterOrder(self, order):\n        assert(order.getId() in self.__activeOrders)\n        assert(order.getId() is not None)\n        del self.__activeOrders[order.getId()]\n\n    # Factory method for testing purposes.\n    def buildHTTPClient(self, clientId, key, secret):\n        return httpclient.HTTPClient(clientId, key, secret)\n\n    def refreshAccountBalance(self):\n        \"\"\"Refreshes cash and BTC balance.\"\"\"\n\n        self.__stop = True  # Stop running in case of errors.\n        common.logger.info(\"Retrieving account balance.\")\n        balance = self.__httpClient.getAccountBalance()\n\n        # Cash\n        self.__cash = round(balance.getUSDAvailable(), 2)\n        common.logger.info(\"%s USD\" % (self.__cash))\n        # BTC\n        btc = balance.getBTCAvailable()\n        if btc:\n            self.__shares = {common.btc_symbol: btc}\n        else:\n            self.__shares = {}\n        common.logger.info(\"%s BTC\" % (btc))\n\n        self.__stop = False  # No errors. Keep running.\n\n    def refreshOpenOrders(self):\n        self.__stop = True  # Stop running in case of errors.\n        common.logger.info(\"Retrieving open orders.\")\n        openOrders = self.__httpClient.getOpenOrders()\n        for openOrder in openOrders:\n            self._registerOrder(build_order_from_open_order(openOrder, self.getInstrumentTraits(common.btc_symbol)))\n\n        common.logger.info(\"%d open order/s found\" % (len(openOrders)))\n        self.__stop = False  # No errors. Keep running.\n\n    def _startTradeMonitor(self):\n        self.__stop = True  # Stop running in case of errors.\n        common.logger.info(\"Initializing trade monitor.\")\n        self.__tradeMonitor.start()\n        self.__stop = False  # No errors. Keep running.\n\n    def _onUserTrades(self, trades):\n        for trade in trades:\n            order = self.__activeOrders.get(trade.getOrderId())\n            if order is not None:\n                fee = trade.getFee()\n                fillPrice = trade.getBTCUSD()\n                btcAmount = trade.getBTC()\n                dateTime = trade.getDateTime()\n\n                # Update cash and shares.\n                self.refreshAccountBalance()\n                # Update the order.\n                orderExecutionInfo = broker.OrderExecutionInfo(fillPrice, abs(btcAmount), fee, dateTime)\n                order.addExecutionInfo(orderExecutionInfo)\n                if not order.isActive():\n                    self._unregisterOrder(order)\n                # Notify that the order was updated.\n                if order.isFilled():\n                    eventType = broker.OrderEvent.Type.FILLED\n                else:\n                    eventType = broker.OrderEvent.Type.PARTIALLY_FILLED\n                self.notifyOrderEvent(broker.OrderEvent(order, eventType, orderExecutionInfo))\n            else:\n                common.logger.info(\"Trade %d refered to order %d that is not active\" % (trade.getId(), trade.getOrderId()))\n\n    # BEGIN observer.Subject interface\n    def start(self):\n        super(LiveBroker, self).start()\n        self.refreshAccountBalance()\n        self.refreshOpenOrders()\n        self._startTradeMonitor()\n\n    def stop(self):\n        self.__stop = True\n        common.logger.info(\"Shutting down trade monitor.\")\n        self.__tradeMonitor.stop()\n\n    def join(self):\n        if self.__tradeMonitor.isAlive():\n            self.__tradeMonitor.join()\n\n    def eof(self):\n        return self.__stop\n\n    def dispatch(self):\n        # Switch orders from SUBMITTED to ACCEPTED.\n        ordersToProcess = self.__activeOrders.values()\n        for order in ordersToProcess:\n            if order.isSubmitted():\n                order.switchState(broker.Order.State.ACCEPTED)\n                self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.ACCEPTED, None))\n\n        # Dispatch events from the trade monitor.\n        try:\n            eventType, eventData = self.__tradeMonitor.getQueue().get(True, LiveBroker.QUEUE_TIMEOUT)\n\n            if eventType == TradeMonitor.ON_USER_TRADE:\n                self._onUserTrades(eventData)\n            else:\n                common.logger.error(\"Invalid event received to dispatch: %s - %s\" % (eventType, eventData))\n        except Queue.Empty:\n            pass\n\n    def peekDateTime(self):\n        # Return None since this is a realtime subject.\n        return None\n\n    # END observer.Subject interface\n\n    # BEGIN broker.Broker interface\n\n    def getCash(self, includeShort=True):\n        return self.__cash\n\n    def getInstrumentTraits(self, instrument):\n        return common.BTCTraits()\n\n    def getShares(self, instrument):\n        return self.__shares.get(instrument, 0)\n\n    def getPositions(self):\n        return self.__shares\n\n    def getActiveOrders(self, instrument=None):\n        return self.__activeOrders.values()\n\n    def submitOrder(self, order):\n        if order.isInitial():\n            # Override user settings based on Bitstamp limitations.\n            order.setAllOrNone(False)\n            order.setGoodTillCanceled(True)\n\n            if order.isBuy():\n                bitstampOrder = self.__httpClient.buyLimit(order.getLimitPrice(), order.getQuantity())\n            else:\n                bitstampOrder = self.__httpClient.sellLimit(order.getLimitPrice(), order.getQuantity())\n\n            order.setSubmitted(bitstampOrder.getId(), bitstampOrder.getDateTime())\n            self._registerOrder(order)\n            # Switch from INITIAL -> SUBMITTED\n            # IMPORTANT: Do not emit an event for this switch because when using the position interface\n            # the order is not yet mapped to the position and Position.onOrderUpdated will get called.\n            order.switchState(broker.Order.State.SUBMITTED)\n        else:\n            raise Exception(\"The order was already processed\")\n\n    def createMarketOrder(self, action, instrument, quantity, onClose=False):\n        raise Exception(\"Market orders are not supported\")\n\n    def createLimitOrder(self, action, instrument, limitPrice, quantity):\n        if instrument != common.btc_symbol:\n            raise Exception(\"Only BTC instrument is supported\")\n\n        if action == broker.Order.Action.BUY_TO_COVER:\n            action = broker.Order.Action.BUY\n        elif action == broker.Order.Action.SELL_SHORT:\n            action = broker.Order.Action.SELL\n\n        if action not in [broker.Order.Action.BUY, broker.Order.Action.SELL]:\n            raise Exception(\"Only BUY/SELL orders are supported\")\n\n        instrumentTraits = self.getInstrumentTraits(instrument)\n        limitPrice = round(limitPrice, 2)\n        quantity = instrumentTraits.roundQuantity(quantity)\n        return broker.LimitOrder(action, instrument, limitPrice, quantity, instrumentTraits)\n\n    def createStopOrder(self, action, instrument, stopPrice, quantity):\n        raise Exception(\"Stop orders are not supported\")\n\n    def createStopLimitOrder(self, action, instrument, stopPrice, limitPrice, quantity):\n        raise Exception(\"Stop limit orders are not supported\")\n\n    def cancelOrder(self, order):\n        activeOrder = self.__activeOrders.get(order.getId())\n        if activeOrder is None:\n            raise Exception(\"The order is not active anymore\")\n        if activeOrder.isFilled():\n            raise Exception(\"Can't cancel order that has already been filled\")\n\n        self.__httpClient.cancelOrder(order.getId())\n        self._unregisterOrder(order)\n        order.switchState(broker.Order.State.CANCELED)\n\n        # Update cash and shares.\n        self.refreshAccountBalance()\n\n        # Notify that the order was canceled.\n        self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.CANCELED, \"User requested cancellation\"))\n\n    # END broker.Broker interface\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport time\nimport Queue\n\nfrom pyalgotrade import bar\nfrom pyalgotrade import barfeed\nfrom pyalgotrade import observer\nfrom pyalgotrade.bitstamp import common\nfrom pyalgotrade.bitstamp import wsclient\n\n\nclass TradeBar(bar.Bar):\n    # Optimization to reduce memory footprint.\n    __slots__ = ('__dateTime', '__tradeId', '__price', '__amount')\n\n    def __init__(self, dateTime, trade):\n        self.__dateTime = dateTime\n        self.__tradeId = trade.getId()\n        self.__price = trade.getPrice()\n        self.__amount = trade.getAmount()\n        self.__buy = trade.isBuy()\n\n    def __setstate__(self, state):\n        (self.__dateTime, self.__tradeId, self.__price, self.__amount) = state\n\n    def __getstate__(self):\n        return (self.__dateTime, self.__tradeId, self.__price, self.__amount)\n\n    def setUseAdjustedValue(self, useAdjusted):\n        if useAdjusted:\n            raise Exception(\"Adjusted close is not available\")\n\n    def getTradeId(self):\n        return self.__tradeId\n\n    def getFrequency(self):\n        return bar.Frequency.TRADE\n\n    def getDateTime(self):\n        return self.__dateTime\n\n    def getOpen(self, adjusted=False):\n        return self.__price\n\n    def getHigh(self, adjusted=False):\n        return self.__price\n\n    def getLow(self, adjusted=False):\n        return self.__price\n\n    def getClose(self, adjusted=False):\n        return self.__price\n\n    def getVolume(self):\n        return self.__amount\n\n    def getAdjClose(self):\n        return None\n\n    def getTypicalPrice(self):\n        return self.__price\n\n    def getPrice(self):\n        return self.__price\n\n    def getUseAdjValue(self):\n        return False\n\n    def isBuy(self):\n        return self.__buy\n\n    def isSell(self):\n        return not self.__buy\n\n\nclass LiveTradeFeed(barfeed.BaseBarFeed):\n\n    \"\"\"A real-time BarFeed that builds bars from live trades.\n\n    :param maxLen: The maximum number of values that the :class:`pyalgotrade.dataseries.bards.BarDataSeries` will hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded\n        from the opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n\n    .. note::\n        Note that a Bar will be created for every trade, so open, high, low and close values will all be the same.\n    \"\"\"\n\n    QUEUE_TIMEOUT = 0.01\n\n    def __init__(self, maxLen=None):\n        super(LiveTradeFeed, self).__init__(bar.Frequency.TRADE, maxLen)\n        self.__barDicts = []\n        self.registerInstrument(common.btc_symbol)\n        self.__prevTradeDateTime = None\n        self.__thread = None\n        self.__initializationOk = None\n        self.__enableReconnection = True\n        self.__stopped = False\n        self.__orderBookUpdateEvent = observer.Event()\n\n    # Factory method for testing purposes.\n    def buildWebSocketClientThread(self):\n        return wsclient.WebSocketClientThread()\n\n    def getCurrentDateTime(self):\n        return wsclient.get_current_datetime()\n\n    def enableReconection(self, enableReconnection):\n        self.__enableReconnection = enableReconnection\n\n    def __initializeClient(self):\n        self.__initializationOk = None\n        common.logger.info(\"Initializing websocket client.\")\n\n        try:\n            # Start the thread that runs the client.\n            self.__thread = self.buildWebSocketClientThread()\n            self.__thread.start()\n        except Exception, e:\n            self.__initializationOk = False\n            common.logger.error(\"Error connecting : %s\" % str(e))\n\n        # Wait for initialization to complete.\n        while self.__initializationOk is None and self.__thread.is_alive():\n            self.__dispatchImpl([wsclient.WebSocketClient.ON_CONNECTED])\n\n        if self.__initializationOk:\n            common.logger.info(\"Initialization ok.\")\n        else:\n            common.logger.error(\"Initialization failed.\")\n        return self.__initializationOk\n\n    def __onConnected(self):\n        self.__initializationOk = True\n\n    def __onDisconnected(self):\n        if self.__enableReconnection:\n            initialized = False\n            while not self.__stopped and not initialized:\n                common.logger.info(\"Reconnecting\")\n                initialized = self.__initializeClient()\n                if not initialized:\n                    time.sleep(5)\n        else:\n            self.__stopped = True\n\n    def __dispatchImpl(self, eventFilter):\n        ret = False\n        try:\n            eventType, eventData = self.__thread.getQueue().get(True, LiveTradeFeed.QUEUE_TIMEOUT)\n            if eventFilter is not None and eventType not in eventFilter:\n                return False\n\n            ret = True\n            if eventType == wsclient.WebSocketClient.ON_TRADE:\n                self.__onTrade(eventData)\n            elif eventType == wsclient.WebSocketClient.ON_ORDER_BOOK_UPDATE:\n                self.__orderBookUpdateEvent.emit(eventData)\n            elif eventType == wsclient.WebSocketClient.ON_CONNECTED:\n                self.__onConnected()\n            elif eventType == wsclient.WebSocketClient.ON_DISCONNECTED:\n                self.__onDisconnected()\n            else:\n                ret = False\n                common.logger.error(\"Invalid event received to dispatch: %s - %s\" % (eventType, eventData))\n        except Queue.Empty:\n            pass\n        return ret\n\n    # Bar datetimes should not duplicate. In case trade object datetimes conflict, we just move one slightly forward.\n    def __getTradeDateTime(self, trade):\n        ret = trade.getDateTime()\n        if ret == self.__prevTradeDateTime:\n            ret += datetime.timedelta(microseconds=1)\n        self.__prevTradeDateTime = ret\n        return ret\n\n    def __onTrade(self, trade):\n        # Build a bar for each trade.\n        barDict = {\n            common.btc_symbol: TradeBar(self.__getTradeDateTime(trade), trade)\n            }\n        self.__barDicts.append(barDict)\n\n    def barsHaveAdjClose(self):\n        return False\n\n    def getNextBars(self):\n        ret = None\n        if len(self.__barDicts):\n            ret = bar.Bars(self.__barDicts.pop(0))\n        return ret\n\n    def peekDateTime(self):\n        # Return None since this is a realtime subject.\n        return None\n\n    # This may raise.\n    def start(self):\n        super(LiveTradeFeed, self).start()\n        if self.__thread is not None:\n            raise Exception(\"Already running\")\n        elif not self.__initializeClient():\n            self.__stopped = True\n            raise Exception(\"Initialization failed\")\n\n    def dispatch(self):\n        # Note that we may return True even if we didn't dispatch any Bar\n        # event.\n        ret = False\n        if self.__dispatchImpl(None):\n            ret = True\n        if super(LiveTradeFeed, self).dispatch():\n            ret = True\n        return ret\n\n    # This should not raise.\n    def stop(self):\n        try:\n            self.__stopped = True\n            if self.__thread is not None and self.__thread.is_alive():\n                common.logger.info(\"Shutting down websocket client.\")\n                self.__thread.stop()\n        except Exception, e:\n            common.logger.error(\"Error shutting down client: %s\" % (str(e)))\n\n    # This should not raise.\n    def join(self):\n        if self.__thread is not None:\n            self.__thread.join()\n\n    def eof(self):\n        return self.__stopped\n\n    def getOrderBookUpdateEvent(self):\n        \"\"\"\n        Returns the event that will be emitted when the orderbook gets updated.\n\n        Eventh handlers should receive one parameter:\n         1. A :class:`pyalgotrade.bitstamp.wsclient.OrderBookUpdate` instance.\n\n        :rtype: :class:`pyalgotrade.observer.Event`.\n        \"\"\"\n        return self.__orderBookUpdateEvent\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport threading\nimport Queue\n\nfrom pyalgotrade.websocket import pusher\nfrom pyalgotrade.bitstamp import common\n\n\ndef get_current_datetime():\n    return datetime.datetime.now()\n\n# Bitstamp protocol reference: https://www.bitstamp.net/websocket/\n\n\nclass Trade(pusher.Event):\n    \"\"\"A trade event.\"\"\"\n\n    def __init__(self, dateTime, eventDict):\n        super(Trade, self).__init__(eventDict, True)\n        self.__dateTime = dateTime\n\n    def getDateTime(self):\n        \"\"\"Returns the :class:`datetime.datetime` when this event was received.\"\"\"\n        return self.__dateTime\n\n    def getId(self):\n        \"\"\"Returns the trade id.\"\"\"\n        return self.getData()[\"id\"]\n\n    def getPrice(self):\n        \"\"\"Returns the trade price.\"\"\"\n        return self.getData()[\"price\"]\n\n    def getAmount(self):\n        \"\"\"Returns the trade amount.\"\"\"\n        return self.getData()[\"amount\"]\n\n    def isBuy(self):\n        \"\"\"Returns True if the trade was a buy.\"\"\"\n        return self.getData()[\"type\"] == 0\n\n    def isSell(self):\n        \"\"\"Returns True if the trade was a sell.\"\"\"\n        return self.getData()[\"type\"] == 1\n\n\nclass OrderBookUpdate(pusher.Event):\n    \"\"\"An order book update event.\"\"\"\n\n    def __init__(self, dateTime, eventDict):\n        super(OrderBookUpdate, self).__init__(eventDict, True)\n        self.__dateTime = dateTime\n\n    def getDateTime(self):\n        \"\"\"Returns the :class:`datetime.datetime` when this event was received.\"\"\"\n        return self.__dateTime\n\n    def getBidPrices(self):\n        \"\"\"Returns a list with the top 20 bid prices.\"\"\"\n        return [float(bid[0]) for bid in self.getData()[\"bids\"]]\n\n    def getBidVolumes(self):\n        \"\"\"Returns a list with the top 20 bid volumes.\"\"\"\n        return [float(bid[1]) for bid in self.getData()[\"bids\"]]\n\n    def getAskPrices(self):\n        \"\"\"Returns a list with the top 20 ask prices.\"\"\"\n        return [float(ask[0]) for ask in self.getData()[\"asks\"]]\n\n    def getAskVolumes(self):\n        \"\"\"Returns a list with the top 20 ask volumes.\"\"\"\n        return [float(ask[1]) for ask in self.getData()[\"asks\"]]\n\n\nclass WebSocketClient(pusher.WebSocketClient):\n    PUSHER_APP_KEY = \"de504dc5763aeef9ff52\"\n\n    # Events\n    ON_TRADE = 1\n    ON_ORDER_BOOK_UPDATE = 2\n    ON_CONNECTED = 3\n    ON_DISCONNECTED = 4\n\n    def __init__(self):\n        super(WebSocketClient, self).__init__(WebSocketClient.PUSHER_APP_KEY, 5)\n        self.__queue = Queue.Queue()\n\n    def getQueue(self):\n        return self.__queue\n\n    def onMessage(self, msg):\n        # If we can't handle the message, forward it to Pusher WebSocketClient.\n        event = msg.get(\"event\")\n        if event == \"trade\":\n            self.onTrade(Trade(get_current_datetime(), msg))\n        elif event == \"data\" and msg.get(\"channel\") == \"order_book\":\n            self.onOrderBookUpdate(OrderBookUpdate(get_current_datetime(), msg))\n        else:\n            super(WebSocketClient, self).onMessage(msg)\n\n    ######################################################################\n    # WebSocketClientBase events.\n\n    def onOpened(self):\n        pass\n\n    def onClosed(self, code, reason):\n        common.logger.info(\"Closed. Code: %s. Reason: %s.\" % (code, reason))\n        self.__queue.put((WebSocketClient.ON_DISCONNECTED, None))\n\n    def onDisconnectionDetected(self):\n        common.logger.warning(\"Disconnection detected.\")\n        try:\n            self.stopClient()\n        except Exception, e:\n            common.logger.error(\"Error stopping websocket client: %s.\" % (str(e)))\n        self.__queue.put((WebSocketClient.ON_DISCONNECTED, None))\n\n    ######################################################################\n    # Pusher specific events.\n\n    def onConnectionEstablished(self, event):\n        common.logger.info(\"Connection established.\")\n        self.subscribeChannel(\"live_trades\")\n        self.subscribeChannel(\"order_book\")\n        self.__queue.put((WebSocketClient.ON_CONNECTED, None))\n\n    def onError(self, event):\n        common.logger.error(\"Error: %s\" % (event))\n\n    def onUnknownEvent(self, event):\n        common.logger.warning(\"Unknown event: %s\" % (event))\n\n    ######################################################################\n    # Bitstamp specific\n\n    def onTrade(self, trade):\n        self.__queue.put((WebSocketClient.ON_TRADE, trade))\n\n    def onOrderBookUpdate(self, orderBookUpdate):\n        self.__queue.put((WebSocketClient.ON_ORDER_BOOK_UPDATE, orderBookUpdate))\n\n\nclass WebSocketClientThread(threading.Thread):\n    def __init__(self):\n        super(WebSocketClientThread, self).__init__()\n        self.__wsClient = WebSocketClient()\n\n    def getQueue(self):\n        return self.__wsClient.getQueue()\n\n    def start(self):\n        self.__wsClient.connect()\n        super(WebSocketClientThread, self).start()\n\n    def run(self):\n        self.__wsClient.startClient()\n\n    def stop(self):\n        try:\n            common.logger.info(\"Stopping websocket client.\")\n            self.__wsClient.stopClient()\n        except Exception, e:\n            common.logger.error(\"Error stopping websocket client: %s.\" % (str(e)))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import observer\nfrom pyalgotrade import dispatchprio\n\n\n# This class is used to prevent bugs like the one triggered in testcases.bitstamp_test:TestCase.testRoundingBug.\n# Why not use decimal.Decimal instead ?\n# 1: I'd have to expose this to users. They'd have to deal with decimal.Decimal and it'll break existing users.\n# 2: numpy arrays built using decimal.Decimal instances have dtype=object.\nclass InstrumentTraits(object):\n\n    __metaclass__ = abc.ABCMeta\n\n    # Return the floating point value number rounded.\n    @abc.abstractmethod\n    def roundQuantity(self, quantity):\n        raise NotImplementedError()\n\n\nclass IntegerTraits(InstrumentTraits):\n    def roundQuantity(self, quantity):\n        return int(quantity)\n\n\n######################################################################\n# Orders\n# http://stocks.about.com/od/tradingbasics/a/markords.htm\n# http://www.interactivebrokers.com/en/software/tws/usersguidebook/ordertypes/basic_order_types.htm\n#\n# State chart:\n# INITIAL           -> SUBMITTED\n# INITIAL           -> CANCELED\n# SUBMITTED         -> ACCEPTED\n# SUBMITTED         -> CANCELED\n# ACCEPTED          -> FILLED\n# ACCEPTED          -> PARTIALLY_FILLED\n# ACCEPTED          -> CANCELED\n# PARTIALLY_FILLED  -> PARTIALLY_FILLED\n# PARTIALLY_FILLED  -> FILLED\n# PARTIALLY_FILLED  -> CANCELED\n\nclass Order(object):\n    \"\"\"Base class for orders.\n\n    :param type_: The order type\n    :type type_: :class:`Order.Type`\n    :param action: The order action.\n    :type action: :class:`Order.Action`\n    :param instrument: Instrument identifier.\n    :type instrument: string.\n    :param quantity: Order quantity.\n    :type quantity: int/float.\n\n    .. note::\n        This is a base class and should not be used directly.\n\n        Valid **type** parameter values are:\n\n         * Order.Type.MARKET\n         * Order.Type.LIMIT\n         * Order.Type.STOP\n         * Order.Type.STOP_LIMIT\n\n        Valid **action** parameter values are:\n\n         * Order.Action.BUY\n         * Order.Action.BUY_TO_COVER\n         * Order.Action.SELL\n         * Order.Action.SELL_SHORT\n    \"\"\"\n\n    class Action(object):\n        BUY = 1\n        BUY_TO_COVER = 2\n        SELL = 3\n        SELL_SHORT = 4\n\n    class State(object):\n        INITIAL = 1  # Initial state.\n        SUBMITTED = 2  # Order has been submitted.\n        ACCEPTED = 3  # Order has been acknowledged by the broker.\n        CANCELED = 4  # Order has been canceled.\n        PARTIALLY_FILLED = 5  # Order has been partially filled.\n        FILLED = 6  # Order has been completely filled.\n\n        @classmethod\n        def toString(cls, state):\n            if state == cls.INITIAL:\n                return \"INITIAL\"\n            elif state == cls.SUBMITTED:\n                return \"SUBMITTED\"\n            elif state == cls.ACCEPTED:\n                return \"ACCEPTED\"\n            elif state == cls.CANCELED:\n                return \"CANCELED\"\n            elif state == cls.PARTIALLY_FILLED:\n                return \"PARTIALLY_FILLED\"\n            elif state == cls.FILLED:\n                return \"FILLED\"\n            else:\n                raise Exception(\"Invalid state\")\n\n    class Type(object):\n        MARKET = 1\n        LIMIT = 2\n        STOP = 3\n        STOP_LIMIT = 4\n        NEXT_CUSTOM_TYPE = 1000\n\n    # Valid state transitions.\n    VALID_TRANSITIONS = {\n        State.INITIAL: [State.SUBMITTED, State.CANCELED],\n        State.SUBMITTED: [State.ACCEPTED, State.CANCELED],\n        State.ACCEPTED: [State.PARTIALLY_FILLED, State.FILLED, State.CANCELED],\n        State.PARTIALLY_FILLED: [State.PARTIALLY_FILLED, State.FILLED, State.CANCELED],\n    }\n\n    def __init__(self, type_, action, instrument, quantity, instrumentTraits):\n        if quantity is not None and quantity <= 0:\n            raise Exception(\"Invalid quantity\")\n\n        self.__id = None\n        self.__type = type_\n        self.__action = action\n        self.__instrument = instrument\n        self.__quantity = quantity\n        self.__instrumentTraits = instrumentTraits\n        self.__filled = 0\n        self.__avgFillPrice = None\n        self.__executionInfo = None\n        self.__goodTillCanceled = False\n        self.__commissions = 0\n        self.__allOrNone = False\n        self.__state = Order.State.INITIAL\n        self.__submitDateTime = None\n\n    # This is to check that orders are not compared directly. order ids should be compared.\n#    def __eq__(self, other):\n#        if other is None:\n#            return False\n#        assert(False)\n\n    # This is to check that orders are not compared directly. order ids should be compared.\n#    def __ne__(self, other):\n#        if other is None:\n#            return True\n#        assert(False)\n\n    def _setQuantity(self, quantity):\n        assert self.__quantity is None, \"Can only change the quantity if it was undefined\"\n        assert quantity > 0, \"Invalid quantity\"\n        self.__quantity = quantity\n\n    def getInstrumentTraits(self):\n        return self.__instrumentTraits\n\n    def getId(self):\n        \"\"\"\n        Returns the order id.\n\n        .. note::\n\n            This will be None if the order was not submitted.\n        \"\"\"\n        return self.__id\n\n    def getType(self):\n        \"\"\"Returns the order type. Valid order types are:\n\n         * Order.Type.MARKET\n         * Order.Type.LIMIT\n         * Order.Type.STOP\n         * Order.Type.STOP_LIMIT\n        \"\"\"\n        return self.__type\n\n    def getSubmitDateTime(self):\n        \"\"\"Returns the datetime when the order was submitted.\"\"\"\n        return self.__submitDateTime\n\n    def setSubmitted(self, orderId, dateTime):\n        assert(self.__id is None or orderId == self.__id)\n        self.__id = orderId\n        self.__submitDateTime = dateTime\n\n    def getAction(self):\n        \"\"\"Returns the order action. Valid order actions are:\n\n         * Order.Action.BUY\n         * Order.Action.BUY_TO_COVER\n         * Order.Action.SELL\n         * Order.Action.SELL_SHORT\n        \"\"\"\n        return self.__action\n\n    def getState(self):\n        \"\"\"Returns the order state. Valid order states are:\n\n         * Order.State.INITIAL (the initial state).\n         * Order.State.SUBMITTED\n         * Order.State.ACCEPTED\n         * Order.State.CANCELED\n         * Order.State.PARTIALLY_FILLED\n         * Order.State.FILLED\n        \"\"\"\n        return self.__state\n\n    def isActive(self):\n        \"\"\"Returns True if the order is active.\"\"\"\n        return self.__state not in [Order.State.CANCELED, Order.State.FILLED]\n\n    def isInitial(self):\n        \"\"\"Returns True if the order state is Order.State.INITIAL.\"\"\"\n        return self.__state == Order.State.INITIAL\n\n    def isSubmitted(self):\n        \"\"\"Returns True if the order state is Order.State.SUBMITTED.\"\"\"\n        return self.__state == Order.State.SUBMITTED\n\n    def isAccepted(self):\n        \"\"\"Returns True if the order state is Order.State.ACCEPTED.\"\"\"\n        return self.__state == Order.State.ACCEPTED\n\n    def isCanceled(self):\n        \"\"\"Returns True if the order state is Order.State.CANCELED.\"\"\"\n        return self.__state == Order.State.CANCELED\n\n    def isPartiallyFilled(self):\n        \"\"\"Returns True if the order state is Order.State.PARTIALLY_FILLED.\"\"\"\n        return self.__state == Order.State.PARTIALLY_FILLED\n\n    def isFilled(self):\n        \"\"\"Returns True if the order state is Order.State.FILLED.\"\"\"\n        return self.__state == Order.State.FILLED\n\n    def getInstrument(self):\n        \"\"\"Returns the instrument identifier.\"\"\"\n        return self.__instrument\n\n    def getQuantity(self):\n        \"\"\"Returns the quantity.\"\"\"\n        return self.__quantity\n\n    def getFilled(self):\n        \"\"\"Returns the number of shares that have been executed.\"\"\"\n        return self.__filled\n\n    def getRemaining(self):\n        \"\"\"Returns the number of shares still outstanding.\"\"\"\n        return self.__instrumentTraits.roundQuantity(self.__quantity - self.__filled)\n\n    def getAvgFillPrice(self):\n        \"\"\"Returns the average price of the shares that have been executed, or None if nothing has been filled.\"\"\"\n        return self.__avgFillPrice\n\n    def getCommissions(self):\n        return self.__commissions\n\n    def getGoodTillCanceled(self):\n        \"\"\"Returns True if the order is good till canceled.\"\"\"\n        return self.__goodTillCanceled\n\n    def setGoodTillCanceled(self, goodTillCanceled):\n        \"\"\"Sets if the order should be good till canceled.\n        Orders that are not filled by the time the session closes will be will be automatically canceled\n        if they were not set as good till canceled\n\n        :param goodTillCanceled: True if the order should be good till canceled.\n        :type goodTillCanceled: boolean.\n\n        .. note:: This can't be changed once the order is submitted.\n        \"\"\"\n        if self.__state != Order.State.INITIAL:\n            raise Exception(\"The order has already been submitted\")\n        self.__goodTillCanceled = goodTillCanceled\n\n    def getAllOrNone(self):\n        \"\"\"Returns True if the order should be completely filled or else canceled.\"\"\"\n        return self.__allOrNone\n\n    def setAllOrNone(self, allOrNone):\n        \"\"\"Sets the All-Or-None property for this order.\n\n        :param allOrNone: True if the order should be completely filled.\n        :type allOrNone: boolean.\n\n        .. note:: This can't be changed once the order is submitted.\n        \"\"\"\n        if self.__state != Order.State.INITIAL:\n            raise Exception(\"The order has already been submitted\")\n        self.__allOrNone = allOrNone\n\n    def addExecutionInfo(self, orderExecutionInfo):\n        if orderExecutionInfo.getQuantity() > self.getRemaining():\n            raise Exception(\"Invalid fill size. %s remaining and %s filled\" % (self.getRemaining(), orderExecutionInfo.getQuantity()))\n\n        if self.__avgFillPrice is None:\n            self.__avgFillPrice = orderExecutionInfo.getPrice()\n        else:\n            self.__avgFillPrice = (self.__avgFillPrice * self.__filled + orderExecutionInfo.getPrice() * orderExecutionInfo.getQuantity()) / float(self.__filled + orderExecutionInfo.getQuantity())\n\n        self.__executionInfo = orderExecutionInfo\n        self.__filled = self.getInstrumentTraits().roundQuantity(self.__filled + orderExecutionInfo.getQuantity())\n        self.__commissions += orderExecutionInfo.getCommission()\n\n        if self.getRemaining() == 0:\n            self.switchState(Order.State.FILLED)\n        else:\n            assert(not self.__allOrNone)\n            self.switchState(Order.State.PARTIALLY_FILLED)\n\n    def switchState(self, newState):\n        validTransitions = Order.VALID_TRANSITIONS.get(self.__state, [])\n        if newState not in validTransitions:\n            raise Exception(\"Invalid order state transition from %s to %s\" % (Order.State.toString(self.__state), Order.State.toString(newState)))\n        else:\n            self.__state = newState\n\n    def setState(self, newState):\n        self.__state = newState\n\n    def getExecutionInfo(self):\n        \"\"\"Returns the last execution information for this order, or None if nothing has been filled so far.\n        This will be different every time an order, or part of it, gets filled.\n\n        :rtype: :class:`OrderExecutionInfo`.\n        \"\"\"\n        return self.__executionInfo\n\n    # Returns True if this is a BUY or BUY_TO_COVER order.\n    def isBuy(self):\n        return self.__action in [Order.Action.BUY, Order.Action.BUY_TO_COVER]\n\n    # Returns True if this is a SELL or SELL_SHORT order.\n    def isSell(self):\n        return self.__action in [Order.Action.SELL, Order.Action.SELL_SHORT]\n\n\nclass MarketOrder(Order):\n    \"\"\"Base class for market orders.\n\n    .. note::\n\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, action, instrument, quantity, onClose, instrumentTraits):\n        super(MarketOrder, self).__init__(Order.Type.MARKET, action, instrument, quantity, instrumentTraits)\n        self.__onClose = onClose\n\n    def getFillOnClose(self):\n        \"\"\"Returns True if the order should be filled as close to the closing price as possible (Market-On-Close order).\"\"\"\n        return self.__onClose\n\n\nclass LimitOrder(Order):\n    \"\"\"Base class for limit orders.\n\n    .. note::\n\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, action, instrument, limitPrice, quantity, instrumentTraits):\n        super(LimitOrder, self).__init__(Order.Type.LIMIT, action, instrument, quantity, instrumentTraits)\n        self.__limitPrice = limitPrice\n\n    def getLimitPrice(self):\n        \"\"\"Returns the limit price.\"\"\"\n        return self.__limitPrice\n\n\nclass StopOrder(Order):\n    \"\"\"Base class for stop orders.\n\n    .. note::\n\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, action, instrument, stopPrice, quantity, instrumentTraits):\n        super(StopOrder, self).__init__(Order.Type.STOP, action, instrument, quantity, instrumentTraits)\n        self.__stopPrice = stopPrice\n\n    def getStopPrice(self):\n        \"\"\"Returns the stop price.\"\"\"\n        return self.__stopPrice\n\n\nclass StopLimitOrder(Order):\n    \"\"\"Base class for stop limit orders.\n\n    .. note::\n\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, action, instrument, stopPrice, limitPrice, quantity, instrumentTraits):\n        super(StopLimitOrder, self).__init__(Order.Type.STOP_LIMIT, action, instrument, quantity, instrumentTraits)\n        self.__stopPrice = stopPrice\n        self.__limitPrice = limitPrice\n\n    def getStopPrice(self):\n        \"\"\"Returns the stop price.\"\"\"\n        return self.__stopPrice\n\n    def getLimitPrice(self):\n        \"\"\"Returns the limit price.\"\"\"\n        return self.__limitPrice\n\n\nclass OrderExecutionInfo(object):\n    \"\"\"Execution information for an order.\"\"\"\n    def __init__(self, price, quantity, commission, dateTime):\n        self.__price = price\n        self.__quantity = quantity\n        self.__commission = commission\n        self.__dateTime = dateTime\n\n    def __str__(self):\n        return \"%s - Price: %s - Amount: %s - Fee: %s\" % (self.__dateTime, self.__price, self.__quantity, self.__commission)\n\n    def getPrice(self):\n        \"\"\"Returns the fill price.\"\"\"\n        return self.__price\n\n    def getQuantity(self):\n        \"\"\"Returns the quantity.\"\"\"\n        return self.__quantity\n\n    def getCommission(self):\n        \"\"\"Returns the commission applied.\"\"\"\n        return self.__commission\n\n    def getDateTime(self):\n        \"\"\"Returns the :class:`datatime.datetime` when the order was executed.\"\"\"\n        return self.__dateTime\n\n\nclass OrderEvent(object):\n    class Type:\n        SUBMITTED = 1  # Order has been submitted.\n        ACCEPTED = 2  # Order has been acknowledged by the broker.\n        CANCELED = 3  # Order has been canceled.\n        PARTIALLY_FILLED = 4  # Order has been partially filled.\n        FILLED = 5  # Order has been completely filled.\n\n    def __init__(self, order, eventyType, eventInfo):\n        self.__order = order\n        self.__eventType = eventyType\n        self.__eventInfo = eventInfo\n\n    def getOrder(self):\n        return self.__order\n\n    def getEventType(self):\n        return self.__eventType\n\n    # This depends on the event type:\n    # ACCEPTED: None\n    # CANCELED: A string with the reason why it was canceled.\n    # PARTIALLY_FILLED: An OrderExecutionInfo instance.\n    # FILLED: An OrderExecutionInfo instance.\n    def getEventInfo(self):\n        return self.__eventInfo\n\n\n######################################################################\n# Base broker class\nclass Broker(observer.Subject):\n    \"\"\"Base class for brokers.\n\n    .. note::\n\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self):\n        super(Broker, self).__init__()\n        self.__orderEvent = observer.Event()\n\n    def getDispatchPriority(self):\n        return dispatchprio.BROKER\n\n    def notifyOrderEvent(self, orderEvent):\n        self.__orderEvent.emit(self, orderEvent)\n\n    # Handlers should expect 2 parameters:\n    # 1: broker instance\n    # 2: OrderEvent instance\n    def getOrderUpdatedEvent(self):\n        return self.__orderEvent\n\n    @abc.abstractmethod\n    def getInstrumentTraits(self, instrument):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getCash(self, includeShort=True):\n        \"\"\"\n        Returns the available cash.\n\n        :param includeShort: Include cash from short positions.\n        :type includeShort: boolean.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getShares(self, instrument):\n        \"\"\"Returns the number of shares for an instrument.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getPositions(self):\n        \"\"\"Returns a dictionary that maps instruments to shares.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getActiveOrders(self, instrument=None):\n        \"\"\"Returns a sequence with the orders that are still active.\n\n        :param instrument: An optional instrument identifier to return only the active orders for the given instrument.\n        :type instrument: string.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def submitOrder(self, order):\n        \"\"\"Submits an order.\n\n        :param order: The order to submit.\n        :type order: :class:`Order`.\n\n        .. note::\n            * After this call the order is in SUBMITTED state and an event is not triggered for this transition.\n            * Calling this twice on the same order will raise an exception.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def createMarketOrder(self, action, instrument, quantity, onClose=False):\n        \"\"\"Creates a Market order.\n        A market order is an order to buy or sell a stock at the best available price.\n        Generally, this type of order will be executed immediately. However, the price at which a market order will be executed\n        is not guaranteed.\n\n        :param action: The order action.\n        :type action: Order.Action.BUY, or Order.Action.BUY_TO_COVER, or Order.Action.SELL or Order.Action.SELL_SHORT.\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param quantity: Order quantity.\n        :type quantity: int/float.\n        :param onClose: True if the order should be filled as close to the closing price as possible (Market-On-Close order). Default is False.\n        :type onClose: boolean.\n        :rtype: A :class:`MarketOrder` subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def createLimitOrder(self, action, instrument, limitPrice, quantity):\n        \"\"\"Creates a Limit order.\n        A limit order is an order to buy or sell a stock at a specific price or better.\n        A buy limit order can only be executed at the limit price or lower, and a sell limit order can only be executed at the\n        limit price or higher.\n\n        :param action: The order action.\n        :type action: Order.Action.BUY, or Order.Action.BUY_TO_COVER, or Order.Action.SELL or Order.Action.SELL_SHORT.\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param limitPrice: The order price.\n        :type limitPrice: float\n        :param quantity: Order quantity.\n        :type quantity: int/float.\n        :rtype: A :class:`LimitOrder` subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def createStopOrder(self, action, instrument, stopPrice, quantity):\n        \"\"\"Creates a Stop order.\n        A stop order, also referred to as a stop-loss order, is an order to buy or sell a stock once the price of the stock\n        reaches a specified price, known as the stop price.\n        When the stop price is reached, a stop order becomes a market order.\n        A buy stop order is entered at a stop price above the current market price. Investors generally use a buy stop order\n        to limit a loss or to protect a profit on a stock that they have sold short.\n        A sell stop order is entered at a stop price below the current market price. Investors generally use a sell stop order\n        to limit a loss or to protect a profit on a stock that they own.\n\n        :param action: The order action.\n        :type action: Order.Action.BUY, or Order.Action.BUY_TO_COVER, or Order.Action.SELL or Order.Action.SELL_SHORT.\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: The trigger price.\n        :type stopPrice: float\n        :param quantity: Order quantity.\n        :type quantity: int/float.\n        :rtype: A :class:`StopOrder` subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def createStopLimitOrder(self, action, instrument, stopPrice, limitPrice, quantity):\n        \"\"\"Creates a Stop-Limit order.\n        A stop-limit order is an order to buy or sell a stock that combines the features of a stop order and a limit order.\n        Once the stop price is reached, a stop-limit order becomes a limit order that will be executed at a specified price\n        (or better). The benefit of a stop-limit order is that the investor can control the price at which the order can be executed.\n\n        :param action: The order action.\n        :type action: Order.Action.BUY, or Order.Action.BUY_TO_COVER, or Order.Action.SELL or Order.Action.SELL_SHORT.\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: The trigger price.\n        :type stopPrice: float\n        :param limitPrice: The price for the limit order.\n        :type limitPrice: float\n        :param quantity: Order quantity.\n        :type quantity: int/float.\n        :rtype: A :class:`StopLimitOrder` subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def cancelOrder(self, order):\n        \"\"\"Requests an order to be canceled. If the order is filled an Exception is raised.\n\n        :param order: The order to cancel.\n        :type order: :class:`Order`.\n        \"\"\"\n        raise NotImplementedError()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import broker\nfrom pyalgotrade.broker import fillstrategy\nfrom pyalgotrade import logger\nimport pyalgotrade.bar\n\n\n######################################################################\n# Commission models\n\nclass Commission(object):\n    \"\"\"Base class for implementing different commission schemes.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def calculate(self, order, price, quantity):\n        \"\"\"Calculates the commission for an order execution.\n\n        :param order: The order being executed.\n        :type order: :class:`pyalgotrade.broker.Order`.\n        :param price: The price for each share.\n        :type price: float.\n        :param quantity: The order size.\n        :type quantity: float.\n        :rtype: float.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass NoCommission(Commission):\n    \"\"\"A :class:`Commission` class that always returns 0.\"\"\"\n\n    def calculate(self, order, price, quantity):\n        return 0\n\n\nclass FixedPerTrade(Commission):\n    \"\"\"A :class:`Commission` class that charges a fixed amount for the whole trade.\n\n    :param amount: The commission for an order.\n    :type amount: float.\n    \"\"\"\n    def __init__(self, amount):\n        super(FixedPerTrade, self).__init__()\n        self.__amount = amount\n\n    def calculate(self, order, price, quantity):\n        ret = 0\n        # Only charge the first fill.\n        if order.getExecutionInfo() is None:\n            ret = self.__amount\n        return ret\n\n\nclass TradePercentage(Commission):\n    \"\"\"A :class:`Commission` class that charges a percentage of the whole trade.\n\n    :param percentage: The percentage to charge. 0.01 means 1%, and so on. It must be smaller than 1.\n    :type percentage: float.\n    \"\"\"\n    def __init__(self, percentage):\n        super(TradePercentage, self).__init__()\n        assert(percentage < 1)\n        self.__percentage = percentage\n\n    def calculate(self, order, price, quantity):\n        return price * quantity * self.__percentage\n\n\n######################################################################\n# Orders\n\nclass BacktestingOrder(object):\n    def __init__(self, *args, **kwargs):\n        self.__accepted = None\n\n    def setAcceptedDateTime(self, dateTime):\n        self.__accepted = dateTime\n\n    def getAcceptedDateTime(self):\n        return self.__accepted\n\n    # Override to call the fill strategy using the concrete order type.\n    # return FillInfo or None if the order should not be filled.\n    def process(self, broker_, bar_):\n        raise NotImplementedError()\n\n\nclass MarketOrder(broker.MarketOrder, BacktestingOrder):\n    def __init__(self, action, instrument, quantity, onClose, instrumentTraits):\n        super(MarketOrder, self).__init__(action, instrument, quantity, onClose, instrumentTraits)\n\n    def process(self, broker_, bar_):\n        return broker_.getFillStrategy().fillMarketOrder(broker_, self, bar_)\n\n\nclass LimitOrder(broker.LimitOrder, BacktestingOrder):\n    def __init__(self, action, instrument, limitPrice, quantity, instrumentTraits):\n        super(LimitOrder, self).__init__(action, instrument, limitPrice, quantity, instrumentTraits)\n\n    def process(self, broker_, bar_):\n        return broker_.getFillStrategy().fillLimitOrder(broker_, self, bar_)\n\n\nclass StopOrder(broker.StopOrder, BacktestingOrder):\n    def __init__(self, action, instrument, stopPrice, quantity, instrumentTraits):\n        super(StopOrder, self).__init__(action, instrument, stopPrice, quantity, instrumentTraits)\n        self.__stopHit = False\n\n    def process(self, broker_, bar_):\n        return broker_.getFillStrategy().fillStopOrder(broker_, self, bar_)\n\n    def setStopHit(self, stopHit):\n        self.__stopHit = stopHit\n\n    def getStopHit(self):\n        return self.__stopHit\n\n\n# http://www.sec.gov/answers/stoplim.htm\n# http://www.interactivebrokers.com/en/trading/orders/stopLimit.php\nclass StopLimitOrder(broker.StopLimitOrder, BacktestingOrder):\n    def __init__(self, action, instrument, stopPrice, limitPrice, quantity, instrumentTraits):\n        super(StopLimitOrder, self).__init__(action, instrument, stopPrice, limitPrice, quantity, instrumentTraits)\n        self.__stopHit = False  # Set to true when the limit order is activated (stop price is hit)\n\n    def setStopHit(self, stopHit):\n        self.__stopHit = stopHit\n\n    def getStopHit(self):\n        return self.__stopHit\n\n    def isLimitOrderActive(self):\n        # TODO: Deprecated since v0.15. Use getStopHit instead.\n        return self.__stopHit\n\n    def process(self, broker_, bar_):\n        return broker_.getFillStrategy().fillStopLimitOrder(broker_, self, bar_)\n\n\n######################################################################\n# Broker\n\nclass Broker(broker.Broker):\n    \"\"\"Backtesting broker.\n\n    :param cash: The initial amount of cash.\n    :type cash: int/float.\n    :param barFeed: The bar feed that will provide the bars.\n    :type barFeed: :class:`pyalgotrade.barfeed.BarFeed`\n    :param commission: An object responsible for calculating order commissions.\n    :type commission: :class:`Commission`\n    \"\"\"\n\n    LOGGER_NAME = \"broker.backtesting\"\n\n    def __init__(self, cash, barFeed, commission=None):\n        super(Broker, self).__init__()\n\n        assert(cash >= 0)\n        self.__cash = cash\n        if commission is None:\n            self.__commission = NoCommission()\n        else:\n            self.__commission = commission\n        self.__shares = {}\n        self.__activeOrders = {}\n        self.__useAdjustedValues = False\n        self.__fillStrategy = fillstrategy.DefaultStrategy()\n        self.__logger = logger.getLogger(Broker.LOGGER_NAME)\n\n        # It is VERY important that the broker subscribes to barfeed events before the strategy.\n        barFeed.getNewValuesEvent().subscribe(self.onBars)\n        self.__barFeed = barFeed\n        self.__allowNegativeCash = False\n        self.__nextOrderId = 1\n\n    def _getNextOrderId(self):\n        ret = self.__nextOrderId\n        self.__nextOrderId += 1\n        return ret\n\n    def _getBar(self, bars, instrument):\n        ret = bars.getBar(instrument)\n        if ret is None:\n            ret = self.__barFeed.getLastBar(instrument)\n        return ret\n\n    def _registerOrder(self, order):\n        assert(order.getId() not in self.__activeOrders)\n        assert(order.getId() is not None)\n        self.__activeOrders[order.getId()] = order\n\n    def _unregisterOrder(self, order):\n        assert(order.getId() in self.__activeOrders)\n        assert(order.getId() is not None)\n        del self.__activeOrders[order.getId()]\n\n    def getLogger(self):\n        return self.__logger\n\n    def setAllowNegativeCash(self, allowNegativeCash):\n        self.__allowNegativeCash = allowNegativeCash\n\n    def getCash(self, includeShort=True):\n        ret = self.__cash\n        if not includeShort and self.__barFeed.getCurrentBars() is not None:\n            bars = self.__barFeed.getCurrentBars()\n            for instrument, shares in self.__shares.iteritems():\n                if shares < 0:\n                    instrumentPrice = self._getBar(bars, instrument).getClose(self.getUseAdjustedValues())\n                    ret += instrumentPrice * shares\n        return ret\n\n    def setCash(self, cash):\n        self.__cash = cash\n\n    def getCommission(self):\n        \"\"\"Returns the strategy used to calculate order commissions.\n\n        :rtype: :class:`Commission`.\n        \"\"\"\n        return self.__commission\n\n    def setCommission(self, commission):\n        \"\"\"Sets the strategy to use to calculate order commissions.\n\n        :param commission: An object responsible for calculating order commissions.\n        :type commission: :class:`Commission`.\n        \"\"\"\n\n        self.__commission = commission\n\n    def setFillStrategy(self, strategy):\n        \"\"\"Sets the :class:`pyalgotrade.broker.fillstrategy.FillStrategy` to use.\"\"\"\n        self.__fillStrategy = strategy\n\n    def getFillStrategy(self):\n        \"\"\"Returns the :class:`pyalgotrade.broker.fillstrategy.FillStrategy` currently set.\"\"\"\n        return self.__fillStrategy\n\n    def getUseAdjustedValues(self):\n        return self.__useAdjustedValues\n\n    def setUseAdjustedValues(self, useAdjusted):\n        # Deprecated since v0.15\n        if not self.__barFeed.barsHaveAdjClose():\n            raise Exception(\"The barfeed doesn't support adjusted close values\")\n        self.__useAdjustedValues = useAdjusted\n\n    def getActiveOrders(self, instrument=None):\n        if instrument is None:\n            ret = self.__activeOrders.values()\n        else:\n            ret = [order for order in self.__activeOrders.values() if order.getInstrument() == instrument]\n        return ret\n\n    def _getCurrentDateTime(self):\n        return self.__barFeed.getCurrentDateTime()\n\n    def getInstrumentTraits(self, instrument):\n        return broker.IntegerTraits()\n\n    def getShares(self, instrument):\n        return self.__shares.get(instrument, 0)\n\n    def getPositions(self):\n        return self.__shares\n\n    def getActiveInstruments(self):\n        return [instrument for instrument, shares in self.__shares.iteritems() if shares != 0]\n\n    def __getEquityWithBars(self, bars):\n        ret = self.getCash()\n        if bars is not None:\n            for instrument, shares in self.__shares.iteritems():\n                instrumentPrice = self._getBar(bars, instrument).getClose(self.getUseAdjustedValues())\n                ret += instrumentPrice * shares\n        return ret\n\n    def getEquity(self):\n        \"\"\"Returns the portfolio value (cash + shares).\"\"\"\n        return self.__getEquityWithBars(self.__barFeed.getCurrentBars())\n\n    # Tries to commit an order execution.\n    def commitOrderExecution(self, order, dateTime, fillInfo):\n        price = fillInfo.getPrice()\n        quantity = fillInfo.getQuantity()\n\n        if order.isBuy():\n            cost = price * quantity * -1\n            assert(cost < 0)\n            sharesDelta = quantity\n        elif order.isSell():\n            cost = price * quantity\n            assert(cost > 0)\n            sharesDelta = quantity * -1\n        else:  # Unknown action\n            assert(False)\n\n        commission = self.getCommission().calculate(order, price, quantity)\n        cost -= commission\n        resultingCash = self.getCash() + cost\n\n        # Check that we're ok on cash after the commission.\n        if resultingCash >= 0 or self.__allowNegativeCash:\n\n            # Update the order before updating internal state since addExecutionInfo may raise.\n            # addExecutionInfo should switch the order state.\n            orderExecutionInfo = broker.OrderExecutionInfo(price, quantity, commission, dateTime)\n            order.addExecutionInfo(orderExecutionInfo)\n\n            # Commit the order execution.\n            self.__cash = resultingCash\n            updatedShares = order.getInstrumentTraits().roundQuantity(\n                self.getShares(order.getInstrument()) + sharesDelta\n            )\n            if updatedShares == 0:\n                del self.__shares[order.getInstrument()]\n            else:\n                self.__shares[order.getInstrument()] = updatedShares\n\n            # Let the strategy know that the order was filled.\n            self.__fillStrategy.onOrderFilled(self, order)\n\n            # Notify the order update\n            if order.isFilled():\n                self._unregisterOrder(order)\n                self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.FILLED, orderExecutionInfo))\n            elif order.isPartiallyFilled():\n                self.notifyOrderEvent(\n                    broker.OrderEvent(order, broker.OrderEvent.Type.PARTIALLY_FILLED, orderExecutionInfo)\n                )\n            else:\n                assert(False)\n        else:\n            self.__logger.debug(\"Not enough cash to fill %s order [%s] for %s share/s\" % (\n                order.getInstrument(),\n                order.getId(),\n                order.getRemaining()\n            ))\n\n    def submitOrder(self, order):\n        if order.isInitial():\n            order.setSubmitted(self._getNextOrderId(), self._getCurrentDateTime())\n            self._registerOrder(order)\n            # Switch from INITIAL -> SUBMITTED\n            order.switchState(broker.Order.State.SUBMITTED)\n            self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.SUBMITTED, None))\n        else:\n            raise Exception(\"The order was already processed\")\n\n    # Return True if further processing is needed.\n    def __preProcessOrder(self, order, bar_):\n        ret = True\n\n        # For non-GTC orders we need to check if the order has expired.\n        if not order.getGoodTillCanceled():\n            expired = bar_.getDateTime().date() > order.getAcceptedDateTime().date()\n\n            # Cancel the order if it is expired.\n            if expired:\n                ret = False\n                self._unregisterOrder(order)\n                order.switchState(broker.Order.State.CANCELED)\n                self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.CANCELED, \"Expired\"))\n\n        return ret\n\n    def __postProcessOrder(self, order, bar_):\n        # For non-GTC orders and daily (or greater) bars we need to check if orders should expire right now\n        # before waiting for the next bar.\n        if not order.getGoodTillCanceled():\n            expired = False\n            if self.__barFeed.getFrequency() >= pyalgotrade.bar.Frequency.DAY:\n                expired = bar_.getDateTime().date() >= order.getAcceptedDateTime().date()\n\n            # Cancel the order if it will expire in the next bar.\n            if expired:\n                self._unregisterOrder(order)\n                order.switchState(broker.Order.State.CANCELED)\n                self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.CANCELED, \"Expired\"))\n\n    def __processOrder(self, order, bar_):\n        if not self.__preProcessOrder(order, bar_):\n            return\n\n        # Double dispatch to the fill strategy using the concrete order type.\n        fillInfo = order.process(self, bar_)\n        if fillInfo is not None:\n            self.commitOrderExecution(order, bar_.getDateTime(), fillInfo)\n\n        if order.isActive():\n            self.__postProcessOrder(order, bar_)\n\n    def __onBarsImpl(self, order, bars):\n        # IF WE'RE DEALING WITH MULTIPLE INSTRUMENTS WE SKIP ORDER PROCESSING IF THERE IS NO BAR FOR THE ORDER'S\n        # INSTRUMENT TO GET THE SAME BEHAVIOUR AS IF WERE BE PROCESSING ONLY ONE INSTRUMENT.\n        bar_ = bars.getBar(order.getInstrument())\n        if bar_ is not None:\n            # Switch from SUBMITTED -> ACCEPTED\n            if order.isSubmitted():\n                order.setAcceptedDateTime(bar_.getDateTime())\n                order.switchState(broker.Order.State.ACCEPTED)\n                self.notifyOrderEvent(broker.OrderEvent(order, broker.OrderEvent.Type.ACCEPTED, None))\n\n            if order.isActive():\n                # This may trigger orders to be added/removed from __activeOrders.\n                self.__processOrder(order, bar_)\n            else:\n                # If an order is not active it should be because it was canceled in this same loop and it should\n                # have been removed.\n                assert(order.isCanceled())\n                assert(order not in self.__activeOrders)\n\n    def onBars(self, dateTime, bars):\n        # Let the fill strategy know that new bars are being processed.\n        self.__fillStrategy.onBars(self, bars)\n\n        # This is to froze the orders that will be processed in this event, to avoid new getting orders introduced\n        # and processed on this very same event.\n        ordersToProcess = self.__activeOrders.values()\n\n        for order in ordersToProcess:\n            # This may trigger orders to be added/removed from __activeOrders.\n            self.__onBarsImpl(order, bars)\n\n    def start(self):\n        super(Broker, self).start()\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def eof(self):\n        # If there are no more events in the barfeed, then there is nothing left for us to do since all processing took\n        # place while processing barfeed events.\n        return self.__barFeed.eof()\n\n    def dispatch(self):\n        # All events were already emitted while handling barfeed events.\n        pass\n\n    def peekDateTime(self):\n        return None\n\n    def createMarketOrder(self, action, instrument, quantity, onClose=False):\n        # In order to properly support market-on-close with intraday feeds I'd need to know about different\n        # exchange/market trading hours and support specifying routing an order to a specific exchange/market.\n        # Even if I had all this in place it would be a problem while paper-trading with a live feed since\n        # I can't tell if the next bar will be the last bar of the market session or not.\n        if onClose is True and self.__barFeed.isIntraday():\n            raise Exception(\"Market-on-close not supported with intraday feeds\")\n\n        return MarketOrder(action, instrument, quantity, onClose, self.getInstrumentTraits(instrument))\n\n    def createLimitOrder(self, action, instrument, limitPrice, quantity):\n        return LimitOrder(action, instrument, limitPrice, quantity, self.getInstrumentTraits(instrument))\n\n    def createStopOrder(self, action, instrument, stopPrice, quantity):\n        return StopOrder(action, instrument, stopPrice, quantity, self.getInstrumentTraits(instrument))\n\n    def createStopLimitOrder(self, action, instrument, stopPrice, limitPrice, quantity):\n        return StopLimitOrder(action, instrument, stopPrice, limitPrice, quantity, self.getInstrumentTraits(instrument))\n\n    def cancelOrder(self, order):\n        activeOrder = self.__activeOrders.get(order.getId())\n        if activeOrder is None:\n            raise Exception(\"The order is not active anymore\")\n        if activeOrder.isFilled():\n            raise Exception(\"Can't cancel order that has already been filled\")\n\n        self._unregisterOrder(activeOrder)\n        activeOrder.switchState(broker.Order.State.CANCELED)\n        self.notifyOrderEvent(\n            broker.OrderEvent(activeOrder, broker.OrderEvent.Type.CANCELED, \"User requested cancellation\")\n        )\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import broker\nimport pyalgotrade.bar\nimport slippage\n\n\n# Returns the trigger price for a Limit or StopLimit order, or None if the limit price was not yet penetrated.\ndef get_limit_price_trigger(action, limitPrice, useAdjustedValues, bar):\n    ret = None\n    open_ = bar.getOpen(useAdjustedValues)\n    high = bar.getHigh(useAdjustedValues)\n    low = bar.getLow(useAdjustedValues)\n\n    # If the bar is below the limit price, use the open price.\n    # If the bar includes the limit price, use the open price or the limit price.\n    if action in [broker.Order.Action.BUY, broker.Order.Action.BUY_TO_COVER]:\n        if high < limitPrice:\n            ret = open_\n        elif limitPrice >= low:\n            if open_ < limitPrice:  # The limit price was penetrated on open.\n                ret = open_\n            else:\n                ret = limitPrice\n    # If the bar is above the limit price, use the open price.\n    # If the bar includes the limit price, use the open price or the limit price.\n    elif action in [broker.Order.Action.SELL, broker.Order.Action.SELL_SHORT]:\n        if low > limitPrice:\n            ret = open_\n        elif limitPrice <= high:\n            if open_ > limitPrice:  # The limit price was penetrated on open.\n                ret = open_\n            else:\n                ret = limitPrice\n    else:  # Unknown action\n        assert(False)\n    return ret\n\n\n# Returns the trigger price for a Stop or StopLimit order, or None if the stop price was not yet penetrated.\ndef get_stop_price_trigger(action, stopPrice, useAdjustedValues, bar):\n    ret = None\n    open_ = bar.getOpen(useAdjustedValues)\n    high = bar.getHigh(useAdjustedValues)\n    low = bar.getLow(useAdjustedValues)\n\n    # If the bar is above the stop price, use the open price.\n    # If the bar includes the stop price, use the open price or the stop price. Whichever is better.\n    if action in [broker.Order.Action.BUY, broker.Order.Action.BUY_TO_COVER]:\n        if low > stopPrice:\n            ret = open_\n        elif stopPrice <= high:\n            if open_ > stopPrice:  # The stop price was penetrated on open.\n                ret = open_\n            else:\n                ret = stopPrice\n    # If the bar is below the stop price, use the open price.\n    # If the bar includes the stop price, use the open price or the stop price. Whichever is better.\n    elif action in [broker.Order.Action.SELL, broker.Order.Action.SELL_SHORT]:\n        if high < stopPrice:\n            ret = open_\n        elif stopPrice >= low:\n            if open_ < stopPrice:  # The stop price was penetrated on open.\n                ret = open_\n            else:\n                ret = stopPrice\n    else:  # Unknown action\n        assert(False)\n\n    return ret\n\n\nclass FillInfo(object):\n    def __init__(self, price, quantity):\n        self.__price = price\n        self.__quantity = quantity\n\n    def getPrice(self):\n        return self.__price\n\n    def getQuantity(self):\n        return self.__quantity\n\n\nclass FillStrategy(object):\n    \"\"\"Base class for order filling strategies for the backtester.\"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    def onBars(self, broker_, bars):\n        \"\"\"\n        Override (optional) to get notified when the broker is about to process new bars.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param bars: The current bars.\n        :type bars: :class:`pyalgotrade.bar.Bars`\n        \"\"\"\n        pass\n\n    def onOrderFilled(self, broker_, order):\n        \"\"\"\n        Override (optional) to get notified when an order was filled, or partially filled.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param order: The order filled.\n        :type order: :class:`pyalgotrade.broker.Order`\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def fillMarketOrder(self, broker_, order, bar):\n        \"\"\"Override to return the fill price and quantity for a market order or None if the order can't be filled\n        at the given time.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param order: The order.\n        :type order: :class:`pyalgotrade.broker.MarketOrder`\n        :param bar: The current bar.\n        :type bar: :class:`pyalgotrade.bar.Bar`\n        :rtype: A :class:`FillInfo` or None if the order should not be filled.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def fillLimitOrder(self, broker_, order, bar):\n        \"\"\"Override to return the fill price and quantity for a limit order or None if the order can't be filled\n        at the given time.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param order: The order.\n        :type order: :class:`pyalgotrade.broker.LimitOrder`\n        :param bar: The current bar.\n        :type bar: :class:`pyalgotrade.bar.Bar`\n        :rtype: A :class:`FillInfo` or None if the order should not be filled.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def fillStopOrder(self, broker_, order, bar):\n        \"\"\"Override to return the fill price and quantity for a stop order or None if the order can't be filled\n        at the given time.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param order: The order.\n        :type order: :class:`pyalgotrade.broker.StopOrder`\n        :param bar: The current bar.\n        :type bar: :class:`pyalgotrade.bar.Bar`\n        :rtype: A :class:`FillInfo` or None if the order should not be filled.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def fillStopLimitOrder(self, broker_, order, bar):\n        \"\"\"Override to return the fill price and quantity for a stop limit order or None if the order can't be filled\n        at the given time.\n\n        :param broker_: The broker.\n        :type broker_: :class:`Broker`\n        :param order: The order.\n        :type order: :class:`pyalgotrade.broker.StopLimitOrder`\n        :param bar: The current bar.\n        :type bar: :class:`pyalgotrade.bar.Bar`\n        :rtype: A :class:`FillInfo` or None if the order should not be filled.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DefaultStrategy(FillStrategy):\n    \"\"\"\n    Default fill strategy.\n\n    :param volumeLimit: The proportion of the volume that orders can take up in a bar. Must be > 0 and <= 1.\n        If None, then volume limit is not checked.\n    :type volumeLimit: float\n\n    This strategy works as follows:\n\n    * A :class:`pyalgotrade.broker.MarketOrder` is always filled using the open/close price.\n    * A :class:`pyalgotrade.broker.LimitOrder` will be filled like this:\n        * If the limit price was penetrated with the open price, then the open price is used.\n        * If the bar includes the limit price, then the limit price is used.\n        * Note that when buying the price is penetrated if it gets <= the limit price, and when selling the price\n          is penetrated if it gets >= the limit price\n    * A :class:`pyalgotrade.broker.StopOrder` will be filled like this:\n        * If the stop price was penetrated with the open price, then the open price is used.\n        * If the bar includes the stop price, then the stop price is used.\n        * Note that when buying the price is penetrated if it gets >= the stop price, and when selling the price\n          is penetrated if it gets <= the stop price\n    * A :class:`pyalgotrade.broker.StopLimitOrder` will be filled like this:\n        * If the stop price was penetrated with the open price, or if the bar includes the stop price, then the limit\n          order becomes active.\n        * If the limit order is active:\n            * If the limit order was activated in this same bar and the limit price is penetrated as well, then the\n              best between the stop price and the limit fill price (as described earlier) is used.\n            * If the limit order was activated at a previous bar then the limit fill price (as described earlier)\n              is used.\n\n    .. note::\n        * This is the default strategy used by the Broker.\n        * It uses :class:`pyalgotrade.broker.slippage.NoSlippage` slippage model by default.\n        * If volumeLimit is 0.25, and a certain bar's volume is 100, then no more than 25 shares can be used by all\n          orders that get processed at that bar.\n        * If using trade bars, then all the volume from that bar can be used.\n    \"\"\"\n\n    def __init__(self, volumeLimit=0.25):\n        super(DefaultStrategy, self).__init__()\n        self.__volumeLeft = {}\n        self.__volumeUsed = {}\n        self.setVolumeLimit(volumeLimit)\n        self.setSlippageModel(slippage.NoSlippage())\n\n    def onBars(self, broker_, bars):\n        volumeLeft = {}\n\n        for instrument in bars.getInstruments():\n            bar = bars[instrument]\n            # Reset the volume available for each instrument.\n            if bar.getFrequency() == pyalgotrade.bar.Frequency.TRADE:\n                volumeLeft[instrument] = bar.getVolume()\n            elif self.__volumeLimit is not None:\n                # We can't round here because there is no order to request the instrument traits.\n                volumeLeft[instrument] = bar.getVolume() * self.__volumeLimit\n            # Reset the volume used for each instrument.\n            self.__volumeUsed[instrument] = 0.0\n\n        self.__volumeLeft = volumeLeft\n\n    def getVolumeLeft(self):\n        return self.__volumeLeft\n\n    def getVolumeUsed(self):\n        return self.__volumeUsed\n\n    def onOrderFilled(self, broker_, order):\n        # Update the volume left.\n        if self.__volumeLimit is not None:\n            # We round the volume left here becuase it was not rounded when it was initialized.\n            volumeLeft = order.getInstrumentTraits().roundQuantity(self.__volumeLeft[order.getInstrument()])\n            fillQuantity = order.getExecutionInfo().getQuantity()\n            assert volumeLeft >= fillQuantity, \\\n                \"Invalid fill quantity %s. Not enough volume left %s\" % (fillQuantity, volumeLeft)\n            self.__volumeLeft[order.getInstrument()] = order.getInstrumentTraits().roundQuantity(\n                volumeLeft - fillQuantity\n            )\n\n        # Update the volume used.\n        self.__volumeUsed[order.getInstrument()] = order.getInstrumentTraits().roundQuantity(\n            self.__volumeUsed[order.getInstrument()] + order.getExecutionInfo().getQuantity()\n        )\n\n    def setVolumeLimit(self, volumeLimit):\n        \"\"\"\n        Set the volume limit.\n\n        :param volumeLimit: The proportion of the volume that orders can take up in a bar. Must be > 0 and <= 1.\n            If None, then volume limit is not checked.\n        :type volumeLimit: float\n        \"\"\"\n\n        if volumeLimit is not None:\n            assert volumeLimit > 0 and volumeLimit <= 1, \"Invalid volume limit\"\n        self.__volumeLimit = volumeLimit\n\n    def setSlippageModel(self, slippageModel):\n        \"\"\"\n        Set the slippage model to use.\n\n        :param slippageModel: The slippage model.\n        :type slippageModel: :class:`pyalgotrade.broker.slippage.SlippageModel`\n        \"\"\"\n\n        self.__slippageModel = slippageModel\n\n    def __calculateFillSize(self, broker_, order, bar):\n        ret = 0\n\n        # If self.__volumeLimit is None then allow all the order to get filled.\n        if self.__volumeLimit is not None:\n            maxVolume = self.__volumeLeft.get(order.getInstrument(), 0)\n            maxVolume = order.getInstrumentTraits().roundQuantity(maxVolume)\n        else:\n            maxVolume = order.getRemaining()\n\n        if not order.getAllOrNone():\n            ret = min(maxVolume, order.getRemaining())\n        elif order.getRemaining() <= maxVolume:\n            ret = order.getRemaining()\n\n        return ret\n\n    def fillMarketOrder(self, broker_, order, bar):\n        # Calculate the fill size for the order.\n        fillSize = self.__calculateFillSize(broker_, order, bar)\n        if fillSize == 0:\n            broker_.getLogger().debug(\n                \"Not enough volume to fill %s market order [%s] for %s share/s\" % (\n                    order.getInstrument(),\n                    order.getId(),\n                    order.getRemaining()\n                )\n            )\n            return None\n\n        # Unless its a fill-on-close order, use the open price.\n        if order.getFillOnClose():\n            price = bar.getClose(broker_.getUseAdjustedValues())\n        else:\n            price = bar.getOpen(broker_.getUseAdjustedValues())\n        assert price is not None\n\n        # Don't slip prices when the bar represents the trading activity of a single trade.\n        if bar.getFrequency() != pyalgotrade.bar.Frequency.TRADE:\n            price = self.__slippageModel.calculatePrice(\n                order, price, fillSize, bar, self.__volumeUsed[order.getInstrument()]\n            )\n        return FillInfo(price, fillSize)\n\n    def fillLimitOrder(self, broker_, order, bar):\n        # Calculate the fill size for the order.\n        fillSize = self.__calculateFillSize(broker_, order, bar)\n        if fillSize == 0:\n            broker_.getLogger().debug(\"Not enough volume to fill %s limit order [%s] for %s share/s\" % (\n                order.getInstrument(), order.getId(), order.getRemaining())\n            )\n            return None\n\n        ret = None\n        price = get_limit_price_trigger(order.getAction(), order.getLimitPrice(), broker_.getUseAdjustedValues(), bar)\n        if price is not None:\n            ret = FillInfo(price, fillSize)\n        return ret\n\n    def fillStopOrder(self, broker_, order, bar):\n        ret = None\n\n        # First check if the stop price was hit so the market order becomes active.\n        stopPriceTrigger = None\n        if not order.getStopHit():\n            stopPriceTrigger = get_stop_price_trigger(\n                order.getAction(),\n                order.getStopPrice(),\n                broker_.getUseAdjustedValues(),\n                bar\n            )\n            order.setStopHit(stopPriceTrigger is not None)\n\n        # If the stop price was hit, check if we can fill the market order.\n        if order.getStopHit():\n            # Calculate the fill size for the order.\n            fillSize = self.__calculateFillSize(broker_, order, bar)\n            if fillSize == 0:\n                broker_.getLogger().debug(\"Not enough volume to fill %s stop order [%s] for %s share/s\" % (\n                    order.getInstrument(),\n                    order.getId(),\n                    order.getRemaining()\n                ))\n                return None\n\n            # If we just hit the stop price we'll use it as the fill price.\n            # For the remaining bars we'll use the open price.\n            if stopPriceTrigger is not None:\n                price = stopPriceTrigger\n            else:\n                price = bar.getOpen(broker_.getUseAdjustedValues())\n            assert price is not None\n\n            # Don't slip prices when the bar represents the trading activity of a single trade.\n            if bar.getFrequency() != pyalgotrade.bar.Frequency.TRADE:\n                price = self.__slippageModel.calculatePrice(\n                    order, price, fillSize, bar, self.__volumeUsed[order.getInstrument()]\n                )\n            ret = FillInfo(price, fillSize)\n        return ret\n\n    def fillStopLimitOrder(self, broker_, order, bar):\n        ret = None\n\n        # First check if the stop price was hit so the limit order becomes active.\n        stopPriceTrigger = None\n        if not order.getStopHit():\n            stopPriceTrigger = get_stop_price_trigger(\n                order.getAction(),\n                order.getStopPrice(),\n                broker_.getUseAdjustedValues(),\n                bar\n            )\n            order.setStopHit(stopPriceTrigger is not None)\n\n        # If the stop price was hit, check if we can fill the limit order.\n        if order.getStopHit():\n            # Calculate the fill size for the order.\n            fillSize = self.__calculateFillSize(broker_, order, bar)\n            if fillSize == 0:\n                broker_.getLogger().debug(\"Not enough volume to fill %s stop limit order [%s] for %s share/s\" % (\n                    order.getInstrument(),\n                    order.getId(),\n                    order.getRemaining()\n                ))\n                return None\n\n            price = get_limit_price_trigger(\n                order.getAction(),\n                order.getLimitPrice(),\n                broker_.getUseAdjustedValues(),\n                bar\n            )\n            if price is not None:\n                # If we just hit the stop price, we need to make additional checks.\n                if stopPriceTrigger is not None:\n                    if order.isBuy():\n                        # If the stop price triggered is lower than the limit price, then use that one.\n                        # Else use the limit price.\n                        price = min(stopPriceTrigger, order.getLimitPrice())\n                    else:\n                        # If the stop price triggered is greater than the limit price, then use that one.\n                        # Else use the limit price.\n                        price = max(stopPriceTrigger, order.getLimitPrice())\n\n                ret = FillInfo(price, fillSize)\n\n        return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\n\nclass SlippageModel(object):\n    \"\"\"Base class for slippage models.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def calculatePrice(self, order, price, quantity, bar, volumeUsed):\n        \"\"\"\n        Returns the slipped price per share for an order.\n\n        :param order: The order being filled.\n        :type order: :class:`pyalgotrade.broker.Order`.\n        :param price: The price for each share before slippage.\n        :type price: float.\n        :param quantity: The amount of shares that will get filled at this time for this order.\n        :type quantity: float.\n        :param bar: The current bar.\n        :type bar: :class:`pyalgotrade.bar.Bar`.\n        :param volumeUsed: The volume size that was taken so far from the current bar.\n        :type volumeUsed: float.\n        :rtype: float.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass NoSlippage(SlippageModel):\n    \"\"\"A no slippage model.\"\"\"\n\n    def calculatePrice(self, order, price, quantity, bar, volumeUsed):\n        return price\n\n\nclass VolumeShareSlippage(SlippageModel):\n    \"\"\"\n    A volume share slippage model as defined in Zipline's VolumeShareSlippage model.\n    The slippage is calculated by multiplying the price impact constant by the square of the ratio of the order\n    to the total volume.\n\n    Check https://www.quantopian.com/help#ide-slippage for more details.\n\n    :param priceImpact: Defines how large of an impact your order will have on the backtester's price calculation.\n    :type priceImpact: float.\n    \"\"\"\n\n    def __init__(self, priceImpact=0.1):\n        super(VolumeShareSlippage, self).__init__()\n        self.__priceImpact = priceImpact\n\n    def calculatePrice(self, order, price, quantity, bar, volumeUsed):\n        assert bar.getVolume(), \"Can't use 0 volume bars with VolumeShareSlippage\"\n\n        totalVolume = volumeUsed + quantity\n        volumeShare = totalVolume / float(bar.getVolume())\n        impactPct = volumeShare ** 2 * self.__priceImpact\n        if order.isBuy():\n            ret = price * (1 + impactPct)\n        else:\n            ret = price * (1 - impactPct)\n        return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import observer\nfrom pyalgotrade.utils import collections\n\nDEFAULT_MAX_LEN = 1024\n\n\ndef get_checked_max_len(maxLen):\n    if maxLen is None:\n        maxLen = DEFAULT_MAX_LEN\n    if not maxLen > 0:\n        raise Exception(\"Invalid maximum length\")\n    return maxLen\n\n\n# It is important to inherit object to get __getitem__ to work properly.\n# Check http://code.activestate.com/lists/python-list/621258/\nclass DataSeries(object):\n    \"\"\"Base class for data series.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def __len__(self):\n        \"\"\"Returns the number of elements in the data series.\"\"\"\n        raise NotImplementedError()\n\n    def __getitem__(self, key):\n        \"\"\"Returns the value at a given position/slice. It raises IndexError if the position is invalid,\n        or TypeError if the key type is invalid.\"\"\"\n        if isinstance(key, slice):\n            return [self[i] for i in xrange(*key.indices(len(self)))]\n        elif isinstance(key, int):\n            if key < 0:\n                key += len(self)\n            if key >= len(self) or key < 0:\n                raise IndexError(\"Index out of range\")\n            return self.getValueAbsolute(key)\n        else:\n            raise TypeError(\"Invalid argument type\")\n\n    # This is similar to __getitem__ for ints, but it shouldn't raise for invalid positions.\n    @abc.abstractmethod\n    def getValueAbsolute(self, pos):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getDateTimes(self):\n        \"\"\"Returns a list of :class:`datetime.datetime` associated with each value.\"\"\"\n        raise NotImplementedError()\n\n\nclass SequenceDataSeries(DataSeries):\n    \"\"\"A DataSeries that holds values in a sequence in memory.\n\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n    \"\"\"\n\n    def __init__(self, maxLen=None):\n        super(SequenceDataSeries, self).__init__()\n        maxLen = get_checked_max_len(maxLen)\n\n        self.__newValueEvent = observer.Event()\n        self.__values = collections.ListDeque(maxLen)\n        self.__dateTimes = collections.ListDeque(maxLen)\n\n    def __len__(self):\n        return len(self.__values)\n\n    def __getitem__(self, key):\n        return self.__values[key]\n\n    def setMaxLen(self, maxLen):\n        \"\"\"Sets the maximum number of values to hold and resizes accordingly if necessary.\"\"\"\n        self.__values.resize(maxLen)\n        self.__dateTimes.resize(maxLen)\n\n    def getMaxLen(self):\n        \"\"\"Returns the maximum number of values to hold.\"\"\"\n        return self.__values.getMaxLen()\n\n    # Event handler receives:\n    # 1: Dataseries generating the event\n    # 2: The datetime for the new value\n    # 3: The new value\n    def getNewValueEvent(self):\n        return self.__newValueEvent\n\n    def getValueAbsolute(self, pos):\n        ret = None\n        if pos >= 0 and pos < len(self.__values):\n            ret = self.__values[pos]\n        return ret\n\n    def append(self, value):\n        \"\"\"Appends a value.\"\"\"\n        self.appendWithDateTime(None, value)\n\n    def appendWithDateTime(self, dateTime, value):\n        \"\"\"\n        Appends a value with an associated datetime.\n\n        .. note::\n            If dateTime is not None, it must be greater than the last one.\n        \"\"\"\n\n        if dateTime is not None and len(self.__dateTimes) != 0 and self.__dateTimes[-1] >= dateTime:\n            raise Exception(\"Invalid datetime. It must be bigger than that last one\")\n\n        assert(len(self.__values) == len(self.__dateTimes))\n        self.__dateTimes.append(dateTime)\n        self.__values.append(value)\n\n        self.getNewValueEvent().emit(self, dateTime, value)\n\n    def getDateTimes(self):\n        return self.__dateTimes.data()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import dataseries\n\n\nclass BarDataSeries(dataseries.SequenceDataSeries):\n    \"\"\"A DataSeries of :class:`pyalgotrade.bar.Bar` instances.\n\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n    \"\"\"\n\n    def __init__(self, maxLen=None):\n        super(BarDataSeries, self).__init__(maxLen)\n        self.__openDS = dataseries.SequenceDataSeries(maxLen)\n        self.__closeDS = dataseries.SequenceDataSeries(maxLen)\n        self.__highDS = dataseries.SequenceDataSeries(maxLen)\n        self.__lowDS = dataseries.SequenceDataSeries(maxLen)\n        self.__volumeDS = dataseries.SequenceDataSeries(maxLen)\n        self.__adjCloseDS = dataseries.SequenceDataSeries(maxLen)\n        self.__extraDS = {}\n        self.__useAdjustedValues = False\n\n    def __getOrCreateExtraDS(self, name):\n        ret = self.__extraDS.get(name)\n        if ret is None:\n            ret = dataseries.SequenceDataSeries(self.getMaxLen())\n            self.__extraDS[name] = ret\n        return ret\n\n    def setUseAdjustedValues(self, useAdjusted):\n        self.__useAdjustedValues = useAdjusted\n\n    def append(self, bar):\n        self.appendWithDateTime(bar.getDateTime(), bar)\n\n    def appendWithDateTime(self, dateTime, bar):\n        assert(dateTime is not None)\n        assert(bar is not None)\n        bar.setUseAdjustedValue(self.__useAdjustedValues)\n\n        super(BarDataSeries, self).appendWithDateTime(dateTime, bar)\n\n        self.__openDS.appendWithDateTime(dateTime, bar.getOpen())\n        self.__closeDS.appendWithDateTime(dateTime, bar.getClose())\n        self.__highDS.appendWithDateTime(dateTime, bar.getHigh())\n        self.__lowDS.appendWithDateTime(dateTime, bar.getLow())\n        self.__volumeDS.appendWithDateTime(dateTime, bar.getVolume())\n        self.__adjCloseDS.appendWithDateTime(dateTime, bar.getAdjClose())\n\n        # Process extra columns.\n        for name, value in bar.getExtraColumns().iteritems():\n            extraDS = self.__getOrCreateExtraDS(name)\n            extraDS.appendWithDateTime(dateTime, value)\n\n    def getOpenDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the open prices.\"\"\"\n        return self.__openDS\n\n    def getCloseDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the close prices.\"\"\"\n        return self.__closeDS\n\n    def getHighDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the high prices.\"\"\"\n        return self.__highDS\n\n    def getLowDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the low prices.\"\"\"\n        return self.__lowDS\n\n    def getVolumeDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the volume.\"\"\"\n        return self.__volumeDS\n\n    def getAdjCloseDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the adjusted close prices.\"\"\"\n        return self.__adjCloseDS\n\n    def getPriceDataSeries(self):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` with the close or adjusted close prices.\"\"\"\n        if self.__useAdjustedValues:\n            return self.__adjCloseDS\n        else:\n            return self.__closeDS\n\n    def getExtraDataSeries(self, name):\n        \"\"\"Returns a :class:`pyalgotrade.dataseries.DataSeries` for an extra column.\"\"\"\n        return self.__getOrCreateExtraDS(name)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\n\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.dataseries import bards\nfrom pyalgotrade import bar\nfrom pyalgotrade import resamplebase\n\n\nclass AggFunGrouper(resamplebase.Grouper):\n    def __init__(self, groupDateTime, value, aggfun):\n        super(AggFunGrouper, self).__init__(groupDateTime)\n        self.__values = [value]\n        self.__aggfun = aggfun\n\n    def addValue(self, value):\n        self.__values.append(value)\n\n    def getGrouped(self):\n        return self.__aggfun(self.__values)\n\n\nclass BarGrouper(resamplebase.Grouper):\n    def __init__(self, groupDateTime, bar_, frequency):\n        super(BarGrouper, self).__init__(groupDateTime)\n        self.__open = bar_.getOpen()\n        self.__high = bar_.getHigh()\n        self.__low = bar_.getLow()\n        self.__close = bar_.getClose()\n        self.__volume = bar_.getVolume()\n        self.__adjClose = bar_.getAdjClose()\n        self.__useAdjValue = bar_.getUseAdjValue()\n        self.__frequency = frequency\n\n    def addValue(self, value):\n        self.__high = max(self.__high, value.getHigh())\n        self.__low = min(self.__low, value.getLow())\n        self.__close = value.getClose()\n        self.__adjClose = value.getAdjClose()\n        self.__volume += value.getVolume()\n\n    def getGrouped(self):\n        \"\"\"Return the grouped value.\"\"\"\n        ret = bar.BasicBar(\n            self.getDateTime(),\n            self.__open, self.__high, self.__low, self.__close, self.__volume, self.__adjClose,\n            self.__frequency\n        )\n        ret.setUseAdjustedValue(self.__useAdjValue)\n        return ret\n\n\nclass DSResampler(object):\n    __metaclass__ = abc.ABCMeta\n\n    def initDSResampler(self, dataSeries, frequency):\n        if not resamplebase.is_valid_frequency(frequency):\n            raise Exception(\"Unsupported frequency\")\n\n        self.__frequency = frequency\n        self.__grouper = None\n        self.__range = None\n\n        dataSeries.getNewValueEvent().subscribe(self.__onNewValue)\n\n    @abc.abstractmethod\n    def buildGrouper(self, range_, value, frequency):\n        raise NotImplementedError()\n\n    def __onNewValue(self, dataSeries, dateTime, value):\n        if self.__range is None:\n            self.__range = resamplebase.build_range(dateTime, self.__frequency)\n            self.__grouper = self.buildGrouper(self.__range, value, self.__frequency)\n        elif self.__range.belongs(dateTime):\n            self.__grouper.addValue(value)\n        else:\n            self.appendWithDateTime(self.__grouper.getDateTime(), self.__grouper.getGrouped())\n            self.__range = resamplebase.build_range(dateTime, self.__frequency)\n            self.__grouper = self.buildGrouper(self.__range, value, self.__frequency)\n\n    def pushLast(self):\n        if self.__grouper is not None:\n            self.appendWithDateTime(self.__grouper.getDateTime(), self.__grouper.getGrouped())\n            self.__grouper = None\n            self.__range = None\n\n    def checkNow(self, dateTime):\n        if self.__range is not None and not self.__range.belongs(dateTime):\n            self.appendWithDateTime(self.__grouper.getDateTime(), self.__grouper.getGrouped())\n            self.__grouper = None\n            self.__range = None\n\n\nclass ResampledBarDataSeries(bards.BarDataSeries, DSResampler):\n    \"\"\"A BarDataSeries that will build on top of another, higher frequency, BarDataSeries.\n    Resampling will take place as new values get pushed into the dataseries being resampled.\n\n    :param dataSeries: The DataSeries instance being resampled.\n    :type dataSeries: :class:`pyalgotrade.dataseries.bards.BarDataSeries`\n    :param frequency: The grouping frequency in seconds. Must be > 0.\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded\n        from the opposite end.\n    :type maxLen: int.\n\n    .. note::\n        * Supported resampling frequencies are:\n            * Less than bar.Frequency.DAY\n            * bar.Frequency.DAY\n            * bar.Frequency.MONTH\n    \"\"\"\n\n    def __init__(self, dataSeries, frequency, maxLen=None):\n        if not isinstance(dataSeries, bards.BarDataSeries):\n            raise Exception(\"dataSeries must be a dataseries.bards.BarDataSeries instance\")\n\n        super(ResampledBarDataSeries, self).__init__(maxLen)\n        self.initDSResampler(dataSeries, frequency)\n\n    def checkNow(self, dateTime):\n        \"\"\"Forces a resample check. Depending on the resample frequency, and the current datetime, a new\n        value may be generated.\n\n       :param dateTime: The current datetime.\n       :type dateTime: :class:`datetime.datetime`\n        \"\"\"\n\n        return super(ResampledBarDataSeries, self).checkNow(dateTime)\n\n    def buildGrouper(self, range_, value, frequency):\n        return BarGrouper(range_.getBeginning(), value, frequency)\n\n\nclass ResampledDataSeries(dataseries.SequenceDataSeries, DSResampler):\n    def __init__(self, dataSeries, frequency, aggfun, maxLen=None):\n        super(ResampledDataSeries, self).__init__(maxLen)\n        self.initDSResampler(dataSeries, frequency)\n        self.__aggfun = aggfun\n\n    def buildGrouper(self, range_, value, frequency):\n        return AggFunGrouper(range_.getBeginning(), value, self.__aggfun)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pyalgotrade.technical import roc\nfrom pyalgotrade import dispatcher\n\n\nclass Results(object):\n    \"\"\"Results from the profiler.\"\"\"\n    def __init__(self, eventsDict, lookBack, lookForward):\n        assert(lookBack > 0)\n        assert(lookForward > 0)\n        self.__lookBack = lookBack\n        self.__lookForward = lookForward\n        self.__values = [[] for i in xrange(lookBack+lookForward+1)]\n        self.__eventCount = 0\n\n        # Process events.\n        for instrument, events in eventsDict.items():\n            for event in events:\n                # Skip events which are on the boundary or for some reason are not complete.\n                if event.isComplete():\n                    self.__eventCount += 1\n                    # Compute cumulative returns: (1 + R1)*(1 + R2)*...*(1 + Rn)\n                    values = np.cumprod(event.getValues() + 1)\n                    # Normalize everything to the time of the event\n                    values = values / values[event.getLookBack()]\n                    for t in range(event.getLookBack()*-1, event.getLookForward()+1):\n                        self.setValue(t, values[t+event.getLookBack()])\n\n    def __mapPos(self, t):\n        assert(t >= -1*self.__lookBack and t <= self.__lookForward)\n        return t + self.__lookBack\n\n    def setValue(self, t, value):\n        if value is None:\n            raise Exception(\"Invalid value at time %d\" % (t))\n        pos = self.__mapPos(t)\n        self.__values[pos].append(value)\n\n    def getValues(self, t):\n        pos = self.__mapPos(t)\n        return self.__values[pos]\n\n    def getLookBack(self):\n        return self.__lookBack\n\n    def getLookForward(self):\n        return self.__lookForward\n\n    def getEventCount(self):\n        \"\"\"Returns the number of events occurred. Events that are on the boundary are skipped.\"\"\"\n        return self.__eventCount\n\n\nclass Predicate(object):\n    \"\"\"Base class for event identification. You should subclass this to implement\n    the event identification logic.\"\"\"\n\n    def eventOccurred(self, instrument, bards):\n        \"\"\"Override (**mandatory**) to determine if an event took place in the last bar (bards[-1]).\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param bards: The BarDataSeries for the given instrument.\n        :type bards: :class:`pyalgotrade.dataseries.bards.BarDataSeries`.\n        :rtype: boolean.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass Event(object):\n    def __init__(self, lookBack, lookForward):\n        assert(lookBack > 0)\n        assert(lookForward > 0)\n        self.__lookBack = lookBack\n        self.__lookForward = lookForward\n        self.__values = np.empty((lookBack + lookForward + 1))\n        self.__values[:] = np.NAN\n\n    def __mapPos(self, t):\n        assert(t >= -1*self.__lookBack and t <= self.__lookForward)\n        return t + self.__lookBack\n\n    def isComplete(self):\n        return not any(np.isnan(self.__values))\n\n    def getLookBack(self):\n        return self.__lookBack\n\n    def getLookForward(self):\n        return self.__lookForward\n\n    def setValue(self, t, value):\n        if value is not None:\n            pos = self.__mapPos(t)\n            self.__values[pos] = value\n\n    def getValue(self, t):\n        pos = self.__mapPos(t)\n        return self.__values[pos]\n\n    def getValues(self):\n        return self.__values\n\n\nclass Profiler(object):\n    \"\"\"This class is responsible for scanning over historical data and analyzing returns before\n    and after the events.\n\n    :param predicate: A :class:`Predicate` subclass responsible for identifying events.\n    :type predicate: :class:`Predicate`.\n    :param lookBack: The number of bars before the event to analyze. Must be > 0.\n    :type lookBack: int.\n    :param lookForward: The number of bars after the event to analyze. Must be > 0.\n    :type lookForward: int.\n    \"\"\"\n\n    def __init__(self, predicate, lookBack, lookForward):\n        assert(lookBack > 0)\n        assert(lookForward > 0)\n        self.__predicate = predicate\n        self.__lookBack = lookBack\n        self.__lookForward = lookForward\n        self.__feed = None\n        self.__rets = {}\n        self.__futureRets = {}\n        self.__events = {}\n\n    def __addPastReturns(self, instrument, event):\n        begin = (event.getLookBack() + 1) * -1\n        for t in xrange(begin, 0):\n            try:\n                ret = self.__rets[instrument][t]\n                if ret is not None:\n                    event.setValue(t+1, ret)\n            except IndexError:\n                pass\n\n    def __addCurrentReturns(self, instrument):\n        nextTs = []\n        for event, t in self.__futureRets[instrument]:\n            event.setValue(t, self.__rets[instrument][-1])\n            if t < event.getLookForward():\n                t += 1\n                nextTs.append((event, t))\n        self.__futureRets[instrument] = nextTs\n\n    def __onBars(self, dateTime, bars):\n        for instrument in bars.getInstruments():\n            self.__addCurrentReturns(instrument)\n            eventOccurred = self.__predicate.eventOccurred(instrument, self.__feed[instrument])\n            if eventOccurred:\n                event = Event(self.__lookBack, self.__lookForward)\n                self.__events[instrument].append(event)\n                self.__addPastReturns(instrument, event)\n                # Add next return for this instrument at t=1.\n                self.__futureRets[instrument].append((event, 1))\n\n    def getResults(self):\n        \"\"\"Returns the results of the analysis.\n\n        :rtype: :class:`Results`.\n        \"\"\"\n        return Results(self.__events, self.__lookBack, self.__lookForward)\n\n    def run(self, feed, useAdjustedCloseForReturns=True):\n        \"\"\"Runs the analysis using the bars supplied by the feed.\n\n        :param barFeed: The bar feed to use to run the analysis.\n        :type barFeed: :class:`pyalgotrade.barfeed.BarFeed`.\n        :param useAdjustedCloseForReturns: True if adjusted close values should be used to calculate returns.\n        :type useAdjustedCloseForReturns: boolean.\n        \"\"\"\n\n        if useAdjustedCloseForReturns:\n            assert feed.barsHaveAdjClose(), \"Feed doesn't have adjusted close values\"\n\n        try:\n            self.__feed = feed\n            self.__rets = {}\n            self.__futureRets = {}\n            for instrument in feed.getRegisteredInstruments():\n                self.__events.setdefault(instrument, [])\n                self.__futureRets[instrument] = []\n                if useAdjustedCloseForReturns:\n                    ds = feed[instrument].getAdjCloseDataSeries()\n                else:\n                    ds = feed[instrument].getCloseDataSeries()\n                self.__rets[instrument] = roc.RateOfChange(ds, 1)\n\n            feed.getNewValuesEvent().subscribe(self.__onBars)\n            disp = dispatcher.Dispatcher()\n            disp.addSubject(feed)\n            disp.run()\n        finally:\n            feed.getNewValuesEvent().unsubscribe(self.__onBars)\n\n\ndef build_plot(profilerResults):\n    # Calculate each value.\n    x = []\n    y = []\n    std = []\n    for t in xrange(profilerResults.getLookBack()*-1, profilerResults.getLookForward()+1):\n        x.append(t)\n        values = np.asarray(profilerResults.getValues(t))\n        y.append(values.mean())\n        std.append(values.std())\n\n    # Plot\n    plt.clf()\n    plt.plot(x, y, color='#0000FF')\n    eventT = profilerResults.getLookBack()\n    # stdBegin = eventT + 1\n    # plt.errorbar(x[stdBegin:], y[stdBegin:], std[stdBegin:], alpha=0, ecolor='#AAAAFF')\n    plt.errorbar(x[eventT+1:], y[eventT+1:], std[eventT+1:], alpha=0, ecolor='#AAAAFF')\n    # plt.errorbar(x, y, std, alpha=0, ecolor='#AAAAFF')\n    plt.axhline(y=y[eventT], xmin=-1*profilerResults.getLookBack(), xmax=profilerResults.getLookForward(), color='#000000')\n    plt.xlim(profilerResults.getLookBack()*-1-0.5, profilerResults.getLookForward()+0.5)\n    plt.xlabel('Time')\n    plt.ylabel('Cumulative returns')\n\n\ndef plot(profilerResults):\n    \"\"\"Plots the result of the analysis.\n\n    :param profilerResults: The result of the analysis\n    :type profilerResults: :class:`Results`.\n    \"\"\"\n\n    build_plot(profilerResults)\n    plt.show()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import observer\nfrom pyalgotrade import dataseries\n\n\ndef feed_iterator(feed):\n    feed.start()\n    try:\n        while not feed.eof():\n            yield feed.getNextValuesAndUpdateDS()\n    finally:\n        feed.stop()\n        feed.join()\n\n\nclass BaseFeed(observer.Subject):\n    \"\"\"Base class for feeds.\n\n    :param maxLen: The maximum number of values that each :class:`pyalgotrade.dataseries.DataSeries` will hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded\n        from the opposite end.\n    :type maxLen: int.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, maxLen):\n        super(BaseFeed, self).__init__()\n\n        maxLen = dataseries.get_checked_max_len(maxLen)\n\n        self.__ds = {}\n        self.__event = observer.Event()\n        self.__maxLen = maxLen\n\n    def reset(self):\n        keys = self.__ds.keys()\n        self.__ds = {}\n        for key in keys:\n            self.registerDataSeries(key)\n\n    # Subclasses should implement this and return the appropriate dataseries for the given key.\n    @abc.abstractmethod\n    def createDataSeries(self, key, maxLen):\n        raise NotImplementedError()\n\n    # Subclasses should implement this and return a tuple with two elements:\n    # 1: datetime.datetime.\n    # 2: dictionary or dict-like object.\n    @abc.abstractmethod\n    def getNextValues(self):\n        raise NotImplementedError()\n\n    def registerDataSeries(self, key):\n        if key not in self.__ds:\n            self.__ds[key] = self.createDataSeries(key, self.__maxLen)\n\n    def getNextValuesAndUpdateDS(self):\n        dateTime, values = self.getNextValues()\n        if dateTime is not None:\n            for key, value in values.items():\n                # Get or create the datseries for each key.\n                try:\n                    ds = self.__ds[key]\n                except KeyError:\n                    ds = self.createDataSeries(key, self.__maxLen)\n                    self.__ds[key] = ds\n                ds.appendWithDateTime(dateTime, value)\n        return (dateTime, values)\n\n    def __iter__(self):\n        return feed_iterator(self)\n\n    def getNewValuesEvent(self):\n        \"\"\"Returns the event that will be emitted when new values are available.\n        To subscribe you need to pass in a callable object that receives two parameters:\n\n         1. A :class:`datetime.datetime` instance.\n         2. The new value.\n        \"\"\"\n        return self.__event\n\n    def dispatch(self):\n        dateTime, values = self.getNextValuesAndUpdateDS()\n        if dateTime is not None:\n            self.__event.emit(dateTime, values)\n        return dateTime is not None\n\n    def getKeys(self):\n        return self.__ds.keys()\n\n    def __getitem__(self, key):\n        \"\"\"Returns the :class:`pyalgotrade.dataseries.DataSeries` for a given key.\"\"\"\n        return self.__ds[key]\n\n    def __contains__(self, key):\n        \"\"\"Returns True if a :class:`pyalgotrade.dataseries.DataSeries` for the given key is available.\"\"\"\n        return key in self.__ds\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\nimport datetime\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.utils import csvutils\nfrom pyalgotrade.feed import memfeed\n\n\n# Interface for csv row parsers.\nclass RowParser(object):\n\n    __metaclass__ = abc.ABCMeta\n\n    # Parses a row and returns a tuple with with two elements:\n    # 1: datetime.datetime.\n    # 2: dictionary or dict-like object.\n    @abc.abstractmethod\n    def parseRow(self, csvRowDict):\n        raise NotImplementedError()\n\n    # Returns a list of field names. If None, then the first row in the CSV should have the field names.\n    @abc.abstractmethod\n    def getFieldNames(self):\n        raise NotImplementedError()\n\n    # Returns the delimiter.\n    @abc.abstractmethod\n    def getDelimiter(self):\n        raise NotImplementedError()\n\n\n# Interface for bar filters.\nclass RowFilter(object):\n\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def includeRow(self, dateTime, values):\n        raise NotImplementedError()\n\n\nclass DateRangeFilter(RowFilter):\n    def __init__(self, fromDate=None, toDate=None):\n        self.__fromDate = fromDate\n        self.__toDate = toDate\n\n    def includeRow(self, dateTime, values):\n        if self.__toDate and dateTime > self.__toDate:\n            return False\n        if self.__fromDate and dateTime < self.__fromDate:\n            return False\n        return True\n\n\nclass BaseFeed(memfeed.MemFeed):\n    def __init__(self, rowParser, maxLen=None):\n        super(BaseFeed, self).__init__(maxLen)\n\n        self.__rowParser = rowParser\n        self.__rowFilter = None\n\n    def setRowFilter(self, rowFilter):\n        self.__rowFilter = rowFilter\n\n    def addValuesFromCSV(self, path):\n        # Load the values from the csv file\n        values = []\n        reader = csvutils.FastDictReader(open(path, \"r\"), fieldnames=self.__rowParser.getFieldNames(), delimiter=self.__rowParser.getDelimiter())\n        for row in reader:\n            dateTime, rowValues = self.__rowParser.parseRow(row)\n            if dateTime is not None and (self.__rowFilter is None or self.__rowFilter.includeRow(dateTime, rowValues)):\n                values.append((dateTime, rowValues))\n\n        self.addValues(values)\n\n\n# This row parser doesn't support CSV files that have date and time in different columns.\nclass BasicRowParser(RowParser):\n    def __init__(self, dateTimeColumn, dateTimeFormat, converter, delimiter=\",\", timezone=None):\n        self.__dateTimeColumn = dateTimeColumn\n        self.__dateTimeFormat = dateTimeFormat\n        self.__converter = converter\n        self.__delimiter = delimiter\n        self.__timezone = timezone\n        self.__timeDelta = None\n\n    def parseRow(self, csvRowDict):\n        dateTime = datetime.datetime.strptime(csvRowDict[self.__dateTimeColumn], self.__dateTimeFormat)\n        # Localize the datetime if a timezone was given.\n        if self.__timezone is not None:\n            if self.__timeDelta is not None:\n                dateTime += self.__timeDelta\n            dateTime = dt.localize(dateTime, self.__timezone)\n        # Convert the values\n        values = {}\n        for key, value in csvRowDict.items():\n            if key != self.__dateTimeColumn:\n                values[key] = self.__converter(key, value)\n        return (dateTime, values)\n\n    def getFieldNames(self):\n        return None\n\n    def getDelimiter(self):\n        return self.__delimiter\n\n    def setTimeDelta(self, timeDelta):\n        self.__timeDelta = timeDelta\n\n\ndef float_or_string(column, value):\n    return csvutils.float_or_string(value)\n\n\nclass Feed(BaseFeed):\n    \"\"\"A feed that loads values from CSV formatted files.\n\n    :param dateTimeColumn: The name of the column that has the datetime information.\n    :type dateTimeColumn: string.\n    :param dateTimeFormat: The datetime format. datetime.datetime.strptime will be used to parse the column.\n    :type dateTimeFormat: string.\n    :param converter: A function with two parameters (column name and value) used to convert the string\n        value to something else. The default coverter will try to convert the value to a float. If that fails\n        the original string is returned.\n    :type converter: function.\n    :param delimiter: The string used to separate values.\n    :type delimiter: string.\n    :param timezone: The timezone to use to localize datetimes. Check :mod:`pyalgotrade.marketsession`.\n    :type timezone: A pytz timezone.\n    :param maxLen: The maximum number of values that each :class:`pyalgotrade.dataseries.DataSeries` will hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n    \"\"\"\n\n    def __init__(self, dateTimeColumn, dateTimeFormat, converter=None, delimiter=\",\", timezone=None, maxLen=None):\n        if converter is None:\n            converter = float_or_string\n        self.__rowParser = BasicRowParser(dateTimeColumn, dateTimeFormat, converter, delimiter, timezone)\n\n        super(Feed, self).__init__(self.__rowParser, maxLen)\n\n    def addValuesFromCSV(self, path):\n        \"\"\"Loads values from a file.\n\n        :param path: The path to the CSV file.\n        :type path: string.\n        \"\"\"\n        return super(Feed, self).addValuesFromCSV(path)\n\n    def setDateRange(self, fromDateTime, toDateTime):\n        self.setRowFilter(DateRangeFilter(fromDateTime, toDateTime))\n\n    def setTimeDelta(self, timeDelta):\n        self.__rowParser.setTimeDelta(timeDelta)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import feed\nfrom pyalgotrade import dataseries\n\n\nclass MemFeed(feed.BaseFeed):\n    def __init__(self, maxLen=None):\n        super(MemFeed, self).__init__(maxLen)\n\n        self.__values = []\n        self.__nextIdx = 0\n\n    def reset(self):\n        self.__nextIdx = 0\n        feed.BaseFeed.reset(self)\n\n    def start(self):\n        super(MemFeed, self).start()\n        # Now that all the data is in place, sort it to dispatch it in order.\n        cmpFun = lambda x, y: cmp(x[0], y[0])\n        self.__values.sort(cmpFun)\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def eof(self):\n        if self.__nextIdx < len(self.__values):\n            return False\n        else:\n            return True\n\n    def peekDateTime(self):\n        ret = None\n        if self.__nextIdx < len(self.__values):\n            ret = self.__values[self.__nextIdx][0]\n        return ret\n\n    def createDataSeries(self, key, maxLen):\n        return dataseries.SequenceDataSeries(maxLen)\n\n    def getNextValues(self):\n        ret = (None, None)\n        if self.__nextIdx < len(self.__values):\n            ret = self.__values[self.__nextIdx]\n            self.__nextIdx += 1\n        return ret\n\n    # Add values to the feed. values should be a sequence of tuples. The tuples should have two elements:\n    # 1: datetime.datetime.\n    # 2: dictionary or dict-like object.\n    def addValues(self, values):\n        # Register a dataseries for each item.\n        for key in values[0][1].keys():\n            self.registerDataSeries(key)\n\n        self.__values.extend(values)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\n\nfrom pyalgotrade import dispatchprio\n\n\nclass Event(object):\n    def __init__(self):\n        self.__handlers = []\n        self.__toSubscribe = []\n        self.__toUnsubscribe = []\n        self.__emitting = False\n\n    def __applyChanges(self):\n        if len(self.__toSubscribe):\n            for handler in self.__toSubscribe:\n                if handler not in self.__handlers:\n                    self.__handlers.append(handler)\n            self.__toSubscribe = []\n\n        if len(self.__toUnsubscribe):\n            for handler in self.__toUnsubscribe:\n                self.__handlers.remove(handler)\n            self.__toUnsubscribe = []\n\n    def subscribe(self, handler):\n        if self.__emitting:\n            self.__toSubscribe.append(handler)\n        elif handler not in self.__handlers:\n            self.__handlers.append(handler)\n\n    def unsubscribe(self, handler):\n        if self.__emitting:\n            self.__toUnsubscribe.append(handler)\n        else:\n            self.__handlers.remove(handler)\n\n    def emit(self, *args, **kwargs):\n        try:\n            self.__emitting = True\n            for handler in self.__handlers:\n                handler(*args, **kwargs)\n        finally:\n            self.__emitting = False\n            self.__applyChanges()\n\n\nclass Subject(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self):\n        self.__dispatchPrio = dispatchprio.LAST\n\n    # This may raise.\n    @abc.abstractmethod\n    def start(self):\n        pass\n\n    # This should not raise.\n    @abc.abstractmethod\n    def stop(self):\n        raise NotImplementedError()\n\n    # This should not raise.\n    @abc.abstractmethod\n    def join(self):\n        raise NotImplementedError()\n\n    # Return True if there are not more events to dispatch.\n    @abc.abstractmethod\n    def eof(self):\n        raise NotImplementedError()\n\n    # Dispatch events. If True is returned, it means that at least one event was dispatched.\n    @abc.abstractmethod\n    def dispatch(self):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def peekDateTime(self):\n        # Return the datetime for the next event.\n        # This is needed to properly synchronize non-realtime subjects.\n        # Return None since this is a realtime subject.\n        raise NotImplementedError()\n\n    def getDispatchPriority(self):\n        # Returns a priority used to sort subjects within the dispatch queue.\n        # The return value should never change once this subject is added to the dispatcher.\n        return self.__dispatchPrio\n\n    def setDispatchPriority(self, dispatchPrio):\n        self.__dispatchPrio = dispatchPrio\n\n    def onDispatcherRegistered(self, dispatcher):\n        # Called when the subject is registered with a dispatcher.\n        pass\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport threading\n\n\nclass Parameters(object):\n    def __init__(self, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n\n\nclass ParameterSource(object):\n    \"\"\"\n    Source for backtesting parameters. This class is thread safe.\n    \"\"\"\n    def __init__(self, params):\n        self.__iter = iter(params)\n        self.__lock = threading.Lock()\n\n    def getNext(self, count):\n        \"\"\"\n        Returns the next parameters to use in a backtest.\n        If there are no more parameters to try then an empty list is returned.\n\n        :param count: The max number of parameters to return.\n        :type count: int\n        :rtype: list of Parameters.\n        \"\"\"\n\n        assert count > 0, \"Invalid number of parameters\"\n\n        ret = []\n        with self.__lock:\n            if self.__iter is not None:\n                try:\n                    while count > 0:\n                        params = self.__iter.next()\n                        # Backward compatibility when parameters don't yield Parameters.\n                        if not isinstance(params, Parameters):\n                            params = Parameters(*params)\n                        ret.append(params)\n                        count -= 1\n                except StopIteration:\n                    self.__iter = None\n        return ret\n\n    def eof(self):\n        with self.__lock:\n            return self.__iter is None\n\n\nclass ResultSinc(object):\n    \"\"\"\n    Sinc for backtest results. This class is thread safe.\n    \"\"\"\n    def __init__(self):\n        self.__lock = threading.Lock()\n        self.__bestResult = None\n        self.__bestParameters = None\n\n    def push(self, result, parameters):\n        \"\"\"\n        Push strategy results obtained by running the strategy with the given parameters.\n\n        :param result: The result obtained by running the strategy with the given parameters.\n        :type result: float\n        :param parameters: The parameters that yield the given result.\n        :type parameters: Parameters\n        \"\"\"\n        with self.__lock:\n            if result is not None and (self.__bestResult is None or result > self.__bestResult):\n                self.__bestResult = result\n                self.__bestParameters = parameters\n\n    def getBest(self):\n        with self.__lock:\n            ret = self.__bestResult, self.__bestParameters\n        return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport socket\nimport threading\n\nfrom pyalgotrade.optimizer import base\nfrom pyalgotrade.optimizer import server\nfrom pyalgotrade.optimizer import worker\nfrom pyalgotrade.optimizer import xmlrpcserver\n\nlogger = logging.getLogger(__name__)\n\n\nclass ServerThread(threading.Thread):\n    def __init__(self, server):\n        super(ServerThread, self).__init__()\n        self.__server = server\n\n    def run(self):\n        self.__results = self.__server.serve()\n\n\ndef worker_process(strategyClass, port, logLevel):\n    class Worker(worker.Worker):\n        def runStrategy(self, barFeed, *args, **kwargs):\n            strat = strategyClass(barFeed, *args, **kwargs)\n            strat.run()\n            return strat.getResult()\n\n    # Create a worker and run it.\n    try:\n        name = \"worker-%s\" % (os.getpid())\n        w = Worker(\"localhost\", port, name)\n        w.getLogger().setLevel(logLevel)\n        w.run()\n    except Exception, e:\n        w.getLogger().exception(\"Failed to run worker: %s\" % (e))\n\n\ndef find_port():\n    while True:\n        ret = random.randint(1025, 65536)\n        try:\n            s = socket.socket()\n            s.bind((\"localhost\", ret))\n            s.close()\n            return ret\n        except socket.error:\n            pass\n\n\ndef wait_process(p):\n    timeout = 10\n    p.join(timeout)\n    while p.is_alive():\n        p.join(timeout)\n\n\ndef run(strategyClass, barFeed, strategyParameters, workerCount=None, logLevel=logging.ERROR):\n    \"\"\"Executes many instances of a strategy in parallel and finds the parameters that yield the best results.\n\n    :param strategyClass: The strategy class.\n    :param barFeed: The bar feed to use to backtest the strategy.\n    :type barFeed: :class:`pyalgotrade.barfeed.BarFeed`.\n    :param strategyParameters: The set of parameters to use for backtesting. An iterable object where **each element is\n        a tuple that holds parameter values**.\n    :param workerCount: The number of strategies to run in parallel. If None then as many workers as CPUs are used.\n    :type workerCount: int.\n    :param logLevel: The log level. Defaults to **logging.ERROR**.\n    :rtype: A :class:`Results` instance with the best results found.\n    \"\"\"\n\n    assert(workerCount is None or workerCount > 0)\n    if workerCount is None:\n        workerCount = multiprocessing.cpu_count()\n\n    ret = None\n    workers = []\n    port = find_port()\n    if port is None:\n        raise Exception(\"Failed to find a port to listen\")\n\n    # Build and start the server thread before the worker processes.\n    # We'll manually stop the server once workers have finished.\n    paramSource = base.ParameterSource(strategyParameters)\n    resultSinc = base.ResultSinc()\n    srv = xmlrpcserver.Server(paramSource, resultSinc, barFeed, \"localhost\", port, False)\n    serverThread = ServerThread(srv)\n    serverThread.start()\n\n    try:\n        # Build the worker processes.\n        for i in range(workerCount):\n            workers.append(multiprocessing.Process(\n                target=worker_process,\n                args=(strategyClass, port, logLevel))\n            )\n\n        logger.info(\"Executing workers\")\n\n        # Start workers\n        for process in workers:\n            process.start()\n\n        # Wait workers\n        for process in workers:\n            wait_process(process)\n\n        logger.info(\"All workers finished\")\n    finally:\n        # Stop and wait the server to finish.\n        srv.stop()\n        serverThread.join()\n\n        bestResult, bestParameters = resultSinc.getBest()\n        if bestResult is not None:\n            ret = server.Results(bestParameters.args, bestResult)\n\n    return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport xmlrpclib\nimport pickle\nimport time\nimport socket\nimport random\nimport multiprocessing\n\nimport pyalgotrade.logger\nfrom pyalgotrade import barfeed\n\n\ndef call_function(function, *args, **kwargs):\n    return function(*args, **kwargs)\n\n\ndef call_and_retry_on_network_error(function, retryCount, *args, **kwargs):\n    ret = None\n    while retryCount > 0:\n        retryCount -= 1\n        try:\n            ret = call_function(function, *args, **kwargs)\n            return ret\n        except socket.error:\n            time.sleep(random.randint(1, 3))\n    ret = call_function(function, *args, **kwargs)\n    return ret\n\n\nclass Worker(object):\n    def __init__(self, address, port, workerName=None):\n        url = \"http://%s:%s/PyAlgoTradeRPC\" % (address, port)\n        self.__server = xmlrpclib.ServerProxy(url, allow_none=True)\n        self.__logger = pyalgotrade.logger.getLogger(workerName)\n        if workerName is None:\n            self.__workerName = socket.gethostname()\n        else:\n            self.__workerName = workerName\n\n    def getLogger(self):\n        return self.__logger\n\n    def getInstrumentsAndBars(self):\n        ret = call_and_retry_on_network_error(self.__server.getInstrumentsAndBars, 10)\n        ret = pickle.loads(ret)\n        return ret\n\n    def getBarsFrequency(self):\n        ret = call_and_retry_on_network_error(self.__server.getBarsFrequency, 10)\n        ret = int(ret)\n        return ret\n\n    def getNextJob(self):\n        ret = call_and_retry_on_network_error(self.__server.getNextJob, 10)\n        ret = pickle.loads(ret)\n        return ret\n\n    def pushJobResults(self, jobId, result, parameters):\n        jobId = pickle.dumps(jobId)\n        result = pickle.dumps(result)\n        parameters = pickle.dumps(parameters)\n        workerName = pickle.dumps(self.__workerName)\n        call_and_retry_on_network_error(self.__server.pushJobResults, 10, jobId, result, parameters, workerName)\n\n    def __processJob(self, job, barsFreq, instruments, bars):\n        bestResult = None\n        parameters = job.getNextParameters()\n        bestParams = parameters\n        while parameters is not None:\n            # Wrap the bars into a feed.\n            feed = barfeed.OptimizerBarFeed(barsFreq, instruments, bars)\n            # Run the strategy.\n            self.getLogger().info(\"Running strategy with parameters %s\" % (str(parameters)))\n            result = None\n            try:\n                result = self.runStrategy(feed, *parameters)\n            except Exception, e:\n                self.getLogger().exception(\"Error running strategy with parameters %s: %s\" % (str(parameters), e))\n            self.getLogger().info(\"Result %s\" % result)\n            if bestResult is None or result > bestResult:\n                bestResult = result\n                bestParams = parameters\n            # Run with the next set of parameters.\n            parameters = job.getNextParameters()\n\n        assert(bestParams is not None)\n        self.pushJobResults(job.getId(), bestResult, bestParams)\n\n    # Run the strategy and return the result.\n    def runStrategy(self, feed, parameters):\n        raise Exception(\"Not implemented\")\n\n    def run(self):\n        try:\n            self.getLogger().info(\"Started running\")\n            # Get the instruments and bars.\n            instruments, bars = self.getInstrumentsAndBars()\n            barsFreq = self.getBarsFrequency()\n\n            # Process jobs\n            job = self.getNextJob()\n            while job is not None:\n                self.__processJob(job, barsFreq, instruments, bars)\n                job = self.getNextJob()\n            self.getLogger().info(\"Finished running\")\n        except Exception, e:\n            self.getLogger().exception(\"Finished running with errors: %s\" % (e))\n\n\ndef worker_process(strategyClass, address, port, workerName):\n    class MyWorker(Worker):\n        def runStrategy(self, barFeed, *args, **kwargs):\n            strat = strategyClass(barFeed, *args, **kwargs)\n            strat.run()\n            return strat.getResult()\n\n    # Create a worker and run it.\n    w = MyWorker(address, port, workerName)\n    w.run()\n\n\ndef run(strategyClass, address, port, workerCount=None, workerName=None):\n    \"\"\"Executes one or more worker processes that will run a strategy with the bars and parameters supplied by the server.\n\n    :param strategyClass: The strategy class.\n    :param address: The address of the server.\n    :type address: string.\n    :param port: The port where the server is listening for incoming connections.\n    :type port: int.\n    :param workerCount: The number of worker processes to run. If None then as many workers as CPUs are used.\n    :type workerCount: int.\n    :param workerName: A name for the worker. A name that identifies the worker. If None, the hostname is used.\n    :type workerName: string.\n    \"\"\"\n\n    assert(workerCount is None or workerCount > 0)\n    if workerCount is None:\n        workerCount = multiprocessing.cpu_count()\n\n    workers = []\n    # Build the worker processes.\n    for i in range(workerCount):\n        workers.append(multiprocessing.Process(target=worker_process, args=(strategyClass, address, port, workerName)))\n\n    # Start workers\n    for process in workers:\n        process.start()\n\n    # Wait workers\n    for process in workers:\n        process.join()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport SimpleXMLRPCServer\nimport pickle\nimport threading\nimport time\n\nimport pyalgotrade.logger\nfrom pyalgotrade.optimizer import base\n\nlogger = pyalgotrade.logger.getLogger(__name__)\n\n\nclass AutoStopThread(threading.Thread):\n    def __init__(self, server):\n        super(AutoStopThread, self).__init__()\n        self.__server = server\n\n    def run(self):\n        while self.__server.jobsPending():\n            time.sleep(1)\n        self.__server.stop()\n\n\nclass Job(object):\n    def __init__(self, strategyParameters):\n        self.__strategyParameters = strategyParameters\n        self.__bestResult = None\n        self.__bestParameters = None\n        self.__id = id(self)\n\n    def getId(self):\n        return self.__id\n\n    def getNextParameters(self):\n        ret = None\n        if len(self.__strategyParameters):\n            ret = self.__strategyParameters.pop()\n        return ret\n\n\n# Restrict to a particular path.\nclass RequestHandler(SimpleXMLRPCServer.SimpleXMLRPCRequestHandler):\n    rpc_paths = ('/PyAlgoTradeRPC',)\n\n\nclass Server(SimpleXMLRPCServer.SimpleXMLRPCServer):\n    defaultBatchSize = 200\n\n    def __init__(self, paramSource, resultSinc, barFeed, address, port, autoStop=True):\n        SimpleXMLRPCServer.SimpleXMLRPCServer.__init__(self, (address, port), requestHandler=RequestHandler, logRequests=False, allow_none=True)\n        # super(Server, self).__init__((address, port), requestHandler=RequestHandler, logRequests=False, allow_none=True)\n\n        self.__paramSource = paramSource\n        self.__resultSinc = resultSinc\n        self.__barFeed = barFeed\n        self.__instrumentsAndBars = None  # Pickle'd instruments and bars for faster retrieval.\n        self.__barsFreq = None\n        self.__activeJobs = {}\n        self.__activeJobsLock = threading.Lock()\n        self.__forcedStop = False\n        self.__bestResult = None\n        if autoStop:\n            self.__autoStopThread = AutoStopThread(self)\n        else:\n            self.__autoStopThread = None\n\n        self.register_introspection_functions()\n        self.register_function(self.getInstrumentsAndBars, 'getInstrumentsAndBars')\n        self.register_function(self.getBarsFrequency, 'getBarsFrequency')\n        self.register_function(self.getNextJob, 'getNextJob')\n        self.register_function(self.pushJobResults, 'pushJobResults')\n\n    def getInstrumentsAndBars(self):\n        return self.__instrumentsAndBars\n\n    def getBarsFrequency(self):\n        return str(self.__barsFreq)\n\n    def getNextJob(self):\n        ret = None\n\n        # Get the next set of parameters.\n        params = self.__paramSource.getNext(self.defaultBatchSize)\n        params = map(lambda p: p.args, params)\n\n        # Map the active job\n        if len(params):\n            ret = Job(params)\n            with self.__activeJobsLock:\n                self.__activeJobs[ret.getId()] = ret\n\n        return pickle.dumps(ret)\n\n    def jobsPending(self):\n        if self.__forcedStop:\n            return False\n\n        jobsPending = not self.__paramSource.eof()\n\n        with self.__activeJobsLock:\n            activeJobs = len(self.__activeJobs) > 0\n\n        return jobsPending or activeJobs\n\n    def pushJobResults(self, jobId, result, parameters, workerName):\n        jobId = pickle.loads(jobId)\n        result = pickle.loads(result)\n        parameters = pickle.loads(parameters)\n\n        # Remove the job mapping.\n        with self.__activeJobsLock:\n            try:\n                del self.__activeJobs[jobId]\n            except KeyError:\n                # The job's results were already submitted.\n                return\n\n        if result is None or result > self.__bestResult:\n            logger.info(\"Best result so far %s with parameters %s\" % (result, parameters))\n            self.__bestResult = result\n\n        self.__resultSinc.push(result, base.Parameters(*parameters))\n\n    def stop(self):\n        self.shutdown()\n\n    def serve(self):\n        try:\n            # Initialize instruments, bars and parameters.\n            logger.info(\"Loading bars\")\n            loadedBars = []\n            for dateTime, bars in self.__barFeed:\n                loadedBars.append(bars)\n            instruments = self.__barFeed.getRegisteredInstruments()\n            self.__instrumentsAndBars = pickle.dumps((instruments, loadedBars))\n            self.__barsFreq = self.__barFeed.getFrequency()\n\n            if self.__autoStopThread:\n                self.__autoStopThread.start()\n\n            logger.info(\"Waiting for workers\")\n            self.serve_forever()\n\n            if self.__autoStopThread:\n                self.__autoStopThread.join()\n        finally:\n            self.__forcedStop = True\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport collections\n\nimport broker\nfrom pyalgotrade import warninghelpers\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\n\n\ndef get_last_value(dataSeries):\n    ret = None\n    try:\n        ret = dataSeries[-1]\n    except IndexError:\n        pass\n    return ret\n\n\ndef _filter_datetimes(dateTimes, fromDate=None, toDate=None):\n    class DateTimeFilter(object):\n        def __init__(self, fromDate=None, toDate=None):\n            self.__fromDate = fromDate\n            self.__toDate = toDate\n\n        def includeDateTime(self, dateTime):\n            if self.__toDate and dateTime > self.__toDate:\n                return False\n            if self.__fromDate and dateTime < self.__fromDate:\n                return False\n            return True\n\n    dateTimeFilter = DateTimeFilter(fromDate, toDate)\n    return filter(lambda x: dateTimeFilter.includeDateTime(x), dateTimes)\n\n\ndef _post_plot_fun(subPlot, mplSubplot):\n    # Legend\n    mplSubplot.legend(subPlot.getAllSeries().keys(), shadow=True, loc=\"best\")\n    # Don't scale the Y axis\n    mplSubplot.yaxis.set_major_formatter(ticker.ScalarFormatter(useOffset=False))\n\n\nclass Series(object):\n    def __init__(self):\n        self.__values = {}\n\n    def getColor(self):\n        return None\n\n    def addValue(self, dateTime, value):\n        self.__values[dateTime] = value\n\n    def getValue(self, dateTime):\n        return self.__values.get(dateTime, None)\n\n    def getValues(self):\n        return self.__values\n\n    def getMarker(self):\n        raise NotImplementedError()\n\n    def needColor(self):\n        raise NotImplementedError()\n\n    def plot(self, mplSubplot, dateTimes, color):\n        values = []\n        for dateTime in dateTimes:\n            values.append(self.getValue(dateTime))\n        mplSubplot.plot(dateTimes, values, color=color, marker=self.getMarker())\n\n\nclass BuyMarker(Series):\n    def getColor(self):\n        return 'g'\n\n    def getMarker(self):\n        return \"^\"\n\n    def needColor(self):\n        return True\n\n\nclass SellMarker(Series):\n    def getColor(self):\n        return 'r'\n\n    def getMarker(self):\n        return \"v\"\n\n    def needColor(self):\n        return True\n\n\nclass CustomMarker(Series):\n    def __init__(self):\n        super(CustomMarker, self).__init__()\n        self.__marker = \"o\"\n\n    def needColor(self):\n        return True\n\n    def setMarker(self, marker):\n        self.__marker = marker\n\n    def getMarker(self):\n        return self.__marker\n\n\nclass LineMarker(Series):\n    def __init__(self):\n        super(LineMarker, self).__init__()\n        self.__marker = \" \"\n\n    def needColor(self):\n        return True\n\n    def setMarker(self, marker):\n        self.__marker = marker\n\n    def getMarker(self):\n        return self.__marker\n\n\nclass InstrumentMarker(Series):\n    def __init__(self):\n        super(InstrumentMarker, self).__init__()\n        self.__useAdjClose = None\n        self.__marker = \" \"\n\n    def needColor(self):\n        return True\n\n    def setMarker(self, marker):\n        self.__marker = marker\n\n    def getMarker(self):\n        return self.__marker\n\n    def setUseAdjClose(self, useAdjClose):\n        # Force close/adj_close instead of price.\n        self.__useAdjClose = useAdjClose\n\n    def getValue(self, dateTime):\n        # If not using candlesticks, the return the closing price.\n        ret = Series.getValue(self, dateTime)\n        if ret is not None:\n            if self.__useAdjClose is None:\n                ret = ret.getPrice()\n            elif self.__useAdjClose:\n                ret = ret.getAdjClose()\n            else:\n                ret = ret.getClose()\n        return ret\n\n\nclass HistogramMarker(Series):\n    def needColor(self):\n        return True\n\n    def getColorForValue(self, value, default):\n        return default\n\n    def plot(self, mplSubplot, dateTimes, color):\n        validDateTimes = []\n        values = []\n        colors = []\n        for dateTime in dateTimes:\n            value = self.getValue(dateTime)\n            if value is not None:\n                validDateTimes.append(dateTime)\n                values.append(value)\n                colors.append(self.getColorForValue(value, color))\n        mplSubplot.bar(validDateTimes, values, color=colors)\n\n\nclass MACDMarker(HistogramMarker):\n    def getColorForValue(self, value, default):\n        ret = default\n        if value >= 0:\n            ret = \"g\"\n        else:\n            ret = \"r\"\n        return ret\n\n\nclass Subplot(object):\n    \"\"\" \"\"\"\n    colors = ['b', 'c', 'm', 'y', 'k']\n\n    def __init__(self):\n        self.__series = {}  # Series by name.\n        self.__callbacks = {}  # Maps a function to a Series.\n        self.__nextColor = 0\n\n    def __getColor(self, series):\n        ret = series.getColor()\n        if ret is None:\n            ret = Subplot.colors[self.__nextColor % len(Subplot.colors)]\n            self.__nextColor += 1\n        return ret\n\n    def isEmpty(self):\n        return len(self.__series) == 0\n\n    def getAllSeries(self):\n        return self.__series\n\n    def addDataSeries(self, label, dataSeries, defaultClass=LineMarker):\n        \"\"\"Add a DataSeries to the subplot.\n\n        :param label: A name for the DataSeries values.\n        :type label: string.\n        :param dataSeries: The DataSeries to add.\n        :type dataSeries: :class:`pyalgotrade.dataseries.DataSeries`.\n        \"\"\"\n        callback = lambda bars: get_last_value(dataSeries)\n        self.__callbacks[callback] = self.getSeries(label, defaultClass)\n\n    def addCallback(self, label, callback, defaultClass=LineMarker):\n        \"\"\"Add a callback that will be called on each bar.\n\n        :param label: A name for the series values.\n        :type label: string.\n        :param callback: A function that receives a :class:`pyalgotrade.bar.Bars` instance as a parameter and returns a number or None.\n        \"\"\"\n        self.__callbacks[callback] = self.getSeries(label, defaultClass)\n\n    def addLine(self, label, level):\n        \"\"\"Add a horizontal line to the plot.\n\n        :param label: A label.\n        :type label: string.\n        :param level: The position for the line.\n        :type level: int/float.\n        \"\"\"\n        self.addCallback(label, lambda x: level)\n\n    def onBars(self, bars):\n        dateTime = bars.getDateTime()\n        for cb, series in self.__callbacks.iteritems():\n            series.addValue(dateTime, cb(bars))\n\n    def getSeries(self, name, defaultClass=LineMarker):\n        try:\n            ret = self.__series[name]\n        except KeyError:\n            ret = defaultClass()\n            self.__series[name] = ret\n        return ret\n\n    def getCustomMarksSeries(self, name):\n        return self.getSeries(name, CustomMarker)\n\n    def plot(self, mplSubplot, dateTimes, postPlotFun=_post_plot_fun):\n        for series in self.__series.values():\n            color = None\n            if series.needColor():\n                color = self.__getColor(series)\n            series.plot(mplSubplot, dateTimes, color)\n\n        postPlotFun(self, mplSubplot)\n\n\nclass InstrumentSubplot(Subplot):\n    \"\"\"A Subplot responsible for plotting an instrument.\"\"\"\n    def __init__(self, instrument, plotBuySell):\n        super(InstrumentSubplot, self).__init__()\n        self.__instrument = instrument\n        self.__plotBuySell = plotBuySell\n        self.__instrumentSeries = self.getSeries(instrument, InstrumentMarker)\n\n    def setUseAdjClose(self, useAdjClose):\n        self.__instrumentSeries.setUseAdjClose(useAdjClose)\n\n    def onBars(self, bars):\n        super(InstrumentSubplot, self).onBars(bars)\n        bar = bars.getBar(self.__instrument)\n        if bar:\n            dateTime = bars.getDateTime()\n            self.__instrumentSeries.addValue(dateTime, bar)\n\n    def onOrderEvent(self, broker_, orderEvent):\n        order = orderEvent.getOrder()\n        if self.__plotBuySell and orderEvent.getEventType() in (broker.OrderEvent.Type.PARTIALLY_FILLED, broker.OrderEvent.Type.FILLED) and order.getInstrument() == self.__instrument:\n            action = order.getAction()\n            execInfo = orderEvent.getEventInfo()\n            if action in [broker.Order.Action.BUY, broker.Order.Action.BUY_TO_COVER]:\n                self.getSeries(\"Buy\", BuyMarker).addValue(execInfo.getDateTime(), execInfo.getPrice())\n            elif action in [broker.Order.Action.SELL, broker.Order.Action.SELL_SHORT]:\n                self.getSeries(\"Sell\", SellMarker).addValue(execInfo.getDateTime(), execInfo.getPrice())\n\n\nclass StrategyPlotter(object):\n    \"\"\"Class responsible for plotting a strategy execution.\n\n    :param strat: The strategy to plot.\n    :type strat: :class:`pyalgotrade.strategy.BaseStrategy`.\n    :param plotAllInstruments: Set to True to get a subplot for each instrument available.\n    :type plotAllInstruments: boolean.\n    :param plotBuySell: Set to True to get the buy/sell events plotted for each instrument available.\n    :type plotBuySell: boolean.\n    :param plotPortfolio: Set to True to get the portfolio value (shares + cash) plotted.\n    :type plotPortfolio: boolean.\n    \"\"\"\n\n    def __init__(self, strat, plotAllInstruments=True, plotBuySell=True, plotPortfolio=True):\n        self.__dateTimes = set()\n\n        self.__plotAllInstruments = plotAllInstruments\n        self.__plotBuySell = plotBuySell\n        self.__barSubplots = {}\n        self.__namedSubplots = collections.OrderedDict()\n        self.__portfolioSubplot = None\n        if plotPortfolio:\n            self.__portfolioSubplot = Subplot()\n\n        strat.getBarsProcessedEvent().subscribe(self.__onBarsProcessed)\n        strat.getBroker().getOrderUpdatedEvent().subscribe(self.__onOrderEvent)\n\n    def __checkCreateInstrumentSubplot(self, instrument):\n        if instrument not in self.__barSubplots:\n            self.getInstrumentSubplot(instrument)\n\n    def __onBarsProcessed(self, strat, bars):\n        dateTime = bars.getDateTime()\n        self.__dateTimes.add(dateTime)\n\n        if self.__plotAllInstruments:\n            for instrument in bars.getInstruments():\n                self.__checkCreateInstrumentSubplot(instrument)\n\n        # Notify named subplots.\n        for subplot in self.__namedSubplots.values():\n            subplot.onBars(bars)\n\n        # Notify bar subplots.\n        for subplot in self.__barSubplots.values():\n            subplot.onBars(bars)\n\n        # Feed the portfolio evolution subplot.\n        if self.__portfolioSubplot:\n            self.__portfolioSubplot.getSeries(\"Portfolio\").addValue(dateTime, strat.getBroker().getEquity())\n            # This is in case additional dataseries were added to the portfolio subplot.\n            self.__portfolioSubplot.onBars(bars)\n\n    def __onOrderEvent(self, broker_, orderEvent):\n        # Notify BarSubplots\n        for subplot in self.__barSubplots.values():\n            subplot.onOrderEvent(broker_, orderEvent)\n\n    def getInstrumentSubplot(self, instrument):\n        \"\"\"Returns the InstrumentSubplot for a given instrument\n\n        :rtype: :class:`InstrumentSubplot`.\n        \"\"\"\n        try:\n            ret = self.__barSubplots[instrument]\n        except KeyError:\n            ret = InstrumentSubplot(instrument, self.__plotBuySell)\n            self.__barSubplots[instrument] = ret\n        return ret\n\n    def getOrCreateSubplot(self, name):\n        \"\"\"Returns a Subplot by name. If the subplot doesn't exist, it gets created.\n\n        :param name: The name of the Subplot to get or create.\n        :type name: string.\n        :rtype: :class:`Subplot`.\n        \"\"\"\n        try:\n            ret = self.__namedSubplots[name]\n        except KeyError:\n            ret = Subplot()\n            self.__namedSubplots[name] = ret\n        return ret\n\n    def getPortfolioSubplot(self):\n        \"\"\"Returns the subplot where the portfolio values get plotted.\n\n        :rtype: :class:`Subplot`.\n        \"\"\"\n        return self.__portfolioSubplot\n\n    def __buildFigureImpl(self, fromDateTime=None, toDateTime=None, postPlotFun=_post_plot_fun):\n        dateTimes = _filter_datetimes(self.__dateTimes, fromDateTime, toDateTime)\n        dateTimes.sort()\n\n        subplots = []\n        subplots.extend(self.__barSubplots.values())\n        subplots.extend(self.__namedSubplots.values())\n        if self.__portfolioSubplot is not None:\n            subplots.append(self.__portfolioSubplot)\n\n        # Build each subplot.\n        fig, axes = plt.subplots(nrows=len(subplots), sharex=True, squeeze=False)\n        mplSubplots = []\n        for i, subplot in enumerate(subplots):\n            axesSubplot = axes[i][0]\n            if not subplot.isEmpty():\n                mplSubplots.append(axesSubplot)\n                subplot.plot(axesSubplot, dateTimes, postPlotFun=postPlotFun)\n                axesSubplot.grid(True)\n\n        return (fig, mplSubplots)\n\n    def buildFigure(self, fromDateTime=None, toDateTime=None):\n        # Deprecated in v0.18.\n        warninghelpers.deprecation_warning(\"buildFigure will be deprecated in the next version. Use buildFigureAndSubplots.\", stacklevel=2)\n\n        fig, _ = self.buildFigureAndSubplots(fromDateTime, toDateTime)\n        return fig\n\n    def buildFigureAndSubplots(self, fromDateTime=None, toDateTime=None, postPlotFun=_post_plot_fun):\n        \"\"\"Builds a matplotlib.figure.Figure with the subplots. Must be called after running the strategy.\n\n        :param fromDateTime: An optional starting datetime.datetime. Everything before it won't get plotted.\n        :type fromDateTime: datetime.datetime\n        :param toDateTime: An optional ending datetime.datetime. Everything after it won't get plotted.\n        :type toDateTime: datetime.datetime\n        :rtype: A 2 element tuple with matplotlib.figure.Figure and subplots.\n        \"\"\"\n        fig, mplSubplots = self.__buildFigureImpl(fromDateTime, toDateTime, postPlotFun=postPlotFun)\n        fig.autofmt_xdate()\n        return fig, mplSubplots\n\n    def plot(self, fromDateTime=None, toDateTime=None, postPlotFun=_post_plot_fun):\n        \"\"\"Plots the strategy execution. Must be called after running the strategy.\n\n        :param fromDateTime: An optional starting datetime.datetime. Everything before it won't get plotted.\n        :type fromDateTime: datetime.datetime\n        :param toDateTime: An optional ending datetime.datetime. Everything after it won't get plotted.\n        :type toDateTime: datetime.datetime\n        \"\"\"\n\n        fig, mplSubplots = self.__buildFigureImpl(fromDateTime, toDateTime, postPlotFun=postPlotFun)\n        fig.autofmt_xdate()\n        plt.show()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport abc\nimport datetime\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade import bar\n\n\nclass TimeRange(object):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def belongs(self, dateTime):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getBeginning(self):\n        raise NotImplementedError()\n\n    # 1 past the end\n    @abc.abstractmethod\n    def getEnding(self):\n        raise NotImplementedError()\n\n\nclass IntraDayRange(TimeRange):\n    def __init__(self, dateTime, frequency):\n        super(IntraDayRange, self).__init__()\n        assert isinstance(frequency, int)\n        assert frequency > 1\n        assert frequency < bar.Frequency.DAY\n\n        ts = int(dt.datetime_to_timestamp(dateTime))\n        slot = int(ts / frequency)\n        slotTs = slot * frequency\n        self.__begin = dt.timestamp_to_datetime(slotTs, not dt.datetime_is_naive(dateTime))\n        if not dt.datetime_is_naive(dateTime):\n            self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n        self.__end = self.__begin + datetime.timedelta(seconds=frequency)\n\n    def belongs(self, dateTime):\n        return dateTime >= self.__begin and dateTime < self.__end\n\n    def getBeginning(self):\n        return self.__begin\n\n    def getEnding(self):\n        return self.__end\n\n\nclass DayRange(TimeRange):\n    def __init__(self, dateTime):\n        super(DayRange, self).__init__()\n        self.__begin = datetime.datetime(dateTime.year, dateTime.month, dateTime.day)\n        if not dt.datetime_is_naive(dateTime):\n            self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n        self.__end = self.__begin + datetime.timedelta(days=1)\n\n    def belongs(self, dateTime):\n        return dateTime >= self.__begin and dateTime < self.__end\n\n    def getBeginning(self):\n        return self.__begin\n\n    def getEnding(self):\n        return self.__end\n\n\nclass MonthRange(TimeRange):\n    def __init__(self, dateTime):\n        super(MonthRange, self).__init__()\n        self.__begin = datetime.datetime(dateTime.year, dateTime.month, 1)\n\n        # Calculate the ending date.\n        if dateTime.month == 12:\n            self.__end = datetime.datetime(dateTime.year + 1, 1, 1)\n        else:\n            self.__end = datetime.datetime(dateTime.year, dateTime.month + 1, 1)\n\n        if not dt.datetime_is_naive(dateTime):\n            self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n            self.__end = dt.localize(self.__end, dateTime.tzinfo)\n\n    def belongs(self, dateTime):\n        return dateTime >= self.__begin and dateTime < self.__end\n\n    def getBeginning(self):\n        return self.__begin\n\n    def getEnding(self):\n        return self.__end\n\n\ndef is_valid_frequency(frequency):\n    assert(isinstance(frequency, int))\n    assert(frequency > 1)\n\n    if frequency < bar.Frequency.DAY:\n        ret = True\n    elif frequency == bar.Frequency.DAY:\n        ret = True\n    elif frequency == bar.Frequency.MONTH:\n        ret = True\n    else:\n        ret = False\n    return ret\n\n\ndef build_range(dateTime, frequency):\n    assert(isinstance(frequency, int))\n    assert(frequency > 1)\n\n    if frequency < bar.Frequency.DAY:\n        ret = IntraDayRange(dateTime, frequency)\n    elif frequency == bar.Frequency.DAY:\n        ret = DayRange(dateTime)\n    elif frequency == bar.Frequency.MONTH:\n        ret = MonthRange(dateTime)\n    else:\n        raise Exception(\"Unsupported frequency\")\n    return ret\n\n\nclass Grouper(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, groupDateTime):\n        self.__groupDateTime = groupDateTime\n\n    def getDateTime(self):\n        return self.__groupDateTime\n\n    @abc.abstractmethod\n    def addValue(self, value):\n        \"\"\"Add a value to the group.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def getGrouped(self):\n        \"\"\"Return the grouped value.\"\"\"\n        raise NotImplementedError()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport abc\nimport logging\n\nimport pyalgotrade.broker\nfrom pyalgotrade.broker import backtesting\nfrom pyalgotrade import observer\nfrom pyalgotrade import dispatcher\nimport pyalgotrade.strategy.position\nfrom pyalgotrade import logger\nfrom pyalgotrade.barfeed import resampled\n\n\nclass BaseStrategy(object):\n    \"\"\"Base class for strategies.\n\n    :param barFeed: The bar feed that will supply the bars.\n    :type barFeed: :class:`pyalgotrade.barfeed.BaseBarFeed`.\n    :param broker: The broker that will handle orders.\n    :type broker: :class:`pyalgotrade.broker.Broker`.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    LOGGER_NAME = \"strategy\"\n\n    def __init__(self, barFeed, broker):\n        self.__barFeed = barFeed\n        self.__broker = broker\n        self.__activePositions = set()\n        self.__orderToPosition = {}\n        self.__barsProcessedEvent = observer.Event()\n        self.__analyzers = []\n        self.__namedAnalyzers = {}\n        self.__resampledBarFeeds = []\n        self.__dispatcher = dispatcher.Dispatcher()\n        self.__broker.getOrderUpdatedEvent().subscribe(self.__onOrderEvent)\n        self.__barFeed.getNewValuesEvent().subscribe(self.__onBars)\n\n        self.__dispatcher.getStartEvent().subscribe(self.onStart)\n        self.__dispatcher.getIdleEvent().subscribe(self.__onIdle)\n\n        # It is important to dispatch broker events before feed events, specially if we're backtesting.\n        self.__dispatcher.addSubject(self.__broker)\n        self.__dispatcher.addSubject(self.__barFeed)\n\n        # Initialize logging.\n        self.__logger = logger.getLogger(BaseStrategy.LOGGER_NAME)\n\n    # Only valid for testing purposes.\n    def _setBroker(self, broker):\n        self.__broker = broker\n\n    def setUseEventDateTimeInLogs(self, useEventDateTime):\n        if useEventDateTime:\n            logger.Formatter.DATETIME_HOOK = self.getDispatcher().getCurrentDateTime\n        else:\n            logger.Formatter.DATETIME_HOOK = None\n\n    def getLogger(self):\n        return self.__logger\n\n    def getActivePositions(self):\n        return self.__activePositions\n\n    def getOrderToPosition(self):\n        return self.__orderToPosition\n\n    def getDispatcher(self):\n        return self.__dispatcher\n\n    def getResult(self):\n        return self.getBroker().getEquity()\n\n    def getBarsProcessedEvent(self):\n        return self.__barsProcessedEvent\n\n    def getUseAdjustedValues(self):\n        return False\n\n    def registerPositionOrder(self, position, order):\n        self.__activePositions.add(position)\n        assert(order.isActive())  # Why register an inactive order ?\n        self.__orderToPosition[order.getId()] = position\n\n    def unregisterPositionOrder(self, position, order):\n        del self.__orderToPosition[order.getId()]\n\n    def unregisterPosition(self, position):\n        assert(not position.isOpen())\n        self.__activePositions.remove(position)\n\n    def __notifyAnalyzers(self, lambdaExpression):\n        for s in self.__analyzers:\n            lambdaExpression(s)\n\n    def attachAnalyzerEx(self, strategyAnalyzer, name=None):\n        if strategyAnalyzer not in self.__analyzers:\n            if name is not None:\n                if name in self.__namedAnalyzers:\n                    raise Exception(\"A different analyzer named '%s' was already attached\" % name)\n                self.__namedAnalyzers[name] = strategyAnalyzer\n\n            strategyAnalyzer.beforeAttach(self)\n            self.__analyzers.append(strategyAnalyzer)\n            strategyAnalyzer.attached(self)\n\n    def getLastPrice(self, instrument):\n        ret = None\n        bar = self.getFeed().getLastBar(instrument)\n        if bar is not None:\n            ret = bar.getPrice()\n        return ret\n\n    def getFeed(self):\n        \"\"\"Returns the :class:`pyalgotrade.barfeed.BaseBarFeed` that this strategy is using.\"\"\"\n        return self.__barFeed\n\n    def getBroker(self):\n        \"\"\"Returns the :class:`pyalgotrade.broker.Broker` used to handle order executions.\"\"\"\n        return self.__broker\n\n    def getCurrentDateTime(self):\n        \"\"\"Returns the :class:`datetime.datetime` for the current :class:`pyalgotrade.bar.Bars`.\"\"\"\n        return self.__barFeed.getCurrentDateTime()\n\n    def marketOrder(self, instrument, quantity, onClose=False, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Submits a market order.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param quantity: The amount of shares. Positive means buy, negative means sell.\n        :type quantity: int/float.\n        :param onClose: True if the order should be filled as close to the closing price as possible (Market-On-Close order). Default is False.\n        :type onClose: boolean.\n        :param goodTillCanceled: True if the order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the order should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.broker.MarketOrder` submitted.\n        \"\"\"\n\n        ret = None\n        if quantity > 0:\n            ret = self.getBroker().createMarketOrder(pyalgotrade.broker.Order.Action.BUY, instrument, quantity, onClose)\n        elif quantity < 0:\n            ret = self.getBroker().createMarketOrder(pyalgotrade.broker.Order.Action.SELL, instrument, quantity*-1, onClose)\n        if ret:\n            ret.setGoodTillCanceled(goodTillCanceled)\n            ret.setAllOrNone(allOrNone)\n            self.getBroker().submitOrder(ret)\n        return ret\n\n    def limitOrder(self, instrument, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Submits a limit order.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: The amount of shares. Positive means buy, negative means sell.\n        :type quantity: int/float.\n        :param goodTillCanceled: True if the order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the order should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.broker.LimitOrder` submitted.\n        \"\"\"\n\n        ret = None\n        if quantity > 0:\n            ret = self.getBroker().createLimitOrder(pyalgotrade.broker.Order.Action.BUY, instrument, limitPrice, quantity)\n        elif quantity < 0:\n            ret = self.getBroker().createLimitOrder(pyalgotrade.broker.Order.Action.SELL, instrument, limitPrice, quantity*-1)\n        if ret:\n            ret.setGoodTillCanceled(goodTillCanceled)\n            ret.setAllOrNone(allOrNone)\n            self.getBroker().submitOrder(ret)\n        return ret\n\n    def stopOrder(self, instrument, stopPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Submits a stop order.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: Stop price.\n        :type stopPrice: float.\n        :param quantity: The amount of shares. Positive means buy, negative means sell.\n        :type quantity: int/float.\n        :param goodTillCanceled: True if the order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the order should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.broker.StopOrder` submitted.\n        \"\"\"\n\n        ret = None\n        if quantity > 0:\n            ret = self.getBroker().createStopOrder(pyalgotrade.broker.Order.Action.BUY, instrument, stopPrice, quantity)\n        elif quantity < 0:\n            ret = self.getBroker().createStopOrder(pyalgotrade.broker.Order.Action.SELL, instrument, stopPrice, quantity*-1)\n        if ret:\n            ret.setGoodTillCanceled(goodTillCanceled)\n            ret.setAllOrNone(allOrNone)\n            self.getBroker().submitOrder(ret)\n        return ret\n\n    def stopLimitOrder(self, instrument, stopPrice, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Submits a stop limit order.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: Stop price.\n        :type stopPrice: float.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: The amount of shares. Positive means buy, negative means sell.\n        :type quantity: int/float.\n        :param goodTillCanceled: True if the order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the order should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.broker.StopLimitOrder` submitted.\n        \"\"\"\n\n        ret = None\n        if quantity > 0:\n            ret = self.getBroker().createStopLimitOrder(pyalgotrade.broker.Order.Action.BUY, instrument, stopPrice, limitPrice, quantity)\n        elif quantity < 0:\n            ret = self.getBroker().createStopLimitOrder(pyalgotrade.broker.Order.Action.SELL, instrument, stopPrice, limitPrice, quantity*-1)\n        if ret:\n            ret.setGoodTillCanceled(goodTillCanceled)\n            ret.setAllOrNone(allOrNone)\n            self.getBroker().submitOrder(ret)\n        return ret\n\n    def enterLong(self, instrument, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a buy :class:`pyalgotrade.broker.MarketOrder` to enter a long position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.LongPosition(self, instrument, None, None, quantity, goodTillCanceled, allOrNone)\n\n    def enterShort(self, instrument, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a sell short :class:`pyalgotrade.broker.MarketOrder` to enter a short position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.ShortPosition(self, instrument, None, None, quantity, goodTillCanceled, allOrNone)\n\n    def enterLongLimit(self, instrument, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a buy :class:`pyalgotrade.broker.LimitOrder` to enter a long position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.LongPosition(self, instrument, None, limitPrice, quantity, goodTillCanceled, allOrNone)\n\n    def enterShortLimit(self, instrument, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a sell short :class:`pyalgotrade.broker.LimitOrder` to enter a short position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.ShortPosition(self, instrument, None, limitPrice, quantity, goodTillCanceled, allOrNone)\n\n    def enterLongStop(self, instrument, stopPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a buy :class:`pyalgotrade.broker.StopOrder` to enter a long position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: Stop price.\n        :type stopPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.LongPosition(self, instrument, stopPrice, None, quantity, goodTillCanceled, allOrNone)\n\n    def enterShortStop(self, instrument, stopPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a sell short :class:`pyalgotrade.broker.StopOrder` to enter a short position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: Stop price.\n        :type stopPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.ShortPosition(self, instrument, stopPrice, None, quantity, goodTillCanceled, allOrNone)\n\n    def enterLongStopLimit(self, instrument, stopPrice, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a buy :class:`pyalgotrade.broker.StopLimitOrder` order to enter a long position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: Stop price.\n        :type stopPrice: float.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.LongPosition(self, instrument, stopPrice, limitPrice, quantity, goodTillCanceled, allOrNone)\n\n    def enterShortStopLimit(self, instrument, stopPrice, limitPrice, quantity, goodTillCanceled=False, allOrNone=False):\n        \"\"\"Generates a sell short :class:`pyalgotrade.broker.StopLimitOrder` order to enter a short position.\n\n        :param instrument: Instrument identifier.\n        :type instrument: string.\n        :param stopPrice: The Stop price.\n        :type stopPrice: float.\n        :param limitPrice: Limit price.\n        :type limitPrice: float.\n        :param quantity: Entry order quantity.\n        :type quantity: int.\n        :param goodTillCanceled: True if the entry order is good till canceled. If False then the order gets automatically canceled when the session closes.\n        :type goodTillCanceled: boolean.\n        :param allOrNone: True if the orders should be completely filled or not at all.\n        :type allOrNone: boolean.\n        :rtype: The :class:`pyalgotrade.strategy.position.Position` entered.\n        \"\"\"\n\n        return pyalgotrade.strategy.position.ShortPosition(self, instrument, stopPrice, limitPrice, quantity, goodTillCanceled, allOrNone)\n\n    def onEnterOk(self, position):\n        \"\"\"Override (optional) to get notified when the order submitted to enter a position was filled. The default implementation is empty.\n\n        :param position: A position returned by any of the enterLongXXX or enterShortXXX methods.\n        :type position: :class:`pyalgotrade.strategy.position.Position`.\n        \"\"\"\n        pass\n\n    def onEnterCanceled(self, position):\n        \"\"\"Override (optional) to get notified when the order submitted to enter a position was canceled. The default implementation is empty.\n\n        :param position: A position returned by any of the enterLongXXX or enterShortXXX methods.\n        :type position: :class:`pyalgotrade.strategy.position.Position`.\n        \"\"\"\n        pass\n\n    # Called when the exit order for a position was filled.\n    def onExitOk(self, position):\n        \"\"\"Override (optional) to get notified when the order submitted to exit a position was filled. The default implementation is empty.\n\n        :param position: A position returned by any of the enterLongXXX or enterShortXXX methods.\n        :type position: :class:`pyalgotrade.strategy.position.Position`.\n        \"\"\"\n        pass\n\n    # Called when the exit order for a position was canceled.\n    def onExitCanceled(self, position):\n        \"\"\"Override (optional) to get notified when the order submitted to exit a position was canceled. The default implementation is empty.\n\n        :param position: A position returned by any of the enterLongXXX or enterShortXXX methods.\n        :type position: :class:`pyalgotrade.strategy.position.Position`.\n        \"\"\"\n        pass\n\n    \"\"\"Base class for strategies. \"\"\"\n    def onStart(self):\n        \"\"\"Override (optional) to get notified when the strategy starts executing. The default implementation is empty. \"\"\"\n        pass\n\n    def onFinish(self, bars):\n        \"\"\"Override (optional) to get notified when the strategy finished executing. The default implementation is empty.\n\n        :param bars: The last bars processed.\n        :type bars: :class:`pyalgotrade.bar.Bars`.\n        \"\"\"\n        pass\n\n    def onIdle(self):\n        \"\"\"Override (optional) to get notified when there are no events.\n\n       .. note::\n            In a pure backtesting scenario this will not be called.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def onBars(self, bars):\n        \"\"\"Override (**mandatory**) to get notified when new bars are available. The default implementation raises an Exception.\n\n        **This is the method to override to enter your trading logic and enter/exit positions**.\n\n        :param bars: The current bars.\n        :type bars: :class:`pyalgotrade.bar.Bars`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def onOrderUpdated(self, order):\n        \"\"\"Override (optional) to get notified when an order gets updated.\n\n        :param order: The order updated.\n        :type order: :class:`pyalgotrade.broker.Order`.\n        \"\"\"\n        pass\n\n    def __onIdle(self):\n        # Force a resample check to avoid depending solely on the underlying\n        # barfeed events.\n        for resampledBarFeed in self.__resampledBarFeeds:\n            resampledBarFeed.checkNow(self.getCurrentDateTime())\n\n        self.onIdle()\n\n    def __onOrderEvent(self, broker_, orderEvent):\n        order = orderEvent.getOrder()\n        self.onOrderUpdated(order)\n\n        # Notify the position about the order event.\n        pos = self.__orderToPosition.get(order.getId(), None)\n        if pos is not None:\n            # Unlink the order from the position if its not active anymore.\n            if not order.isActive():\n                self.unregisterPositionOrder(pos, order)\n\n            pos.onOrderEvent(orderEvent)\n\n    def __onBars(self, dateTime, bars):\n        # THE ORDER HERE IS VERY IMPORTANT\n\n        # 1: Let analyzers process bars.\n        self.__notifyAnalyzers(lambda s: s.beforeOnBars(self, bars))\n\n        # 2: Let the strategy process current bars and submit orders.\n        self.onBars(bars)\n\n        # 3: Notify that the bars were processed.\n        self.__barsProcessedEvent.emit(self, bars)\n\n    def run(self):\n        \"\"\"Call once (**and only once**) to run the strategy.\"\"\"\n        self.__dispatcher.run()\n\n        if self.__barFeed.getCurrentBars() is not None:\n            self.onFinish(self.__barFeed.getCurrentBars())\n        else:\n            raise Exception(\"Feed was empty\")\n\n    def stop(self):\n        \"\"\"Stops a running strategy.\"\"\"\n        self.__dispatcher.stop()\n\n    def attachAnalyzer(self, strategyAnalyzer):\n        \"\"\"Adds a :class:`pyalgotrade.stratanalyzer.StrategyAnalyzer`.\"\"\"\n        self.attachAnalyzerEx(strategyAnalyzer)\n\n    def getNamedAnalyzer(self, name):\n        return self.__namedAnalyzers.get(name, None)\n\n    def debug(self, msg):\n        \"\"\"Logs a message with level DEBUG on the strategy logger.\"\"\"\n        self.getLogger().debug(msg)\n\n    def info(self, msg):\n        \"\"\"Logs a message with level INFO on the strategy logger.\"\"\"\n        self.getLogger().info(msg)\n\n    def warning(self, msg):\n        \"\"\"Logs a message with level WARNING on the strategy logger.\"\"\"\n        self.getLogger().warning(msg)\n\n    def error(self, msg):\n        \"\"\"Logs a message with level ERROR on the strategy logger.\"\"\"\n        self.getLogger().error(msg)\n\n    def critical(self, msg):\n        \"\"\"Logs a message with level CRITICAL on the strategy logger.\"\"\"\n        self.getLogger().critical(msg)\n\n    def resampleBarFeed(self, frequency, callback):\n        \"\"\"\n        Builds a resampled barfeed that groups bars by a certain frequency.\n\n        :param frequency: The grouping frequency in seconds. Must be > 0.\n        :param callback: A function similar to onBars that will be called when new bars are available.\n        :rtype: :class:`pyalgotrade.barfeed.BaseBarFeed`.\n        \"\"\"\n        ret = resampled.ResampledBarFeed(self.getFeed(), frequency)\n        ret.getNewValuesEvent().subscribe(callback)\n        self.getDispatcher().addSubject(ret)\n        self.__resampledBarFeeds.append(ret)\n        return ret\n\n\nclass BacktestingStrategy(BaseStrategy):\n    \"\"\"Base class for backtesting strategies.\n\n    :param barFeed: The bar feed to use to backtest the strategy.\n    :type barFeed: :class:`pyalgotrade.barfeed.BaseBarFeed`.\n    :param cash_or_brk: The starting capital or a broker instance.\n    :type cash_or_brk: int/float or :class:`pyalgotrade.broker.Broker`.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, barFeed, cash_or_brk=1000000):\n        # The broker should subscribe to barFeed events before the strategy.\n        # This is to avoid executing orders submitted in the current tick.\n\n        if isinstance(cash_or_brk, pyalgotrade.broker.Broker):\n            broker = cash_or_brk\n        else:\n            broker = backtesting.Broker(cash_or_brk, barFeed)\n\n        BaseStrategy.__init__(self, barFeed, broker)\n        self.__useAdjustedValues = False\n        self.setUseEventDateTimeInLogs(True)\n        self.setDebugMode(True)\n\n    def getUseAdjustedValues(self):\n        return self.__useAdjustedValues\n\n    def setUseAdjustedValues(self, useAdjusted):\n        self.getFeed().setUseAdjustedValues(useAdjusted)\n        self.getBroker().setUseAdjustedValues(useAdjusted)\n        self.__useAdjustedValues = useAdjusted\n\n    def setDebugMode(self, debugOn):\n        \"\"\"Enable/disable debug level messages in the strategy and backtesting broker.\n        This is enabled by default.\"\"\"\n        level = logging.DEBUG if debugOn else logging.INFO\n        self.getLogger().setLevel(level)\n        self.getBroker().getLogger().setLevel(level)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade.stratanalyzer import returns\nfrom pyalgotrade import warninghelpers\nfrom pyalgotrade import broker\n\nimport datetime\n\n\nclass PositionState(object):\n    def onEnter(self, position):\n        pass\n\n    # Raise an exception if an order can't be submitted in the current state.\n    def canSubmitOrder(self, position, order):\n        raise NotImplementedError()\n\n    def onOrderEvent(self, position, orderEvent):\n        raise NotImplementedError()\n\n    def isOpen(self, position):\n        raise NotImplementedError()\n\n    def exit(self, position, stopPrice=None, limitPrice=None, goodTillCanceled=None):\n        raise NotImplementedError()\n\n\nclass WaitingEntryState(PositionState):\n    def canSubmitOrder(self, position, order):\n        if position.entryActive():\n            raise Exception(\"The entry order is still active\")\n\n    def onOrderEvent(self, position, orderEvent):\n        # Only entry order events are valid in this state.\n        assert(position.getEntryOrder().getId() == orderEvent.getOrder().getId())\n\n        if orderEvent.getEventType() in (broker.OrderEvent.Type.FILLED, broker.OrderEvent.Type.PARTIALLY_FILLED):\n            position.switchState(OpenState())\n            position.getStrategy().onEnterOk(position)\n        elif orderEvent.getEventType() == broker.OrderEvent.Type.CANCELED:\n            assert(position.getEntryOrder().getFilled() == 0)\n            position.switchState(ClosedState())\n            position.getStrategy().onEnterCanceled(position)\n\n    def isOpen(self, position):\n        return True\n\n    def exit(self, position, stopPrice=None, limitPrice=None, goodTillCanceled=None):\n        assert(position.getShares() == 0)\n        assert(position.getEntryOrder().isActive())\n        position.getStrategy().getBroker().cancelOrder(position.getEntryOrder())\n\n\nclass OpenState(PositionState):\n    def onEnter(self, position):\n        entryDateTime = position.getEntryOrder().getExecutionInfo().getDateTime()\n        position.setEntryDateTime(entryDateTime)\n\n    def canSubmitOrder(self, position, order):\n        # Only exit orders should be submitted in this state.\n        pass\n\n    def onOrderEvent(self, position, orderEvent):\n        if position.getExitOrder() and position.getExitOrder().getId() == orderEvent.getOrder().getId():\n            if orderEvent.getEventType() == broker.OrderEvent.Type.FILLED:\n                if position.getShares() == 0:\n                    position.switchState(ClosedState())\n                    position.getStrategy().onExitOk(position)\n            elif orderEvent.getEventType() == broker.OrderEvent.Type.CANCELED:\n                assert(position.getShares() != 0)\n                position.getStrategy().onExitCanceled(position)\n        elif position.getEntryOrder().getId() == orderEvent.getOrder().getId():\n            # Nothing to do since the entry order may be completely filled or canceled after a partial fill.\n            assert(position.getShares() != 0)\n        else:\n            raise Exception(\"Invalid order event '%s' in OpenState\" % (orderEvent.getEventType()))\n\n    def isOpen(self, position):\n        return True\n\n    def exit(self, position, stopPrice=None, limitPrice=None, goodTillCanceled=None):\n        assert(position.getShares() != 0)\n\n        # Fail if a previous exit order is active.\n        if position.exitActive():\n            raise Exception(\"Exit order is active and it should be canceled first\")\n\n        # If the entry order is active, request cancellation.\n        if position.entryActive():\n            position.getStrategy().getBroker().cancelOrder(position.getEntryOrder())\n\n        position._submitExitOrder(stopPrice, limitPrice, goodTillCanceled)\n\n\nclass ClosedState(PositionState):\n    def onEnter(self, position):\n        # Set the exit datetime if the exit order was filled.\n        if position.exitFilled():\n            exitDateTime = position.getExitOrder().getExecutionInfo().getDateTime()\n            position.setExitDateTime(exitDateTime)\n\n        assert(position.getShares() == 0)\n        position.getStrategy().unregisterPosition(position)\n\n    def canSubmitOrder(self, position, order):\n        raise Exception(\"The position is closed\")\n\n    def onOrderEvent(self, position, orderEvent):\n        raise Exception(\"Invalid order event '%s' in ClosedState\" % (orderEvent.getEventType()))\n\n    def isOpen(self, position):\n        return False\n\n    def exit(self, position, stopPrice=None, limitPrice=None, goodTillCanceled=None):\n        pass\n\n\nclass Position(object):\n    \"\"\"Base class for positions.\n\n    Positions are higher level abstractions for placing orders.\n    They are escentially a pair of entry-exit orders and allow\n    to track returns and PnL easier that placing orders manually.\n\n    :param strategy: The strategy that this position belongs to.\n    :type strategy: :class:`pyalgotrade.strategy.BaseStrategy`.\n    :param entryOrder: The order used to enter the position.\n    :type entryOrder: :class:`pyalgotrade.broker.Order`\n    :param goodTillCanceled: True if the entry order should be set as good till canceled.\n    :type goodTillCanceled: boolean.\n    :param allOrNone: True if the orders should be completely filled or not at all.\n    :type allOrNone: boolean.\n\n    .. note::\n        This is a base class and should not be used directly.\n    \"\"\"\n\n    def __init__(self, strategy, entryOrder, goodTillCanceled, allOrNone):\n        # The order must be created but not submitted.\n        assert(entryOrder.isInitial())\n\n        self.__state = None\n        self.__activeOrders = {}\n        self.__shares = 0\n        self.__strategy = strategy\n        self.__entryOrder = None\n        self.__entryDateTime = None\n        self.__exitOrder = None\n        self.__exitDateTime = None\n        self.__posTracker = returns.PositionTracker(entryOrder.getInstrumentTraits())\n        self.__allOrNone = allOrNone\n\n        self.switchState(WaitingEntryState())\n\n        entryOrder.setGoodTillCanceled(goodTillCanceled)\n        entryOrder.setAllOrNone(allOrNone)\n        self.__submitAndRegisterOrder(entryOrder)\n        self.__entryOrder = entryOrder\n\n    def __submitAndRegisterOrder(self, order):\n        assert(order.isInitial())\n\n        # Check if an order can be submitted in the current state.\n        self.__state.canSubmitOrder(self, order)\n\n        # This may raise an exception, so we wan't to submit the order before moving forward and registering\n        # the order in the strategy.\n        self.getStrategy().getBroker().submitOrder(order)\n\n        self.__activeOrders[order.getId()] = order\n        self.getStrategy().registerPositionOrder(self, order)\n\n    def setEntryDateTime(self, dateTime):\n        self.__entryDateTime = dateTime\n\n    def setExitDateTime(self, dateTime):\n        self.__exitDateTime = dateTime\n\n    def switchState(self, newState):\n        self.__state = newState\n        self.__state.onEnter(self)\n\n    def getStrategy(self):\n        return self.__strategy\n\n    def getLastPrice(self):\n        return self.__strategy.getLastPrice(self.getInstrument())\n\n    def getActiveOrders(self):\n        return self.__activeOrders.values()\n\n    def getShares(self):\n        \"\"\"Returns the number of shares.\n        This will be a possitive number for a long position, and a negative number for a short position.\n\n        .. note::\n            If the entry order was not filled, or if the position is closed, then the number of shares will be 0.\n        \"\"\"\n        return self.__shares\n\n    def entryActive(self):\n        \"\"\"Returns True if the entry order is active.\"\"\"\n        return self.__entryOrder is not None and self.__entryOrder.isActive()\n\n    def entryFilled(self):\n        \"\"\"Returns True if the entry order was filled.\"\"\"\n        return self.__entryOrder is not None and self.__entryOrder.isFilled()\n\n    def exitActive(self):\n        \"\"\"Returns True if the exit order is active.\"\"\"\n        return self.__exitOrder is not None and self.__exitOrder.isActive()\n\n    def exitFilled(self):\n        \"\"\"Returns True if the exit order was filled.\"\"\"\n        return self.__exitOrder is not None and self.__exitOrder.isFilled()\n\n    def getEntryOrder(self):\n        \"\"\"Returns the :class:`pyalgotrade.broker.Order` used to enter the position.\"\"\"\n        return self.__entryOrder\n\n    def getExitOrder(self):\n        \"\"\"Returns the :class:`pyalgotrade.broker.Order` used to exit the position. If this position hasn't been closed yet, None is returned.\"\"\"\n        return self.__exitOrder\n\n    def getInstrument(self):\n        \"\"\"Returns the instrument used for this position.\"\"\"\n        return self.__entryOrder.getInstrument()\n\n    def getReturn(self, includeCommissions=True):\n        \"\"\"\n        Calculates cumulative percentage returns up to this point.\n        If the position is not closed, these will be unrealized returns.\n        \"\"\"\n\n        # Deprecated in v0.18.\n        if includeCommissions is False:\n            warninghelpers.deprecation_warning(\"includeCommissions will be deprecated in the next version.\", stacklevel=2)\n\n        ret = 0\n        price = self.getLastPrice()\n        if price is not None:\n            ret = self.__posTracker.getReturn(price, includeCommissions)\n        return ret\n\n    def getPnL(self, includeCommissions=True):\n        \"\"\"\n        Calculates PnL up to this point.\n        If the position is not closed, these will be unrealized PnL.\n        \"\"\"\n\n        # Deprecated in v0.18.\n        if includeCommissions is False:\n            warninghelpers.deprecation_warning(\"includeCommissions will be deprecated in the next version.\", stacklevel=2)\n\n        ret = 0\n        price = self.getLastPrice()\n        if price is not None:\n            ret = self.__posTracker.getPnL(price=price, includeCommissions=includeCommissions)\n        return ret\n\n    def cancelEntry(self):\n        \"\"\"Cancels the entry order if its active.\"\"\"\n        if self.entryActive():\n            self.getStrategy().getBroker().cancelOrder(self.getEntryOrder())\n\n    def cancelExit(self):\n        \"\"\"Cancels the exit order if its active.\"\"\"\n        if self.exitActive():\n            self.getStrategy().getBroker().cancelOrder(self.getExitOrder())\n\n    def exitMarket(self, goodTillCanceled=None):\n        \"\"\"Submits a market order to close this position.\n\n        :param goodTillCanceled: True if the exit order is good till canceled. If False then the order gets automatically canceled when the session closes. If None, then it will match the entry order.\n        :type goodTillCanceled: boolean.\n\n        .. note::\n            * If the position is closed (entry canceled or exit filled) this won't have any effect.\n            * If the exit order for this position is pending, an exception will be raised. The exit order should be canceled first.\n            * If the entry order is active, cancellation will be requested.\n        \"\"\"\n\n        self.__state.exit(self, None, None, goodTillCanceled)\n\n    def exitLimit(self, limitPrice, goodTillCanceled=None):\n        \"\"\"Submits a limit order to close this position.\n\n        :param limitPrice: The limit price.\n        :type limitPrice: float.\n        :param goodTillCanceled: True if the exit order is good till canceled. If False then the order gets automatically canceled when the session closes. If None, then it will match the entry order.\n        :type goodTillCanceled: boolean.\n\n        .. note::\n            * If the position is closed (entry canceled or exit filled) this won't have any effect.\n            * If the exit order for this position is pending, an exception will be raised. The exit order should be canceled first.\n            * If the entry order is active, cancellation will be requested.\n        \"\"\"\n\n        self.__state.exit(self, None, limitPrice, goodTillCanceled)\n\n    def exitStop(self, stopPrice, goodTillCanceled=None):\n        \"\"\"Submits a stop order to close this position.\n\n        :param stopPrice: The stop price.\n        :type stopPrice: float.\n        :param goodTillCanceled: True if the exit order is good till canceled. If False then the order gets automatically canceled when the session closes. If None, then it will match the entry order.\n        :type goodTillCanceled: boolean.\n\n        .. note::\n            * If the position is closed (entry canceled or exit filled) this won't have any effect.\n            * If the exit order for this position is pending, an exception will be raised. The exit order should be canceled first.\n            * If the entry order is active, cancellation will be requested.\n        \"\"\"\n\n        self.__state.exit(self, stopPrice, None, goodTillCanceled)\n\n    def exitStopLimit(self, stopPrice, limitPrice, goodTillCanceled=None):\n        \"\"\"Submits a stop limit order to close this position.\n\n        :param stopPrice: The stop price.\n        :type stopPrice: float.\n        :param limitPrice: The limit price.\n        :type limitPrice: float.\n        :param goodTillCanceled: True if the exit order is good till canceled. If False then the order gets automatically canceled when the session closes. If None, then it will match the entry order.\n        :type goodTillCanceled: boolean.\n\n        .. note::\n            * If the position is closed (entry canceled or exit filled) this won't have any effect.\n            * If the exit order for this position is pending, an exception will be raised. The exit order should be canceled first.\n            * If the entry order is active, cancellation will be requested.\n        \"\"\"\n\n        self.__state.exit(self, stopPrice, limitPrice, goodTillCanceled)\n\n    def _submitExitOrder(self, stopPrice, limitPrice, goodTillCanceled):\n        assert(not self.exitActive())\n\n        exitOrder = self.buildExitOrder(stopPrice, limitPrice)\n\n        # If goodTillCanceled was not set, match the entry order.\n        if goodTillCanceled is None:\n            goodTillCanceled = self.__entryOrder.getGoodTillCanceled()\n        exitOrder.setGoodTillCanceled(goodTillCanceled)\n\n        exitOrder.setAllOrNone(self.__allOrNone)\n\n        self.__submitAndRegisterOrder(exitOrder)\n        self.__exitOrder = exitOrder\n\n    def onOrderEvent(self, orderEvent):\n        self.__updatePosTracker(orderEvent)\n\n        order = orderEvent.getOrder()\n        if not order.isActive():\n            del self.__activeOrders[order.getId()]\n\n        # Update the number of shares.\n        if orderEvent.getEventType() in (broker.OrderEvent.Type.PARTIALLY_FILLED, broker.OrderEvent.Type.FILLED):\n            execInfo = orderEvent.getEventInfo()\n            # roundQuantity is used to prevent bugs like the one triggered in testcases.bitstamp_test:TestCase.testRoundingBug\n            if order.isBuy():\n                self.__shares = order.getInstrumentTraits().roundQuantity(self.__shares + execInfo.getQuantity())\n            else:\n                self.__shares = order.getInstrumentTraits().roundQuantity(self.__shares - execInfo.getQuantity())\n\n        self.__state.onOrderEvent(self, orderEvent)\n\n    def __updatePosTracker(self, orderEvent):\n        if orderEvent.getEventType() in (broker.OrderEvent.Type.PARTIALLY_FILLED, broker.OrderEvent.Type.FILLED):\n            order = orderEvent.getOrder()\n            execInfo = orderEvent.getEventInfo()\n            if order.isBuy():\n                self.__posTracker.buy(execInfo.getQuantity(), execInfo.getPrice(), execInfo.getCommission())\n            else:\n                self.__posTracker.sell(execInfo.getQuantity(), execInfo.getPrice(), execInfo.getCommission())\n\n    def buildExitOrder(self, stopPrice, limitPrice):\n        raise NotImplementedError()\n\n    def isOpen(self):\n        \"\"\"Returns True if the position is open.\"\"\"\n        return self.__state.isOpen(self)\n\n    def getAge(self):\n        \"\"\"Returns the duration in open state.\n\n        :rtype: datetime.timedelta.\n\n        .. note::\n            * If the position is open, then the difference between the entry datetime and the datetime of the last bar is returned.\n            * If the position is closed, then the difference between the entry datetime and the exit datetime is returned.\n        \"\"\"\n        ret = datetime.timedelta()\n        if self.__entryDateTime is not None:\n            if self.__exitDateTime is not None:\n                last = self.__exitDateTime\n            else:\n                last = self.__strategy.getCurrentDateTime()\n            ret = last - self.__entryDateTime\n        return ret\n\n\n# This class is reponsible for order management in long positions.\nclass LongPosition(Position):\n    def __init__(self, strategy, instrument, stopPrice, limitPrice, quantity, goodTillCanceled, allOrNone):\n        if limitPrice is None and stopPrice is None:\n            entryOrder = strategy.getBroker().createMarketOrder(broker.Order.Action.BUY, instrument, quantity, False)\n        elif limitPrice is not None and stopPrice is None:\n            entryOrder = strategy.getBroker().createLimitOrder(broker.Order.Action.BUY, instrument, limitPrice, quantity)\n        elif limitPrice is None and stopPrice is not None:\n            entryOrder = strategy.getBroker().createStopOrder(broker.Order.Action.BUY, instrument, stopPrice, quantity)\n        elif limitPrice is not None and stopPrice is not None:\n            entryOrder = strategy.getBroker().createStopLimitOrder(broker.Order.Action.BUY, instrument, stopPrice, limitPrice, quantity)\n        else:\n            assert(False)\n\n        super(LongPosition, self).__init__(strategy, entryOrder, goodTillCanceled, allOrNone)\n\n    def buildExitOrder(self, stopPrice, limitPrice):\n        quantity = self.getShares()\n        assert(quantity > 0)\n        if limitPrice is None and stopPrice is None:\n            ret = self.getStrategy().getBroker().createMarketOrder(broker.Order.Action.SELL, self.getInstrument(), quantity, False)\n        elif limitPrice is not None and stopPrice is None:\n            ret = self.getStrategy().getBroker().createLimitOrder(broker.Order.Action.SELL, self.getInstrument(), limitPrice, quantity)\n        elif limitPrice is None and stopPrice is not None:\n            ret = self.getStrategy().getBroker().createStopOrder(broker.Order.Action.SELL, self.getInstrument(), stopPrice, quantity)\n        elif limitPrice is not None and stopPrice is not None:\n            ret = self.getStrategy().getBroker().createStopLimitOrder(broker.Order.Action.SELL, self.getInstrument(), stopPrice, limitPrice, quantity)\n        else:\n            assert(False)\n\n        return ret\n\n\n# This class is reponsible for order management in short positions.\nclass ShortPosition(Position):\n    def __init__(self, strategy, instrument, stopPrice, limitPrice, quantity, goodTillCanceled, allOrNone):\n        if limitPrice is None and stopPrice is None:\n            entryOrder = strategy.getBroker().createMarketOrder(broker.Order.Action.SELL_SHORT, instrument, quantity, False)\n        elif limitPrice is not None and stopPrice is None:\n            entryOrder = strategy.getBroker().createLimitOrder(broker.Order.Action.SELL_SHORT, instrument, limitPrice, quantity)\n        elif limitPrice is None and stopPrice is not None:\n            entryOrder = strategy.getBroker().createStopOrder(broker.Order.Action.SELL_SHORT, instrument, stopPrice, quantity)\n        elif limitPrice is not None and stopPrice is not None:\n            entryOrder = strategy.getBroker().createStopLimitOrder(broker.Order.Action.SELL_SHORT, instrument, stopPrice, limitPrice, quantity)\n        else:\n            assert(False)\n\n        super(ShortPosition, self).__init__(strategy, entryOrder, goodTillCanceled, allOrNone)\n\n    def buildExitOrder(self, stopPrice, limitPrice):\n        quantity = self.getShares() * -1\n        assert(quantity > 0)\n        if limitPrice is None and stopPrice is None:\n            ret = self.getStrategy().getBroker().createMarketOrder(broker.Order.Action.BUY_TO_COVER, self.getInstrument(), quantity, False)\n        elif limitPrice is not None and stopPrice is None:\n            ret = self.getStrategy().getBroker().createLimitOrder(broker.Order.Action.BUY_TO_COVER, self.getInstrument(), limitPrice, quantity)\n        elif limitPrice is None and stopPrice is not None:\n            ret = self.getStrategy().getBroker().createStopOrder(broker.Order.Action.BUY_TO_COVER, self.getInstrument(), stopPrice, quantity)\n        elif limitPrice is not None and stopPrice is not None:\n            ret = self.getStrategy().getBroker().createStopLimitOrder(broker.Order.Action.BUY_TO_COVER, self.getInstrument(), stopPrice, limitPrice, quantity)\n        else:\n            assert(False)\n\n        return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\n\ndef compute_diff(values1, values2):\n    assert(len(values1) == len(values2))\n    ret = []\n    for i in range(len(values1)):\n        v1 = values1[i]\n        v2 = values2[i]\n        if v1 is not None and v2 is not None:\n            diff = v1 - v2\n        else:\n            diff = None\n        ret.append(diff)\n    return ret\n\n\ndef _get_stripped(values1, values2, alignLeft):\n    if len(values1) > len(values2):\n        if alignLeft:\n            values1 = values1[0:len(values2)]\n        else:\n            values1 = values1[len(values1)-len(values2):]\n    elif len(values2) > len(values1):\n        if alignLeft:\n            values2 = values2[0:len(values1)]\n        else:\n            values2 = values2[len(values2)-len(values1):]\n    return values1, values2\n\n\ndef _cross_impl(values1, values2, start, end, signCheck):\n    # Get both set of values.\n    values1, values2 = _get_stripped(values1[start:end], values2[start:end], start > 0)\n\n    # Compute differences and check sign changes.\n    ret = 0\n    diffs = compute_diff(values1, values2)\n    diffs = filter(lambda x: x != 0, diffs)\n    prevDiff = None\n    for diff in diffs:\n        if prevDiff is not None and not signCheck(prevDiff) and signCheck(diff):\n            ret += 1\n        prevDiff = diff\n    return ret\n\n\n# Note:\n# Up to version 0.12 CrossAbove and CrossBelow were DataSeries.\n# In version 0.13 SequenceDataSeries was refactored to support specifying a limit to the amount\n# of values to hold. This was introduced mainly to reduce memory footprint.\n# This change had a huge impact on the way DataSeries filters were implemented since they were\n# mosly views and didn't hold any actual values. For example, a SMA(200) didn't hold any values at all\n# but rather calculate those on demand by requesting 200 values from the DataSeries being wrapped.\n# Now that the DataSeries being wrapped may not hold so many values, DataSeries filters were refactored\n# to an event based model and they will calculate and hold resulting values as new values get added to\n# the underlying DataSeries (the one being wrapped).\n# Since it was too complicated to make CrossAbove and CrossBelow filters work with this new model (\n# mainly because the underlying DataSeries may not get new values added at the same time, or one after\n# another) I decided to turn those into functions, cross_above and cross_below.\n\ndef cross_above(values1, values2, start=-2, end=None):\n    \"\"\"Checks for a cross above conditions over the specified period between two DataSeries objects.\n\n    It returns the number of times values1 crossed above values2 during the given period.\n\n    :param values1: The DataSeries that crosses.\n    :type values1: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param values2: The DataSeries being crossed.\n    :type values2: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param start: The start of the range.\n    :type start: int.\n    :param end: The end of the range.\n    :type end: int.\n\n    .. note::\n        The default start and end values check for cross above conditions over the last 2 values.\n    \"\"\"\n    return _cross_impl(values1, values2, start, end, lambda x: x > 0)\n\n\ndef cross_below(values1, values2, start=-2, end=None):\n    \"\"\"Checks for a cross below conditions over the specified period between two DataSeries objects.\n\n    It returns the number of times values1 crossed below values2 during the given period.\n\n    :param values1: The DataSeries that crosses.\n    :type values1: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param values2: The DataSeries being crossed.\n    :type values2: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param start: The start of the range.\n    :type start: int.\n    :param end: The end of the range.\n    :type end: int.\n\n    .. note::\n        The default start and end values check for cross below conditions over the last 2 values.\n    \"\"\"\n    return _cross_impl(values1, values2, start, end, lambda x: x < 0)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import technical\nfrom pyalgotrade.utils import collections\nfrom pyalgotrade.utils import dt\n\nimport numpy as np\nfrom scipy import stats\n\n\n# Using scipy.stats.linregress instead of numpy.linalg.lstsq because of this:\n# http://stackoverflow.com/questions/20736255/numpy-linalg-lstsq-with-big-values\ndef lsreg(x, y):\n    x = np.asarray(x)\n    y = np.asarray(y)\n    res = stats.linregress(x, y)\n    return res[0], res[1]\n\n\nclass LeastSquaresRegressionWindow(technical.EventWindow):\n    def __init__(self, windowSize):\n        assert(windowSize > 1)\n        super(LeastSquaresRegressionWindow, self).__init__(windowSize)\n        self.__timestamps = collections.NumPyDeque(windowSize)\n\n    def onNewValue(self, dateTime, value):\n        technical.EventWindow.onNewValue(self, dateTime, value)\n        if value is not None:\n            timestamp = dt.datetime_to_timestamp(dateTime)\n            if len(self.__timestamps):\n                assert(timestamp > self.__timestamps[-1])\n            self.__timestamps.append(timestamp)\n\n    def __getValueAtImpl(self, timestamp):\n        ret = None\n        if self.windowFull():\n            a, b = lsreg(self.__timestamps.data(), self.getValues())\n            ret = a * timestamp + b\n        return ret\n\n    def getTimeStamps(self):\n        return self.__timestamps\n\n    def getValueAt(self, dateTime):\n        return self.__getValueAtImpl(dt.datetime_to_timestamp(dateTime))\n\n    def getValue(self):\n        ret = None\n        if self.windowFull():\n            ret = self.__getValueAtImpl(self.__timestamps.data()[-1])\n        return ret\n\n\nclass LeastSquaresRegression(technical.EventBasedFilter):\n    \"\"\"Calculates values based on a least-squares regression.\n\n    :param dataSeries: The DataSeries instance being filtered.\n    :type dataSeries: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param windowSize: The number of values to use to calculate the regression.\n    :type windowSize: int.\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n    \"\"\"\n    def __init__(self, dataSeries, windowSize, maxLen=None):\n        super(LeastSquaresRegression, self).__init__(dataSeries, LeastSquaresRegressionWindow(windowSize), maxLen)\n\n    def getValueAt(self, dateTime):\n        \"\"\"Calculates the value at a given time based on the regression line.\n\n        :param dateTime: The datetime to calculate the value at.\n            Will return None if there are not enough values in the underlying DataSeries.\n        :type dateTime: :class:`datetime.datetime`.\n        \"\"\"\n        return self.getEventWindow().getValueAt(dateTime)\n\n\nclass SlopeEventWindow(technical.EventWindow):\n    def __init__(self, windowSize):\n        super(SlopeEventWindow, self).__init__(windowSize)\n        self.__x = np.asarray(range(windowSize))\n\n    def getValue(self):\n        ret = None\n        if self.windowFull():\n            y = self.getValues()\n            ret = lsreg(self.__x, y)[0]\n        return ret\n\n\nclass Slope(technical.EventBasedFilter):\n    \"\"\"The Slope filter calculates the slope of a least-squares regression line.\n\n    :param dataSeries: The DataSeries instance being filtered.\n    :type dataSeries: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param period: The number of values to use to calculate the slope.\n    :type period: int.\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n\n    .. note::\n        This filter ignores the time elapsed between the different values.\n    \"\"\"\n\n    def __init__(self, dataSeries, period, maxLen=None):\n        super(Slope, self).__init__(dataSeries, SlopeEventWindow(period), maxLen)\n\n\nclass TrendEventWindow(SlopeEventWindow):\n    def __init__(self, windowSize, positiveThreshold, negativeThreshold):\n        if negativeThreshold > positiveThreshold:\n            raise Exception(\"Invalid thresholds\")\n\n        super(TrendEventWindow, self).__init__(windowSize)\n        self.__positiveThreshold = positiveThreshold\n        self.__negativeThreshold = negativeThreshold\n\n    def getValue(self):\n        ret = super(TrendEventWindow, self).getValue()\n        if ret is not None:\n            if ret > self.__positiveThreshold:\n                ret = True\n            elif ret < self.__negativeThreshold:\n                ret = False\n            else:  # Between negative and postive thresholds.\n                ret = None\n        return ret\n\n\nclass Trend(technical.EventBasedFilter):\n    def __init__(self, dataSeries, trendDays, positiveThreshold=0, negativeThreshold=0, maxLen=None):\n        super(Trend, self).__init__(dataSeries, TrendEventWindow(trendDays, positiveThreshold, negativeThreshold), maxLen)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import technical\n\n\n# RSI = 100 - 100 / (1 + RS)\n# RS = Average gain / Average loss\n# First Average Gain = Sum of Gains over the past 14 periods / 14\n# First Average Loss = Sum of Losses over the past 14 periods / 14\n# Average Gain = [(previous Average Gain) x 13 + current Gain] / 14\n# Average Loss = [(previous Average Loss) x 13 + current Loss] / 14\n#\n# RSI is 0 when the Average Gain equals zero. Assuming a 14-period RSI, a zero RSI value means prices moved lower all\n# 14 periods. There were no gains to measure.\n# RSI is 100 when the Average Loss equals zero. This means prices moved higher all 14 periods.\n# There were no losses to measure.\n#\n# If Average Loss equals zero, a \"divide by zero\" situation occurs for RS and RSI is set to 100 by definition.\n# Similarly, RSI equals 0 when Average Gain equals zero.\n#\n# RSI is considered overbought when above 70 and oversold when below 30.\n# These traditional levels can also be adjusted to better fit the security or analytical requirements.\n# Raising overbought to 80 or lowering oversold to 20 will reduce the number of overbought/oversold readings.\n# Short-term traders sometimes use 2-period RSI to look for overbought readings above 80 and oversold readings below 20.\n\ndef gain_loss_one(prevValue, nextValue):\n    change = nextValue - prevValue\n    if change < 0:\n        gain = 0\n        loss = abs(change)\n    else:\n        gain = change\n        loss = 0\n    return gain, loss\n\n\n# [begin, end)\ndef avg_gain_loss(values, begin, end):\n    rangeLen = end - begin\n    if rangeLen < 2:\n        return None\n\n    gain = 0\n    loss = 0\n    for i in xrange(begin+1, end):\n        currGain, currLoss = gain_loss_one(values[i-1], values[i])\n        gain += currGain\n        loss += currLoss\n    return (gain/float(rangeLen-1), loss/float(rangeLen-1))\n\n\ndef rsi(values, period):\n    assert(period > 1)\n    if len(values) < period + 1:\n        return None\n\n    avgGain, avgLoss = avg_gain_loss(values, 0, period)\n    for i in xrange(period, len(values)):\n        gain, loss = gain_loss_one(values[i-1], values[i])\n        avgGain = (avgGain * (period - 1) + gain) / float(period)\n        avgLoss = (avgLoss * (period - 1) + loss) / float(period)\n\n    if avgLoss == 0:\n        return 100\n    rs = avgGain / avgLoss\n    return 100 - 100 / (1 + rs)\n\n\nclass RSIEventWindow(technical.EventWindow):\n    def __init__(self, period):\n        assert(period > 1)\n        # We need N + 1 samples to calculate N averages because they are calculated based on the diff with previous values.\n        super(RSIEventWindow, self).__init__(period + 1)\n        self.__value = None\n        self.__prevGain = None\n        self.__prevLoss = None\n        self.__period = period\n\n    def onNewValue(self, dateTime, value):\n        super(RSIEventWindow, self).onNewValue(dateTime, value)\n\n        # Formula from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi\n        if value is not None and self.windowFull():\n            if self.__prevGain is None:\n                assert(self.__prevLoss is None)\n                avgGain, avgLoss = avg_gain_loss(self.getValues(), 0, len(self.getValues()))\n            else:\n                # Rest of averages are smoothed\n                assert(self.__prevLoss is not None)\n                prevValue = self.getValues()[-2]\n                currValue = self.getValues()[-1]\n                currGain, currLoss = gain_loss_one(prevValue, currValue)\n                avgGain = (self.__prevGain * (self.__period-1) + currGain) / float(self.__period)\n                avgLoss = (self.__prevLoss * (self.__period-1) + currLoss) / float(self.__period)\n\n            if avgLoss == 0:\n                self.__value = 100\n            else:\n                rs = avgGain / avgLoss\n                self.__value = 100 - 100 / (1 + rs)\n\n            self.__prevGain = avgGain\n            self.__prevLoss = avgLoss\n\n    def getValue(self):\n        return self.__value\n\n\nclass RSI(technical.EventBasedFilter):\n    \"\"\"Relative Strength Index filter as described in http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi.\n\n    :param dataSeries: The DataSeries instance being filtered.\n    :type dataSeries: :class:`pyalgotrade.dataseries.DataSeries`.\n    :param period: The period. Note that if period is **n**, then **n+1** values are used. Must be > 1.\n    :type period: int.\n    :param maxLen: The maximum number of values to hold.\n        Once a bounded length is full, when new items are added, a corresponding number of items are discarded from the\n        opposite end. If None then dataseries.DEFAULT_MAX_LEN is used.\n    :type maxLen: int.\n    \"\"\"\n\n    def __init__(self, dataSeries, period, maxLen=None):\n        super(RSI, self).__init__(dataSeries, RSIEventWindow(period), maxLen)\n",
          "# -*- coding: utf-8 -*-\n# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n.. moduleauthor:: Maciej Å»ok <maciek.zok@gmail.com>\n\"\"\"\n\nimport os\nimport datetime\n\nimport pyalgotrade.logger\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import googlefeed\nfrom pyalgotrade.utils import csvutils\n\n\ndef download_csv(instrument, begin, end):\n    url = \"http://www.google.com/finance/historical\"\n    params = {\n        \"q\": instrument,\n        \"startdate\": begin.strftime(\"%Y-%m-%d\"),\n        \"enddate\": end.strftime(\"%Y-%m-%d\"),\n        \"output\": \"csv\",\n    }\n\n    return csvutils.download_csv(url, url_params=params, content_type=\"application/vnd.ms-excel\")\n\n\ndef download_daily_bars(instrument, year, csvFile):\n    \"\"\"Download daily bars from Google Finance for a given year.\n\n    :param instrument: Instrument identifier.\n    :type instrument: string.\n    :param year: The year.\n    :type year: int.\n    :param csvFile: The path to the CSV file to write.\n    :type csvFile: string.\n    \"\"\"\n\n    bars = download_csv(instrument,\n                        datetime.date(year, 1, 1),\n                        datetime.date(year, 12, 31))\n    f = open(csvFile, \"w\")\n    f.write(bars)\n    f.close()\n\n\ndef build_feed(instruments, fromYear, toYear, storage, frequency=bar.Frequency.DAY, timezone=None, skipErrors=False):\n    \"\"\"Build and load a :class:`pyalgotrade.barfeed.googlefeed.Feed` using CSV files downloaded from Google Finance.\n    CSV files are downloaded if they haven't been downloaded before.\n\n    :param instruments: Instrument identifiers.\n    :type instruments: list.\n    :param fromYear: The first year.\n    :type fromYear: int.\n    :param toYear: The last year.\n    :type toYear: int.\n    :param storage: The path were the files will be loaded from, or downloaded to.\n    :type storage: string.\n    :param frequency: The frequency of the bars. Only **pyalgotrade.bar.Frequency.DAY** is currently supported.\n    :param timezone: The default timezone to use to localize bars. Check :mod:`pyalgotrade.marketsession`.\n    :type timezone: A pytz timezone.\n    :param skipErrors: True to keep on loading/downloading files in case of errors.\n    :type skipErrors: boolean.\n    :rtype: :class:`pyalgotrade.barfeed.googlefeed.Feed`.\n    \"\"\"\n\n    logger = pyalgotrade.logger.getLogger(\"googlefinance\")\n    ret = googlefeed.Feed(frequency, timezone)\n\n    if not os.path.exists(storage):\n        logger.info(\"Creating {dirname} directory\".format(dirname=storage))\n        os.mkdir(storage)\n\n    for year in range(fromYear, toYear+1):\n        for instrument in instruments:\n            fileName = os.path.join(\n                storage,\n                \"{instrument}-{year}-googlefinance.csv\".format(\n                    instrument=instrument, year=year))\n            if not os.path.exists(fileName):\n                logger.info(\n                    \"Downloading {instrument} {year} to {filename}\".format(\n                        instrument=instrument, year=year, filename=fileName))\n                try:\n                    if frequency == bar.Frequency.DAY:\n                        download_daily_bars(instrument, year, fileName)\n                    else:\n                        raise Exception(\"Invalid frequency\")\n                except Exception, e:\n                    if skipErrors:\n                        logger.error(str(e))\n                        continue\n                    else:\n                        raise e\n            ret.addBarsFromCSV(instrument, fileName)\n    return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport os\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import quandlfeed\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.utils import csvutils\nimport pyalgotrade.logger\n\n\n# http://www.quandl.com/help/api\n\ndef download_csv(sourceCode, tableCode, begin, end, frequency, authToken):\n    url = \"http://www.quandl.com/api/v1/datasets/%s/%s.csv\" % (sourceCode, tableCode)\n    params = {\n        \"trim_start\": begin.strftime(\"%Y-%m-%d\"),\n        \"trim_end\": end.strftime(\"%Y-%m-%d\"),\n        \"collapse\": frequency\n    }\n    if authToken is not None:\n        params[\"auth_token\"] = authToken\n\n    return csvutils.download_csv(url, params)\n\n\ndef download_daily_bars(sourceCode, tableCode, year, csvFile, authToken=None):\n    \"\"\"Download daily bars from Quandl for a given year.\n\n    :param sourceCode: The dataset's source code.\n    :type sourceCode: string.\n    :param tableCode: The dataset's table code.\n    :type tableCode: string.\n    :param year: The year.\n    :type year: int.\n    :param csvFile: The path to the CSV file to write.\n    :type csvFile: string.\n    :param authToken: Optional. An authentication token needed if you're doing more than 50 calls per day.\n    :type authToken: string.\n    \"\"\"\n\n    bars = download_csv(sourceCode, tableCode, datetime.date(year, 1, 1), datetime.date(year, 12, 31), \"daily\", authToken)\n    f = open(csvFile, \"w\")\n    f.write(bars)\n    f.close()\n\n\ndef download_weekly_bars(sourceCode, tableCode, year, csvFile, authToken=None):\n    \"\"\"Download weekly bars from Quandl for a given year.\n\n    :param sourceCode: The dataset's source code.\n    :type sourceCode: string.\n    :param tableCode: The dataset's table code.\n    :type tableCode: string.\n    :param year: The year.\n    :type year: int.\n    :param csvFile: The path to the CSV file to write.\n    :type csvFile: string.\n    :param authToken: Optional. An authentication token needed if you're doing more than 50 calls per day.\n    :type authToken: string.\n    \"\"\"\n\n    begin = dt.get_first_monday(year) - datetime.timedelta(days=1)  # Start on a sunday\n    end = dt.get_last_monday(year) - datetime.timedelta(days=1)  # Start on a sunday\n    bars = download_csv(sourceCode, tableCode, begin, end, \"weekly\", authToken)\n    f = open(csvFile, \"w\")\n    f.write(bars)\n    f.close()\n\n\ndef build_feed(sourceCode, tableCodes, fromYear, toYear, storage, frequency=bar.Frequency.DAY, timezone=None,\n               skipErrors=False, noAdjClose=False, authToken=None, columnNames={}, forceDownload=False\n               ):\n    \"\"\"Build and load a :class:`pyalgotrade.barfeed.quandlfeed.Feed` using CSV files downloaded from Quandl.\n    CSV files are downloaded if they haven't been downloaded before.\n\n    :param sourceCode: The dataset source code.\n    :type sourceCode: string.\n    :param tableCodes: The dataset table codes.\n    :type tableCodes: list.\n    :param fromYear: The first year.\n    :type fromYear: int.\n    :param toYear: The last year.\n    :type toYear: int.\n    :param storage: The path were the files will be loaded from, or downloaded to.\n    :type storage: string.\n    :param frequency: The frequency of the bars. Only **pyalgotrade.bar.Frequency.DAY** or **pyalgotrade.bar.Frequency.WEEK**\n        are supported.\n    :param timezone: The default timezone to use to localize bars. Check :mod:`pyalgotrade.marketsession`.\n    :type timezone: A pytz timezone.\n    :param skipErrors: True to keep on loading/downloading files in case of errors.\n    :type skipErrors: boolean.\n    :param noAdjClose: True if the instruments don't have adjusted close values.\n    :type noAdjClose: boolean.\n    :param authToken: Optional. An authentication token needed if you're doing more than 50 calls per day.\n    :type authToken: string.\n    :param columnNames: Optional. A dictionary to map column names. Valid key values are:\n\n        * datetime\n        * open\n        * high\n        * low\n        * close\n        * volume\n        * adj_close\n\n    :type columnNames: dict.\n\n    :rtype: :class:`pyalgotrade.barfeed.quandlfeed.Feed`.\n    \"\"\"\n\n    logger = pyalgotrade.logger.getLogger(\"quandl\")\n    ret = quandlfeed.Feed(frequency, timezone)\n    if noAdjClose:\n        ret.setNoAdjClose()\n\n    # Additional column names.\n    for col, name in columnNames.iteritems():\n        ret.setColumnName(col, name)\n\n    if not os.path.exists(storage):\n        logger.info(\"Creating %s directory\" % (storage))\n        os.mkdir(storage)\n\n    for year in range(fromYear, toYear+1):\n        for tableCode in tableCodes:\n            fileName = os.path.join(storage, \"%s-%s-%d-quandl.csv\" % (sourceCode, tableCode, year))\n            if not os.path.exists(fileName) or forceDownload:\n                logger.info(\"Downloading %s %d to %s\" % (tableCode, year, fileName))\n                try:\n                    if frequency == bar.Frequency.DAY:\n                        download_daily_bars(sourceCode, tableCode, year, fileName, authToken)\n                    elif frequency == bar.Frequency.WEEK:\n                        download_weekly_bars(sourceCode, tableCode, year, fileName, authToken)\n                    else:\n                        raise Exception(\"Invalid frequency\")\n                except Exception, e:\n                    if skipErrors:\n                        logger.error(str(e))\n                        continue\n                    else:\n                        raise e\n            ret.addBarsFromCSV(tableCode, fileName)\n    return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\nimport datetime\n\nimport pyalgotrade.logger\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.utils import csvutils\n\n\ndef __adjust_month(month):\n    if month > 12 or month < 1:\n        raise Exception(\"Invalid month\")\n    month -= 1  # Months for yahoo are 0 based\n    return month\n\n\ndef download_csv(instrument, begin, end, frequency):\n    url = \"http://ichart.finance.yahoo.com/table.csv?s=%s&a=%d&b=%d&c=%d&d=%d&e=%d&f=%d&g=%s&ignore=.csv\" % (instrument, __adjust_month(begin.month), begin.day, begin.year, __adjust_month(end.month), end.day, end.year, frequency)\n    return csvutils.download_csv(url)\n\n\ndef download_daily_bars(instrument, year, csvFile):\n    \"\"\"Download daily bars from Yahoo! Finance for a given year.\n\n    :param instrument: Instrument identifier.\n    :type instrument: string.\n    :param year: The year.\n    :type year: int.\n    :param csvFile: The path to the CSV file to write.\n    :type csvFile: string.\n    \"\"\"\n\n    bars = download_csv(instrument, datetime.date(year, 1, 1), datetime.date(year, 12, 31), \"d\")\n    f = open(csvFile, \"w\")\n    f.write(bars)\n    f.close()\n\n\ndef download_weekly_bars(instrument, year, csvFile):\n    \"\"\"Download weekly bars from Yahoo! Finance for a given year.\n\n    :param instrument: Instrument identifier.\n    :type instrument: string.\n    :param year: The year.\n    :type year: int.\n    :param csvFile: The path to the CSV file to write.\n    :type csvFile: string.\n    \"\"\"\n\n    begin = dt.get_first_monday(year)\n    end = dt.get_last_monday(year) + datetime.timedelta(days=6)\n    bars = download_csv(instrument, begin, end, \"w\")\n    f = open(csvFile, \"w\")\n    f.write(bars)\n    f.close()\n\n\ndef build_feed(instruments, fromYear, toYear, storage, frequency=bar.Frequency.DAY, timezone=None, skipErrors=False):\n    \"\"\"Build and load a :class:`pyalgotrade.barfeed.yahoofeed.Feed` using CSV files downloaded from Yahoo! Finance.\n    CSV files are downloaded if they haven't been downloaded before.\n\n    :param instruments: Instrument identifiers.\n    :type instruments: list.\n    :param fromYear: The first year.\n    :type fromYear: int.\n    :param toYear: The last year.\n    :type toYear: int.\n    :param storage: The path were the files will be loaded from, or downloaded to.\n    :type storage: string.\n    :param frequency: The frequency of the bars. Only **pyalgotrade.bar.Frequency.DAY** or **pyalgotrade.bar.Frequency.WEEK**\n        are supported.\n    :param timezone: The default timezone to use to localize bars. Check :mod:`pyalgotrade.marketsession`.\n    :type timezone: A pytz timezone.\n    :param skipErrors: True to keep on loading/downloading files in case of errors.\n    :type skipErrors: boolean.\n    :rtype: :class:`pyalgotrade.barfeed.yahoofeed.Feed`.\n    \"\"\"\n\n    logger = pyalgotrade.logger.getLogger(\"yahoofinance\")\n    ret = yahoofeed.Feed(frequency, timezone)\n\n    if not os.path.exists(storage):\n        logger.info(\"Creating %s directory\" % (storage))\n        os.mkdir(storage)\n\n    for year in range(fromYear, toYear+1):\n        for instrument in instruments:\n            fileName = os.path.join(storage, \"%s-%d-yahoofinance.csv\" % (instrument, year))\n            if not os.path.exists(fileName):\n                logger.info(\"Downloading %s %d to %s\" % (instrument, year, fileName))\n                try:\n                    if frequency == bar.Frequency.DAY:\n                        download_daily_bars(instrument, year, fileName)\n                    elif frequency == bar.Frequency.WEEK:\n                        download_weekly_bars(instrument, year, fileName)\n                    else:\n                        raise Exception(\"Invalid frequency\")\n                except Exception, e:\n                    if skipErrors:\n                        logger.error(str(e))\n                        continue\n                    else:\n                        raise e\n            ret.addBarsFromCSV(instrument, fileName)\n    return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport Queue\nimport threading\nimport json\n\nfrom pyalgotrade import observer\nimport pyalgotrade.logger\n\nimport tweepy\nfrom tweepy import streaming\n\nlogger = pyalgotrade.logger.getLogger(\"twitter\")\n\n\n# This listener just pushs data into a queue.\nclass Listener(streaming.StreamListener):\n    def __init__(self, queue):\n        super(Listener, self).__init__()\n        self.__queue = queue\n\n    def on_connect(self):\n        logger.info(\"Connected.\")\n\n    def on_timeout(self):\n        logger.error(\"Timeout.\")\n\n    def on_data(self, data):\n        self.__queue.put(data)\n        return True\n\n    def on_error(self, status):\n        logger.error(status)\n        return False\n\n\n# https://dev.twitter.com/docs/streaming-apis/parameters\nclass TwitterFeed(observer.Subject):\n    \"\"\"Class responsible for connecting to Twitter's public stream API and dispatching events.\n    Check https://dev.twitter.com/docs/streaming-apis/streams/public for more information.\n\n    :param consumerKey: Consumer key.\n    :type consumerKey: string.\n    :param consumerSecret: Consumer secret.\n    :type consumerSecret: string.\n    :param accessToken: Access token.\n    :type accessToken: string.\n    :param accessTokenSecret: Access token secret.\n    :type accessTokenSecret: string.\n    :param track: A list of phrases which will be used to determine what Tweets will be delivered\n        on the stream. A phrase may be one or more terms separated by spaces, and a phrase will match\n        if all of the terms in the phrase are present in the Tweet, regardless of order and ignoring case.\n    :type track: list.\n    :param follow: A list of user IDs, indicating the users whose Tweets should be delivered on the\n        stream. Following protected users is not supported.\n    :type follow: list.\n    :param languages: A list of language IDs a defined in http://tools.ietf.org/html/bcp47.\n    :type languages: list.\n\n    .. note::\n        * Go to http://dev.twitter.com and create an app. The consumer key and secret will be generated for you after that.\n        * Create an access token under the \"Your access token\" section.\n        * At least **track** or **follow** have to be set.\n    \"\"\"\n\n    QUEUE_TIMEOUT = 0.01\n    MAX_EVENTS_PER_DISPATCH = 50\n\n    def __init__(self, consumerKey, consumerSecret, accessToken, accessTokenSecret, track=[], follow=[], languages=[]):\n        assert isinstance(track, list), \"track must be a list\"\n        assert isinstance(follow, list), \"follow must be a list\"\n        assert isinstance(languages, list), \"languages must be a list\"\n\n        super(TwitterFeed, self).__init__()\n\n        self.__event = observer.Event()\n        self.__queue = Queue.Queue()\n        self.__thread = None\n        self.__running = False\n\n        listener = Listener(self.__queue)\n        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n        auth.set_access_token(accessToken, accessTokenSecret)\n        self.__stream = tweepy.Stream(auth, listener)\n        self.__track = track\n        self.__follow = follow\n        self.__languages = languages\n\n    def __threadMain(self):\n        try:\n            logger.info(\"Initializing client.\")\n            self.__stream.filter(track=self.__track, follow=self.__follow, languages=self.__languages)\n        finally:\n            logger.info(\"Client finished.\")\n            self.__running = False\n\n    def __dispatchImpl(self):\n        ret = False\n        try:\n            nextTweet = json.loads(self.__queue.get(True, TwitterFeed.QUEUE_TIMEOUT))\n            ret = True\n            self.__event.emit(nextTweet)\n        except Queue.Empty:\n            pass\n        return ret\n\n    def subscribe(self, callback):\n        \"\"\"Subscribe to Twitter events. The event handler will receive a dictionary with the data as defined in:\n        https://dev.twitter.com/docs/streaming-apis/messages#Public_stream_messages.\n        \"\"\"\n        return self.__event.subscribe(callback)\n\n    def start(self):\n        super(TwitterFeed, self).start()\n        if self.__thread is not None:\n            raise Exception(\"Already running\")\n\n        # Start the thread that will run the client.\n        self.__thread = threading.Thread(target=self.__threadMain)\n        self.__thread.start()\n        self.__running = True\n\n    def stop(self):\n        try:\n            if self.__thread is not None and self.__thread.is_alive():\n                logger.info(\"Shutting down client.\")\n                self.__stream.disconnect()\n        except Exception, e:\n            logger.error(\"Error disconnecting stream: %s.\" % (str(e)))\n\n    def join(self):\n        if self.__thread is not None:\n            self.__thread.join()\n        assert(not self.__running)\n\n    def eof(self):\n        return not self.__running\n\n    def dispatch(self):\n        ret = False\n        dispatched = TwitterFeed.MAX_EVENTS_PER_DISPATCH\n        while self.__dispatchImpl() and dispatched > 0:\n            ret = True\n            dispatched -= 1\n        return ret\n\n    def peekDateTime(self):\n        return None\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport csv\nimport requests\n\nimport logging\nlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\n\n# A faster (but limited) version of csv.DictReader\nclass FastDictReader(object):\n    def __init__(self, f, fieldnames=None, dialect=\"excel\", *args, **kwargs):\n        self.__fieldNames = fieldnames\n        self.reader = csv.reader(f, dialect, *args, **kwargs)\n        if self.__fieldNames is None:\n            self.__fieldNames = self.reader.next()\n        self.__dict = {}\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        # Skip empty rows.\n        row = self.reader.next()\n        while row == []:\n            row = self.reader.next()\n\n        # Check that the row has the right number of columns.\n        assert(len(self.__fieldNames) == len(row))\n\n        # Copy the row values into the dict.\n        for i in xrange(len(self.__fieldNames)):\n            self.__dict[self.__fieldNames[i]] = row[i]\n\n        return self.__dict\n\n\ndef download_csv(url, url_params=None, content_type=\"text/csv\"):\n    response = requests.get(url, params=url_params)\n\n    response.raise_for_status()\n    response_content_type = response.headers['content-type']\n    if response_content_type != content_type:\n        raise Exception(\"Invalid content-type: %s\" % response_content_type)\n\n    ret = response.text\n\n    # Remove the BOM\n    while not ret[0].isalnum():\n        ret = ret[1:]\n\n    return ret\n\n\ndef float_or_string(value):\n    try:\n        ret = float(value)\n    except Exception:\n        ret = value\n    return ret\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport json\nimport time\n\nfrom ws4py.client import tornadoclient\nimport tornado\nimport pyalgotrade.logger\n\n\nlogger = pyalgotrade.logger.getLogger(\"websocket.client\")\n\n\n# This class is responsible for sending keep alive messages and detecting disconnections\n# from the server.\nclass KeepAliveMgr(object):\n    def __init__(self, wsClient, maxInactivity, responseTimeout):\n        assert(maxInactivity > 0)\n        assert(responseTimeout > 0)\n        self.__callback = None\n        self.__wsClient = wsClient\n        self.__activityTimeout = maxInactivity\n        self.__responseTimeout = responseTimeout\n        self.__lastSeen = None\n        self.__kaSent = None  # timestamp when the last keep alive was sent.\n\n    def _keepAlive(self):\n        if self.__lastSeen is None:\n            return\n\n        # Check if we're under the inactivity threshold.\n        inactivity = (time.time() - self.__lastSeen)\n        if inactivity <= self.__activityTimeout:\n            return\n\n        # Send keep alive if it was not sent,\n        # or check if we have to timeout waiting for the keep alive response.\n        try:\n            if self.__kaSent is None:\n                self.sendKeepAlive()\n                self.__kaSent = time.time()\n            elif (time.time() - self.__kaSent) > self.__responseTimeout:\n                self.__wsClient.onDisconnectionDetected()\n        except Exception:\n            # Treat an error sending the keep-alive as a diconnection.\n            # print \"Error sending keep alive\", e\n            self.__wsClient.onDisconnectionDetected()\n\n    def getWSClient(self):\n        return self.__wsClient\n\n    def setAlive(self):\n        self.__lastSeen = time.time()\n        self.__kaSent = None\n\n    def start(self):\n        # Check every second.\n        self.__callback = tornado.ioloop.PeriodicCallback(self._keepAlive, 1000, self.__wsClient.getIOLoop())\n        self.__callback.start()\n\n    def stop(self):\n        if self.__callback is not None:\n            self.__callback.stop()\n\n    # Override to send the keep alive msg.\n    def sendKeepAlive(self):\n        raise NotImplementedError()\n\n    # Return True if the response belongs to a keep alive message, False otherwise.\n    def handleResponse(self, msg):\n        raise NotImplementedError()\n\n\n# Base clase for websocket clients.\n# To use it call connect and startClient, and stopClient.\nclass WebSocketClientBase(tornadoclient.TornadoWebSocketClient):\n    def __init__(self, url):\n        super(WebSocketClientBase, self).__init__(url)\n        self.__keepAliveMgr = None\n        self.__connected = False\n\n    # This is to avoid a stack trace because TornadoWebSocketClient is not implementing _cleanup.\n    def _cleanup(self):\n        ret = None\n        try:\n            ret = super(WebSocketClientBase, self)._cleanup()\n        except Exception:\n            pass\n        return ret\n\n    def getIOLoop(self):\n        return tornado.ioloop.IOLoop.instance()\n\n    # Must be set before calling startClient().\n    def setKeepAliveMgr(self, keepAliveMgr):\n        if self.__keepAliveMgr is not None:\n            raise Exception(\"KeepAliveMgr already set\")\n        self.__keepAliveMgr = keepAliveMgr\n\n    def received_message(self, message):\n        try:\n            msg = json.loads(message.data)\n\n            if self.__keepAliveMgr is not None:\n                self.__keepAliveMgr.setAlive()\n                if self.__keepAliveMgr.handleResponse(msg):\n                    return\n\n            self.onMessage(msg)\n        except Exception, e:\n            self.onUnhandledException(e)\n\n    def opened(self):\n        self.__connected = True\n        if self.__keepAliveMgr is not None:\n            self.__keepAliveMgr.start()\n            self.__keepAliveMgr.setAlive()\n        self.onOpened()\n\n    def closed(self, code, reason=None):\n        wasConnected = self.__connected\n        self.__connected = False\n        if self.__keepAliveMgr:\n            self.__keepAliveMgr.stop()\n            self.__keepAliveMgr = None\n        tornado.ioloop.IOLoop.instance().stop()\n\n        if wasConnected:\n            self.onClosed(code, reason)\n\n    def isConnected(self):\n        return self.__connected\n\n    def startClient(self):\n        tornado.ioloop.IOLoop.instance().start()\n\n    def stopClient(self):\n        try:\n            if self.__connected:\n                self.close()\n            self.close_connection()\n        except Exception, e:\n            logger.warning(\"Failed to close connection: %s\" % (e))\n\n    ######################################################################\n    # Overrides\n\n    def onUnhandledException(self, exception):\n        logger.critical(\"Unhandled exception\", exc_info=exception)\n        raise\n\n    def onOpened(self):\n        pass\n\n    def onMessage(self, msg):\n        raise NotImplementedError()\n\n    def onClosed(self, code, reason):\n        pass\n\n    def onDisconnectionDetected(self):\n        pass\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport json\nimport urllib\n\nimport pyalgotrade\nfrom pyalgotrade.websocket import client\nimport pyalgotrade.logger\n\n\nlogger = pyalgotrade.logger.getLogger(\"pusher\")\n\n\n# Pusher protocol reference: http://pusher.com/docs/pusher_protocol\n# Every message on a Pusher WebSocket connection is packaged as an event.\n# The data field is sent as a string (check 'Double encoding' in the protocol reference). If dataIsJSON is True, it is decoded.\nclass Event(object):\n    def __init__(self, eventDict, dataIsJSON):\n        self.__eventDict = eventDict\n        self.__data = eventDict.get(\"data\")\n        if self.__data is not None and dataIsJSON:\n            self.__data = json.loads(self.__data)\n\n    def __str__(self):\n        return str(self.__eventDict)\n\n    def getDict(self):\n        return self.__eventDict\n\n    def getData(self):\n        return self.__data\n\n    def getType(self):\n        return self.__eventDict.get(\"event\")\n\n\nclass PingKeepAliveMgr(client.KeepAliveMgr):\n    def __init__(self, wsClient, maxInactivity, responseTimeout):\n        super(PingKeepAliveMgr, self).__init__(wsClient, maxInactivity, responseTimeout)\n\n    # Override to send the keep alive msg.\n    def sendKeepAlive(self):\n        logger.debug(\"Sending pusher:ping.\")\n        self.getWSClient().sendPing()\n\n    # Return True if the response belongs to a keep alive message, False otherwise.\n    def handleResponse(self, msg):\n        ret = msg.get(\"event\") == \"pusher:pong\"\n        if ret:\n            logger.debug(\"Received pusher:pong.\")\n        return ret\n\n\nclass WebSocketClient(client.WebSocketClientBase):\n    def __init__(self, appKey, protocol=5, maxInactivity=120, responseTimeout=30):\n        params = {\n            \"protocol\": protocol,\n            \"client\": \"Python-PyAlgoTrade\",\n            \"version\": pyalgotrade.__version__\n            }\n        url = \"ws://ws.pusherapp.com/app/%s?%s\" % (appKey, urllib.urlencode(params))\n        super(WebSocketClient, self).__init__(url)\n        self.setKeepAliveMgr(PingKeepAliveMgr(self, maxInactivity, responseTimeout))\n\n    def sendEvent(self, eventType, eventData):\n        msgDict = {\"event\": eventType}\n        if eventData:\n            msgDict[\"data\"] = eventData\n        msg = json.dumps(msgDict)\n        self.send(msg, False)\n\n    def subscribeChannel(self, channel):\n        self.sendEvent(\"pusher:subscribe\", {\"channel\": channel})\n\n    def sendPing(self):\n        self.sendEvent(\"pusher:ping\", None)\n\n    def sendPong(self):\n        self.sendEvent(\"pusher:pong\", None)\n\n    def onMessage(self, msg):\n        eventType = msg.get(\"event\")\n\n        if eventType == \"pusher:error\":\n            self.onError(Event(msg, False))\n        elif eventType == \"pusher:ping\":\n            self.sendPong()\n        elif eventType == \"pusher:connection_established\":\n            self.onConnectionEstablished(Event(msg, True))\n        elif eventType == \"pusher_internal:subscription_succeeded\":\n            self.onSubscriptionSucceeded(Event(msg, True))\n        else:\n            # If we can't handle the message, notify the most concrete class.\n            self.onUnknownEvent(Event(msg, False))\n\n    ######################################################################\n    # Override for Pusher specific events.\n\n    def onConnectionEstablished(self, event):\n        pass\n\n    def onSubscriptionSucceeded(self, event):\n        pass\n\n    def onError(self, event):\n        raise NotImplementedError()\n\n    def onUnknownEvent(self, event):\n        raise NotImplementedError()\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.technical import bollinger\nfrom pyalgotrade.stratanalyzer import sharpe\n\n\nclass BBands(strategy.BacktestingStrategy):\n    def __init__(self, feed, instrument, bBandsPeriod):\n        super(BBands, self).__init__(feed)\n        self.__instrument = instrument\n        self.__bbands = bollinger.BollingerBands(feed[instrument].getCloseDataSeries(), bBandsPeriod, 2)\n\n    def getBollingerBands(self):\n        return self.__bbands\n\n    def onBars(self, bars):\n        lower = self.__bbands.getLowerBand()[-1]\n        upper = self.__bbands.getUpperBand()[-1]\n        if lower is None:\n            return\n\n        shares = self.getBroker().getShares(self.__instrument)\n        bar = bars[self.__instrument]\n        if shares == 0 and bar.getClose() < lower:\n            sharesToBuy = int(self.getBroker().getCash(False) / bar.getClose())\n            self.marketOrder(self.__instrument, sharesToBuy)\n        elif shares > 0 and bar.getClose() > upper:\n            self.marketOrder(self.__instrument, -1*shares)\n\n\ndef main(plot):\n    instrument = \"yhoo\"\n    bBandsPeriod = 40\n\n    # Download the bars.\n    feed = yahoofinance.build_feed([instrument], 2011, 2012, \".\")\n\n    strat = BBands(feed, instrument, bBandsPeriod)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, True, True, True)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"upper\", strat.getBollingerBands().getUpperBand())\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"middle\", strat.getBollingerBands().getMiddleBand())\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"lower\", strat.getBollingerBands().getLowerBand())\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade import bar\nfrom pyalgotrade import strategy\nfrom pyalgotrade import plotter\nfrom pyalgotrade.technical import vwap\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade.bitstamp import broker\nfrom pyalgotrade import broker as basebroker\n\n\nclass VWAPMomentum(strategy.BacktestingStrategy):\n    MIN_TRADE = 5\n\n    def __init__(self, feed, brk, instrument, vwapWindowSize, buyThreshold, sellThreshold):\n        super(VWAPMomentum, self).__init__(feed, brk)\n        self.__instrument = instrument\n        self.__vwap = vwap.VWAP(feed[instrument], vwapWindowSize)\n        self.__buyThreshold = buyThreshold\n        self.__sellThreshold = sellThreshold\n\n    def _getActiveOrders(self):\n        orders = self.getBroker().getActiveOrders()\n        buy = filter(lambda o: o.isBuy(), orders)\n        sell = filter(lambda o: o.isSell(), orders)\n        return buy, sell\n\n    def _cancelOrders(self, orders):\n        brk = self.getBroker()\n        for o in orders:\n            self.info(\"Canceling order %s\" % (o.getId()))\n            brk.cancelOrder(o)\n\n    def _buySignal(self, price):\n        buyOrders, sellOrders = self._getActiveOrders()\n        self._cancelOrders(sellOrders)\n\n        brk = self.getBroker()\n        cashAvail = brk.getCash() * 0.98\n        size = round(cashAvail / price, 3)\n        if len(buyOrders) == 0 and price*size > VWAPMomentum.MIN_TRADE:\n            self.info(\"Buy %s at %s\" % (size, price))\n            try:\n                self.limitOrder(self.__instrument, price, size)\n            except Exception, e:\n                self.error(\"Failed to buy: %s\" % (e))\n\n    def _sellSignal(self, price):\n        buyOrders, sellOrders = self._getActiveOrders()\n        self._cancelOrders(buyOrders)\n\n        brk = self.getBroker()\n        shares = brk.getShares(self.__instrument)\n        if len(sellOrders) == 0 and shares > 0:\n            self.info(\"Sell %s at %s\" % (shares, price))\n            self.limitOrder(self.__instrument, price, shares*-1)\n\n    def getVWAP(self):\n        return self.__vwap\n\n    def onBars(self, bars):\n        vwap = self.__vwap[-1]\n        if vwap is None:\n            return\n\n        price = bars[self.__instrument].getClose()\n        if price > vwap * (1 + self.__buyThreshold):\n            self._buySignal(price)\n        elif price < vwap * (1 - self.__sellThreshold):\n            self._sellSignal(price)\n\n    def onOrderUpdated(self, order):\n        if order.isBuy():\n            orderType = \"Buy\"\n        else:\n            orderType = \"Sell\"\n        self.info(\"%s order %d updated - Status: %s - %s\" % (\n            orderType,\n            order.getId(),\n            basebroker.Order.State.toString(order.getState()),\n            order.getExecutionInfo(),\n        ))\n\n\ndef main(plot):\n    instrument = \"BTC\"\n    initialCash = 1000\n    vwapWindowSize = 100\n    buyThreshold = 0.02\n    sellThreshold = 0.01\n\n    barFeed = csvfeed.GenericBarFeed(bar.Frequency.MINUTE*30)\n    barFeed.addBarsFromCSV(instrument, \"30min-bitstampUSD.csv\")\n    brk = broker.BacktestingBroker(initialCash, barFeed)\n    strat = VWAPMomentum(barFeed, brk, instrument, vwapWindowSize, buyThreshold, sellThreshold)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"VWAP\", strat.getVWAP())\n\n    strat.run()\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.stratanalyzer import returns\nfrom pyalgotrade.stratanalyzer import sharpe\nfrom pyalgotrade.utils import stats\n\n\nclass MyStrategy(strategy.BacktestingStrategy):\n    def __init__(self, feed):\n        super(MyStrategy, self).__init__(feed, 1000000)\n\n        # We wan't to use adjusted close prices instead of close.\n        self.setUseAdjustedValues(True)\n\n        # Place the orders to get them processed on the first bar.\n        orders = {\n            \"aeti\": 297810,\n            \"egan\": 81266,\n            \"glng\": 11095,\n            \"simo\": 17293,\n        }\n        for instrument, quantity in orders.items():\n            self.marketOrder(instrument, quantity, onClose=True, allOrNone=True)\n\n    def onBars(self, bars):\n        pass\n\n# Load the yahoo feed from CSV files.\nfeed = yahoofeed.Feed()\nfeed.addBarsFromCSV(\"aeti\", \"aeti-2011-yahoofinance.csv\")\nfeed.addBarsFromCSV(\"egan\", \"egan-2011-yahoofinance.csv\")\nfeed.addBarsFromCSV(\"glng\", \"glng-2011-yahoofinance.csv\")\nfeed.addBarsFromCSV(\"simo\", \"simo-2011-yahoofinance.csv\")\n\n# Evaluate the strategy with the feed's bars.\nmyStrategy = MyStrategy(feed)\n\n# Attach returns and sharpe ratio analyzers.\nretAnalyzer = returns.Returns()\nmyStrategy.attachAnalyzer(retAnalyzer)\nsharpeRatioAnalyzer = sharpe.SharpeRatio()\nmyStrategy.attachAnalyzer(sharpeRatioAnalyzer)\n\n# Run the strategy\nmyStrategy.run()\n\n# Print the results.\nprint \"Final portfolio value: $%.2f\" % myStrategy.getResult()\nprint \"Anual return: %.2f %%\" % (retAnalyzer.getCumulativeReturns()[-1] * 100)\nprint \"Average daily return: %.2f %%\" % (stats.mean(retAnalyzer.getReturns()) * 100)\nprint \"Std. dev. daily return: %.4f\" % (stats.stddev(retAnalyzer.getReturns()))\nprint \"Sharpe ratio: %.2f\" % (sharpeRatioAnalyzer.getSharpeRatio(0))\n",
          "import csv\nimport datetime\nimport os\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade import strategy\nfrom pyalgotrade.utils import stats\nfrom pyalgotrade.stratanalyzer import returns\nfrom pyalgotrade.stratanalyzer import sharpe\n\n\nclass OrdersFile:\n    def __init__(self, ordersFile):\n        self.__orders = {}\n        self.__firstDate = None\n        self.__lastDate = None\n        self.__instruments = []\n\n        # Load orders from the file.\n        reader = csv.DictReader(open(ordersFile, \"r\"), fieldnames=[\"year\", \"month\", \"day\", \"symbol\", \"action\", \"qty\"])\n        for row in reader:\n            dateTime = datetime.datetime(int(row[\"year\"]), int(row[\"month\"]), int(row[\"day\"]))\n            self.__orders.setdefault(dateTime, [])\n            order = (row[\"symbol\"], row[\"action\"], int(row[\"qty\"]))\n            self.__orders[dateTime].append(order)\n\n            # As we process the file, store instruments, first date, and last date.\n            if row[\"symbol\"] not in self.__instruments:\n                self.__instruments.append(row[\"symbol\"])\n\n            if self.__firstDate is None:\n                self.__firstDate = dateTime\n            else:\n                self.__firstDate = min(self.__firstDate, dateTime)\n\n            if self.__lastDate is None:\n                self.__lastDate = dateTime\n            else:\n                self.__lastDate = max(self.__lastDate, dateTime)\n\n    def getFirstDate(self):\n        return self.__firstDate\n\n    def getLastDate(self):\n        return self.__lastDate\n\n    def getInstruments(self):\n        return self.__instruments\n\n    def getOrders(self, dateTime):\n        return self.__orders.get(dateTime, [])\n\n\nclass MyStrategy(strategy.BacktestingStrategy):\n    def __init__(self, feed, cash, ordersFile, useAdjustedClose):\n        # Suscribe to the feed bars event before the broker just to place the orders properly.\n        feed.getNewValuesEvent().subscribe(self.__onBarsBeforeBroker)\n        super(MyStrategy, self).__init__(feed, cash)\n        self.__ordersFile = ordersFile\n        self.setUseAdjustedValues(useAdjustedClose)\n        # We will allow buying more shares than cash allows.\n        self.getBroker().setAllowNegativeCash(True)\n\n    def __onBarsBeforeBroker(self, dateTime, bars):\n        for instrument, action, quantity in self.__ordersFile.getOrders(dateTime):\n            if action.lower() == \"buy\":\n                self.marketOrder(instrument, quantity, onClose=True)\n            else:\n                self.marketOrder(instrument, quantity*-1, onClose=True)\n\n    def onOrderUpdated(self, order):\n        if order.isCanceled():\n            raise Exception(\"Order canceled. Ran out of cash ?\")\n\n    def onBars(self, bars):\n        portfolioValue = self.getBroker().getEquity()\n        self.info(\"Portfolio value: $%.2f\" % (portfolioValue))\n\n\ndef main():\n    # Load the orders file.\n    ordersFile = OrdersFile(\"orders.csv\")\n    print \"First date\", ordersFile.getFirstDate()\n    print \"Last date\", ordersFile.getLastDate()\n    print \"Symbols\", ordersFile.getInstruments()\n\n    # Load the data from QSTK storage. QS environment variable has to be defined.\n    if os.getenv(\"QS\") is None:\n        raise Exception(\"QS environment variable not defined\")\n    feed = yahoofeed.Feed()\n    feed.setBarFilter(csvfeed.DateRangeFilter(ordersFile.getFirstDate(), ordersFile.getLastDate()))\n    feed.setDailyBarTime(datetime.time(0, 0, 0))  # This is to match the dates loaded with the ones in the orders file.\n    for symbol in ordersFile.getInstruments():\n        feed.addBarsFromCSV(symbol, os.path.join(os.getenv(\"QS\"), \"QSData\", \"Yahoo\", symbol + \".csv\"))\n\n    # Run the strategy.\n    cash = 1000000\n    useAdjustedClose = True\n    myStrategy = MyStrategy(feed, cash, ordersFile, useAdjustedClose)\n\n    # Attach returns and sharpe ratio analyzers.\n    retAnalyzer = returns.Returns()\n    myStrategy.attachAnalyzer(retAnalyzer)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    myStrategy.attachAnalyzer(sharpeRatioAnalyzer)\n\n    myStrategy.run()\n\n    # Print the results.\n    print \"Final portfolio value: $%.2f\" % myStrategy.getResult()\n    print \"Anual return: %.2f %%\" % (retAnalyzer.getCumulativeReturns()[-1] * 100)\n    print \"Average daily return: %.2f %%\" % (stats.mean(retAnalyzer.getReturns()) * 100)\n    print \"Std. dev. daily return: %.4f\" % (stats.stddev(retAnalyzer.getReturns()))\n    print \"Sharpe ratio: %.2f\" % (sharpeRatioAnalyzer.getSharpeRatio(0))\n\nmain()\n",
          "from pyalgotrade.feed import csvfeed\n\nfeed = csvfeed.Feed(\"Date\", \"%Y-%m-%d\")\nfeed.addValuesFromCSV(\"quandl_gold_2.csv\")\nfor dateTime, value in feed:\n    print dateTime, value\n",
          "from pyalgotrade import eventprofiler\nfrom pyalgotrade.technical import stats\nfrom pyalgotrade.technical import roc\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade.tools import yahoofinance\n\n# Event inspired on an example from Ernie Chan's book:\n# 'Algorithmic Trading: Winning Strategies and Their Rationale'\n\n\nclass BuyOnGap(eventprofiler.Predicate):\n    def __init__(self, feed):\n        super(BuyOnGap, self).__init__()\n\n        stdDevPeriod = 90\n        smaPeriod = 20\n        self.__returns = {}\n        self.__stdDev = {}\n        self.__ma = {}\n        for instrument in feed.getRegisteredInstruments():\n            priceDS = feed[instrument].getAdjCloseDataSeries()\n            # Returns over the adjusted close values.\n            self.__returns[instrument] = roc.RateOfChange(priceDS, 1)\n            # StdDev over those returns.\n            self.__stdDev[instrument] = stats.StdDev(self.__returns[instrument], stdDevPeriod)\n            # MA over the adjusted close values.\n            self.__ma[instrument] = ma.SMA(priceDS, smaPeriod)\n\n    def __gappedDown(self, instrument, bards):\n        ret = False\n        if self.__stdDev[instrument][-1] is not None:\n            prevBar = bards[-2]\n            currBar = bards[-1]\n            low2OpenRet = (currBar.getOpen(True) - prevBar.getLow(True)) / float(prevBar.getLow(True))\n            if low2OpenRet < (self.__returns[instrument][-1] - self.__stdDev[instrument][-1]):\n                ret = True\n        return ret\n\n    def __aboveSMA(self, instrument, bards):\n        ret = False\n        if self.__ma[instrument][-1] is not None and bards[-1].getOpen(True) > self.__ma[instrument][-1]:\n            ret = True\n        return ret\n\n    def eventOccurred(self, instrument, bards):\n        ret = False\n        if self.__gappedDown(instrument, bards) and self.__aboveSMA(instrument, bards):\n            ret = True\n        return ret\n\n\ndef main(plot):\n    instruments = [\"AA\", \"AES\", \"AIG\"]\n    feed = yahoofinance.build_feed(instruments, 2008, 2009, \".\")\n\n    predicate = BuyOnGap(feed)\n    eventProfiler = eventprofiler.Profiler(predicate, 5, 5)\n    eventProfiler.run(feed, True)\n\n    results = eventProfiler.getResults()\n    print \"%d events found\" % (results.getEventCount())\n    if plot:\n        eventprofiler.plot(results)\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade.technical import cumret\nfrom pyalgotrade.stratanalyzer import sharpe\nfrom pyalgotrade.stratanalyzer import returns\n\n\nclass MarketTiming(strategy.BacktestingStrategy):\n    def __init__(self, feed, instrumentsByClass, initialCash):\n        super(MarketTiming, self).__init__(feed, initialCash)\n        self.setUseAdjustedValues(True)\n        self.__instrumentsByClass = instrumentsByClass\n        self.__rebalanceMonth = None\n        self.__sharesToBuy = {}\n        # Initialize indicators for each instrument.\n        self.__sma = {}\n        for assetClass in instrumentsByClass:\n            for instrument in instrumentsByClass[assetClass]:\n                priceDS = feed[instrument].getPriceDataSeries()\n                self.__sma[instrument] = ma.SMA(priceDS, 200)\n\n    def _shouldRebalance(self, dateTime):\n        return dateTime.month != self.__rebalanceMonth\n\n    def _getRank(self, instrument):\n        # If the price is below the SMA, then this instrument doesn't rank at\n        # all.\n        smas = self.__sma[instrument]\n        price = self.getLastPrice(instrument)\n        if len(smas) == 0 or smas[-1] is None or price < smas[-1]:\n            return None\n\n        # Rank based on 20 day returns.\n        ret = None\n        lookBack = 20\n        priceDS = self.getFeed()[instrument].getPriceDataSeries()\n        if len(priceDS) >= lookBack and smas[-1] is not None and smas[-1*lookBack] is not None:\n            ret = (priceDS[-1] - priceDS[-1*lookBack]) / float(priceDS[-1*lookBack])\n        return ret\n\n    def _getTopByClass(self, assetClass):\n        # Find the instrument with the highest rank.\n        ret = None\n        highestRank = None\n        for instrument in self.__instrumentsByClass[assetClass]:\n            rank = self._getRank(instrument)\n            if rank is not None and (highestRank is None or rank > highestRank):\n                highestRank = rank\n                ret = instrument\n        return ret\n\n    def _getTop(self):\n        ret = {}\n        for assetClass in self.__instrumentsByClass:\n            ret[assetClass] = self._getTopByClass(assetClass)\n        return ret\n\n    def _placePendingOrders(self):\n        remainingCash = self.getBroker().getCash() * 0.9  # Use less chash just in case price changes too much.\n\n        for instrument in self.__sharesToBuy:\n            orderSize = self.__sharesToBuy[instrument]\n            if orderSize > 0:\n                # Adjust the order size based on available cash.\n                lastPrice = self.getLastPrice(instrument)\n                cost = orderSize * lastPrice\n                while cost > remainingCash and orderSize > 0:\n                    orderSize -= 1\n                    cost = orderSize * lastPrice\n                if orderSize > 0:\n                    remainingCash -= cost\n                    assert(remainingCash >= 0)\n\n            if orderSize != 0:\n                self.info(\"Placing market order for %d %s shares\" % (orderSize, instrument))\n                self.marketOrder(instrument, orderSize, goodTillCanceled=True)\n                self.__sharesToBuy[instrument] -= orderSize\n\n    def _logPosSize(self):\n        totalEquity = self.getBroker().getEquity()\n        positions = self.getBroker().getPositions()\n        for instrument in self.getBroker().getPositions():\n            posSize = positions[instrument] * self.getLastPrice(instrument) / totalEquity * 100\n            self.info(\"%s - %0.2f %%\" % (instrument, posSize))\n\n    def _rebalance(self):\n        self.info(\"Rebalancing\")\n\n        # Cancel all active/pending orders.\n        for order in self.getBroker().getActiveOrders():\n            self.getBroker().cancelOrder(order)\n\n        cashPerAssetClass = self.getBroker().getEquity() / float(len(self.__instrumentsByClass))\n        self.__sharesToBuy = {}\n\n        # Calculate which positions should be open during the next period.\n        topByClass = self._getTop()\n        for assetClass in topByClass:\n            instrument = topByClass[assetClass]\n            self.info(\"Best for class %s: %s\" % (assetClass, instrument))\n            if instrument is not None:\n                lastPrice = self.getLastPrice(instrument)\n                cashForInstrument = cashPerAssetClass - self.getBroker().getShares(instrument) * lastPrice\n                # This may yield a negative value and we have to reduce this\n                # position.\n                self.__sharesToBuy[instrument] = int(cashForInstrument / lastPrice)\n\n        # Calculate which positions should be closed.\n        for instrument in self.getBroker().getPositions():\n            if instrument not in topByClass.values():\n                currentShares = self.getBroker().getShares(instrument)\n                assert(instrument not in self.__sharesToBuy)\n                self.__sharesToBuy[instrument] = currentShares * -1\n\n    def getSMA(self, instrument):\n        return self.__sma[instrument]\n\n    def onBars(self, bars):\n        currentDateTime = bars.getDateTime()\n\n        if self._shouldRebalance(currentDateTime):\n            self.__rebalanceMonth = currentDateTime.month\n            self._rebalance()\n\n        self._placePendingOrders()\n\n\ndef main(plot):\n    initialCash = 10000\n    instrumentsByClass = {\n        \"US Stocks\": [\"VTI\"],\n        \"Foreign Stocks\": [\"VEU\"],\n        \"US 10 Year Government Bonds\": [\"IEF\"],\n        \"Real Estate\": [\"VNQ\"],\n        \"Commodities\": [\"DBC\"],\n    }\n\n    # Download the bars.\n    instruments = [\"SPY\"]\n    for assetClass in instrumentsByClass:\n        instruments.extend(instrumentsByClass[assetClass])\n    feed = yahoofinance.build_feed(instruments, 2007, 2013, \"data\", skipErrors=True)\n\n    strat = MarketTiming(feed, instrumentsByClass, initialCash)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n    returnsAnalyzer = returns.Returns()\n    strat.attachAnalyzer(returnsAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, False, False, True)\n        plt.getOrCreateSubplot(\"cash\").addCallback(\"Cash\", lambda x: strat.getBroker().getCash())\n        # Plot strategy vs. SPY cumulative returns.\n        plt.getOrCreateSubplot(\"returns\").addDataSeries(\"SPY\", cumret.CumulativeReturn(feed[\"SPY\"].getPriceDataSeries()))\n        plt.getOrCreateSubplot(\"returns\").addDataSeries(\"Strategy\", returnsAnalyzer.getCumulativeReturns())\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n    print \"Returns: %.2f %%\" % (returnsAnalyzer.getCumulativeReturns()[-1] * 100)\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "import rsi2\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.stratanalyzer import sharpe\n\n\ndef main(plot):\n    instrument = \"DIA\"\n    entrySMA = 200\n    exitSMA = 5\n    rsiPeriod = 2\n    overBoughtThreshold = 90\n    overSoldThreshold = 10\n\n    # Download the bars.\n    feed = yahoofinance.build_feed([instrument], 2009, 2012, \".\")\n\n    strat = rsi2.RSI2(feed, instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, True, False, True)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"Entry SMA\", strat.getEntrySMA())\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"Exit SMA\", strat.getExitSMA())\n        plt.getOrCreateSubplot(\"rsi\").addDataSeries(\"RSI\", strat.getRSI())\n        plt.getOrCreateSubplot(\"rsi\").addLine(\"Overbought\", overBoughtThreshold)\n        plt.getOrCreateSubplot(\"rsi\").addLine(\"Oversold\", overSoldThreshold)\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.stratanalyzer import returns\nfrom pyalgotrade.stratanalyzer import sharpe\nfrom pyalgotrade.stratanalyzer import drawdown\nfrom pyalgotrade.stratanalyzer import trades\nimport sma_crossover\n\n# Load the yahoo feed from the CSV file\nfeed = yahoofeed.Feed()\nfeed.addBarsFromCSV(\"orcl\", \"orcl-2000.csv\")\n\n# Evaluate the strategy with the feed's bars.\nmyStrategy = sma_crossover.SMACrossOver(feed, \"orcl\", 20)\n\n# Attach different analyzers to a strategy before executing it.\nretAnalyzer = returns.Returns()\nmyStrategy.attachAnalyzer(retAnalyzer)\nsharpeRatioAnalyzer = sharpe.SharpeRatio()\nmyStrategy.attachAnalyzer(sharpeRatioAnalyzer)\ndrawDownAnalyzer = drawdown.DrawDown()\nmyStrategy.attachAnalyzer(drawDownAnalyzer)\ntradesAnalyzer = trades.Trades()\nmyStrategy.attachAnalyzer(tradesAnalyzer)\n\n# Run the strategy.\nmyStrategy.run()\n\nprint \"Final portfolio value: $%.2f\" % myStrategy.getResult()\nprint \"Cumulative returns: %.2f %%\" % (retAnalyzer.getCumulativeReturns()[-1] * 100)\nprint \"Sharpe ratio: %.2f\" % (sharpeRatioAnalyzer.getSharpeRatio(0.05))\nprint \"Max. drawdown: %.2f %%\" % (drawDownAnalyzer.getMaxDrawDown() * 100)\nprint \"Longest drawdown duration: %s\" % (drawDownAnalyzer.getLongestDrawDownDuration())\n\nprint\nprint \"Total trades: %d\" % (tradesAnalyzer.getCount())\nif tradesAnalyzer.getCount() > 0:\n    profits = tradesAnalyzer.getAll()\n    print \"Avg. profit: $%2.f\" % (profits.mean())\n    print \"Profits std. dev.: $%2.f\" % (profits.std())\n    print \"Max. profit: $%2.f\" % (profits.max())\n    print \"Min. profit: $%2.f\" % (profits.min())\n    returns = tradesAnalyzer.getAllReturns()\n    print \"Avg. return: %2.f %%\" % (returns.mean() * 100)\n    print \"Returns std. dev.: %2.f %%\" % (returns.std() * 100)\n    print \"Max. return: %2.f %%\" % (returns.max() * 100)\n    print \"Min. return: %2.f %%\" % (returns.min() * 100)\n\nprint\nprint \"Profitable trades: %d\" % (tradesAnalyzer.getProfitableCount())\nif tradesAnalyzer.getProfitableCount() > 0:\n    profits = tradesAnalyzer.getProfits()\n    print \"Avg. profit: $%2.f\" % (profits.mean())\n    print \"Profits std. dev.: $%2.f\" % (profits.std())\n    print \"Max. profit: $%2.f\" % (profits.max())\n    print \"Min. profit: $%2.f\" % (profits.min())\n    returns = tradesAnalyzer.getPositiveReturns()\n    print \"Avg. return: %2.f %%\" % (returns.mean() * 100)\n    print \"Returns std. dev.: %2.f %%\" % (returns.std() * 100)\n    print \"Max. return: %2.f %%\" % (returns.max() * 100)\n    print \"Min. return: %2.f %%\" % (returns.min() * 100)\n\nprint\nprint \"Unprofitable trades: %d\" % (tradesAnalyzer.getUnprofitableCount())\nif tradesAnalyzer.getUnprofitableCount() > 0:\n    losses = tradesAnalyzer.getLosses()\n    print \"Avg. loss: $%2.f\" % (losses.mean())\n    print \"Losses std. dev.: $%2.f\" % (losses.std())\n    print \"Max. loss: $%2.f\" % (losses.min())\n    print \"Min. loss: $%2.f\" % (losses.max())\n    returns = tradesAnalyzer.getNegativeReturns()\n    print \"Avg. return: %2.f %%\" % (returns.mean() * 100)\n    print \"Returns std. dev.: %2.f %%\" % (returns.std() * 100)\n    print \"Max. return: %2.f %%\" % (returns.max() * 100)\n    print \"Min. return: %2.f %%\" % (returns.min() * 100)\n",
          "import sma_crossover\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.stratanalyzer import sharpe\n\n\ndef main(plot):\n    instrument = \"aapl\"\n    smaPeriod = 163\n\n    # Download the bars.\n    feed = yahoofinance.build_feed([instrument], 2011, 2012, \".\")\n\n    strat = sma_crossover.SMACrossOver(feed, instrument, smaPeriod)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, True, False, True)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"sma\", strat.getSMA())\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.dataseries import aligned\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.stratanalyzer import sharpe\n\nimport numpy as np\nimport statsmodels.api as sm\n\n\ndef get_beta(values1, values2):\n    # http://statsmodels.sourceforge.net/stable/regression.html\n    model = sm.OLS(values1, values2)\n    results = model.fit()\n    return results.params[0]\n\n\nclass StatArbHelper:\n    def __init__(self, ds1, ds2, windowSize):\n        # We're going to use datetime aligned versions of the dataseries.\n        self.__ds1, self.__ds2 = aligned.datetime_aligned(ds1, ds2)\n        self.__windowSize = windowSize\n        self.__hedgeRatio = None\n        self.__spread = None\n        self.__spreadMean = None\n        self.__spreadStd = None\n        self.__zScore = None\n\n    def getSpread(self):\n        return self.__spread\n\n    def getSpreadMean(self):\n        return self.__spreadMean\n\n    def getSpreadStd(self):\n        return self.__spreadStd\n\n    def getZScore(self):\n        return self.__zScore\n\n    def getHedgeRatio(self):\n        return self.__hedgeRatio\n\n    def __updateHedgeRatio(self, values1, values2):\n        self.__hedgeRatio = get_beta(values1, values2)\n\n    def __updateSpreadMeanAndStd(self, values1, values2):\n        if self.__hedgeRatio is not None:\n            spread = values1 - values2 * self.__hedgeRatio\n            self.__spreadMean = spread.mean()\n            self.__spreadStd = spread.std(ddof=1)\n\n    def __updateSpread(self):\n        if self.__hedgeRatio is not None:\n            self.__spread = self.__ds1[-1] - self.__hedgeRatio * self.__ds2[-1]\n\n    def __updateZScore(self):\n        if self.__spread is not None and self.__spreadMean is not None and self.__spreadStd is not None:\n            self.__zScore = (self.__spread - self.__spreadMean) / float(self.__spreadStd)\n\n    def update(self):\n        if len(self.__ds1) >= self.__windowSize:\n            values1 = np.asarray(self.__ds1[-1*self.__windowSize:])\n            values2 = np.asarray(self.__ds2[-1*self.__windowSize:])\n            self.__updateHedgeRatio(values1, values2)\n            self.__updateSpread()\n            self.__updateSpreadMeanAndStd(values1, values2)\n            self.__updateZScore()\n\n\nclass StatArb(strategy.BacktestingStrategy):\n    def __init__(self, feed, instrument1, instrument2, windowSize):\n        super(StatArb, self).__init__(feed)\n        self.setUseAdjustedValues(True)\n        self.__statArbHelper = StatArbHelper(feed[instrument1].getAdjCloseDataSeries(), feed[instrument2].getAdjCloseDataSeries(), windowSize)\n        self.__i1 = instrument1\n        self.__i2 = instrument2\n\n        # These are used only for plotting purposes.\n        self.__spread = dataseries.SequenceDataSeries()\n        self.__hedgeRatio = dataseries.SequenceDataSeries()\n\n    def getSpreadDS(self):\n        return self.__spread\n\n    def getHedgeRatioDS(self):\n        return self.__hedgeRatio\n\n    def __getOrderSize(self, bars, hedgeRatio):\n        cash = self.getBroker().getCash(False)\n        price1 = bars[self.__i1].getAdjClose()\n        price2 = bars[self.__i2].getAdjClose()\n        size1 = int(cash / (price1 + hedgeRatio * price2))\n        size2 = int(size1 * hedgeRatio)\n        return (size1, size2)\n\n    def buySpread(self, bars, hedgeRatio):\n        amount1, amount2 = self.__getOrderSize(bars, hedgeRatio)\n        self.marketOrder(self.__i1, amount1)\n        self.marketOrder(self.__i2, amount2 * -1)\n\n    def sellSpread(self, bars, hedgeRatio):\n        amount1, amount2 = self.__getOrderSize(bars, hedgeRatio)\n        self.marketOrder(self.__i1, amount1 * -1)\n        self.marketOrder(self.__i2, amount2)\n\n    def reducePosition(self, instrument):\n        currentPos = self.getBroker().getShares(instrument)\n        if currentPos > 0:\n            self.marketOrder(instrument, currentPos * -1)\n        elif currentPos < 0:\n            self.marketOrder(instrument, currentPos * -1)\n\n    def onBars(self, bars):\n        self.__statArbHelper.update()\n\n        # These is used only for plotting purposes.\n        self.__spread.appendWithDateTime(bars.getDateTime(), self.__statArbHelper.getSpread())\n        self.__hedgeRatio.appendWithDateTime(bars.getDateTime(), self.__statArbHelper.getHedgeRatio())\n\n        if bars.getBar(self.__i1) and bars.getBar(self.__i2):\n            hedgeRatio = self.__statArbHelper.getHedgeRatio()\n            zScore = self.__statArbHelper.getZScore()\n            if zScore is not None:\n                currentPos = abs(self.getBroker().getShares(self.__i1)) + abs(self.getBroker().getShares(self.__i2))\n                if abs(zScore) <= 1 and currentPos != 0:\n                    self.reducePosition(self.__i1)\n                    self.reducePosition(self.__i2)\n                elif zScore <= -2 and currentPos == 0:  # Buy spread when its value drops below 2 standard deviations.\n                    self.buySpread(bars, hedgeRatio)\n                elif zScore >= 2 and currentPos == 0:  # Short spread when its value rises above 2 standard deviations.\n                    self.sellSpread(bars, hedgeRatio)\n\n\ndef main(plot):\n    instruments = [\"gld\", \"gdx\"]\n    windowSize = 50\n\n    # Download the bars.\n    feed = yahoofinance.build_feed(instruments, 2006, 2012, \".\")\n\n    strat = StatArb(feed, instruments[0], instruments[1], windowSize)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, False, False, True)\n        plt.getOrCreateSubplot(\"hedge\").addDataSeries(\"Hedge Ratio\", strat.getHedgeRatioDS())\n        plt.getOrCreateSubplot(\"spread\").addDataSeries(\"Spread\", strat.getSpreadDS())\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n\n    if plot:\n        plt.plot()\n\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "from pyalgotrade import dataseries\nfrom pyalgotrade import technical\n\n\n# An EventWindow is responsible for making calculations using a window of values.\nclass Accumulator(technical.EventWindow):\n    def getValue(self):\n        ret = None\n        if self.windowFull():\n            ret = self.getValues().sum()\n        return ret\n\n# Build a sequence based DataSeries.\nseqDS = dataseries.SequenceDataSeries()\n# Wrap it with a filter that will get fed as new values get added to the underlying DataSeries.\naccum = technical.EventBasedFilter(seqDS, Accumulator(3))\n\n# Put in some values.\nfor i in range(0, 50):\n    seqDS.append(i)\n\n# Get some values.\nprint accum[0]  # Not enough values yet.\nprint accum[1]  # Not enough values yet.\nprint accum[2]  # Ok, now we should have at least 3 values.\nprint accum[3]\n\n# Get the last value, which should be equal to 49 + 48 + 47.\nprint accum[-1]\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.technical import ma\n\n\nclass MyStrategy(strategy.BacktestingStrategy):\n    def __init__(self, feed, instrument, smaPeriod):\n        super(MyStrategy, self).__init__(feed, 1000)\n        self.__position = None\n        self.__instrument = instrument\n        # We'll use adjusted close values instead of regular close values.\n        self.setUseAdjustedValues(True)\n        self.__sma = ma.SMA(feed[instrument].getPriceDataSeries(), smaPeriod)\n\n    def onEnterOk(self, position):\n        execInfo = position.getEntryOrder().getExecutionInfo()\n        self.info(\"BUY at $%.2f\" % (execInfo.getPrice()))\n\n    def onEnterCanceled(self, position):\n        self.__position = None\n\n    def onExitOk(self, position):\n        execInfo = position.getExitOrder().getExecutionInfo()\n        self.info(\"SELL at $%.2f\" % (execInfo.getPrice()))\n        self.__position = None\n\n    def onExitCanceled(self, position):\n        # If the exit was canceled, re-submit it.\n        self.__position.exitMarket()\n\n    def onBars(self, bars):\n        # Wait for enough bars to be available to calculate a SMA.\n        if self.__sma[-1] is None:\n            return\n\n        bar = bars[self.__instrument]\n        # If a position was not opened, check if we should enter a long position.\n        if self.__position is None:\n            if bar.getPrice() > self.__sma[-1]:\n                # Enter a buy market order for 10 shares. The order is good till canceled.\n                self.__position = self.enterLong(self.__instrument, 10, True)\n        # Check if we have to exit the position.\n        elif bar.getPrice() < self.__sma[-1] and not self.__position.exitActive():\n            self.__position.exitMarket()\n\n\ndef run_strategy(smaPeriod):\n    # Load the yahoo feed from the CSV file\n    feed = yahoofeed.Feed()\n    feed.addBarsFromCSV(\"orcl\", \"orcl-2000.csv\")\n\n    # Evaluate the strategy with the feed.\n    myStrategy = MyStrategy(feed, \"orcl\", smaPeriod)\n    myStrategy.run()\n    print \"Final portfolio value: $%.2f\" % myStrategy.getBroker().getEquity()\n\nrun_strategy(15)\n",
          "import itertools\nfrom pyalgotrade.optimizer import local\nfrom pyalgotrade.barfeed import yahoofeed\nimport rsi2\n\n\ndef parameters_generator():\n    instrument = [\"dia\"]\n    entrySMA = range(150, 251)\n    exitSMA = range(5, 16)\n    rsiPeriod = range(2, 11)\n    overBoughtThreshold = range(75, 96)\n    overSoldThreshold = range(5, 26)\n    return itertools.product(instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold)\n\n\n# The if __name__ == '__main__' part is necessary if running on Windows.\nif __name__ == '__main__':\n    # Load the feed from the CSV files.\n    feed = yahoofeed.Feed()\n    feed.addBarsFromCSV(\"dia\", \"dia-2009.csv\")\n    feed.addBarsFromCSV(\"dia\", \"dia-2010.csv\")\n    feed.addBarsFromCSV(\"dia\", \"dia-2011.csv\")\n\n    local.run(rsi2.RSI2, feed, parameters_generator())\n",
          "import itertools\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.optimizer import server\n\n\ndef parameters_generator():\n    instrument = [\"dia\"]\n    entrySMA = range(150, 251)\n    exitSMA = range(5, 16)\n    rsiPeriod = range(2, 11)\n    overBoughtThreshold = range(75, 96)\n    overSoldThreshold = range(5, 26)\n    return itertools.product(instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold)\n\n# The if __name__ == '__main__' part is necessary if running on Windows.\nif __name__ == '__main__':\n    # Load the feed from the CSV files.\n    feed = yahoofeed.Feed()\n    feed.addBarsFromCSV(\"dia\", \"dia-2009.csv\")\n    feed.addBarsFromCSV(\"dia\", \"dia-2010.csv\")\n    feed.addBarsFromCSV(\"dia\", \"dia-2011.csv\")\n\n    # Run the server.\n    server.serve(feed, parameters_generator(), \"localhost\", 5000)\n",
          "from pyalgotrade import strategy\nfrom pyalgotrade import plotter\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade.technical import vwap\nfrom pyalgotrade.stratanalyzer import sharpe\n\n\nclass VWAPMomentum(strategy.BacktestingStrategy):\n    def __init__(self, feed, instrument, vwapWindowSize, threshold):\n        super(VWAPMomentum, self).__init__(feed)\n        self.__instrument = instrument\n        self.__vwap = vwap.VWAP(feed[instrument], vwapWindowSize)\n        self.__threshold = threshold\n\n    def getVWAP(self):\n        return self.__vwap\n\n    def onBars(self, bars):\n        vwap = self.__vwap[-1]\n        if vwap is None:\n            return\n\n        shares = self.getBroker().getShares(self.__instrument)\n        price = bars[self.__instrument].getClose()\n        notional = shares * price\n\n        if price > vwap * (1 + self.__threshold) and notional < 1000000:\n            self.marketOrder(self.__instrument, 100)\n        elif price < vwap * (1 - self.__threshold) and notional > 0:\n            self.marketOrder(self.__instrument, -100)\n\n\ndef main(plot):\n    instrument = \"aapl\"\n    vwapWindowSize = 5\n    threshold = 0.01\n\n    # Download the bars.\n    feed = yahoofinance.build_feed([instrument], 2011, 2012, \".\")\n\n    strat = VWAPMomentum(feed, instrument, vwapWindowSize, threshold)\n    sharpeRatioAnalyzer = sharpe.SharpeRatio()\n    strat.attachAnalyzer(sharpeRatioAnalyzer)\n\n    if plot:\n        plt = plotter.StrategyPlotter(strat, True, False, True)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"vwap\", strat.getVWAP())\n\n    strat.run()\n    print \"Sharpe ratio: %.2f\" % sharpeRatioAnalyzer.getSharpeRatio(0.05)\n\n    if plot:\n        plt.plot()\n\nif __name__ == \"__main__\":\n    main(True)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport cPickle\n\nimport common\n\nfrom pyalgotrade import bar\n\n\nclass BasicBarTestCase(common.TestCase):\n    def testInvalidConstruction(self):\n        with self.assertRaises(Exception):\n            bar.BasicBar(datetime.datetime.now(), 2, 1, 1, 1, 1, 1, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            bar.BasicBar(datetime.datetime.now(), 1, 1, 1, 2, 1, 1, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            bar.BasicBar(datetime.datetime.now(), 1, 2, 1.5, 1, 1, 1, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            bar.BasicBar(datetime.datetime.now(), 2, 2, 1.5, 1, 1, 1, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            bar.BasicBar(datetime.datetime.now(), 1, 1, 1.5, 1, 1, 1, bar.Frequency.DAY)\n\n    def testTypicalPrice(self):\n        b = bar.BasicBar(datetime.datetime.now(), 2, 3, 1, 2.1, 10, 5, bar.Frequency.DAY)\n        self.assertEquals(b.getTypicalPrice(), (3 + 1 + 2.1) / 3)\n\n    def testGetPrice(self):\n        b = bar.BasicBar(datetime.datetime.now(), 2, 3, 1, 2.1, 10, 5, bar.Frequency.DAY)\n        self.assertEquals(b.getPrice(), b.getClose())\n        b.setUseAdjustedValue(True)\n        self.assertEquals(b.getPrice(), b.getAdjClose())\n\n    def testPickle(self):\n        b1 = bar.BasicBar(datetime.datetime.now(), 2, 3, 1, 2.1, 10, 5, bar.Frequency.DAY)\n        b2 = cPickle.loads(cPickle.dumps(b1))\n        self.assertEquals(b1.getDateTime(), b2.getDateTime())\n        self.assertEquals(b1.getOpen(), b2.getOpen())\n        self.assertEquals(b1.getHigh(), b2.getHigh())\n        self.assertEquals(b1.getLow(), b2.getLow())\n        self.assertEquals(b1.getClose(), b2.getClose())\n        self.assertEquals(b1.getVolume(), b2.getVolume())\n        self.assertEquals(b1.getAdjClose(), b2.getAdjClose())\n        self.assertEquals(b1.getFrequency(), b2.getFrequency())\n        self.assertEquals(b1.getPrice(), b2.getPrice())\n        self.assertEquals(b1.getOpen(True), b2.getOpen(True))\n        self.assertEquals(b1.getHigh(True), b2.getHigh(True))\n        self.assertEquals(b1.getLow(True), b2.getLow(True))\n        self.assertEquals(b1.getClose(True), b2.getClose(True))\n\n    def testNoAdjClose(self):\n        b = bar.BasicBar(datetime.datetime.now(), 2, 3, 1, 2.1, 10, None, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            b.setUseAdjustedValue(True)\n        with self.assertRaises(Exception):\n            b.getOpen(True)\n        with self.assertRaises(Exception):\n            b.getHigh(True)\n        with self.assertRaises(Exception):\n            b.getLow(True)\n        with self.assertRaises(Exception):\n            b.getClose(True)\n\n\nclass BarsTestCase(common.TestCase):\n    def testEmptyDict(self):\n        with self.assertRaises(Exception):\n            bar.Bars({})\n\n    def testInvalidDateTimes(self):\n        b1 = bar.BasicBar(datetime.datetime.now(), 2, 3, 1, 2.1, 10, 5, bar.Frequency.DAY)\n        b2 = bar.BasicBar(datetime.datetime.now() + datetime.timedelta(days=1), 2, 3, 1, 2.1, 10, 5, bar.Frequency.DAY)\n        with self.assertRaises(Exception):\n            bar.Bars({\"a\": b1, \"b\": b2})\n\n    def testBasic(self):\n        dt = datetime.datetime.now()\n        b1 = bar.BasicBar(dt, 1, 1, 1, 1, 10, 1, bar.Frequency.DAY)\n        b2 = bar.BasicBar(dt, 2, 2, 2, 2, 10, 2, bar.Frequency.DAY)\n        bars = bar.Bars({\"a\": b1, \"b\": b2})\n        self.assertEquals(bars[\"a\"].getClose(), 1)\n        self.assertEquals(bars[\"b\"].getClose(), 2)\n        self.assertTrue(\"a\" in bars)\n        self.assertEquals(bars.items(), [(\"a\", b1), (\"b\", b2)])\n        self.assertEquals(bars.keys(), [\"a\", \"b\"])\n        self.assertEquals(bars.getInstruments(), [\"a\", \"b\"])\n        self.assertEquals(bars.getDateTime(), dt)\n        self.assertEquals(bars.getBar(\"a\").getClose(), 1)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import barfeed\nfrom pyalgotrade.barfeed import common as bfcommon\nfrom pyalgotrade import bar\nfrom pyalgotrade import dispatcher\n\n\ndef check_base_barfeed(testCase, barFeed, barsHaveAdjClose):\n    called = {\"called\": True}\n\n    def callback(dateTime, bars):\n        called[\"called\"] = True\n        testCase.assertEquals(barFeed.getCurrentDateTime(), dateTime)\n\n    testCase.assertEquals(barFeed.getCurrentDateTime(), None)\n    testCase.assertEquals(barFeed.barsHaveAdjClose(), barsHaveAdjClose)\n    if not barsHaveAdjClose:\n        with testCase.assertRaisesRegexp(Exception, \"The barfeed doesn't support adjusted close values.*\"):\n            barFeed.setUseAdjustedValues(True)\n\n    d = dispatcher.Dispatcher()\n    d.addSubject(barFeed)\n    barFeed.getNewValuesEvent().subscribe(callback)\n    d.run()\n\n    testCase.assertEquals(called[\"called\"], True)\n\n\nclass OptimizerBarFeedTestCase(common.TestCase):\n    def testDateTimesNotInOrder(self):\n        bars = [\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 2), 1, 1, 1, 1, 1, 1, bar.Frequency.DAY)}),\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 1), 1, 1, 1, 1, 1, 1, bar.Frequency.DAY)}),\n        ]\n        f = barfeed.OptimizerBarFeed(bar.Frequency.DAY, [\"orcl\"], bars)\n        with self.assertRaisesRegexp(Exception, \"Bar date times are not in order.*\"):\n            for dt, b in f:\n                pass\n\n    def testBaseBarFeed(self):\n        bars = [\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 1), 1, 1, 1, 1, 1, 1, bar.Frequency.DAY)}),\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 2), 1, 1, 1, 1, 1, 1, bar.Frequency.DAY)}),\n        ]\n        barFeed = barfeed.OptimizerBarFeed(bar.Frequency.DAY, [\"orcl\"], bars)\n        check_base_barfeed(self, barFeed, True)\n\n    def testBaseBarFeedNoAdjClose(self):\n        bars = [\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 1), 1, 1, 1, 1, 1, None, bar.Frequency.DAY)}),\n            bar.Bars({\"orcl\": bar.BasicBar(datetime.datetime(2001, 1, 2), 1, 1, 1, 1, 1, None, bar.Frequency.DAY)}),\n        ]\n        barFeed = barfeed.OptimizerBarFeed(bar.Frequency.DAY, [\"orcl\"], bars)\n        check_base_barfeed(self, barFeed, False)\n\n    def testEmtpy(self):\n        barFeed = barfeed.OptimizerBarFeed(bar.Frequency.DAY, [\"orcl\"], [])\n        self.assertEquals(barFeed.barsHaveAdjClose(), False)\n\n\nclass CommonTestCase(common.TestCase):\n    def testSanitize(self):\n        self.assertEqual(bfcommon.sanitize_ohlc(10, 12, 9, 10), (10, 12, 9, 10))\n        self.assertEqual(bfcommon.sanitize_ohlc(10, 12, 9, 13), (10, 13, 9, 13))\n        self.assertEqual(bfcommon.sanitize_ohlc(10, 9, 9, 10), (10, 10, 9, 10))\n        self.assertEqual(bfcommon.sanitize_ohlc(10, 12, 11, 10), (10, 12, 10, 10))\n        self.assertEqual(bfcommon.sanitize_ohlc(10, 12, 10, 9), (10, 12, 9, 9))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport time\nimport threading\nimport Queue\nimport json\n\nimport common as tc_common\nimport test_strategy\n\nfrom pyalgotrade import broker as basebroker\nfrom pyalgotrade.bitstamp import barfeed\nfrom pyalgotrade.bitstamp import broker\nfrom pyalgotrade.bitstamp import wsclient\nfrom pyalgotrade.bitstamp import httpclient\nfrom pyalgotrade.bitstamp import common\nfrom pyalgotrade.bitcoincharts import barfeed as btcbarfeed\nfrom pyalgotrade import strategy\nfrom pyalgotrade import dispatcher\n\n\nclass WebSocketClientThreadMock(threading.Thread):\n    def __init__(self, events):\n        threading.Thread.__init__(self)\n        self.__queue = Queue.Queue()\n        self.__queue.put((wsclient.WebSocketClient.ON_CONNECTED, None))\n        for event in events:\n            self.__queue.put(event)\n        self.__queue.put((wsclient.WebSocketClient.ON_DISCONNECTED, None))\n        self.__stop = False\n\n    def getQueue(self):\n        return self.__queue\n\n    def start(self):\n        threading.Thread.start(self)\n\n    def run(self):\n        while not self.__queue.empty() and not self.__stop:\n            time.sleep(0.01)\n\n    def stop(self):\n        self.__stop = True\n\n\nclass TestingLiveTradeFeed(barfeed.LiveTradeFeed):\n    def __init__(self):\n        barfeed.LiveTradeFeed.__init__(self)\n        # Disable reconnections so the test finishes when ON_DISCONNECTED is pushed.\n        self.enableReconection(False)\n        self.__events = []\n\n    def addTrade(self, dateTime, tid, price, amount):\n        dataDict = {\n            \"id\": tid,\n            \"price\": price,\n            \"amount\": amount,\n            \"type\": 0,\n            }\n        eventDict = {}\n        eventDict[\"data\"] = json.dumps(dataDict)\n        self.__events.append((wsclient.WebSocketClient.ON_TRADE, wsclient.Trade(dateTime, eventDict)))\n\n    def buildWebSocketClientThread(self):\n        return WebSocketClientThreadMock(self.__events)\n\n\nclass HTTPClientMock(object):\n    class UserTransactionType:\n        MARKET_TRADE = 2\n\n    def __init__(self):\n        self.__userTransactions = []\n        self.__openOrders = []\n        self.__btcAvailable = 0.0\n        self.__usdAvailable = 0.0\n        self.__nextTxId = 1\n        self.__nextOrderId = 1000\n        self.__userTransactionsRequested = False\n\n    def setUSDAvailable(self, usd):\n        self.__usdAvailable = usd\n\n    def setBTCAvailable(self, btc):\n        self.__btcAvailable = btc\n\n    def addOpenOrder(self, orderId, btcAmount, usdAmount):\n        jsonDict = {\n            'id': orderId,\n            'datetime': str(datetime.datetime.now()),\n            'type': 0 if btcAmount > 0 else 1,\n            'price': str(usdAmount),\n            'amount': str(abs(btcAmount)),\n        }\n        self.__openOrders.append(jsonDict)\n\n    def addUserTransaction(self, orderId, btcAmount, usdAmount, fillPrice, fee):\n        jsonDict = {\n            'btc': str(btcAmount),\n            'btc_usd': str(fillPrice),\n            'datetime': str(datetime.datetime.now()),\n            'fee': str(fee),\n            'id': self.__nextTxId,\n            'order_id': orderId,\n            'type': 2,\n            'usd': str(usdAmount)\n        }\n        self.__userTransactions.insert(0, jsonDict)\n        self.__nextTxId += 1\n\n    def getAccountBalance(self):\n        jsonDict = {\n            'btc_available': str(self.__btcAvailable),\n            # 'btc_balance': '0',\n            # 'btc_reserved': '0',\n            # 'fee': '0.5000',\n            'usd_available': str(self.__usdAvailable),\n            # 'usd_balance': '0.00',\n            # 'usd_reserved': '0'\n        }\n        return httpclient.AccountBalance(jsonDict)\n\n    def getOpenOrders(self):\n        return [httpclient.Order(jsonDict) for jsonDict in self.__openOrders]\n\n    def cancelOrder(self, orderId):\n        pass\n\n    def _buildOrder(self, price, amount):\n        jsonDict = {\n            'id': self.__nextOrderId,\n            'datetime': str(datetime.datetime.now()),\n            'type': 0 if amount > 0 else 1,\n            'price': str(price),\n            'amount': str(abs(amount)),\n        }\n        self.__nextOrderId += 1\n        return httpclient.Order(jsonDict)\n\n    def buyLimit(self, limitPrice, quantity):\n        assert(quantity > 0)\n        return self._buildOrder(limitPrice, quantity)\n\n    def sellLimit(self, limitPrice, quantity):\n        assert(quantity > 0)\n        return self._buildOrder(limitPrice, quantity)\n\n    def getUserTransactions(self, transactionType=None):\n        # The first call is to retrieve user transactions that should have been\n        # processed already.\n        if not self.__userTransactionsRequested:\n            self.__userTransactionsRequested = True\n            return []\n        else:\n            return [httpclient.UserTransaction(jsonDict) for jsonDict in self.__userTransactions]\n\n\nclass TestingLiveBroker(broker.LiveBroker):\n    def __init__(self, clientId, key, secret):\n        self.__httpClient = HTTPClientMock()\n        broker.LiveBroker.__init__(self, clientId, key, secret)\n\n    def buildHTTPClient(self, clientId, key, secret):\n        return self.__httpClient\n\n    def getHTTPClient(self):\n        return self.__httpClient\n\n\nclass TestStrategy(test_strategy.BaseTestStrategy):\n    def __init__(self, feed, brk):\n        super(TestStrategy, self).__init__(feed, brk)\n        self.bid = None\n        self.ask = None\n\n        # Subscribe to order book update events to get bid/ask prices to trade.\n        feed.getOrderBookUpdateEvent().subscribe(self.__onOrderBookUpdate)\n\n    def __onOrderBookUpdate(self, orderBookUpdate):\n        bid = orderBookUpdate.getBidPrices()[0]\n        ask = orderBookUpdate.getAskPrices()[0]\n\n        if bid != self.bid or ask != self.ask:\n            self.bid = bid\n            self.ask = ask\n\n\nclass InstrumentTraitsTestCase(tc_common.TestCase):\n    def testInstrumentTraits(self):\n        traits = common.BTCTraits()\n        self.assertEquals(traits.roundQuantity(0), 0)\n        self.assertEquals(traits.roundQuantity(1), 1)\n        self.assertEquals(traits.roundQuantity(1.1 + 1.1 + 1.1), 3.3)\n        self.assertEquals(traits.roundQuantity(1.1 + 1.1 + 1.1 - 3.3), 0)\n        self.assertEquals(traits.roundQuantity(0.00441376), 0.00441376)\n        self.assertEquals(traits.roundQuantity(0.004413764), 0.00441376)\n\n\nclass BacktestingTestCase(tc_common.TestCase):\n    def testBitcoinChartsFeed(self):\n\n        class TestStrategy(strategy.BaseStrategy):\n            def __init__(self, feed, brk):\n                strategy.BaseStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if not self.pos:\n                    self.pos = self.enterLongLimit(\"BTC\", 5.83, 1, True)\n\n        barFeed = btcbarfeed.CSVTradeFeed()\n        barFeed.addBarsFromCSV(tc_common.get_data_file_path(\"bitstampUSD.csv\"))\n        brk = broker.BacktestingBroker(100, barFeed)\n        strat = TestStrategy(barFeed, brk)\n        strat.run()\n        self.assertEquals(strat.pos.getShares(), 1)\n        self.assertEquals(strat.pos.entryActive(), False)\n        self.assertEquals(strat.pos.isOpen(), True)\n        self.assertEquals(strat.pos.getEntryOrder().getAvgFillPrice(), 5.83)\n\n    def testMinTrade(self):\n        class TestStrategy(strategy.BaseStrategy):\n            def __init__(self, feed, brk):\n                strategy.BaseStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if not self.pos:\n                    self.pos = self.enterLongLimit(\"BTC\", 4.99, 1, True)\n\n        barFeed = btcbarfeed.CSVTradeFeed()\n        barFeed.addBarsFromCSV(tc_common.get_data_file_path(\"bitstampUSD.csv\"))\n        brk = broker.BacktestingBroker(100, barFeed)\n        strat = TestStrategy(barFeed, brk)\n        with self.assertRaisesRegexp(Exception, \"Trade must be >= 5\"):\n            strat.run()\n\n\nclass PaperTradingTestCase(tc_common.TestCase):\n    def testBuyWithPartialFill(self):\n\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterLongLimit(\"BTC\", 100, 1, True)\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 101, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 0.2)\n\n        brk = broker.PaperTradingBroker(1000, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertTrue(strat.pos.isOpen())\n        self.assertEquals(round(strat.pos.getShares(), 3), 0.3)\n        self.assertEquals(len(strat.posExecutionInfo), 1)\n        self.assertEquals(strat.pos.getEntryOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n\n    def testBuyAndSellWithPartialFill1(self):\n\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterLongLimit(\"BTC\", 100, 1, True)\n                elif bars.getDateTime() == datetime.datetime(2000, 1, 3):\n                    self.pos.exitLimit(101)\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 101, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 0.2)\n        barFeed.addTrade(datetime.datetime(2000, 1, 4), 1, 100, 0.2)\n        barFeed.addTrade(datetime.datetime(2000, 1, 5), 1, 101, 0.2)\n\n        brk = broker.PaperTradingBroker(1000, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertTrue(strat.pos.isOpen())\n        self.assertEquals(round(strat.pos.getShares(), 3), 0.1)\n        self.assertEquals(len(strat.posExecutionInfo), 1)\n        self.assertEquals(strat.pos.getEntryOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n        self.assertEquals(strat.pos.getExitOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n\n    def testBuyAndSellWithPartialFill2(self):\n\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterLongLimit(\"BTC\", 100, 1, True)\n                elif bars.getDateTime() == datetime.datetime(2000, 1, 3):\n                    self.pos.exitLimit(101)\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 101, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 0.2)\n        barFeed.addTrade(datetime.datetime(2000, 1, 4), 1, 100, 0.2)\n        barFeed.addTrade(datetime.datetime(2000, 1, 5), 1, 101, 0.2)\n        barFeed.addTrade(datetime.datetime(2000, 1, 6), 1, 102, 5)\n\n        brk = broker.PaperTradingBroker(1000, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertFalse(strat.pos.isOpen())\n        self.assertEquals(strat.pos.getShares(), 0)\n        self.assertEquals(len(strat.posExecutionInfo), 2)\n        self.assertEquals(strat.pos.getEntryOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n        self.assertEquals(strat.pos.getExitOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n\n    def testRoundingBugWithTrades(self):\n        # Unless proper rounding is in place 0.01 - 0.00441376 - 0.00445547 - 0.00113077 == 6.50521303491e-19\n        # instead of 0.\n\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterLongLimit(\"BTC\", 1000, 0.01, True)\n                elif self.pos.entryFilled() and not self.pos.getExitOrder():\n                    self.pos.exitLimit(1000, True)\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 1000, 1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 1000, 0.01)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 1000, 0.00441376)\n        barFeed.addTrade(datetime.datetime(2000, 1, 4), 1, 1000, 0.00445547)\n        barFeed.addTrade(datetime.datetime(2000, 1, 5), 1, 1000, 0.00113077)\n\n        brk = broker.PaperTradingBroker(1000, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(brk.getShares(\"BTC\"), 0)\n        self.assertEquals(strat.pos.getEntryOrder().getAvgFillPrice(), 1000)\n        self.assertEquals(strat.pos.getExitOrder().getAvgFillPrice(), 1000)\n        self.assertEquals(strat.pos.getEntryOrder().getFilled(), 0.01)\n        self.assertEquals(strat.pos.getExitOrder().getFilled(), 0.01)\n        self.assertEquals(strat.pos.getEntryOrder().getRemaining(), 0)\n        self.assertEquals(strat.pos.getExitOrder().getRemaining(), 0)\n        self.assertEquals(strat.pos.getEntryOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n        self.assertEquals(strat.pos.getExitOrder().getSubmitDateTime().date(), wsclient.get_current_datetime().date())\n\n        self.assertFalse(strat.pos.isOpen())\n        self.assertEquals(len(strat.posExecutionInfo), 2)\n        self.assertEquals(strat.pos.getShares(), 0.0)\n\n    def testInvalidOrders(self):\n        barFeed = TestingLiveTradeFeed()\n        brk = broker.PaperTradingBroker(1000, barFeed)\n        with self.assertRaises(Exception):\n            brk.createLimitOrder(basebroker.Order.Action.BUY, \"none\", 1, 1)\n        with self.assertRaises(Exception):\n            brk.createLimitOrder(basebroker.Order.Action.SELL_SHORT, \"none\", 1, 1)\n        with self.assertRaises(Exception):\n            brk.createMarketOrder(basebroker.Order.Action.BUY, \"none\", 1)\n        with self.assertRaises(Exception):\n            brk.createStopOrder(basebroker.Order.Action.BUY, \"none\", 1, 1)\n        with self.assertRaises(Exception):\n            brk.createStopLimitOrder(basebroker.Order.Action.BUY, \"none\", 1, 1, 1)\n\n    def testBuyWithoutCash(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.errors = 0\n\n            def onBars(self, bars):\n                try:\n                    self.limitOrder(\"BTC\", 10, 1)\n                except Exception:\n                    self.errors += 1\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 0.1)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 101, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 0.2)\n\n        brk = broker.PaperTradingBroker(0, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(strat.errors, 4)\n        self.assertEquals(brk.getShares(\"BTC\"), 0)\n        self.assertEquals(brk.getCash(), 0)\n\n    def testRanOutOfCash(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.errors = 0\n\n            def onBars(self, bars):\n                try:\n                    self.limitOrder(\"BTC\", 100, 0.1)\n                except Exception:\n                    self.errors += 1\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 10)\n\n        brk = broker.PaperTradingBroker(10.025, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(strat.errors, 2)\n        self.assertEquals(brk.getShares(\"BTC\"), 0.1)\n        self.assertEquals(brk.getCash(), 0)\n\n    def testSellWithoutBTC(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.errors = 0\n\n            def onBars(self, bars):\n                try:\n                    self.limitOrder(\"BTC\", 100, -0.1)\n                except Exception:\n                    self.errors += 1\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 10)\n\n        brk = broker.PaperTradingBroker(0, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(strat.errors, 2)\n        self.assertEquals(brk.getShares(\"BTC\"), 0)\n        self.assertEquals(brk.getCash(), 0)\n\n    def testRanOutOfCoins(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.errors = 0\n                self.bought = False\n\n            def onBars(self, bars):\n                if not self.bought:\n                    self.limitOrder(\"BTC\", 100, 0.1)\n                    self.bought = True\n                else:\n                    try:\n                        self.limitOrder(\"BTC\", 100, -0.1)\n                    except Exception:\n                        self.errors += 1\n\n        barFeed = TestingLiveTradeFeed()\n        barFeed.addTrade(datetime.datetime(2000, 1, 1), 1, 100, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 2), 1, 100, 10)\n        barFeed.addTrade(datetime.datetime(2000, 1, 3), 1, 100, 10)\n\n        brk = broker.PaperTradingBroker(10.05, barFeed)\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(strat.errors, 1)\n        self.assertEquals(brk.getShares(\"BTC\"), 0)\n        self.assertEquals(brk.getCash(), 10)\n\n\nclass LiveTradingTestCase(tc_common.TestCase):\n    def testMapUserTransactionsToOrderEvents(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n\n            def onBars(self, bars):\n                self.stop()\n\n        barFeed = TestingLiveTradeFeed()\n        # This is to hit onBars and stop strategy execution.\n        barFeed.addTrade(datetime.datetime.now(), 1, 100, 1)\n\n        brk = TestingLiveBroker(None, None, None)\n        httpClient = brk.getHTTPClient()\n        httpClient.setUSDAvailable(0)\n        httpClient.setBTCAvailable(0.1)\n\n        httpClient.addOpenOrder(1, -0.1, 578.79)\n        httpClient.addOpenOrder(2, 0.1, 567.21)\n\n        httpClient.addUserTransaction(1, -0.04557395, 26.38, 578.79, 0.14)\n        httpClient.addUserTransaction(2, 0.04601436, -26.10, 567.21, 0.14)\n\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(len(strat.orderExecutionInfo), 2)\n        self.assertEquals(strat.orderExecutionInfo[0].getPrice(), 578.79)\n        self.assertEquals(strat.orderExecutionInfo[0].getQuantity(), 0.04557395)\n        self.assertEquals(strat.orderExecutionInfo[0].getCommission(), 0.14)\n        self.assertEquals(strat.orderExecutionInfo[0].getDateTime().date(), datetime.datetime.now().date())\n        self.assertEquals(strat.orderExecutionInfo[1].getPrice(), 567.21)\n        self.assertEquals(strat.orderExecutionInfo[1].getQuantity(), 0.04601436)\n        self.assertEquals(strat.orderExecutionInfo[1].getCommission(), 0.14)\n        self.assertEquals(strat.orderExecutionInfo[1].getDateTime().date(), datetime.datetime.now().date())\n\n    def testCancelOrder(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n\n            def onBars(self, bars):\n                order = self.getBroker().getActiveOrders()[0]\n                self.getBroker().cancelOrder(order)\n                self.stop()\n\n        barFeed = TestingLiveTradeFeed()\n        # This is to hit onBars and stop strategy execution.\n        barFeed.addTrade(datetime.datetime.now(), 1, 100, 1)\n\n        brk = TestingLiveBroker(None, None, None)\n        httpClient = brk.getHTTPClient()\n        httpClient.setUSDAvailable(0)\n        httpClient.setBTCAvailable(0)\n        httpClient.addOpenOrder(1, 0.1, 578.79)\n\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertEquals(brk.getShares(\"BTC\"), 0)\n        self.assertEquals(brk.getCash(), 0)\n        self.assertEquals(len(strat.orderExecutionInfo), 1)\n        self.assertEquals(strat.orderExecutionInfo[0], None)\n        self.assertEquals(len(strat.ordersUpdated), 1)\n        self.assertTrue(strat.ordersUpdated[0].isCanceled())\n\n    def testBuyAndSell(self):\n        class Strategy(TestStrategy):\n            def __init__(self, feed, brk):\n                TestStrategy.__init__(self, feed, brk)\n                self.buyOrder = None\n                self.sellOrder = None\n\n            def onOrderUpdated(self, order):\n                TestStrategy.onOrderUpdated(self, order)\n\n                if order == self.buyOrder and order.isPartiallyFilled():\n                    if self.sellOrder is None:\n                        self.sellOrder = self.limitOrder(common.btc_symbol, 10, -0.5)\n                        brk.getHTTPClient().addUserTransaction(self.sellOrder.getId(), -0.5, 5, 10, 0.01)\n                elif order == self.sellOrder and order.isFilled():\n                    self.stop()\n\n            def onBars(self, bars):\n                if self.buyOrder is None:\n                    self.buyOrder = self.limitOrder(common.btc_symbol, 10, 1)\n                    brk.getHTTPClient().addUserTransaction(self.buyOrder.getId(), 0.5, -5, 10, 0.01)\n\n        barFeed = TestingLiveTradeFeed()\n        # This is to get onBars called once.\n        barFeed.addTrade(datetime.datetime.now(), 1, 100, 1)\n\n        brk = TestingLiveBroker(None, None, None)\n        httpClient = brk.getHTTPClient()\n        httpClient.setUSDAvailable(10)\n        httpClient.setBTCAvailable(0)\n\n        strat = Strategy(barFeed, brk)\n        strat.run()\n\n        self.assertTrue(strat.buyOrder.isPartiallyFilled())\n        self.assertTrue(strat.sellOrder.isFilled())\n        # 2 events for each order: 1 for accepted, 1 for fill.\n        self.assertEquals(len(strat.orderExecutionInfo), 4)\n        self.assertEquals(strat.orderExecutionInfo[0], None)\n        self.assertEquals(strat.orderExecutionInfo[1].getPrice(), 10)\n        self.assertEquals(strat.orderExecutionInfo[1].getQuantity(), 0.5)\n        self.assertEquals(strat.orderExecutionInfo[1].getCommission(), 0.01)\n        self.assertEquals(strat.orderExecutionInfo[1].getDateTime().date(), datetime.datetime.now().date())\n        self.assertEquals(strat.orderExecutionInfo[2], None)\n        self.assertEquals(strat.orderExecutionInfo[3].getPrice(), 10)\n        self.assertEquals(strat.orderExecutionInfo[3].getQuantity(), 0.5)\n        self.assertEquals(strat.orderExecutionInfo[3].getCommission(), 0.01)\n        self.assertEquals(strat.orderExecutionInfo[3].getDateTime().date(), datetime.datetime.now().date())\n\n\nclass WebSocketTestCase(tc_common.TestCase):\n    def testBarFeed(self):\n        events = {\n            \"on_bars\": False,\n            \"on_order_book_updated\": False,\n            \"break\": False,\n            \"start\": datetime.datetime.now()\n        }\n\n        disp = dispatcher.Dispatcher()\n        barFeed = barfeed.LiveTradeFeed()\n        disp.addSubject(barFeed)\n\n        def on_bars(dateTime, bars):\n            bars[common.btc_symbol]\n            events[\"on_bars\"] = True\n            if events[\"on_order_book_updated\"] is True:\n                disp.stop()\n\n        def on_order_book_updated(orderBookUpdate):\n            events[\"on_order_book_updated\"] = True\n            if events[\"on_bars\"] is True:\n                disp.stop()\n\n        def on_idle():\n            # Stop after 5 minutes.\n            if (datetime.datetime.now() - events[\"start\"]).seconds > 60*5:\n                disp.stop()\n\n        # Subscribe to events.\n        barFeed.getNewValuesEvent().subscribe(on_bars)\n        barFeed.getOrderBookUpdateEvent().subscribe(on_order_book_updated)\n        disp.getIdleEvent().subscribe(on_idle)\n        disp.run()\n\n        # Check that we received both events.\n        self.assertTrue(events[\"on_bars\"])\n        self.assertTrue(events[\"on_order_book_updated\"])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import broker\nfrom pyalgotrade.broker import backtesting\nfrom pyalgotrade import bar\nfrom pyalgotrade import barfeed\n\n\nclass OrderUpdateCallback:\n    def __init__(self, broker_):\n        self.eventCount = 0\n        self.events = []\n        broker_.getOrderUpdatedEvent().subscribe(self.onOrderEvent)\n\n    def onOrderEvent(self, broker_, orderEvent):\n        self.eventCount += 1\n        self.events.append(orderEvent)\n\n\nclass BarsBuilder(object):\n    def __init__(self, instrument, frequency):\n        self.__instrument = instrument\n        self.__frequency = frequency\n        self.__nextDateTime = datetime.datetime(2011, 1, 1)\n        if frequency == bar.Frequency.TRADE:\n            self.__delta = datetime.timedelta(milliseconds=1)\n        elif frequency == bar.Frequency.SECOND:\n            self.__delta = datetime.timedelta(seconds=1)\n        elif frequency == bar.Frequency.MINUTE:\n            self.__delta = datetime.timedelta(minutes=1)\n        elif frequency == bar.Frequency.HOUR:\n            self.__delta = datetime.timedelta(hours=1)\n        elif frequency == bar.Frequency.DAY:\n            self.__delta = datetime.timedelta(days=1)\n        else:\n            raise Exception(\"Invalid frequency\")\n\n    def getCurrentDateTime(self):\n        return self.__nextDateTime\n\n    def advance(self, sessionClose):\n        if sessionClose:\n            self.__nextDateTime = datetime.datetime(self.__nextDateTime.year, self.__nextDateTime.month, self.__nextDateTime.day)\n            self.__nextDateTime += datetime.timedelta(days=1)\n        else:\n            self.__nextDateTime += self.__delta\n\n    # sessionClose is True if the next bars should start at a different date.\n    def nextBars(self, openPrice, highPrice, lowPrice, closePrice, volume=None, sessionClose=False):\n        if volume is None:\n            volume = closePrice*10\n        bar_ = bar.BasicBar(self.__nextDateTime, openPrice, highPrice, lowPrice, closePrice, volume, closePrice, self.__frequency)\n        ret = {self.__instrument: bar_}\n        self.advance(sessionClose)\n        return bar.Bars(ret)\n\n    # sessionClose is True if the next bars should start at a different date.\n    def nextBar(self, openPrice, highPrice, lowPrice, closePrice, volume=None, sessionClose=False):\n        return self.nextBars(openPrice, highPrice, lowPrice, closePrice, volume, sessionClose)[self.__instrument]\n\n    # sessionClose is True if the next bars should start at a different date.\n    def nextTuple(self, openPrice, highPrice, lowPrice, closePrice, volume=None, sessionClose=False):\n        ret = self.nextBars(openPrice, highPrice, lowPrice, closePrice, volume, sessionClose)\n        return (ret.getDateTime(), ret)\n\n\nclass DecimalTraits(broker.InstrumentTraits):\n    def __init__(self, decimals):\n        self.__decimals = decimals\n\n    def roundQuantity(self, quantity):\n        return round(quantity, self.__decimals)\n\n\nclass BarFeed(barfeed.BaseBarFeed):\n    def __init__(self, instrument, frequency):\n        barfeed.BaseBarFeed.__init__(self, frequency)\n        self.__builder = BarsBuilder(instrument, frequency)\n        self.__nextBars = None\n\n    def getCurrentDateTime(self):\n        return self.__builder.getCurrentDateTime()\n\n    def start(self):\n        raise NotImplementedError()\n\n    def stop(self):\n        raise NotImplementedError()\n\n    def join(self):\n        raise NotImplementedError()\n\n    def eof(self):\n        raise NotImplementedError()\n\n    def peekDateTime(self):\n        raise NotImplementedError()\n\n    def dispatchBars(self, openPrice, highPrice, lowPrice, closePrice, volume=None, sessionClose=False):\n        self.__nextBars = self.__builder.nextBars(openPrice, highPrice, lowPrice, closePrice, volume, sessionClose)\n        self.dispatch()\n\n    def barsHaveAdjClose(self):\n        raise True\n\n    def getNextBars(self):\n        return self.__nextBars\n\n\nclass BaseTestCase(common.TestCase):\n    TestInstrument = \"orcl\"\n\n    def buildBroker(self, *args, **kwargs):\n        return backtesting.Broker(*args, **kwargs)\n\n    def buildBarFeed(self, *args, **kwargs):\n        return BarFeed(*args, **kwargs)\n\n\nclass CommissionTestCase(common.TestCase):\n    def testNoCommission(self):\n        comm = backtesting.NoCommission()\n        self.assertEqual(comm.calculate(None, 1, 1), 0)\n\n    def testFixedPerTrade(self):\n        comm = backtesting.FixedPerTrade(1.2)\n        order = backtesting.MarketOrder(broker.Order.Action.BUY, \"orcl\", 1, False, broker.IntegerTraits())\n        self.assertEqual(comm.calculate(order, 1, 1), 1.2)\n\n    def testTradePercentage(self):\n        comm = backtesting.TradePercentage(0.1)\n        self.assertEqual(comm.calculate(None, 1, 1), 0.1)\n        self.assertEqual(comm.calculate(None, 2, 2), 0.4)\n\n\nclass BrokerTestCase(BaseTestCase):\n    def testOneCancelsAnother(self):\n        orders = {}\n\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        def onOrderEvent(broker_, orderEvent):\n            if orderEvent.getEventType() == broker.OrderEvent.Type.FILLED and orderEvent.getOrder().getId() == orders[\"sell\"].getId():\n                brk.cancelOrder(orders[\"stoploss\"])\n\n        # Buy order.\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n\n        brk.getOrderUpdatedEvent().subscribe(onOrderEvent)\n\n        # Create a sell limit and a stop loss order.\n        order = brk.createLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 11, 1)\n        orders[\"sell\"] = order\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        order = brk.createStopOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 8, 1)\n        orders[\"stoploss\"] = order\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 5, 12)\n\n        # Only one order (the sell limit order) should have got filled. The other one should be canceled.\n        self.assertEqual(brk.getShares(BaseTestCase.TestInstrument), 0)\n        self.assertTrue(orders[\"sell\"].isFilled())\n        self.assertTrue(orders[\"stoploss\"].isCanceled())\n\n    def testRegressionGetActiveOrders(self):\n        activeOrders = []\n\n        def onOrderEvent(brk, orderEvent):\n            if orderEvent.getEventType() != broker.OrderEvent.Type.SUBMITTED:\n                activeOrders.append(len(brk.getActiveOrders()))\n\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        brk.getOrderUpdatedEvent().subscribe(onOrderEvent)\n        o1 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(o1.getSubmitDateTime(), None)\n        brk.submitOrder(o1)\n        self.assertEquals(o1.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        o2 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(o2.getSubmitDateTime(), None)\n        brk.submitOrder(o2)\n        self.assertEquals(o2.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        self.assertEqual(o1.getFilled(), 0)\n        self.assertEqual(o2.getFilled(), 0)\n        self.assertEqual(o1.getRemaining(), o1.getQuantity())\n        self.assertEqual(o2.getRemaining(), o2.getQuantity())\n\n        barFeed.dispatchBars(10, 15, 8, 12)\n\n        self.assertNotEquals(o1.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertNotEquals(o2.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        self.assertEqual(o1.getFilled(), 1)\n        self.assertEqual(o2.getFilled(), 1)\n        self.assertEqual(o1.getRemaining(), 0)\n        self.assertEqual(o2.getRemaining(), 0)\n        self.assertEqual(brk.getCash(), 1000 - 10*2)\n        self.assertEqual(len(activeOrders), 4)\n        self.assertEqual(activeOrders[0], 2)  # First order gets accepted, both orders are active.\n        self.assertEqual(activeOrders[1], 1)  # First order gets filled, one order is active.\n        self.assertEqual(activeOrders[2], 1)  # Second order gets accepted, one order is active.\n        self.assertEqual(activeOrders[3], 0)  # Second order gets filled, zero orders are active.\n\n    def testVolumeLimitMinuteBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 3)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 3)\n\n        # The order should not get filled if there is not enough volume.\n        barFeed.dispatchBars(10, 15, 8, 12, volume=3)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 3)\n\n        # The order should now get filled since there is enough volume.\n        barFeed.dispatchBars(10, 15, 8, 12, volume=12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 3)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testVolumeLimitTradeBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.TRADE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Try with different order types.\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 3)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 3)\n\n        # The order should get partially filled.\n        barFeed.dispatchBars(10, 15, 8, 12, volume=1)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 2)\n\n        # The order should now get completely filled.\n        barFeed.dispatchBars(10, 15, 8, 12, volume=3)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 3)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testCancelationEvent(self):\n        orderStates = []\n\n        def onOrderEvent(broker, orderEvent):\n            orderStates.append(order.getState())\n\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n        brk.getOrderUpdatedEvent().subscribe(onOrderEvent)\n\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        barFeed.dispatchBars(10, 15, 8, 12)\n        # Check that cancelation event gets emited right away.\n        brk.cancelOrder(order)\n        self.assertTrue(broker.Order.State.CANCELED in orderStates)\n\n    def testSkipOrderSubmittedDuringEvent(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n        ordersUpdated = []\n\n        def onOrderEvent(broker_, orderEvent):\n            if orderEvent.getEventType() != broker.OrderEvent.Type.SUBMITTED:\n                ordersUpdated.append(orderEvent.getOrder())\n                newOrder = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n                self.assertEquals(newOrder.getSubmitDateTime(), None)\n                brk.submitOrder(newOrder)\n                self.assertEquals(newOrder.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        brk.getOrderUpdatedEvent().subscribe(onOrderEvent)\n\n        # The first order gets submitted.\n        firstOrder = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2, 1)\n        self.assertEquals(firstOrder.getSubmitDateTime(), None)\n        brk.submitOrder(firstOrder)\n        self.assertEquals(firstOrder.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEquals(len(ordersUpdated), 0)\n\n        # The first order gets accepted, and the second one gets submitted..\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertEquals(len(ordersUpdated), 1)  # First order got accepted.\n        self.assertTrue(firstOrder in ordersUpdated)\n        self.assertEquals(len(brk.getActiveOrders()), 2)  # Both orders are active.\n        # Check that the first one was accepted, and the second one submitted.\n        for activeOrder in brk.getActiveOrders():\n            if activeOrder.getId() == firstOrder.getId():\n                self.assertTrue(activeOrder.isAccepted())\n            else:\n                self.assertTrue(activeOrder.isSubmitted())\n\n        # Second order should get accepted and filled.\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertEquals(len(ordersUpdated), 3)\n        self.assertTrue(firstOrder.isAccepted())\n\n    def testPartialFillAndCancel(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.DAY)\n        brk = self.buildBroker(1000, barFeed)\n        cb = OrderUpdateCallback(brk)\n\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 2 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        self.assertTrue(order.isCanceled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        self.assertEqual(len(cb.events), 4)\n        self.assertEqual(cb.events[0].getEventType(), broker.OrderEvent.Type.SUBMITTED)\n        self.assertEqual(cb.events[1].getEventType(), broker.OrderEvent.Type.ACCEPTED)\n        self.assertEqual(cb.events[2].getEventType(), broker.OrderEvent.Type.PARTIALLY_FILLED)\n        self.assertEqual(cb.events[3].getEventType(), broker.OrderEvent.Type.CANCELED)\n\n    def testVolumeLimitPerBar1(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        order1 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2)\n        self.assertEquals(order1.getSubmitDateTime(), None)\n        brk.submitOrder(order1)\n        self.assertEquals(order1.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        order2 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2)\n        self.assertEquals(order2.getSubmitDateTime(), None)\n        brk.submitOrder(order2)\n        self.assertEquals(order2.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        # 2 should get filled for the first order.\n        self.assertTrue(order1.isFilled())\n        self.assertEqual(order1.getFilled(), 2)\n        self.assertEqual(order1.getRemaining(), 0)\n        self.assertEqual(order1.getAvgFillPrice(), 12)\n        self.assertEqual(order1.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order1.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order1.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled for the second order.\n        self.assertTrue(order2.isAccepted())\n        self.assertEqual(order2.getFilled(), 0)\n        self.assertEqual(order2.getRemaining(), 2)\n        self.assertEqual(order2.getAvgFillPrice(), None)\n        self.assertEqual(order2.getExecutionInfo(), None)\n\n        barFeed.dispatchBars(13, 15, 8, 12, 10)\n        # 2 should get filled for the second order.\n        self.assertTrue(order2.isFilled())\n        self.assertEqual(order2.getFilled(), 2)\n        self.assertEqual(order2.getRemaining(), 0)\n        self.assertEqual(order2.getAvgFillPrice(), 13)\n        self.assertEqual(order2.getExecutionInfo().getPrice(), 13)\n        self.assertEqual(order2.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order2.getExecutionInfo().getCommission(), 0)\n\n    def testVolumeLimitPerBar2(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        order1 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order1.getSubmitDateTime(), None)\n        brk.submitOrder(order1)\n        self.assertEquals(order1.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        order2 = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order2.getSubmitDateTime(), None)\n        brk.submitOrder(order2)\n        self.assertEquals(order2.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        # 1 should get filled for the first order.\n        self.assertTrue(order1.isFilled())\n        self.assertEqual(order1.getFilled(), 1)\n        self.assertEqual(order1.getRemaining(), 0)\n        self.assertEqual(order1.getAvgFillPrice(), 12)\n        self.assertEqual(order1.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order1.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order1.getExecutionInfo().getCommission(), 0)\n        # 1 should get filled for the second order.\n        self.assertTrue(order2.isFilled())\n        self.assertEqual(order2.getFilled(), 1)\n        self.assertEqual(order2.getRemaining(), 0)\n        self.assertEqual(order2.getAvgFillPrice(), 12)\n        self.assertEqual(order2.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order2.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order2.getExecutionInfo().getCommission(), 0)\n\n    def testGetActiveOrders(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        order1 = brk.createMarketOrder(broker.Order.Action.BUY, \"ins1\", 1)\n        self.assertEquals(order1.getSubmitDateTime(), None)\n        brk.submitOrder(order1)\n        self.assertEquals(order1.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        order2 = brk.createMarketOrder(broker.Order.Action.BUY, \"ins2\", 1)\n        self.assertEquals(order2.getSubmitDateTime(), None)\n        brk.submitOrder(order2)\n        self.assertEquals(order2.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        self.assertEqual(len(brk.getActiveOrders()), 2)\n        self.assertEqual(len(brk.getActiveOrders(\"ins1\")), 1)\n        self.assertEqual(len(brk.getActiveOrders(\"ins2\")), 1)\n        self.assertEqual(len(brk.getActiveOrders(\"ins3\")), 0)\n\n\nclass MarketOrderTestCase(BaseTestCase):\n    def testGetPositions(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        cash = 1000000\n        brk = backtesting.Broker(cash, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        brk.submitOrder(order)\n        barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, 555.00)\n        self.assertTrue(order.isFilled())\n        self.assertEquals(brk.getPositions().get(BaseTestCase.TestInstrument), 1)\n\n        # Sell\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 1)\n        brk.submitOrder(order)\n        barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, 555.00)\n        self.assertTrue(order.isFilled())\n        self.assertEquals(brk.getPositions().get(BaseTestCase.TestInstrument), None)\n\n    def testBuyPartialWithTwoDecimals(self):\n        class Broker(backtesting.Broker):\n            def getInstrumentTraits(self, instrument):\n                return DecimalTraits(2)\n\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        cash = 1000000\n        brk = Broker(cash, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 500)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 138.75 should get filled.\n        barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, 555.00)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 138.75)\n        self.assertEqual(order.getFilled(), 138.75)\n        self.assertEqual(order.getRemaining(), 361.25)\n        self.assertEqual(order.getAvgFillPrice(), 12.03)\n        self.assertEqual(brk.getShares(BaseTestCase.TestInstrument), 138.75)\n        self.assertEqual(brk.getEquity(), cash)\n        self.assertEqual(brk.getFillStrategy().getVolumeLeft()[BaseTestCase.TestInstrument], 0)\n\n        # 361.25 should get filled.\n        barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, 2345.00)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 361.25)\n        self.assertEqual(order.getFilled(), 500)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12.03)\n        self.assertEqual(brk.getShares(BaseTestCase.TestInstrument), 500)\n        self.assertEqual(brk.getEquity(), cash)\n        self.assertEqual(brk.getFillStrategy().getVolumeLeft()[BaseTestCase.TestInstrument], 586.25 - 361.25)\n\n    def testBuyPartialWithEightDecimals(self):\n        quantityPresicion = 8\n        cashPresicion = 2\n        maxFill = 0.25\n\n        class Broker(backtesting.Broker):\n            def getInstrumentTraits(self, instrument):\n                return DecimalTraits(quantityPresicion)\n\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        cash = 1000000\n        brk = Broker(cash, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        volumes = [0.0001, 0.1, 0.0000001, 0.00000001, 0.132401]\n        volumeFill = [(volume, round(volume*maxFill, quantityPresicion)) for volume in volumes]\n        cumFilled = 0\n        for volume, expectedFill in volumeFill:\n            cumFilled += expectedFill  # I'm not rounding here so I can carry errors.\n            barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, volume)\n            # print expectedFill, cumFilled\n            self.assertTrue(order.isPartiallyFilled())\n            if expectedFill > 0:\n                self.assertEqual(order.getExecutionInfo().getQuantity(), expectedFill)\n            self.assertEqual(order.getFilled(), round(cumFilled, quantityPresicion))\n            self.assertEqual(order.getRemaining(), 1 - cumFilled)\n            self.assertEqual(round(order.getAvgFillPrice(), cashPresicion), 12.03)\n            self.assertEqual(brk.getShares(BaseTestCase.TestInstrument), round(cumFilled, quantityPresicion))\n            self.assertEqual(round(brk.getEquity(), cashPresicion), cash)\n            self.assertEqual(round(brk.getFillStrategy().getVolumeLeft()[BaseTestCase.TestInstrument], quantityPresicion), 0)\n\n        # Full fill\n        filledSoFar = order.getFilled()\n        volume = 10\n        cumFilled += expectedFill  # I'm not rounding here so I can carry errors.\n        barFeed.dispatchBars(12.03, 12.03, 12.03, 12.03, volume)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1 - filledSoFar)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12.03)\n        self.assertEqual(brk.getShares(BaseTestCase.TestInstrument), 1)\n        self.assertEqual(brk.getEquity(), cash)\n        self.assertEqual(round(brk.getFillStrategy().getVolumeLeft()[BaseTestCase.TestInstrument], quantityPresicion), round((volume*maxFill) - (1-filledSoFar), quantityPresicion))\n\n    def testBuySellPartial(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 2 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 5 should get filled.\n        barFeed.dispatchBars(13, 15, 8, 12, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), (12 * 2 + 13 * 5) / 7.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 13)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 3 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (12 * 5 + 13 * 5) / 10.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 3)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 0 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 2)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        # 1 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 4)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 9)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 9 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 100)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 9)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testBuyAndSell(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(11, barFeed)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 1)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 11)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testFailToBuy(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(5, barFeed)\n\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n\n        # Fail to buy. No money.\n        cb = OrderUpdateCallback(brk)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 8, 12, sessionClose=True)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 2)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Fail to buy. No money. Canceled due to session close.\n        cb = OrderUpdateCallback(brk)\n        barFeed.dispatchBars(11, 15, 8, 12)\n        self.assertTrue(order.isCanceled())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertTrue(cb.eventCount == 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n    def testBuy_GTC(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(5, barFeed)\n\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        order.setGoodTillCanceled(True)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Fail to buy. No money.\n        cb = OrderUpdateCallback(brk)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # Set sessionClose to true test that the order doesn't get canceled.\n        barFeed.dispatchBars(10, 15, 8, 12, sessionClose=True)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 2)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        barFeed.dispatchBars(2, 15, 1, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 2)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 2)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 3)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertTrue(cb.eventCount == 1)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testBuyAndSellInTwoSteps(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(20.4, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 2)\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(round(brk.getCash(), 1) == 0.4)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 2)\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(round(brk.getCash(), 1) == 10.4)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell again\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(11, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 11)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 11)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(round(brk.getCash(), 1) == 21.4)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testPortfolioValue(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(11, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 1)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n\n        barFeed.dispatchBars(11, 11, 11, 11)\n        self.assertEqual(brk.getEquity(), 11 + 1)\n        barFeed.dispatchBars(1, 1, 1, 1)\n        self.assertEqual(brk.getEquity(), 1 + 1)\n\n    def testBuyWithCommission(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1020, barFeed, commission=backtesting.FixedPerTrade(10))\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 100)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 100)\n        barFeed.dispatchBars(10, 15, 8, 12, volume=500)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 10)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 100)\n        self.assertEqual(order.getFilled(), 100)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testSellShort_1(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Short sell\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(200, 200, 200, 200)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 200)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 1200)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertTrue(brk.getEquity() == 1000 + 100)\n        barFeed.dispatchBars(0, 0, 0, 0)\n        self.assertTrue(brk.getEquity() == 1000 + 200)\n        barFeed.dispatchBars(30, 30, 30, 30)\n        self.assertTrue(brk.getEquity() == 1000 + 170)\n\n        # Buy at the same price.\n        order = brk.createMarketOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(200, 200, 200, 200)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 200)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 1000)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n\n    def testSellShort_2(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Short sell 1\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 100)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(brk.getCash() == 1100)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertTrue(brk.getEquity() == 1000)\n        barFeed.dispatchBars(0, 0, 0, 0)\n        self.assertTrue(brk.getEquity() == 1000 + 100)\n        barFeed.dispatchBars(70, 70, 70, 70)\n        self.assertTrue(brk.getEquity() == 1000 + 30)\n        barFeed.dispatchBars(200, 200, 200, 200)\n        self.assertTrue(brk.getEquity() == 1000 - 100)\n\n        # Buy 2 and earn 50\n        order = brk.createMarketOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 2)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 2)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(50, 50, 50, 50)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 50)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertTrue(brk.getCash() == 1000)  # +50 from short sell operation, -50 from buy operation.\n        barFeed.dispatchBars(50, 50, 50, 50)\n        self.assertTrue(brk.getEquity() == 1000 + 50)\n        barFeed.dispatchBars(70, 70, 70, 70)\n        self.assertTrue(brk.getEquity() == 1000 + 50 + 20)\n\n        # Sell 1 and earn 50\n        order = brk.createMarketOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 100)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        barFeed.dispatchBars(70, 70, 70, 70)\n        self.assertTrue(brk.getEquity() == 1000 + 50 + 50)\n\n    def testSellShort_3(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(100, barFeed)\n\n        # Buy 1\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 100)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertTrue(brk.getCash() == 0)\n\n        # Sell 2\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 2)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 2)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 100)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        self.assertTrue(brk.getCash() == 200)\n\n        # Buy 1\n        order = brk.createMarketOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(100, 100, 100, 100)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 100)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertTrue(brk.getCash() == 100)\n\n    def testSellShortWithCommission(self):\n        sharePrice = 100\n        commission = 10\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1010, barFeed, commission=backtesting.FixedPerTrade(commission))\n\n        # Sell 10 shares\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 10)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(sharePrice, sharePrice, sharePrice, sharePrice)\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), sharePrice)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 10)\n        self.assertTrue(brk.getCash() == 2000)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -10)\n\n        # Buy the 10 shares sold short plus 9 extra\n        order = brk.createMarketOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 19)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 19)\n        barFeed.dispatchBars(sharePrice, sharePrice, sharePrice, sharePrice)\n        self.assertEqual(order.getFilled(), 19)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), sharePrice)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 9)\n        self.assertTrue(brk.getCash() == sharePrice - commission)\n\n    def testCancel(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(100, barFeed)\n\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        brk.cancelOrder(order)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 10, 10, 10)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(order.isCanceled())\n\n    def testTradePercentageWithPartialFills(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n        commPercentage = 0.1\n        brk.setCommission(backtesting.TradePercentage(0.1))\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getCommissions(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 12*2*commPercentage)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 12*2*commPercentage)\n        # 5 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 12*7*commPercentage)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 12*5*commPercentage)\n        # 3 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 12*10*commPercentage)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 3)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 12*3*commPercentage)\n\n    def testFixedPerTradeWithPartialFills(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n        brk.setCommission(backtesting.FixedPerTrade(1.2))\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getCommissions(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 1.2)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 1.2)  # Commision applied in the first fill.\n        # 5 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 1.2)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 3 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getCommissions(), 1.2)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 3)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testDailyMarketOnClose(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.DAY)\n        cash = 1000000\n        brk = backtesting.Broker(cash, barFeed)\n\n        # Buy\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 2, onClose=True)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n\n        # 2 should get filled at the closing price.\n        barFeed.dispatchBars(12, 15, 8, 14, 10)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 14)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 14)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n\n    def testIntradayMarketOnClose(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        cash = 1000000\n        brk = backtesting.Broker(cash, barFeed)\n\n        with self.assertRaisesRegexp(Exception, \"Market-on-close not supported with intraday feeds\"):\n            brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1, onClose=True)\n\n\nclass LimitOrderTestCase(BaseTestCase):\n    def testBuySellPartial(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 2 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 5 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 3 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 10)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 3)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell\n        order = brk.createLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 10, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 0 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 2)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        # 1 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 4)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 9)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 9 should get filled.\n        barFeed.dispatchBars(12, 15, 8, 12, 100)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 12)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 9)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testBuyAndSell_HitTargetPrice(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(20, barFeed)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 10, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(12, 15, 8, 12)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Sell\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 15, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 17, 8, 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 15)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 15)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 25)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testBuyAndSell_GetBetterPrice(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(20, barFeed)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 14, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(12, 15, 8, 12)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 12)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 8)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Sell\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 15, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(16, 17, 8, 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 16)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 16)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 24)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testBuyAndSell_GappingBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(20, barFeed)\n\n        # Buy. Bar is below the target price.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 20, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 8, 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Sell. Bar is above the target price.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 30, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(35, 40, 32, 35)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 35)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 35)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 45)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testFailToBuy(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(5, barFeed)\n\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 5, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Fail to buy (couldn't get specific price).\n        cb = OrderUpdateCallback(brk)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 8, 12, sessionClose=True)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 2)\n\n        # Fail to buy (couldn't get specific price). Canceled due to session close.\n        cb = OrderUpdateCallback(brk)\n        barFeed.dispatchBars(11, 15, 8, 12)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(order.isCanceled())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertTrue(cb.eventCount == 1)\n\n    def testBuy_GTC(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(10, barFeed)\n\n        order = brk.createLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 4, 2)\n        order.setGoodTillCanceled(True)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 2)\n\n        # Fail to buy (couldn't get specific price).\n        cb = OrderUpdateCallback(brk)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # Set sessionClose to true test that the order doesn't get canceled.\n        barFeed.dispatchBars(10, 15, 8, 12, sessionClose=True)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 2)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertTrue(order.getExecutionInfo() is None)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 2)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        barFeed.dispatchBars(2, 15, 1, 12)\n        self.assertEqual(order.getFilled(), 2)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 2)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 2)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 6)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 2)\n        self.assertEqual(cb.eventCount, 1)\n\n\nclass StopOrderTestCase(BaseTestCase):\n    def testStopHitWithoutVolume(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15.\n        order = brk.createStopOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 0 should get filled. There is not enough volume.\n        barFeed.dispatchBars(18, 19, 17.01, 18, 3)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n\n    def testBuySellPartial_ActivateAndThenFill(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15.\n        order = brk.createStopOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 0 should get filled. The stop price should have not been hit.\n        barFeed.dispatchBars(12, 14, 8, 12, 10)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), False)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 0 should get filled. The stop price should have been hit but there is not enough volume.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 3)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 5 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 18)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 18)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 5 should get filled.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 50)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (18 + 17.1) / 2.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17.1)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell. Stop <= 19.\n        order = brk.createStopOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 19, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 0 should get filled. The stop price should have not been hit.\n        barFeed.dispatchBars(19.1, 19.5, 19.1, 19.4, 10)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), False)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 0 should get filled. The stop price should have been hit but there is not enough volume.\n        barFeed.dispatchBars(18, 18, 16, 18, 3)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 5 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(16, 21, 15, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), (20*5 + 16*2)/7.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 16)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 9)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getAvgFillPrice(), (20*5 + 16*2 + 21*2)/9.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 1 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 10)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (20*5 + 16*2 + 21*2 + 20)/10.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testBuySellPartial_ActivateAndFill(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15.\n        order = brk.createStopOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 5 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 18)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 18)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled. There is not enough volume.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 3)\n        self.assertEqual(order.getAvgFillPrice(), 18)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        # 5 should get filled.\n        barFeed.dispatchBars(16, 18, 16, 18, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (18 + 16) / 2.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 16)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell. Stop <= 19.\n        order = brk.createStopOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 19, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 5 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 20)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 19)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 19)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled. There is not enough volume.\n        barFeed.dispatchBars(18, 18, 16, 18, 3)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getAvgFillPrice(), 19)\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        # 2 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), (19*5 + 20*2)/7.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 9)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getAvgFillPrice(), (19*5 + 20*2 + 21*2)/9.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 1 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (19*5 + 20*2 + 21*3)/10.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testLongPosStopLoss(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Create stop loss order.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createStopOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 9, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 10, 12)  # Stop loss not hit.\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertFalse(order.isFilled())\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 2)\n        barFeed.dispatchBars(10, 15, 8, 12)  # Stop loss hit.\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 9)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 9)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5+9)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testLongPosStopLoss_GappingBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Create stop loss order.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createStopOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 9, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 10, 12)  # Stop loss not hit.\n        self.assertFalse(order.isFilled())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 1)\n        self.assertEqual(cb.eventCount, 2)\n        barFeed.dispatchBars(5, 8, 4, 7)  # Stop loss hit.\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 5)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 5)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 5+5)  # Fill the stop loss order at open price.\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testShortPosStopLoss(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Sell short\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 15+10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Create stop loss order.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createStopOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 11, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(8, 10, 7, 9)  # Stop loss not hit.\n        self.assertFalse(order.isFilled())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 15+10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        self.assertEqual(cb.eventCount, 2)\n        barFeed.dispatchBars(10, 15, 8, 12)  # Stop loss hit.\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 11)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 15-1)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n    def testShortPosStopLoss_GappingBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Sell short\n        cb = OrderUpdateCallback(brk)\n        order = brk.createMarketOrder(broker.Order.Action.SELL_SHORT, BaseTestCase.TestInstrument, 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        barFeed.dispatchBars(10, 15, 8, 12)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertTrue(order.getExecutionInfo().getCommission() == 0)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 15+10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        self.assertEqual(cb.eventCount, 3)\n\n        # Create stop loss order.\n        cb = OrderUpdateCallback(brk)\n        order = brk.createStopOrder(broker.Order.Action.BUY_TO_COVER, BaseTestCase.TestInstrument, 11, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        barFeed.dispatchBars(8, 10, 7, 9)  # Stop loss not hit.\n        self.assertFalse(order.isFilled())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertTrue(len(brk.getActiveOrders()) == 1)\n        self.assertTrue(brk.getCash() == 15+10)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == -1)\n        self.assertEqual(cb.eventCount, 2)\n        barFeed.dispatchBars(15, 20, 13, 14)  # Stop loss hit.\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 15)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 15)\n        self.assertTrue(len(brk.getActiveOrders()) == 0)\n        self.assertTrue(brk.getCash() == 15-5)\n        self.assertTrue(brk.getShares(BaseTestCase.TestInstrument) == 0)\n        self.assertEqual(cb.eventCount, 3)\n\n\nclass StopLimitOrderTestCase(BaseTestCase):\n    def testStopHitWithoutVolume(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15. Buy <= 17.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 17, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 0 should get filled. There is not enough volume.\n        barFeed.dispatchBars(18, 19, 15, 18, 3)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n\n    def testRegressionBarGapsAboveStop(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15. Buy <= 17.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 17, 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 1 should get filled at 17. Before the bug was fixed it was filled at 15.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testBuySellPartial_ActivateAndThenFill(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15. Buy <= 17.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 17, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 0 should get filled. The stop price should have not been hit.\n        barFeed.dispatchBars(12, 14, 8, 12, 10)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), False)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 0 should get filled. The stop price should have been hit.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 10)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 5 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 5 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell. Stop <= 19. Sell >= 20.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 19, 20, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 0 should get filled. The stop price should have not been hit.\n        barFeed.dispatchBars(19.1, 19.5, 19.1, 19.4, 10)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), False)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 0 should get filled. The stop price should have been hit.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 10)\n        self.assertEqual(order.getStopHit(), True)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n        # 5 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 9)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getAvgFillPrice(), (20*7 + 21*2) / 9.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 1 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (20*7 + 21*3) / 10.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testBuySellPartial_ActivateAndFill(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(1000, barFeed)\n\n        # Buy. Stop >= 15. Buy <= 17.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, 15, 17, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n\n        # 5 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled.\n        barFeed.dispatchBars(17.1, 18, 17.01, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 17)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 5 should get filled.\n        barFeed.dispatchBars(16, 18, 16, 18, 20)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (17+16)/2.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 16)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n        # Sell. Stop <= 19. Sell >= 20.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, 19, 20, 10)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        # 5 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 0 should get filled.\n        barFeed.dispatchBars(18, 18, 16, 18, 20)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 5)\n        self.assertEqual(order.getRemaining(), 5)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 5)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(20, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 7)\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getAvgFillPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 20)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 2 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getFilled(), 9)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getAvgFillPrice(), (20*7 + 21*2) / 9.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n        # 1 should get filled.\n        barFeed.dispatchBars(21, 21, 17, 18, 10)\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getFilled(), 10)\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getAvgFillPrice(), (20*7 + 21*3) / 10.0)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 21)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getCommission(), 0)\n\n    def testFillOpen(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 10. Buy <= 12.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=10, limitPrice=12, quantity=1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(13, 15, 13, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit (bars include the price). Fill at open price.\n        barFeed.dispatchBars(11, 15, 10, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 11)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 11)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 8. Sell >= 6.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=8, limitPrice=6, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(4, 5, 3, 4)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit (bars include the price). Fill at open price.\n        barFeed.dispatchBars(7, 8, 6, 7)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 7)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 7)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testFillOpen_GappingBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 10. Buy <= 12.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=10, limitPrice=12, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(13, 18, 13, 17)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit (bars don't include the price). Fill at open price.\n        barFeed.dispatchBars(7, 9, 6, 8)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 7)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 7)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 8. Sell >= 6.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=8, limitPrice=6, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(4, 5, 3, 4)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit (bars don't include the price). Fill at open price.\n        barFeed.dispatchBars(10, 12, 8, 10)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testFillLimit(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 10. Buy <= 12.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=10, limitPrice=12, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(13, 15, 13, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(13, 15, 10, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 12)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 12)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 8. Sell >= 6.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=8, limitPrice=6, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(4, 5, 3, 4)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(5, 7, 5, 6)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 6)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 6)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testHitStopAndLimit(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 10. Buy <= 12.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=10, limitPrice=12, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price hit. Fill at stop price.\n        barFeed.dispatchBars(9, 15, 8, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 8. Sell >= 6.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=8, limitPrice=6, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price hit. Fill at stop price.\n        barFeed.dispatchBars(9, 10, 5, 8)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 8)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 8)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testInvertedPrices_FillOpen(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 12. Buy <= 10.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=12, limitPrice=10, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(11, 12, 10.5, 11)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at open price.\n        barFeed.dispatchBars(9, 15, 8, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 9)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 9)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 6. Sell >= 8.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=6, limitPrice=8, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(7, 7, 6, 7)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at open price.\n        barFeed.dispatchBars(9, 10, 8, 9)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 9)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 9)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testInvertedPrices_FillOpen_GappingBars(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 12. Buy <= 10.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=12, limitPrice=10, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(11, 12, 10.5, 11)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at open price.\n        barFeed.dispatchBars(7, 9, 6, 8)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 7)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 7)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 6. Sell >= 8.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=6, limitPrice=8, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(7, 7, 6, 7)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at open price.\n        barFeed.dispatchBars(10, 10, 9, 9)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testInvertedPrices_FillLimit(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 12. Buy <= 10.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=12, limitPrice=10, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(8, 9, 7, 8)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(11, 12, 10.5, 11)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(11, 13, 8, 9)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 6. Sell >= 8.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=6, limitPrice=8, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price not hit. Limit price not hit.\n        barFeed.dispatchBars(9, 10, 9, 10)\n        self.assertFalse(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price not hit.\n        barFeed.dispatchBars(7, 7, 6, 7)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(7, 10, 6, 9)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 8)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 8)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n    def testInvertedPrices_HitStopAndLimit(self):\n        barFeed = self.buildBarFeed(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        brk = self.buildBroker(15, barFeed)\n\n        # Buy. Stop >= 12. Buy <= 10.\n        order = brk.createStopLimitOrder(broker.Order.Action.BUY, BaseTestCase.TestInstrument, stopPrice=12, limitPrice=10, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(9, 15, 8, 14)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 10)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 10)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n\n        # Sell. Stop <= 6. Sell >= 8.\n        order = brk.createStopLimitOrder(broker.Order.Action.SELL, BaseTestCase.TestInstrument, stopPrice=6, limitPrice=8, quantity=1)\n        self.assertEquals(order.getSubmitDateTime(), None)\n        brk.submitOrder(order)\n        self.assertEquals(order.getSubmitDateTime(), barFeed.getCurrentDateTime())\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getRemaining(), 1)\n\n        # Stop price hit. Limit price hit. Fill at limit price.\n        barFeed.dispatchBars(6, 10, 5, 7)\n        self.assertTrue(order.getStopHit())\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getAvgFillPrice(), 8)\n        self.assertTrue(order.getExecutionInfo().getPrice() == 8)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getRemaining(), 0)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import broker\n\n\nclass DefaultTraits(broker.InstrumentTraits):\n    def roundQuantity(self, quantity):\n        return int(quantity)\n\n\nclass OrderTestCase(common.TestCase):\n    def __buildAcceptedLimitOrder(self, action, limitPrice, quantity):\n        ret = broker.LimitOrder(action, \"orcl\", limitPrice, quantity, DefaultTraits())\n        self.assertEquals(ret.getSubmitDateTime(), None)\n        ret.switchState(broker.Order.State.SUBMITTED)\n        ret.setSubmitted(1, datetime.datetime.now())\n        self.assertNotEquals(ret.getSubmitDateTime(), None)\n        ret.switchState(broker.Order.State.ACCEPTED)\n        return ret\n\n    def testCompleteFill(self):\n        order = self.__buildAcceptedLimitOrder(broker.Order.Action.BUY, 1, 1)\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n\n        order.addExecutionInfo(broker.OrderExecutionInfo(0.9, 1, 0, datetime.datetime.now()))\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getFilled(), 1)\n        self.assertEqual(order.getAvgFillPrice(), 0.9)\n\n    def testCompleteFillInvalidSize(self):\n        order = self.__buildAcceptedLimitOrder(broker.Order.Action.BUY, 1, 1)\n        with self.assertRaises(Exception):\n            order.addExecutionInfo(broker.OrderExecutionInfo(1, 1.001, 0, datetime.datetime.now()))\n        self.assertTrue(order.isAccepted())\n        self.assertEqual(order.getRemaining(), 1)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n\n    def testPartialFill(self):\n        order = self.__buildAcceptedLimitOrder(broker.Order.Action.BUY, 2, 11)\n        self.assertEqual(order.getRemaining(), 11)\n        self.assertEqual(order.getFilled(), 0)\n        self.assertEqual(order.getAvgFillPrice(), None)\n        self.assertEqual(order.getExecutionInfo(), None)\n\n        order.addExecutionInfo(broker.OrderExecutionInfo(1, 8, 0, datetime.datetime.now()))\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getRemaining(), 3)\n        self.assertEqual(order.getFilled(), 8)\n        self.assertEqual(order.getAvgFillPrice(), 1)\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 8)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 1)\n\n        order.addExecutionInfo(broker.OrderExecutionInfo(1.5, 1, 0, datetime.datetime.now()))\n        self.assertTrue(order.isPartiallyFilled())\n        self.assertEqual(order.getRemaining(), 2)\n        self.assertEqual(order.getFilled(), 9)\n        self.assertEqual(round(order.getAvgFillPrice(), 4), round(1.055555556, 4))\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 1)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 1.5)\n\n        order.addExecutionInfo(broker.OrderExecutionInfo(1.123, 2, 0, datetime.datetime.now()))\n        self.assertTrue(order.isFilled())\n        self.assertEqual(order.getRemaining(), 0)\n        self.assertEqual(order.getFilled(), 11)\n        self.assertEqual(round(order.getAvgFillPrice(), 4), round(1.067818182, 4))\n        self.assertEqual(order.getExecutionInfo().getQuantity(), 2)\n        self.assertEqual(order.getExecutionInfo().getPrice(), 1.123)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade.bitcoincharts import barfeed\nfrom pyalgotrade.utils import dt\n\n\nclass TestCase(common.TestCase):\n    def testLoadNoFilter(self):\n        feed = barfeed.CSVTradeFeed()\n        feed.addBarsFromCSV(common.get_data_file_path(\"bitstampUSD.csv\"))\n        loaded = [(dateTime, bars) for dateTime, bars in feed]\n\n        self.assertEquals(len(loaded), 9999)\n\n        self.assertEquals(loaded[0][0], dt.as_utc(datetime.datetime(2011, 9, 13, 13, 53, 36)))\n        self.assertEquals(loaded[0][1][\"BTC\"].getDateTime(), dt.as_utc(datetime.datetime(2011, 9, 13, 13, 53, 36)))\n        self.assertEquals(loaded[0][1][\"BTC\"].getClose(), 5.8)\n        self.assertEquals(loaded[0][1][\"BTC\"].getPrice(), 5.8)\n        self.assertEquals(loaded[0][1][\"BTC\"].getVolume(), 1.0)\n\n        self.assertEquals(loaded[-1][0], dt.as_utc(datetime.datetime(2012, 5, 31, 8, 41, 18, 5)))\n        self.assertEquals(loaded[-1][1][\"BTC\"].getDateTime(), dt.as_utc(datetime.datetime(2012, 5, 31, 8, 41, 18, 5)))\n        self.assertEquals(loaded[-1][1][\"BTC\"].getClose(), 5.1)\n        self.assertEquals(loaded[-1][1][\"BTC\"].getPrice(), 5.1)\n        self.assertEquals(loaded[-1][1][\"BTC\"].getVolume(), 0.39215686)\n\n    def testLoadFilterFrom(self):\n        feed = barfeed.CSVTradeFeed()\n        feed.addBarsFromCSV(common.get_data_file_path(\"bitstampUSD.csv\"), \"bitstampUSD\", fromDateTime=dt.as_utc(datetime.datetime(2012, 5, 29)))\n        loaded = [(dateTime, bars) for dateTime, bars in feed]\n\n        self.assertEquals(len(loaded), 646)\n\n        self.assertEquals(loaded[0][0], dt.as_utc(datetime.datetime(2012, 5, 29, 1, 47, 52)))\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getDateTime(), dt.as_utc(datetime.datetime(2012, 5, 29, 1, 47, 52)))\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getClose(), 5.07)\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getPrice(), 5.07)\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getVolume(), 1.39081288)\n\n        self.assertEquals(loaded[-1][0], dt.as_utc(datetime.datetime(2012, 5, 31, 8, 41, 18, 5)))\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getDateTime(), dt.as_utc(datetime.datetime(2012, 5, 31, 8, 41, 18, 5)))\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getClose(), 5.1)\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getPrice(), 5.1)\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getVolume(), 0.39215686)\n\n    def testLoadFilterFromAndTo(self):\n        feed = barfeed.CSVTradeFeed()\n        feed.addBarsFromCSV(common.get_data_file_path(\"bitstampUSD.csv\"), \"bitstampUSD\", fromDateTime=dt.as_utc(datetime.datetime(2012, 5, 29)), toDateTime=datetime.datetime(2012, 5, 31))\n        loaded = [(dateTime, bars) for dateTime, bars in feed]\n\n        self.assertEquals(len(loaded), 579)\n\n        self.assertEquals(loaded[0][0], dt.as_utc(datetime.datetime(2012, 5, 29, 1, 47, 52)))\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getDateTime(), dt.as_utc(datetime.datetime(2012, 5, 29, 1, 47, 52)))\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getClose(), 5.07)\n        self.assertEquals(loaded[0][1][\"bitstampUSD\"].getVolume(), 1.39081288)\n\n        self.assertEquals(loaded[-1][0], dt.as_utc(datetime.datetime(2012, 5, 30, 23, 49, 21)))\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getDateTime(), dt.as_utc(datetime.datetime(2012, 5, 30, 23, 49, 21)))\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getClose(), 5.14)\n        self.assertEquals(loaded[-1][1][\"bitstampUSD\"].getVolume(), 20)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport csv\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport unittest\n\n# Force matplotlib to not use any Xwindows backend.\nimport matplotlib\nmatplotlib.use('Agg')\n\nfrom pyalgotrade import dataseries\n\n\nclass RunResults(object):\n    def __init__(self, retcode, output):\n        self.__retcode = retcode\n        self.__output = output\n\n    def exit_ok(self):\n        return self.__retcode == 0\n\n    def get_output(self):\n        return self.__output\n\n    def get_output_lines(self, skip_last_line_if_empty=False):\n        ret = self.__output.splitlines()\n        # Skip the last, empty line.\n        if skip_last_line_if_empty and len(ret[:-1]) == 0:\n            ret = ret[:-1]\n        return ret\n\n\ndef run_cmd(cmd):\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n    return RunResults(retcode, output)\n\n\ndef run_python_code(code):\n    cmd = [\"python\"]\n    cmd.append(\"-u\")\n    cmd.append(\"-c\")\n    cmd.append(code)\n    return run_cmd(cmd)\n\n\ndef run_sample_script(script, params=[]):\n    cmd = [\"python\"]\n    cmd.append(\"-u\")\n    cmd.append(os.path.join(\"samples\", script))\n    cmd.extend(params)\n    return run_cmd(cmd)\n\n\ndef get_file_lines(fileName):\n    rawLines = open(fileName, \"r\").read().splitlines()\n    return [rawLine.strip() for rawLine in rawLines]\n\n\ndef compare_head(fileName, lines, path=\"samples\"):\n    assert(len(lines) > 0)\n    fileLines = get_file_lines(os.path.join(path, fileName))\n    return fileLines[0:len(lines)] == lines\n\n\ndef compare_tail(fileName, lines, path=\"samples\"):\n    assert(len(lines) > 0)\n    fileLines = get_file_lines(os.path.join(path, fileName))\n    return fileLines[len(lines)*-1:] == lines\n\n\ndef head_file(fileName, line_count, path=\"samples\"):\n    assert(line_count > 0)\n    fileLines = get_file_lines(os.path.join(path, fileName))\n    return fileLines[0:line_count]\n\n\ndef tail_file(fileName, line_count, path=\"samples\"):\n    assert(line_count > 0)\n    lines = get_file_lines(os.path.join(path, fileName))\n    return lines[line_count*-1:]\n\n\ndef load_test_csv(path):\n    inputSeq = []\n    expectedSeq = []\n    csvFile = open(path, \"r\")\n    reader = csv.DictReader(csvFile)\n    for row in reader:\n        inputSeq.append(float(row[\"Input\"]))\n        expected = row[\"Expected\"]\n        if not expected:\n            expected = None\n        else:\n            expected = float(expected)\n        expectedSeq.append(expected)\n\n    return inputSeq, expectedSeq\n\n\ndef get_data_file_path(fileName):\n    return os.path.join(os.path.split(__file__)[0], \"data\", fileName)\n\n\ndef test_from_csv(testcase, filename, filterClassBuilder, roundDecimals=2, maxLen=None):\n    inputValues, expectedValues = load_test_csv(get_data_file_path(filename))\n    inputDS = dataseries.SequenceDataSeries(maxLen=maxLen)\n    filterDS = filterClassBuilder(inputDS)\n    for i in xrange(len(inputValues)):\n        inputDS.append(inputValues[i])\n        value = safe_round(filterDS[i], roundDecimals)\n        expectedValue = safe_round(expectedValues[i], roundDecimals)\n        testcase.assertEqual(value, expectedValue)\n\n\ndef safe_round(number, ndigits):\n    ret = None\n    if number is not None:\n        ret = round(number, ndigits)\n    return ret\n\n\nclass CopyFiles:\n    def __init__(self, files, dst):\n        self.__files = files\n        self.__dst = dst\n        self.__toRemove = []\n\n    def __enter__(self):\n        for src in self.__files:\n            shutil.copy2(src, self.__dst)\n            if os.path.isdir(self.__dst):\n                self.__toRemove.append(os.path.join(self.__dst, os.path.basename(src)))\n            else:\n                self.__toRemove.append(self.__dst)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for src in self.__toRemove:\n            os.remove(src)\n\n\nclass TmpDir(object):\n    def __init__(self):\n        self.__tmpdir = None\n\n    def __enter__(self):\n        self.__tmpdir = tempfile.mkdtemp()\n        return self.__tmpdir\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.__tmpdir is not None:\n            shutil.rmtree(self.__tmpdir)\n\n\nclass TestCase(unittest.TestCase):\n    pass\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport os\n\nimport common\nimport feed_test\n\nfrom pyalgotrade.feed import csvfeed\nfrom pyalgotrade import dispatcher\nfrom pyalgotrade import marketsession\nfrom pyalgotrade.utils import dt\n\n\nclass TestCase(common.TestCase):\n    def testBaseFeedInterface(self):\n        feed = csvfeed.Feed(\"Date\", \"%Y-%m-%d\")\n        feed.addValuesFromCSV(common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        feed_test.tstBaseFeedInterface(self, feed)\n\n    def testFeedWithBars(self):\n        feed = csvfeed.Feed(\"Date\", \"%Y-%m-%d\")\n        feed.addValuesFromCSV(common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n\n        self.assertEqual(len(feed.getKeys()), 6)\n        for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]:\n            self.assertEqual(len(feed[col]), 0)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]:\n            self.assertEqual(len(feed[col]), 252)\n\n        self.assertEqual(feed[\"Open\"][-1], 30.87)\n        self.assertEqual(feed[\"High\"][-1], 31.31)\n        self.assertEqual(feed[\"Low\"][-1], 28.69)\n        self.assertEqual(feed[\"Close\"][-1], 29.06)\n        self.assertEqual(feed[\"Volume\"][-1], 31655500)\n        self.assertEqual(feed[\"Adj Close\"][-1], 28.41)\n\n    def testFeedWithQuandl(self):\n        class RowFilter(csvfeed.RowFilter):\n            def includeRow(self, dateTime, values):\n                return dateTime.year == 2013\n\n        feed = csvfeed.Feed(\"Date\", \"%Y-%m-%d\", maxLen=40, timezone=marketsession.USEquities.timezone)\n        feed.setRowFilter(RowFilter())\n        feed.setTimeDelta(datetime.timedelta(hours=23, minutes=59, seconds=59))\n        feed.addValuesFromCSV(os.path.join(\"samples\", \"data\", \"quandl_gold_2.csv\"))\n\n        for col in [\"USD\", \"GBP\", \"EUR\"]:\n            self.assertEqual(len(feed[col]), 0)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        for col in [\"USD\", \"GBP\", \"EUR\"]:\n            self.assertEqual(len(feed[col]), 39)\n\n        self.assertEqual(feed[\"USD\"][-1], 1333.0)\n        self.assertEqual(feed[\"GBP\"][-1], 831.203)\n        self.assertEqual(feed[\"EUR\"][-1], 986.75)\n        self.assertFalse(dt.datetime_is_naive(feed[\"USD\"].getDateTimes()[-1]))\n        self.assertEqual(\n            feed[\"USD\"].getDateTimes()[-1],\n            dt.localize(datetime.datetime(2013, 9, 29, 23, 59, 59), marketsession.USEquities.timezone)\n        )\n\n    def testReset(self):\n        feed = csvfeed.Feed(\"Date\", \"%Y-%m-%d\")\n        feed.addValuesFromCSV(common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        keys = feed.getKeys()\n        key = keys[0]\n        values = feed[key]\n\n        feed.reset()\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        reloadedKeys = feed.getKeys()\n        reloadedValues = feed[key]\n\n        self.assertEqual(keys.sort(), reloadedKeys.sort())\n        self.assertNotEqual(values, reloadedValues)\n        self.assertEqual(len(values), len(reloadedValues))\n        for i in range(len(values)):\n            self.assertEqual(values[i], reloadedValues[i])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.dataseries import bards\nfrom pyalgotrade.dataseries import aligned\nfrom pyalgotrade import bar\n\n\nclass TestSequenceDataSeries(common.TestCase):\n    def testEmpty(self):\n        ds = dataseries.SequenceDataSeries()\n        self.assertTrue(len(ds) == 0)\n        with self.assertRaises(IndexError):\n            ds[-1]\n        with self.assertRaises(IndexError):\n            ds[-2]\n        with self.assertRaises(IndexError):\n            ds[0]\n        with self.assertRaises(IndexError):\n            ds[1]\n\n    def testNonEmpty(self):\n        ds = dataseries.SequenceDataSeries()\n        for value in range(10):\n            ds.append(value)\n        self.assertTrue(len(ds) == 10)\n        self.assertTrue(ds[-1] == 9)\n        self.assertTrue(ds[-2] == 8)\n        self.assertTrue(ds[0] == 0)\n        self.assertTrue(ds[1] == 1)\n\n        self.assertTrue(ds[-1:] == [9])\n        self.assertTrue(ds[-2:] == [8, 9])\n        self.assertTrue(ds[-2:-1] == [8])\n        self.assertTrue(ds[-3:-1] == [7, 8])\n\n        self.assertTrue(ds[1:4] == [1, 2, 3])\n        self.assertTrue(ds[9:10] == [9])\n        self.assertTrue(ds[9:11] == [9])\n        self.assertTrue(ds[9:] == [9])\n\n    def testSeqLikeOps(self):\n        seq = range(10)\n        ds = dataseries.SequenceDataSeries()\n        for value in seq:\n            ds.append(value)\n\n        # Test length and every item.\n        self.assertEqual(len(ds), len(seq))\n        for i in xrange(len(seq)):\n            self.assertEqual(ds[i], seq[i])\n\n        # Test negative indices\n        self.assertEqual(ds[-1], seq[-1])\n        self.assertEqual(ds[-2], seq[-2])\n        self.assertEqual(ds[-9], seq[-9])\n\n        # Test slices\n        sl = slice(0, 1, 2)\n        self.assertEqual(ds[sl], seq[sl])\n        sl = slice(0, 9, 2)\n        self.assertEqual(ds[sl], seq[sl])\n        sl = slice(0, -1, 1)\n        self.assertEqual(ds[sl], seq[sl])\n\n        for i in xrange(-100, 100):\n            self.assertEqual(ds[i:], seq[i:])\n\n        for step in xrange(1, 10):\n            for i in xrange(-100, 100):\n                self.assertEqual(ds[i::step], seq[i::step])\n\n    def testBounded(self):\n        ds = dataseries.SequenceDataSeries(maxLen=2)\n        for i in xrange(100):\n            ds.append(i)\n            if i > 0:\n                self.assertEqual(ds[0], i - 1)\n                self.assertEqual(ds[1], i)\n        self.assertEqual(len(ds), 2)\n\n    def testResize1(self):\n        ds = dataseries.SequenceDataSeries(100)\n        for i in xrange(100):\n            ds.append(i)\n\n        self.assertEqual(len(ds), 100)\n        self.assertEqual(len(ds.getDateTimes()), 100)\n        self.assertEqual(ds[0], 0)\n        self.assertEqual(ds[-1], 99)\n\n        ds.setMaxLen(2)\n        self.assertEqual(len(ds), 2)\n        self.assertEqual(len(ds.getDateTimes()), 2)\n        self.assertEqual(ds[0], 98)\n        self.assertEqual(ds[1], 99)\n\n    def testResize2(self):\n        ds = dataseries.SequenceDataSeries()\n        for i in xrange(100):\n            ds.append(i)\n\n        ds.setMaxLen(1000)\n        self.assertEqual(len(ds), 100)\n        self.assertEqual(len(ds.getDateTimes()), 100)\n        self.assertEqual(ds[0], 0)\n        self.assertEqual(ds[-1], 99)\n\n        ds.setMaxLen(10000)\n        self.assertEqual(len(ds), 100)\n        self.assertEqual(len(ds.getDateTimes()), 100)\n        self.assertEqual(ds[0], 0)\n        self.assertEqual(ds[-1], 99)\n\n        ds.setMaxLen(10)\n        self.assertEqual(len(ds), 10)\n        self.assertEqual(len(ds.getDateTimes()), 10)\n        self.assertEqual(ds[0], 90)\n        self.assertEqual(ds[-1], 99)\n\n\nclass TestBarDataSeries(common.TestCase):\n    def testEmpty(self):\n        ds = bards.BarDataSeries()\n        with self.assertRaises(IndexError):\n            ds[-1]\n        with self.assertRaises(IndexError):\n            ds[0]\n        with self.assertRaises(IndexError):\n            ds[1000]\n\n    def testAppendInvalidDatetime(self):\n        ds = bards.BarDataSeries()\n        for i in range(10):\n            now = datetime.datetime.now() + datetime.timedelta(seconds=i)\n            ds.append(bar.BasicBar(now, 0, 0, 0, 0, 0, 0, bar.Frequency.SECOND))\n            # Adding the same datetime twice should fail\n            with self.assertRaises(Exception):\n                ds.append(bar.BasicBar(now, 0, 0, 0, 0, 0, 0, bar.Frequency.SECOND))\n            # Adding a previous datetime should fail\n            with self.assertRaises(Exception):\n                ds.append(bar.BasicBar(now - datetime.timedelta(seconds=i), 0, 0, 0, 0, 0, 0, bar.Frequency.SECOND))\n\n    def testNonEmpty(self):\n        ds = bards.BarDataSeries()\n        for i in range(10):\n            ds.append(bar.BasicBar(datetime.datetime.now() + datetime.timedelta(seconds=i), 0, 0, 0, 0, 0, 0, bar.Frequency.SECOND))\n\n        for i in range(0, 10):\n            self.assertTrue(ds[i].getOpen() == 0)\n\n    def __testGetValue(self, ds, itemCount, value):\n        for i in range(0, itemCount):\n            self.assertTrue(ds[i] == value)\n\n    def testNestedDataSeries(self):\n        ds = bards.BarDataSeries()\n        for i in range(10):\n            ds.append(bar.BasicBar(datetime.datetime.now() + datetime.timedelta(seconds=i), 2, 4, 1, 3, 10, 3, bar.Frequency.SECOND))\n\n        self.__testGetValue(ds.getOpenDataSeries(), 10, 2)\n        self.__testGetValue(ds.getCloseDataSeries(), 10, 3)\n        self.__testGetValue(ds.getHighDataSeries(), 10, 4)\n        self.__testGetValue(ds.getLowDataSeries(), 10, 1)\n        self.__testGetValue(ds.getVolumeDataSeries(), 10, 10)\n        self.__testGetValue(ds.getAdjCloseDataSeries(), 10, 3)\n        self.__testGetValue(ds.getPriceDataSeries(), 10, 3)\n\n    def testSeqLikeOps(self):\n        seq = []\n        ds = bards.BarDataSeries()\n        for i in range(10):\n            bar_ = bar.BasicBar(datetime.datetime.now() + datetime.timedelta(seconds=i), 2, 4, 1, 3, 10, 3, bar.Frequency.SECOND)\n            ds.append(bar_)\n            seq.append(bar_)\n\n        self.assertEqual(ds[-1], seq[-1])\n        self.assertEqual(ds[-2], seq[-2])\n        self.assertEqual(ds[0], seq[0])\n        self.assertEqual(ds[1], seq[1])\n        self.assertEqual(ds[-2:][-1], seq[-2:][-1])\n\n    def testDateTimes(self):\n        ds = bards.BarDataSeries()\n        firstDt = datetime.datetime.now()\n        for i in range(10):\n            ds.append(bar.BasicBar(firstDt + datetime.timedelta(seconds=i), 2, 4, 1, 3, 10, 3, bar.Frequency.SECOND))\n\n        for i in range(10):\n            self.assertEqual(ds[i].getDateTime(), ds.getDateTimes()[i])\n            self.assertEqual(ds.getDateTimes()[i], firstDt + datetime.timedelta(seconds=i))\n\n\nclass TestDateAlignedDataSeries(common.TestCase):\n    def testNotAligned(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            if i % 2 == 0:\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            else:\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ds1), len(ds2))\n\n        for ads in [ads1, ads2]:\n            self.assertEqual(len(ads), 0)\n            self.assertEqual(ads.getValueAbsolute(0), None)\n            self.assertEqual(ads.getDateTimes(), [])\n\n    def testFullyAligned(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ds1), len(ds2))\n\n        for ads in [ads1, ads2]:\n            self.assertEqual(len(ads), size)\n            for i in range(size):\n                self.assertEqual(ads.getValueAbsolute(i), i)\n                self.assertEqual(ads.getDateTimes()[i], now + datetime.timedelta(seconds=i))\n\n    def testPartiallyAligned(self):\n        size = 20\n        commonDateTimes = []\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            if i % 3 == 0:\n                commonDateTimes.append(now + datetime.timedelta(seconds=i))\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            elif i % 2 == 0:\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            else:\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ads1), len(ads2))\n        self.assertEqual(ads1[:], ads2[:])\n        self.assertEqual(ads1.getDateTimes(), commonDateTimes)\n        self.assertEqual(ads2.getDateTimes(), commonDateTimes)\n\n    def testIncremental(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            self.assertEqual(len(ads1), len(ads2))\n            self.assertEqual(ads1[:], ads2[:])\n            self.assertEqual(ads1.getDateTimes()[:], ads2.getDateTimes()[:])\n\n    def testNotAligned_Recursive(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n        # Align the aligned dataseries again with respect to each other.\n        ads1, ads2 = aligned.datetime_aligned(ads1, ads2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            if i % 2 == 0:\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            else:\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ds1), len(ds2))\n\n        for ads in [ads1, ads2]:\n            self.assertEqual(len(ads), 0)\n            self.assertEqual(ads.getValueAbsolute(0), None)\n            self.assertEqual(ads.getDateTimes(), [])\n\n    def testFullyAligned_Recursive(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n        # Align the aligned dataseries again with respect to each other.\n        ads1, ads2 = aligned.datetime_aligned(ads1, ads2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ds1), len(ds2))\n\n        for ads in [ads1, ads2]:\n            self.assertEqual(len(ads), size)\n            for i in range(size):\n                self.assertEqual(ads.getValueAbsolute(i), i)\n                self.assertEqual(ads.getDateTimes()[i], now + datetime.timedelta(seconds=i))\n\n    def testPartiallyAligned_Recursive(self):\n        size = 20\n        commonDateTimes = []\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n        # Align the aligned dataseries again with respect to each other.\n        ads1, ads2 = aligned.datetime_aligned(ads1, ads2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            if i % 3 == 0:\n                commonDateTimes.append(now + datetime.timedelta(seconds=i))\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            elif i % 2 == 0:\n                ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            else:\n                ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n\n        self.assertEqual(len(ads1), len(ads2))\n        self.assertEqual(ads1[:], ads2[:])\n        self.assertEqual(ads1.getDateTimes(), commonDateTimes)\n        self.assertEqual(ads2.getDateTimes(), commonDateTimes)\n\n    def testIncremental_Recursive(self):\n        size = 20\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n        # Align the aligned dataseries again with respect to each other.\n        ads1, ads2 = aligned.datetime_aligned(ads1, ads2)\n\n        now = datetime.datetime.now()\n        for i in range(size):\n            ds1.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            ds2.appendWithDateTime(now + datetime.timedelta(seconds=i), i)\n            self.assertEqual(len(ads1), len(ads2))\n            self.assertEqual(ads1[:], ads2[:])\n            self.assertEqual(ads1.getDateTimes()[:], ads2.getDateTimes()[:])\n\n    def testBounded(self):\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2, 1)\n\n        now = datetime.datetime.now()\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=1), 1)\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=2), 2)\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=3), 3)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=2), 2)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=3), 3)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=4), 4)\n        self.assertEqual(ads1[:], [3])\n        self.assertEqual(ads2[:], [3])\n\n    def testBoundedSources(self):\n        ds1 = dataseries.SequenceDataSeries(1)\n        ds2 = dataseries.SequenceDataSeries(1)\n        ads1, ads2 = aligned.datetime_aligned(ds1, ds2)\n\n        now = datetime.datetime.now()\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=1), 1)\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=2), 2)\n        ds1.appendWithDateTime(now + datetime.timedelta(seconds=3), 3)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=2), 2)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=3), 3)\n        ds2.appendWithDateTime(now + datetime.timedelta(seconds=4), 4)\n        self.assertEqual(ads1[:], [2, 3])\n        self.assertEqual(ads2[:], [2, 3])\n\n\nclass TestUpdatedDefaultMaxLen(common.TestCase):\n    def setUp(self):\n        super(TestUpdatedDefaultMaxLen, self).setUp()\n        self.__default_max_len = dataseries.DEFAULT_MAX_LEN\n        dataseries.DEFAULT_MAX_LEN = 2048\n\n    def tearDown(self):\n        super(TestUpdatedDefaultMaxLen, self).tearDown()\n        dataseries.DEFAULT_MAX_LEN = self.__default_max_len\n\n    def testMaxLen(self):\n        ds = dataseries.SequenceDataSeries()\n        for i in range(3000):\n            ds.append(i)\n        self.assertEqual(len(ds), 2048)\n        self.assertEqual(ds[0], 952)\n        self.assertEqual(ds[-1], 2999)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\n\nimport common\nimport feed_test\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import sqlitefeed\nfrom pyalgotrade import bar\nfrom pyalgotrade import marketsession\n\n\nclass TemporarySQLiteFeed:\n    def __init__(self, dbFilePath, frequency, maxLen=None):\n        if os.path.exists(dbFilePath):\n            raise Exception(\"File exists\")\n\n        self.__dbFilePath = dbFilePath\n        self.__frequency = frequency\n        self.__feed = None\n        self.__maxLen = maxLen\n\n    def __enter__(self):\n        self.__feed = sqlitefeed.Feed(self.__dbFilePath, self.__frequency, maxLen=self.__maxLen)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.__feed.getDatabase().disconnect()  # This is for the feed to release the file and get it deleted.\n        self.__feed = None\n        os.remove(self.__dbFilePath)\n\n    def getFeed(self):\n        return self.__feed\n\n\nclass SQLiteFeedTestCase(common.TestCase):\n    dbName = \"SQLiteFeedTestCase.sqlite\"\n\n    def testBaseFeedInterface(self):\n        tmpFeed = TemporarySQLiteFeed(SQLiteFeedTestCase.dbName, bar.Frequency.DAY)\n        with tmpFeed:\n            # Load bars using a Yahoo! feed.\n            yahooFeed = yahoofeed.Feed()\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.timezone)\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"), marketsession.USEquities.timezone)\n\n            # Fill the database using the bars from the Yahoo! feed.\n            sqliteFeed = tmpFeed.getFeed()\n            sqliteFeed.getDatabase().addBarsFromFeed(yahooFeed)\n\n            # Load the SQLite feed and process all bars.\n            sqliteFeed.loadBars(\"orcl\")\n            feed_test.tstBaseFeedInterface(self, sqliteFeed)\n\n    def testLoadDailyBars(self):\n        tmpFeed = TemporarySQLiteFeed(SQLiteFeedTestCase.dbName, bar.Frequency.DAY)\n        with tmpFeed:\n            # Load bars using a Yahoo! feed.\n            yahooFeed = yahoofeed.Feed()\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.timezone)\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"), marketsession.USEquities.timezone)\n\n            # Fill the database using the bars from the Yahoo! feed.\n            sqliteFeed = tmpFeed.getFeed()\n            sqliteFeed.getDatabase().addBarsFromFeed(yahooFeed)\n\n            # Load the SQLite feed and process all bars.\n            sqliteFeed.loadBars(\"orcl\")\n            for bars in sqliteFeed:\n                pass\n\n            # Check that both dataseries have the same bars.\n            yahooDS = yahooFeed[\"orcl\"]\n            sqliteDS = sqliteFeed[\"orcl\"]\n            self.assertEqual(len(yahooDS), len(sqliteDS))\n            for i in xrange(len(yahooDS)):\n                self.assertEqual(yahooDS[i].getDateTime(), sqliteDS[i].getDateTime())\n                self.assertEqual(yahooDS[i].getOpen(), sqliteDS[i].getOpen())\n                self.assertEqual(yahooDS[i].getHigh(), sqliteDS[i].getHigh())\n                self.assertEqual(yahooDS[i].getLow(), sqliteDS[i].getLow())\n                self.assertEqual(yahooDS[i].getClose(), sqliteDS[i].getClose())\n                self.assertEqual(yahooDS[i].getAdjClose(), sqliteDS[i].getAdjClose())\n\n    def testBounded(self):\n        tmpFeed = TemporarySQLiteFeed(SQLiteFeedTestCase.dbName, bar.Frequency.DAY, maxLen=2)\n        with tmpFeed:\n            # Load bars using a Yahoo! feed.\n            yahooFeed = yahoofeed.Feed(maxLen=1)\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.timezone)\n            yahooFeed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"), marketsession.USEquities.timezone)\n\n            # Fill the database using the bars from the Yahoo! feed.\n            sqliteFeed = tmpFeed.getFeed()\n            sqliteFeed.getDatabase().addBarsFromFeed(yahooFeed)\n\n            # Load the SQLite feed and process all bars.\n            sqliteFeed.loadBars(\"orcl\")\n            for bars in sqliteFeed:\n                pass\n\n            barDS = sqliteFeed[\"orcl\"]\n            self.assertEqual(len(barDS), 2)\n            self.assertEqual(len(barDS.getDateTimes()), 2)\n            self.assertEqual(len(barDS.getCloseDataSeries()), 2)\n            self.assertEqual(len(barDS.getCloseDataSeries().getDateTimes()), 2)\n            self.assertEqual(len(barDS.getOpenDataSeries()), 2)\n            self.assertEqual(len(barDS.getHighDataSeries()), 2)\n            self.assertEqual(len(barDS.getLowDataSeries()), 2)\n            self.assertEqual(len(barDS.getAdjCloseDataSeries()), 2)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\nimport strategy_test\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import membf\nfrom pyalgotrade.stratanalyzer import drawdown\nfrom pyalgotrade import broker\nfrom pyalgotrade import bar\n\n\ndef build_bars_from_closing_prices(closingPrices):\n    ret = []\n\n    nextDateTime = datetime.datetime.now()\n    for closePrice in closingPrices:\n        bar_ = bar.BasicBar(nextDateTime, closePrice, closePrice, closePrice, closePrice, closePrice, closePrice, bar.Frequency.DAY)\n        ret.append(bar_)\n        nextDateTime = nextDateTime + datetime.timedelta(days=1)\n    return ret\n\n\nclass TestBarFeed(membf.BarFeed):\n    def barsHaveAdjClose(self):\n        raise NotImplementedError()\n\n\nclass DDHelperCase(common.TestCase):\n    def testNoDrawDown1(self):\n        helper = drawdown.DrawDownHelper()\n        helper.update(datetime.datetime.now(), 10, 10)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n    def testNoDrawDown2(self):\n        helper = drawdown.DrawDownHelper()\n\n        dt = datetime.datetime.now()\n        helper.update(dt, 10, 10)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 10.01, 10.01)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 11, 11)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n    def testDrawDown1(self):\n        helper = drawdown.DrawDownHelper()\n\n        dt = datetime.datetime.now()\n        helper.update(dt, 10, 10)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 5, 5)\n        self.assertEqual(helper.getMaxDrawDown(), -0.5)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.5)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=1))\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 4, 4)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.6)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=2))\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 4, 4)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.6)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=3))\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 5, 5)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.5)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=4))\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 9, 9)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.1)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=5))\n\n        dt += datetime.timedelta(days=1)\n        helper.update(dt, 9.9, 9.9)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(round(helper.getCurrentDrawDown(), 2), -0.01)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(days=6))\n\n    def testDrawDown2(self):\n        helper = drawdown.DrawDownHelper()\n\n        dt = datetime.datetime.now()\n        helper.update(dt, 10, 10)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 5, 5)\n        self.assertEqual(helper.getMaxDrawDown(), -0.5)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.5)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=1))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 4, 4)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.6)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=2))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 4, 4)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.6)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=3))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 5, 5)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.5)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=4))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 9, 9)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.1)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=5))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 9.9, 9.9)\n        self.assertEqual(helper.getMaxDrawDown(), -0.6)\n        self.assertEqual(round(helper.getCurrentDrawDown(), 2), -0.01)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=6))\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 20, 20)\n        self.assertEqual(helper.getMaxDrawDown(), 0)\n        self.assertEqual(helper.getCurrentDrawDown(), 0)\n        self.assertEqual(helper.getDuration(), datetime.timedelta())\n\n        dt += datetime.timedelta(minutes=1)\n        helper.update(dt, 10, 10)\n        self.assertEqual(helper.getMaxDrawDown(), -0.5)\n        self.assertEqual(helper.getCurrentDrawDown(), -0.5)\n        self.assertEqual(helper.getDuration(), datetime.timedelta(minutes=1))\n\n\nclass AnalyzerTestCase(common.TestCase):\n    def testNoTrades(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"ige\", common.get_data_file_path(\"sharpe-ratio-test-ige.csv\"))\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"sharpe-ratio-test-spy.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, 1000)\n        strat.setBrokerOrdersGTC(True)\n        strat.setUseAdjustedValues(True)\n        stratAnalyzer = drawdown.DrawDown()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == 1000)\n        self.assertEqual(strat.orderUpdatedCalls, 0)\n        self.assertTrue(stratAnalyzer.getMaxDrawDown() == 0)\n        self.assertTrue(stratAnalyzer.getLongestDrawDownDuration() == datetime.timedelta())\n\n    def __testIGE_BrokerImpl(self, quantity):\n        initialCash = 42.09*quantity\n        # This testcase is based on an example from Ernie Chan's book:\n        # 'Quantitative Trading: How to Build Your Own Algorithmic Trading Business'\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"ige\", common.get_data_file_path(\"sharpe-ratio-test-ige.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n        strat.setUseAdjustedValues(True)\n        strat.setBrokerOrdersGTC(True)\n        stratAnalyzer = drawdown.DrawDown()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Disable volume checks to match book results.\n        strat.getBroker().getFillStrategy().setVolumeLimit(None)\n\n        # Manually place the order to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, \"ige\", quantity, True)  # Adj. Close: 42.09\n        order.setGoodTillCanceled(True)\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2007, 11, 13), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, \"ige\", quantity, True)  # Adj. Close: 127.64\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == initialCash + (127.64 - 42.09) * quantity)\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        self.assertTrue(round(stratAnalyzer.getMaxDrawDown(), 5) == 0.31178)\n        self.assertTrue(stratAnalyzer.getLongestDrawDownDuration() == datetime.timedelta(days=623))\n\n    def testIGE_Broker(self):\n        self.__testIGE_BrokerImpl(1)\n\n    def testIGE_Broker2(self):\n        self.__testIGE_BrokerImpl(2)\n\n    def __testManualImpl(self, closingPrices, cash):\n        barFeed = TestBarFeed(bar.Frequency.DAY)\n        bars = build_bars_from_closing_prices(closingPrices)\n        barFeed.addBarsFromSequence(\"orcl\", bars)\n\n        strat = strategy_test.TestStrategy(barFeed, cash)\n        stratAnalyzer = drawdown.DrawDown()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Manually place the order to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, \"orcl\", 1, True)\n        order.setGoodTillCanceled(True)\n        strat.getBroker().submitOrder(order)\n\n        strat.run()\n        return stratAnalyzer\n\n    def testManual_NoDD(self):\n        # No drawdown\n        stratAnalyzer = self.__testManualImpl([10, 10, 10], 10)\n        self.assertEqual(round(stratAnalyzer.getMaxDrawDown(), 2), 0)\n        self.assertEqual(stratAnalyzer.getLongestDrawDownDuration(), datetime.timedelta())\n\n    def testManual_1DD(self):\n        stratAnalyzer = self.__testManualImpl([10, 9, 8], 10)\n        self.assertEqual(round(stratAnalyzer.getMaxDrawDown(), 2), 0.2)\n        self.assertEqual(stratAnalyzer.getLongestDrawDownDuration(), datetime.timedelta(days=2))\n\n    def testManual_2DD(self):\n        stratAnalyzer = self.__testManualImpl([10, 9.5, 9, 8, 11, 8], 10)\n        self.assertEqual(round(stratAnalyzer.getMaxDrawDown(), 2), 0.27)\n        self.assertEqual(stratAnalyzer.getLongestDrawDownDuration(), datetime.timedelta(days=3))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import eventprofiler\nfrom pyalgotrade.barfeed import yahoofeed\n\n\nclass Predicate(eventprofiler.Predicate):\n    def __init__(self, eventDates):\n        self.__dates = eventDates\n\n    def eventOccurred(self, instrument, bards):\n        ret = False\n        if bards[-1].getDateTime().date() in self.__dates:\n            ret = True\n        return ret\n\n\nclass EventProfilerTestCase(common.TestCase):\n    def testNoEvents(self):\n        feed = yahoofeed.Feed()\n        feed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n\n        predicate = Predicate([])\n        eventProfiler = eventprofiler.Profiler(predicate, 5, 5)\n        eventProfiler.run(feed, True)\n        self.assertEqual(eventProfiler.getResults().getEventCount(), 0)\n\n    def testEventsOnBoundary(self):\n        feed = yahoofeed.Feed()\n        feed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n\n        dates = []\n        dates.append(datetime.date(2000, 1, 3))\n        dates.append(datetime.date(2000, 1, 4))\n        dates.append(datetime.date(2000, 1, 5))\n        dates.append(datetime.date(2000, 1, 6))\n        dates.append(datetime.date(2000, 1, 7))\n        dates.append(datetime.date(2000, 1, 10))\n        dates.append(datetime.date(2000, 12, 22))\n        dates.append(datetime.date(2000, 12, 26))\n        dates.append(datetime.date(2000, 12, 27))\n        dates.append(datetime.date(2000, 12, 28))\n        dates.append(datetime.date(2000, 12, 29))\n        predicate = Predicate(dates)\n        eventProfiler = eventprofiler.Profiler(predicate, 5, 5)\n        eventProfiler.run(feed, True)\n        self.assertEqual(eventProfiler.getResults().getEventCount(), 0)\n\n    def testOneEvent(self):\n        feed = yahoofeed.Feed()\n        feed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n\n        predicate = Predicate([datetime.date(2000, 1, 11)])\n        eventProfiler = eventprofiler.Profiler(predicate, 5, 5)\n        eventProfiler.run(feed, True)\n        self.assertEqual(eventProfiler.getResults().getEventCount(), 1)\n        self.assertEqual(eventProfiler.getResults().getValues(0)[0], 1.0)\n        self.assertEqual(round(eventProfiler.getResults().getValues(5)[0], 5), round(1.016745541, 5))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport unittest\nimport datetime\n\nimport broker_backtesting_test\n\nfrom pyalgotrade import broker\nfrom pyalgotrade.broker import fillstrategy\nfrom pyalgotrade.broker import backtesting\nfrom pyalgotrade import bar\n\n\nclass BaseTestCase(unittest.TestCase):\n    TestInstrument = \"orcl\"\n\n\nclass FreeFunctionsTestCase(BaseTestCase):\n    def testStopOrderTriggerBuy(self):\n        barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        # Bar is below\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(5, 5, 5, 5)), None)\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(5, 6, 4, 5)), None)\n        # High touches\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(5, 10, 4, 9)), 10)\n        # High penetrates\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(5, 11, 4, 9)), 10)\n        # Open touches\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(10, 10, 10, 10)), 10)\n        # Open is above\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(11, 12, 4, 9)), 11)\n        # Bar gaps above\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(12, 13, 11, 12)), 12)\n\n    def testStopOrderTriggerSell(self):\n        barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        # Bar is above\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(15, 15, 15, 15)), None)\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(15, 16, 11, 15)), None)\n        # Low touches\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(15, 16, 10, 11)), 10)\n        # Low penetrates\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(15, 16, 9, 11)), 10)\n        # Open touches\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(10, 10, 10, 10)), 10)\n        # Open is below\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(9, 12, 4, 9)), 9)\n        # Bar gaps below\n        self.assertEqual(fillstrategy.get_stop_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(8, 9, 6, 9)), 8)\n\n    def testLimitOrderTriggerBuy(self):\n        barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        # Bar is above\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(15, 15, 15, 15)), None)\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(15, 16, 11, 15)), None)\n        # Low touches\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(15, 16, 10, 11)), 10)\n        # Low penetrates\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(15, 16, 9, 11)), 10)\n        # Open touches\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(10, 10, 10, 10)), 10)\n        # Open is below\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(9, 12, 4, 9)), 9)\n        # Bar gaps below\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.BUY, 10, False, barsBuilder.nextBar(8, 9, 6, 9)), 8)\n\n    def testLimitOrderTriggerSell(self):\n        barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        # Bar is below\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(5, 5, 5, 5)), None)\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(5, 6, 4, 5)), None)\n        # High touches\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(5, 10, 4, 9)), 10)\n        # High penetrates\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(5, 11, 4, 9)), 10)\n        # Open touches\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(10, 10, 10, 10)), 10)\n        # Open is above\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(11, 12, 4, 9)), 11)\n        # Bar gaps above\n        self.assertEqual(fillstrategy.get_limit_price_trigger(broker.Order.Action.SELL, 10, False, barsBuilder.nextBar(12, 13, 11, 12)), 12)\n\n\nclass DefaultStrategyTestCase(BaseTestCase):\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.MINUTE)\n        self.strategy = fillstrategy.DefaultStrategy()\n\n    def __getFilledMarketOrder(self, quantity, price):\n        order = backtesting.MarketOrder(\n            broker.Order.Action.BUY,\n            BaseTestCase.TestInstrument,\n            quantity,\n            False,\n            broker.IntegerTraits()\n        )\n        order.setState(broker.Order.State.ACCEPTED)\n        order.addExecutionInfo(broker.OrderExecutionInfo(price, quantity, 0, datetime.datetime.now()))\n        return order\n\n    def testVolumeLimitPerBar(self):\n        volume = 100\n        self.strategy.onBars(None, self.barsBuilder.nextBars(11, 12, 4, 9, volume))\n        self.assertEquals(self.strategy.getVolumeLeft()[BaseTestCase.TestInstrument], 25)\n        self.assertEquals(self.strategy.getVolumeUsed()[BaseTestCase.TestInstrument], 0)\n\n        self.strategy.onOrderFilled(None, self.__getFilledMarketOrder(24, 11))\n        self.assertEquals(self.strategy.getVolumeLeft()[BaseTestCase.TestInstrument], 1)\n        self.assertEquals(self.strategy.getVolumeUsed()[BaseTestCase.TestInstrument], 24)\n\n        with self.assertRaisesRegexp(Exception, \"Invalid fill quantity 25. Not enough volume left 1\"):\n            self.strategy.onOrderFilled(None, self.__getFilledMarketOrder(25, 11))\n        self.assertEquals(self.strategy.getVolumeLeft()[BaseTestCase.TestInstrument], 1)\n        self.assertEquals(self.strategy.getVolumeUsed()[BaseTestCase.TestInstrument], 24)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\nimport datetime\n\nimport common\n\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import googlefeed\nfrom pyalgotrade.tools import googlefinance\n\n\nclass ToolsTestCase(common.TestCase):\n    def testDownloadAndParseDaily(self):\n        instrument = \"orcl\"\n\n        with common.TmpDir() as tmp_path:\n            path = os.path.join(tmp_path, \"orcl-2010.csv\")\n            googlefinance.download_daily_bars(instrument, 2010, path)\n            bf = googlefeed.Feed()\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            self.assertEqual(bf[instrument][-1].getOpen(), 31.22)\n            self.assertEqual(bf[instrument][-1].getClose(), 31.30)\n\n    def testBuildDailyFeed(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"orcl\"\n            bf = googlefinance.build_feed([instrument], 2010, 2010, storage=tmpPath)\n            bf.loadAll()\n            self.assertEqual(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 31))\n            self.assertEqual(bf[instrument][-1].getOpen(), 31.22)\n            self.assertEqual(bf[instrument][-1].getClose(), 31.30)\n\n    def testInvalidInstrument(self):\n        instrument = \"inexistent\"\n\n        # Don't skip errors.\n        with self.assertRaisesRegexp(Exception, \"400 Client Error: Bad Request\"):\n            with common.TmpDir() as tmpPath:\n                bf = googlefinance.build_feed([instrument], 2100, 2101, storage=tmpPath, frequency=bar.Frequency.DAY)\n\n        # Skip errors.\n        with common.TmpDir() as tmpPath:\n            bf = googlefinance.build_feed(\n                [instrument], 2100, 2101, storage=tmpPath, frequency=bar.Frequency.DAY, skipErrors=True\n            )\n            bf.loadAll()\n            self.assertNotIn(instrument, bf)\n",
          "import threading\nimport BaseHTTPServer\n\n\nclass WebServerThread(threading.Thread):\n    def __init__(self, host, port, handlerClass):\n        super(WebServerThread, self).__init__()\n        self.__host = host\n        self.__port = port\n        self.__handlerClass = handlerClass\n        self.__server = None\n\n    def run(self):\n        def handler_cls_builder(*args, **kwargs):\n            return self.__handlerClass(*args, **kwargs)\n\n        self.__server = BaseHTTPServer.HTTPServer((self.__host, self.__port), handler_cls_builder)\n        self.__server.serve_forever()\n\n    def stop(self):\n        self.__server.shutdown()\n\n\n# handlerClass should be a subclass of (BaseHTTPServer.BaseHTTPRequestHandler.\n# The handler instance will get a thread and server attribute injected.\ndef run_webserver_thread(host, port, handlerClass):\n    wss_thread = WebServerThread(host, port, handlerClass)\n    wss_thread.start()\n    return wss_thread\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade.feed import memfeed\nfrom pyalgotrade import dispatcher\nimport feed_test\n\n\nclass MemFeedTestCase(common.TestCase):\n    def testBaseFeedInterface(self):\n        values = [(datetime.datetime.now() + datetime.timedelta(seconds=i), {\"i\": i}) for i in xrange(100)]\n        feed = memfeed.MemFeed()\n        feed.addValues(values)\n        feed_test.tstBaseFeedInterface(self, feed)\n\n    def testFeed(self):\n        values = [(datetime.datetime.now() + datetime.timedelta(seconds=i), {\"i\": i}) for i in xrange(100)]\n\n        feed = memfeed.MemFeed()\n        feed.addValues(values)\n\n        # Check that the dataseries are available after adding values.\n        self.assertTrue(\"i\" in feed)\n        self.assertEqual(len(feed[\"i\"]), 0)\n        self.assertFalse(\"dt\" in feed)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        self.assertTrue(\"i\" in feed)\n        self.assertFalse(\"dt\" in feed)\n        self.assertEqual(feed[\"i\"][0], 0)\n        self.assertEqual(feed[\"i\"][-1], 99)\n\n    def testReset(self):\n        key = \"i\"\n        values = [(datetime.datetime.now() + datetime.timedelta(seconds=i), {key: i}) for i in xrange(100)]\n\n        feed = memfeed.MemFeed()\n        feed.addValues(values)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n\n        keys = feed.getKeys()\n        values = feed[key]\n\n        feed.reset()\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed)\n        disp.run()\n        reloadedKeys = feed.getKeys()\n        reloadedValues = feed[key]\n\n        self.assertEqual(keys, reloadedKeys)\n        self.assertNotEqual(values, reloadedValues)\n        self.assertEqual(len(values), len(reloadedValues))\n        for i in range(len(values)):\n            self.assertEqual(values[i], reloadedValues[i])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import sqlitefeed\nfrom pyalgotrade import marketsession\nfrom pyalgotrade import strategy\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade.technical import cross\n\n\nclass NikkeiSpyStrategy(strategy.BacktestingStrategy):\n    def __init__(self, feed, smaPeriod):\n        strategy.BacktestingStrategy.__init__(self, feed)\n\n        assert(smaPeriod > 3)\n        self.__lead = \"^n225\"\n        self.__lag = \"spy\"\n        self.__adjClose = feed[self.__lead].getAdjCloseDataSeries()\n        # Exit signal is more sensitive than entry.\n        self.__fastSMA = ma.SMA(self.__adjClose, int(smaPeriod/2))\n        self.__slowSMA = ma.SMA(self.__adjClose, smaPeriod)\n        self.__pos = None\n\n    def onEnterCanceled(self, position):\n        assert(position == self.__pos)\n        self.__pos = None\n\n    def onExitOk(self, position):\n        assert(position == self.__pos)\n        self.__pos = None\n\n    def __calculatePosSize(self):\n        cash = self.getBroker().getCash()\n        lastPrice = self.getFeed()[self.__lag][-1].getClose()\n        ret = cash / lastPrice\n        return int(ret)\n\n    def onBars(self, bars):\n        if bars.getBar(self.__lead):\n            if cross.cross_above(self.__adjClose, self.__slowSMA) == 1 and self.__pos is None:\n                shares = self.__calculatePosSize()\n                if shares:\n                    self.__pos = self.enterLong(self.__lag, shares)\n            elif cross.cross_below(self.__adjClose, self.__fastSMA) == 1 and self.__pos is not None:\n                self.__pos.exitMarket()\n\n\nclass TestCase(common.TestCase):\n    def __testDifferentTimezonesImpl(self, feed):\n        self.assertTrue(\"^n225\" in feed)\n        self.assertTrue(\"spy\" in feed)\n        self.assertTrue(\"cacho\" not in feed)\n        strat = NikkeiSpyStrategy(feed, 34)\n        strat.run()\n        self.assertEqual(round(strat.getResult(), 2), 1033854.48)\n\n    def testDifferentTimezones(self):\n        # Market times in UTC:\n        # - TSE: 0hs ~ 6hs\n        # - US: 14:30hs ~ 21hs\n        feed = yahoofeed.Feed()\n        for year in [2010, 2011]:\n            feed.addBarsFromCSV(\"^n225\", common.get_data_file_path(\"nikkei-%d-yahoofinance.csv\" % year), marketsession.TSE.getTimezone())\n            feed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"spy-%d-yahoofinance.csv\" % year), marketsession.USEquities.getTimezone())\n\n        self.__testDifferentTimezonesImpl(feed)\n\n    def testDifferentTimezones_DBFeed(self):\n        feed = sqlitefeed.Feed(common.get_data_file_path(\"multiinstrument.sqlite\"), bar.Frequency.DAY)\n        feed.loadBars(\"^n225\")\n        feed.loadBars(\"spy\")\n        self.__testDifferentTimezonesImpl(feed)\n\n    def testDifferentTimezones_DBFeed_LocalizedBars(self):\n        feed = sqlitefeed.Feed(common.get_data_file_path(\"multiinstrument.sqlite\"), bar.Frequency.DAY)\n        feed.loadBars(\"^n225\", marketsession.TSE.getTimezone())\n        feed.loadBars(\"spy\", marketsession.USEquities.getTimezone())\n        self.__testDifferentTimezonesImpl(feed)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\nimport barfeed_test\nimport feed_test\n\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade import marketsession\nfrom pyalgotrade import bar\nfrom pyalgotrade.utils import dt\n\n\nclass NinjaTraderTestCase(common.TestCase):\n    def __loadIntradayBarFeed(self, timeZone=None):\n        ret = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE, timeZone)\n        ret.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        ret.loadAll()\n        return ret\n\n    def testBaseFeedInterface(self):\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        feed_test.tstBaseFeedInterface(self, barFeed)\n\n    def testWithTimezone(self):\n        timeZone = marketsession.USEquities.getTimezone()\n        barFeed = self.__loadIntradayBarFeed(timeZone)\n        ds = barFeed.getDataSeries()\n\n        for i, currentBar in enumerate(ds):\n            self.assertFalse(dt.datetime_is_naive(currentBar.getDateTime()))\n            self.assertEqual(ds[i].getDateTime(), ds.getDateTimes()[i])\n\n    def testWithoutTimezone(self):\n        barFeed = self.__loadIntradayBarFeed(None)\n        ds = barFeed.getDataSeries()\n\n        for i, currentBar in enumerate(ds):\n            # Datetime must be set to UTC.\n            self.assertFalse(dt.datetime_is_naive(currentBar.getDateTime()))\n            self.assertEqual(ds[i].getDateTime(), ds.getDateTimes()[i])\n\n    def testWithIntegerTimezone(self):\n        try:\n            barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE, -3)\n            self.assertTrue(False, \"Exception expected\")\n        except Exception, e:\n            self.assertTrue(str(e).find(\"timezone as an int parameter is not supported anymore\") == 0)\n\n        try:\n            barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n            barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"), -5)\n            self.assertTrue(False, \"Exception expected\")\n        except Exception, e:\n            self.assertTrue(str(e).find(\"timezone as an int parameter is not supported anymore\") == 0)\n\n    def testLocalizeAndFilter(self):\n        timezone = marketsession.USEquities.getTimezone()\n        # The prices come from NinjaTrader interface when set to use 'US Equities RTH' session template.\n        prices = {\n            dt.localize(datetime.datetime(2011, 3, 9, 9, 31), timezone): 132.35,\n            dt.localize(datetime.datetime(2011, 3, 9, 16), timezone): 132.39,\n            dt.localize(datetime.datetime(2011, 3, 10, 9, 31), timezone): 130.81,\n            dt.localize(datetime.datetime(2011, 3, 10, 16), timezone): 129.92,\n            dt.localize(datetime.datetime(2011, 3, 11, 9, 31), timezone): 129.72,\n            dt.localize(datetime.datetime(2011, 3, 11, 16), timezone): 130.84,\n        }\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE, timezone)\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011-03.csv\"))\n        for dateTime, bars in barFeed:\n            price = prices.get(bars.getDateTime(), None)\n            if price is not None:\n                self.assertTrue(price == bars.getBar(\"spy\").getClose())\n\n    def testBounded(self):\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE, maxLen=2)\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011-03.csv\"))\n        barFeed.loadAll()\n\n        barDS = barFeed[\"spy\"]\n        self.assertEqual(len(barDS), 2)\n        self.assertEqual(len(barDS.getDateTimes()), 2)\n        self.assertEqual(len(barDS.getCloseDataSeries()), 2)\n        self.assertEqual(len(barDS.getCloseDataSeries().getDateTimes()), 2)\n        self.assertEqual(len(barDS.getOpenDataSeries()), 2)\n        self.assertEqual(len(barDS.getHighDataSeries()), 2)\n        self.assertEqual(len(barDS.getLowDataSeries()), 2)\n        self.assertEqual(len(barDS.getAdjCloseDataSeries()), 2)\n\n    def testBaseBarFeed(self):\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        barfeed_test.check_base_barfeed(self, barFeed, False)\n\n    def testInvalidFrequency(self):\n        with self.assertRaisesRegexp(Exception, \"Invalid frequency.*\"):\n            ninjatraderfeed.Feed(bar.Frequency.WEEK)\n\n    def testReset(self):\n        instrument = \"spy\"\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n        barFeed.addBarsFromCSV(instrument, common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n\n        barFeed.loadAll()\n        instruments = barFeed.getRegisteredInstruments()\n        ds = barFeed[instrument]\n\n        barFeed.reset()\n        barFeed.loadAll()\n        reloadedDs = barFeed[instrument]\n\n        self.assertEqual(len(reloadedDs), len(ds))\n        self.assertNotEqual(reloadedDs, ds)\n        self.assertEqual(instruments, barFeed.getRegisteredInstruments())\n        for i in range(len(ds)):\n            self.assertEqual(ds[i].getDateTime(), reloadedDs[i].getDateTime())\n            self.assertEqual(ds[i].getClose(), reloadedDs[i].getClose())\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport copy\n\nimport common\n\nfrom pyalgotrade import observer\nfrom pyalgotrade import dispatcher\n\n\nclass NonRealtimeFeed(observer.Subject):\n    def __init__(self, datetimes, priority=None):\n        super(NonRealtimeFeed, self).__init__()\n        self.__datetimes = datetimes\n        self.__event = observer.Event()\n        self.__priority = priority\n\n    def getEvent(self):\n        return self.__event\n\n    def start(self):\n        super(NonRealtimeFeed, self).start()\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def eof(self):\n        return len(self.__datetimes) == 0\n\n    def dispatch(self):\n        ret = True\n        self.__event.emit(self.__datetimes.pop(0))\n        return ret\n\n    def peekDateTime(self):\n        return self.__datetimes[0]\n\n    def getDispatchPriority(self):\n        return self.__priority\n\n\nclass RealtimeFeed(observer.Subject):\n    def __init__(self, datetimes, priority=None):\n        super(RealtimeFeed, self).__init__()\n        self.__datetimes = datetimes\n        self.__event = observer.Event()\n        self.__priority = priority\n\n    def getEvent(self):\n        return self.__event\n\n    def start(self):\n        super(RealtimeFeed, self).start()\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def eof(self):\n        return len(self.__datetimes) == 0\n\n    def dispatch(self):\n        ret = True\n        self.__event.emit(self.__datetimes.pop(0))\n        return ret\n\n    def peekDateTime(self):\n        return None\n\n    def getDispatchPriority(self):\n        return self.__priority\n\n\nclass DispatcherTestCase(common.TestCase):\n    def test1NrtFeed(self):\n        values = []\n        now = datetime.datetime.now()\n        datetimes = [now + datetime.timedelta(seconds=i) for i in xrange(10)]\n        nrtFeed = NonRealtimeFeed(copy.copy(datetimes))\n        nrtFeed.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(nrtFeed)\n        disp.run()\n\n        self.assertEqual(values, datetimes)\n\n    def test2NrtFeeds(self):\n        values = []\n        now = datetime.datetime.now()\n        datetimes1 = [now + datetime.timedelta(seconds=i) for i in xrange(10)]\n        datetimes2 = [now + datetime.timedelta(seconds=i+len(datetimes1)) for i in xrange(10)]\n        nrtFeed1 = NonRealtimeFeed(copy.copy(datetimes1))\n        nrtFeed1.getEvent().subscribe(lambda x: values.append(x))\n        nrtFeed2 = NonRealtimeFeed(copy.copy(datetimes2))\n        nrtFeed2.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(nrtFeed1)\n        disp.addSubject(nrtFeed2)\n        disp.run()\n\n        self.assertEqual(len(values), len(datetimes1) + len(datetimes2))\n        self.assertEqual(values[:len(datetimes1)], datetimes1)\n        self.assertEqual(values[len(datetimes1):], datetimes2)\n\n    def test1RtFeed(self):\n        values = []\n        now = datetime.datetime.now()\n        datetimes = [now + datetime.timedelta(seconds=i) for i in xrange(10)]\n        nrtFeed = RealtimeFeed(copy.copy(datetimes))\n        nrtFeed.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(nrtFeed)\n        disp.run()\n\n        self.assertEqual(values, datetimes)\n\n    def test2RtFeeds(self):\n        values = []\n        now = datetime.datetime.now()\n        datetimes1 = [now + datetime.timedelta(seconds=i) for i in xrange(10)]\n        datetimes2 = [now + datetime.timedelta(seconds=i+len(datetimes1)) for i in xrange(10)]\n        nrtFeed1 = RealtimeFeed(copy.copy(datetimes1))\n        nrtFeed1.getEvent().subscribe(lambda x: values.append(x))\n        nrtFeed2 = RealtimeFeed(copy.copy(datetimes2))\n        nrtFeed2.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(nrtFeed1)\n        disp.addSubject(nrtFeed2)\n        disp.run()\n\n        self.assertEqual(len(values), len(datetimes1) + len(datetimes2))\n        for i in xrange(len(datetimes1)):\n            self.assertEqual(values[i*2], datetimes1[i])\n            self.assertEqual(values[i*2+1], datetimes2[i])\n\n    def test2Combined(self):\n        values = []\n        now = datetime.datetime.now()\n        datetimes1 = [now + datetime.timedelta(seconds=i) for i in xrange(10)]\n        datetimes2 = [now + datetime.timedelta(seconds=i+len(datetimes1)) for i in xrange(10)]\n        nrtFeed1 = RealtimeFeed(copy.copy(datetimes1))\n        nrtFeed1.getEvent().subscribe(lambda x: values.append(x))\n        nrtFeed2 = NonRealtimeFeed(copy.copy(datetimes2))\n        nrtFeed2.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(nrtFeed1)\n        disp.addSubject(nrtFeed2)\n        disp.run()\n\n        self.assertEqual(len(values), len(datetimes1) + len(datetimes2))\n        for i in xrange(len(datetimes1)):\n            self.assertEqual(values[i*2], datetimes1[i])\n            self.assertEqual(values[i*2+1], datetimes2[i])\n\n    def testPriority(self):\n        feed4 = RealtimeFeed([], None)\n        feed3 = RealtimeFeed([], None)\n        feed2 = RealtimeFeed([], 3)\n        feed1 = RealtimeFeed([], 0)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed3)\n        disp.addSubject(feed2)\n        disp.addSubject(feed1)\n        self.assertEqual(disp.getSubjects(), [feed1, feed2, feed3])\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed1)\n        disp.addSubject(feed2)\n        disp.addSubject(feed3)\n        self.assertEqual(disp.getSubjects(), [feed1, feed2, feed3])\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed3)\n        disp.addSubject(feed4)\n        disp.addSubject(feed2)\n        disp.addSubject(feed1)\n        self.assertEqual(disp.getSubjects(), [feed1, feed2, feed3, feed4])\n\n    def testDispatchOrder(self):\n        values = []\n        now = datetime.datetime.now()\n        feed1 = NonRealtimeFeed([now], 0)\n        feed2 = RealtimeFeed([now + datetime.timedelta(seconds=1)], None)\n        feed1.getEvent().subscribe(lambda x: values.append(x))\n        feed2.getEvent().subscribe(lambda x: values.append(x))\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(feed2)\n        disp.addSubject(feed1)\n        self.assertEqual(disp.getSubjects(), [feed1, feed2])\n        disp.run()\n        # Check that although feed2 is realtime, feed1 was dispatched before.\n        self.assertTrue(values[0] < values[1])\n\n\nclass EventTestCase(common.TestCase):\n    def testEmitOrder(self):\n        handlersData = []\n\n        def handler3():\n            handlersData.append(3)\n\n        def handler1():\n            handlersData.append(1)\n\n        def handler2():\n            handlersData.append(2)\n\n        event = observer.Event()\n        event.subscribe(handler1)\n        event.subscribe(handler2)\n        event.subscribe(handler3)\n        event.emit()\n        self.assertTrue(handlersData == [1, 2, 3])\n\n        handlersData = []\n        event = observer.Event()\n        event.subscribe(handler3)\n        event.subscribe(handler2)\n        event.subscribe(handler1)\n        event.emit()\n        self.assertTrue(handlersData == [3, 2, 1])\n\n    def testDuplicateHandlers(self):\n        def handler1():\n            handlersData.append(1)\n\n        handlersData = []\n        event = observer.Event()\n        event.subscribe(handler1)\n        event.subscribe(handler1)\n        event.emit()\n        self.assertTrue(handlersData == [1])\n\n    def testReentrancy(self):\n        handlersData = []\n        event = observer.Event()\n\n        def handler2():\n            handlersData.append(2)\n\n        def handler1():\n            handlersData.append(1)\n            event.subscribe(handler2)\n            event.subscribe(handler1)\n\n        event.subscribe(handler1)\n        event.emit()\n        self.assertTrue(handlersData == [1])\n        event.emit()\n        self.assertTrue(handlersData == [1, 1, 2])\n        event.unsubscribe(handler1)\n        event.emit()\n        self.assertTrue(handlersData == [1, 1, 2, 2])\n        event.unsubscribe(handler2)\n        event.emit()\n        self.assertTrue(handlersData == [1, 1, 2, 2])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nimport logging\n\nimport common\n\nfrom pyalgotrade.optimizer import local\nfrom pyalgotrade import strategy\nfrom pyalgotrade.barfeed import yahoofeed\n\nsys.path.append(\"samples\")\nimport sma_crossover\n\n\ndef parameters_generator(instrument, smaFirst, smaLast):\n    for sma in range(smaFirst, smaLast+1):\n        yield(instrument, sma)\n\n\nclass FailingStrategy(strategy.BacktestingStrategy):\n    def __init__(self, barFeed, instrument, smaPeriod):\n        super(FailingStrategy, self).__init__(barFeed)\n\n    def onBars(self, bars):\n        raise Exception(\"oh no!\")\n\n\nclass OptimizerTestCase(common.TestCase):\n    def testLocal(self):\n        barFeed = yahoofeed.Feed()\n        instrument = \"orcl\"\n        barFeed.addBarsFromCSV(instrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        res = local.run(\n            sma_crossover.SMACrossOver, barFeed, parameters_generator(instrument, 5, 100), logLevel=logging.DEBUG\n        )\n        self.assertEquals(round(res.getResult(), 2), 1295462.6)\n        self.assertEquals(res.getParameters()[1], 20)\n\n    def testFailingStrategy(self):\n        barFeed = yahoofeed.Feed()\n        instrument = \"orcl\"\n        barFeed.addBarsFromCSV(instrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        res = local.run(FailingStrategy, barFeed, parameters_generator(instrument, 5, 100), logLevel=logging.DEBUG)\n        self.assertIsNone(res)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nimport os\n\nimport common\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade import plotter\n\nsys.path.append(\"samples\")\nimport sma_crossover\n\n\nclass PlotterTestCase(common.TestCase):\n    def testDownloadAndParseDaily(self):\n        instrument = \"orcl\"\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(instrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        strat = sma_crossover.SMACrossOver(barFeed, instrument, 20)\n        plt = plotter.StrategyPlotter(strat, True, True, True)\n        plt.getInstrumentSubplot(instrument).addDataSeries(\"sma\", strat.getSMA())\n        strat.run()\n\n        with common.TmpDir() as tmpPath:\n            fig, subplots = plt.buildFigureAndSubplots()\n            self.assertIsNotNone(fig)\n            self.assertIsNotNone(subplots)\n            fig = plt.buildFigure()\n            fig.set_size_inches(10, 8)\n            png = os.path.join(tmpPath, \"plotter_test.png\")\n            fig.savefig(png)\n            # Check that file size looks ok.\n            # 118458 on Mac\n            # 116210 on Linux\n            self.assertGreater(\n                os.stat(png).st_size,\n                110000\n            )\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport pytz\n\nimport common\nimport strategy_test\n\nfrom pyalgotrade import bar\nfrom pyalgotrade import strategy\nfrom pyalgotrade.strategy import position\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade import barfeed\nfrom pyalgotrade.barfeed import membf\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade import marketsession\n\n\ndef load_daily_barfeed(instrument):\n    barFeed = yahoofeed.Feed()\n    barFeed.addBarsFromCSV(instrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n    return barFeed\n\n\ndef us_equities_datetime(*args, **kwargs):\n    ret = datetime.datetime(*args, **kwargs)\n    ret = dt.localize(ret, marketsession.USEquities.getTimezone())\n    return ret\n\n\nclass TestBarFeed(membf.BarFeed):\n    def barsHaveAdjClose(self):\n        raise NotImplementedError()\n\n\nclass BaseTestStrategy(strategy.BacktestingStrategy):\n    def __init__(self, barFeed, instrument, cash=1000000):\n        strategy.BacktestingStrategy.__init__(self, barFeed, cash)\n        self.instrument = instrument\n        self.orderUpdatedCalls = 0\n        self.enterOkCalls = 0\n        self.enterCanceledCalls = 0\n        self.exitOkCalls = 0\n        self.exitCanceledCalls = 0\n        self.posExecutionInfo = []\n\n    def onOrderUpdated(self, order):\n        self.orderUpdatedCalls += 1\n\n    def onEnterOk(self, position):\n        self.enterOkCalls += 1\n        self.posExecutionInfo.append(position.getEntryOrder().getExecutionInfo())\n\n    def onEnterCanceled(self, position):\n        self.enterCanceledCalls += 1\n        self.posExecutionInfo.append(position.getEntryOrder().getExecutionInfo())\n\n    def onExitOk(self, position):\n        self.exitOkCalls += 1\n        self.posExecutionInfo.append(position.getExitOrder().getExecutionInfo())\n\n    def onExitCanceled(self, position):\n        self.exitCanceledCalls += 1\n        self.posExecutionInfo.append(position.getExitOrder().getExecutionInfo())\n\n\nclass TestStrategy(BaseTestStrategy):\n    def __init__(self, barFeed, instrument, cash):\n        BaseTestStrategy.__init__(self, barFeed, instrument, cash)\n\n        self.__activePosition = None\n        # Maps dates to a tuple of (method, params)\n        self.__posEntry = {}\n        self.__posExit = {}\n\n        self.__result = 0\n        self.__netProfit = 0\n        self.positions = []\n\n    def addPosEntry(self, dateTime, enterMethod, *args, **kwargs):\n        self.__posEntry.setdefault(dateTime, [])\n        self.__posEntry[dateTime].append((enterMethod, args, kwargs))\n\n    def addPosExitMarket(self, dateTime, *args, **kwargs):\n        self.__posExit.setdefault(dateTime, [])\n        self.__posExit[dateTime].append((position.Position.exitMarket, args, kwargs))\n\n    def addPosExitLimit(self, dateTime, *args, **kwargs):\n        self.__posExit.setdefault(dateTime, [])\n        self.__posExit[dateTime].append((position.Position.exitLimit, args, kwargs))\n\n    def addPosExitStop(self, dateTime, *args, **kwargs):\n        self.__posExit.setdefault(dateTime, [])\n        self.__posExit[dateTime].append((position.Position.exitStop, args, kwargs))\n\n    def addPosExitStopLimit(self, dateTime, *args, **kwargs):\n        self.__posExit.setdefault(dateTime, [])\n        self.__posExit[dateTime].append((position.Position.exitStopLimit, args, kwargs))\n\n    def getResult(self):\n        return self.__result\n\n    def getNetProfit(self):\n        return self.__netProfit\n\n    def getActivePosition(self):\n        return self.__activePosition\n\n    def onEnterOk(self, position):\n        # print \"Enter ok\", position.getEntryOrder().getExecutionInfo().getDateTime()\n        BaseTestStrategy.onEnterOk(self, position)\n        if self.__activePosition is None:\n            self.__activePosition = position\n            assert(position.isOpen())\n            assert(len(position.getActiveOrders()) != 0)\n            assert(position.getShares() != 0)\n\n    def onEnterCanceled(self, position):\n        # print \"Enter canceled\", position.getEntryOrder().getExecutionInfo().getDateTime()\n        BaseTestStrategy.onEnterCanceled(self, position)\n        self.__activePosition = None\n        assert(not position.isOpen())\n        assert(len(position.getActiveOrders()) == 0)\n        assert(position.getShares() == 0)\n\n    def onExitOk(self, position):\n        # print \"Exit ok\", position.getExitOrder().getExecutionInfo().getDateTime()\n        BaseTestStrategy.onExitOk(self, position)\n        self.__result += position.getReturn()\n        self.__netProfit += position.getPnL()\n        self.__activePosition = None\n        assert(not position.isOpen())\n        assert(len(position.getActiveOrders()) == 0)\n        assert(position.getShares() == 0)\n\n    def onExitCanceled(self, position):\n        # print \"Exit canceled\", position.getExitOrder().getExecutionInfo().getDateTime()\n        BaseTestStrategy.onExitCanceled(self, position)\n        assert(position.isOpen())\n        assert(len(position.getActiveOrders()) == 0)\n        assert(position.getShares() != 0)\n\n    def onBars(self, bars):\n        dateTime = bars.getDateTime()\n\n        # Check position entry.\n        for meth, args, kwargs in strategy_test.get_by_datetime_or_date(self.__posEntry, dateTime):\n            if self.__activePosition is not None:\n                raise Exception(\"Only one position allowed at a time\")\n            self.__activePosition = meth(*args, **kwargs)\n            self.positions.append(self.__activePosition)\n\n        # Check position exit.\n        for meth, args, kwargs in strategy_test.get_by_datetime_or_date(self.__posExit, dateTime):\n            if self.__activePosition is None:\n                raise Exception(\"A position was not entered\")\n            meth(self.__activePosition, *args, **kwargs)\n\n\nclass EnterAndExitStrategy(BaseTestStrategy):\n    def onStart(self):\n        self.position = None\n\n    def onBars(self, bars):\n        if self.position is None:\n            self.position = self.enterLong(self.instrument, 1)\n        elif self.position.entryFilled() and not self.position.exitFilled():\n            self.position.exitMarket()\n\n\nclass DoubleExitStrategy(BaseTestStrategy):\n    def onStart(self):\n        self.position = None\n        self.doubleExit = False\n        self.doubleExitFailed = False\n\n    def onBars(self, bars):\n        if self.position is None:\n            self.position = self.enterLong(self.instrument, 1)\n        elif not self.doubleExit:\n            self.doubleExit = True\n            self.position.exitMarket()\n            try:\n                self.position.exitMarket()\n            except Exception:\n                self.doubleExitFailed = True\n\n\nclass CancelEntryStrategy(BaseTestStrategy):\n    def onStart(self):\n        self.position = None\n\n    def onBars(self, bars):\n        if self.position is None:\n            self.position = self.enterLong(self.instrument, 1)\n            self.position.cancelEntry()\n\n\nclass ExitEntryNotFilledStrategy(BaseTestStrategy):\n    def onStart(self):\n        self.position = None\n\n    def onBars(self, bars):\n        if self.position is None:\n            self.position = self.enterLong(self.instrument, 1)\n            self.position.exitMarket()\n\n\nclass ResubmitExitStrategy(BaseTestStrategy):\n    def onStart(self):\n        self.position = None\n        self.exitRequestCanceled = False\n\n    def onBars(self, bars):\n        if self.position is None:\n            self.position = self.enterLong(self.instrument, 1)\n        elif self.position.entryFilled() and not self.position.exitFilled():\n            self.position.exitMarket()\n            if not self.exitRequestCanceled:\n                self.position.cancelExit()\n                self.exitRequestCanceled = True\n\n\nclass BaseTestCase(common.TestCase):\n    TestInstrument = \"doesntmatter\"\n\n    def loadIntradayBarFeed(self):\n        fromMonth = 1\n        toMonth = 1\n        fromDay = 3\n        toDay = 3\n        barFilter = csvfeed.USEquitiesRTH(us_equities_datetime(2011, fromMonth, fromDay, 00, 00), us_equities_datetime(2011, toMonth, toDay, 23, 59))\n        barFeed = ninjatraderfeed.Feed(barfeed.Frequency.MINUTE)\n        barFeed.setBarFilter(barFilter)\n        barFeed.addBarsFromCSV(BaseTestCase.TestInstrument, common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        return barFeed\n\n    def loadDailyBarFeed(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(BaseTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        return barFeed\n\n    def createStrategy(self, useIntradayBarFeed=False):\n        if useIntradayBarFeed:\n            barFeed = self.loadIntradayBarFeed()\n        else:\n            barFeed = self.loadDailyBarFeed()\n\n        strat = TestStrategy(barFeed, BaseTestCase.TestInstrument, 1000)\n        return strat\n\n\nclass LongPosTestCase(BaseTestCase):\n    def testEnterAndExit(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = EnterAndExitStrategy(barFeed, instrument)\n        strat.run()\n\n        self.assertEqual(strat.position.isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        self.assertEqual(len(strat.getActivePositions()), 0)\n        self.assertEqual(len(strat.getOrderToPosition()), 0)\n        self.assertEqual(strat.position.getAge().days, 1)\n\n    def testCancelEntry(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = CancelEntryStrategy(barFeed, instrument)\n        strat.run()\n\n        self.assertEqual(strat.position.isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n        self.assertEqual(strat.orderUpdatedCalls, 2)\n        self.assertEqual(len(strat.getActivePositions()), 0)\n        self.assertEqual(len(strat.getOrderToPosition()), 0)\n        self.assertEqual(strat.position.getAge().total_seconds(), 0)\n\n    def testExitEntryNotFilled(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = ExitEntryNotFilledStrategy(barFeed, instrument)\n        strat.run()\n\n        self.assertEqual(strat.position.isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n        self.assertEqual(strat.orderUpdatedCalls, 2)\n        self.assertEqual(len(strat.getActivePositions()), 0)\n        self.assertEqual(len(strat.getOrderToPosition()), 0)\n        self.assertEqual(strat.position.getAge().total_seconds(), 0)\n\n    def testDoubleExitFails(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = DoubleExitStrategy(barFeed, instrument)\n        strat.run()\n\n        self.assertEqual(strat.position.isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        self.assertEqual(strat.doubleExit, True)\n        self.assertEqual(strat.doubleExitFailed, True)\n        self.assertEqual(len(strat.getActivePositions()), 0)\n        self.assertEqual(len(strat.getOrderToPosition()), 0)\n        self.assertEqual(strat.position.getAge().days, 1)\n\n    def testResubmitExit(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = ResubmitExitStrategy(barFeed, instrument)\n        strat.run()\n\n        self.assertEqual(strat.position.isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 1)\n        self.assertEqual(strat.orderUpdatedCalls, 8)\n        self.assertEqual(len(strat.getActivePositions()), 0)\n        self.assertEqual(len(strat.getOrderToPosition()), 0)\n        self.assertEqual(strat.position.getAge().days, 2)\n\n    def testLongPosition(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-08,27.37,27.50,24.50,24.81,63040000,24.26 - Sell\n        # 2000-11-07,28.37,28.44,26.50,26.56,58950800,25.97 - Exit long\n        # 2000-11-06,30.69,30.69,27.50,27.94,75552300,27.32 - Buy\n        # 2000-11-03,31.50,31.75,29.50,30.31,65020900,29.64 - Enter long\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 3), strat.enterLong, BaseTestCase.TestInstrument, 1, False)\n        strat.addPosExitMarket(datetime.datetime(2000, 11, 7))\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + 27.37 - 30.69, 2))\n        self.assertTrue(round(strat.getResult(), 3) == -0.108)\n        self.assertTrue(round(strat.getNetProfit(), 2) == round(27.37 - 30.69, 2))\n        self.assertEqual(strat.positions[0].getAge().days, 2)\n\n    def testLongPositionAdjClose(self):\n        strat = self.createStrategy()\n        strat.setUseAdjustedValues(True)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-10-13,31.00,35.75,31.00,35.63,38516200,34.84\n        # 2000-10-12,63.81,64.87,61.75,63.00,50892400,30.80\n        # 2000-01-19,56.13,58.25,54.00,57.13,49208800,27.93\n        # 2000-01-18,107.87,114.50,105.62,111.25,66791200,27.19\n\n        strat.addPosEntry(datetime.datetime(2000, 1, 18), strat.enterLong, BaseTestCase.TestInstrument, 1, False)\n        strat.addPosExitMarket(datetime.datetime(2000, 10, 12))\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + 30.31 - 27.44, 2))\n        self.assertTrue(round(strat.getResult(), 3) == 0.105)\n        self.assertTrue(round(strat.getNetProfit(), 2) == round(30.31 - 27.44, 2))\n        self.assertEqual(strat.positions[0].getAge().days, 268)\n\n    def testLongPositionGTC(self):\n        strat = self.createStrategy()\n        strat.getBroker().setCash(48)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-02-07,59.31,60.00,58.42,59.94,44697200,29.30\n        # 2000-02-04,57.63,58.25,56.81,57.81,40925000,28.26 - sell succeeds\n        # 2000-02-03,55.38,57.00,54.25,56.69,55540600,27.71 - exit\n        # 2000-02-02,54.94,56.00,54.00,54.31,63940400,26.55\n        # 2000-02-01,51.25,54.31,50.00,54.00,57108800,26.40\n        # 2000-01-31,47.94,50.13,47.06,49.95,68152400,24.42 - buy succeeds\n        # 2000-01-28,51.50,51.94,46.63,47.38,86400600,23.16 - buy fails\n        # 2000-01-27,55.81,56.69,50.00,51.81,61061800,25.33 - enterLong\n\n        strat.addPosEntry(datetime.datetime(2000, 1, 27), strat.enterLong, BaseTestCase.TestInstrument, 1, True)\n        strat.addPosExitMarket(datetime.datetime(2000, 2, 3))\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(48 + 57.63 - 47.94, 2))\n        self.assertTrue(round(strat.getNetProfit(), 2) == round(57.63 - 47.94, 2))\n        self.assertEqual(strat.positions[0].getAge().days, 4)\n\n    def testEntryCanceled(self):\n        strat = self.createStrategy()\n        strat.getBroker().setCash(10)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-01-28,51.50,51.94,46.63,47.38,86400600,23.16 - buy fails\n        # 2000-01-27,55.81,56.69,50.00,51.81,61061800,25.33 - enterLong\n\n        strat.addPosEntry(datetime.datetime(2000, 1, 27), strat.enterLong, BaseTestCase.TestInstrument, 1, False)\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.enterOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(strat.getBroker().getCash() == 10)\n        self.assertTrue(strat.getNetProfit() == 0)\n\n    def testUnrealized1(self):\n        strat = self.createStrategy(True)\n\n        # 3/Jan/2011 205300 - Enter long\n        # 3/Jan/2011 205400 - entry gets filled at 127.21\n        # 3/Jan/2011 210000 - last bar\n\n        strat.addPosEntry(dt.localize(datetime.datetime(2011, 1, 3, 20, 53), pytz.utc), strat.enterLong, BaseTestCase.TestInstrument, 1, True)\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), True)\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n\n        entryPrice = 127.21\n        lastPrice = strat.getFeed().getCurrentBars()[BaseTestCase.TestInstrument].getClose()\n\n        self.assertEqual(strat.getActivePosition().getReturn(), (lastPrice - entryPrice) / entryPrice)\n        self.assertEqual(strat.getActivePosition().getPnL(), lastPrice - entryPrice)\n\n    def testUnrealized2(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = TestStrategy(barFeed, instrument, 1000)\n        strat.addPosEntry(datetime.date(2000, 12, 13), strat.enterLong, instrument, 1, False)  # Filled on 2000-12-14 at 29.25.\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), True)\n        self.assertEqual(strat.getActivePosition().getPnL(), 29.06 - 29.25)\n        self.assertEqual(strat.getActivePosition().getReturn(), (29.06 - 29.25) / 29.25)\n\n    def testUnrealizedAdjusted(self):\n        instrument = \"orcl\"\n        barFeed = load_daily_barfeed(instrument)\n        strat = TestStrategy(barFeed, instrument, 1000)\n        strat.setUseAdjustedValues(True)\n        strat.addPosEntry(datetime.date(2000, 12, 13), strat.enterLong, instrument, 1, False)  # Filled on 2000-12-14 at 28.60\n        strat.run()\n\n        self.assertEqual(strat.positions[0].isOpen(), True)\n        self.assertEqual(round(strat.getActivePosition().getPnL(), 2), round(28.41 - 28.60, 2))\n        self.assertEqual(round(strat.getActivePosition().getReturn(), 2), round((28.41 - 28.60) / 28.60, 2))\n\n    def testActiveOrdersAndSharesLong(self):\n        instrument = \"orcl\"\n        testCase = self\n\n        class Strategy(strategy.BacktestingStrategy):\n            def __init__(self, barFeed, cash):\n                strategy.BacktestingStrategy.__init__(self, barFeed, cash)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterLong(instrument, 1, True)\n                    # The entry order should be active.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 1)\n                    testCase.assertEqual(self.pos.getShares(), 0)\n                elif self.pos.isOpen():\n                    # At this point the entry order should have been filled.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 0)\n                    testCase.assertEqual(self.pos.getShares(), 1)\n                    self.pos.exitMarket()\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 1)\n                    testCase.assertEqual(self.pos.getShares(), 1)\n                else:\n                    # The position was closed.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 0)\n                    testCase.assertEqual(self.pos.getShares(), 0)\n\n        barFeed = load_daily_barfeed(instrument)\n        strat = Strategy(barFeed, 1000)\n        strat.run()\n\n        self.assertNotEqual(strat.pos, None)\n        self.assertEqual(strat.pos.isOpen(), False)\n        # Entered on 2000-01-04 at 115.50\n        # Exit on 2000-01-05 at 101.62\n        self.assertEqual(strat.pos.getPnL(),  101.62 - 115.50)\n\n    def testIsOpen_NotClosed(self):\n        strat = self.createStrategy()\n        strat.addPosEntry(datetime.datetime(2000, 11, 3), strat.enterLong, BaseTestCase.TestInstrument, 1, False)\n        strat.run()\n        self.assertTrue(strat.getActivePosition().isOpen())\n\n    def testPartialFillGTC1(self):\n        # Open and close after entry has been fully filled.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLong, instrument, 4, True)\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 3))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 11)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 2))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 14)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 5))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC2(self):\n        # Open and close after entry has been partially filled.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLong, instrument, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 2))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 11)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 2))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 12)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 3))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isCanceled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 2)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC3(self):\n        class SkipCancelBroker(object):\n            def __init__(self, decorated):\n                self.__decorated = decorated\n\n            def __getattr__(self, name):\n                return getattr(self.__decorated, name)\n\n            def cancelOrder(self, order):\n                return\n\n        # Open and close after entry has been partially filled.\n        # Cancelations get skipped and the position is left open.\n        # The idea is to simulate a real scenario where cancelation gets submited but the order gets\n        # filled before the cancelation gets processed.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat._setBroker(SkipCancelBroker(strat.getBroker()))\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLong, instrument, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 2))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 1)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 11)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 2))\n\n        self.assertEqual(strat.positions[0].isOpen(), True)\n        self.assertEqual(strat.positions[0].getShares(), 2)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC4(self):\n        class SkipFirstCancelBroker(object):\n            def __init__(self, decorated):\n                self.__decorated = decorated\n                self.__cancelSkipped = False\n\n            def __getattr__(self, name):\n                return getattr(self.__decorated, name)\n\n            def cancelOrder(self, order):\n                if not self.__cancelSkipped:\n                    self.__cancelSkipped = True\n                    return\n                self.__decorated.cancelOrder(order)\n\n        # Open and close after entry has been partially filled.\n        # The first cancelation get skipped and a second exit has to be requested to close the position.\n        # The idea is to simulate a real scenario where cancelation gets submited but the order gets\n        # filled before the cancelation gets processed.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat._setBroker(SkipFirstCancelBroker(strat.getBroker()))\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLong, instrument, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 2))\n        # Retry exit.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 4))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 11)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 2))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 14)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 5))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n\nclass ShortPosTestCase(BaseTestCase):\n    def testActiveOrdersAndSharesShort(self):\n        instrument = \"orcl\"\n        testCase = self\n\n        class Strategy(strategy.BacktestingStrategy):\n            def __init__(self, barFeed, cash):\n                strategy.BacktestingStrategy.__init__(self, barFeed, cash)\n                self.pos = None\n\n            def onBars(self, bars):\n                if self.pos is None:\n                    self.pos = self.enterShort(instrument, 1, True)\n                    # The entry order should be active.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 1)\n                    testCase.assertEqual(self.pos.getShares(), 0)\n                elif self.pos.isOpen():\n                    # At this point the entry order should have been filled.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 0)\n                    testCase.assertEqual(self.pos.getShares(), -1)\n                    self.pos.exitMarket()\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 1)\n                    testCase.assertEqual(self.pos.getShares(), -1)\n                else:\n                    # The position was closed.\n                    testCase.assertEqual(len(self.pos.getActiveOrders()), 0)\n                    testCase.assertEqual(self.pos.getShares(), 0)\n\n        barFeed = load_daily_barfeed(instrument)\n        strat = Strategy(barFeed, 1000)\n        strat.run()\n\n        self.assertNotEqual(strat.pos, None)\n        self.assertEqual(strat.pos.isOpen(), False)\n        # Entered on 2000-01-04 at 115.50\n        # Exit on 2000-01-05 at 101.62\n        self.assertEqual(strat.pos.getPnL(),  115.50 - 101.62)\n\n    def testShortPosition(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-08,27.37,27.50,24.50,24.81,63040000,24.26\n        # 2000-11-07,28.37,28.44,26.50,26.56,58950800,25.97\n        # 2000-11-06,30.69,30.69,27.50,27.94,75552300,27.32\n        # 2000-11-03,31.50,31.75,29.50,30.31,65020900,29.64\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 3), strat.enterShort, BaseTestCase.TestInstrument, 1, False)\n        strat.addPosExitMarket(datetime.datetime(2000, 11, 7))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + 30.69 - 27.37, 2))\n        self.assertTrue(round(strat.getResult(), 3) == round(0.10817856, 3))\n        self.assertTrue(round(strat.getNetProfit(), 2) == round(30.69 - 27.37, 2))\n\n    def testShortPositionAdjClose(self):\n        strat = self.createStrategy()\n        strat.setUseAdjustedValues(True)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-10-13,31.00,35.75,31.00,35.63,38516200,34.84\n        # 2000-10-12,63.81,64.87,61.75,63.00,50892400,30.80\n        # 2000-01-19,56.13,58.25,54.00,57.13,49208800,27.93\n        # 2000-01-18,107.87,114.50,105.62,111.25,66791200,27.19\n\n        strat.addPosEntry(datetime.datetime(2000, 1, 18), strat.enterShort, BaseTestCase.TestInstrument, 1, False)\n        strat.addPosExitMarket(datetime.datetime(2000, 10, 12))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + 27.44 - 30.31, 2))\n        self.assertTrue(round(strat.getResult(), 3) == round(-0.104591837, 3))\n        self.assertTrue(round(strat.getNetProfit(), 2) == round(27.44 - 30.31, 2))\n\n    def testShortPositionExitCanceled(self):\n        strat = self.createStrategy()\n        strat.getBroker().setCash(0)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-12-08,30.06,30.62,29.25,30.06,40054100,29.39\n        # 2000-12-07,29.62,29.94,28.12,28.31,41093000,27.68\n        # .\n        # 2000-11-29,23.19,23.62,21.81,22.87,75408100,22.36\n        # 2000-11-28,23.50,23.81,22.25,22.66,43078300,22.16\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 28), strat.enterShort, BaseTestCase.TestInstrument, 1, False)\n        strat.addPosExitMarket(datetime.datetime(2000, 12, 7))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == 23.19)\n        self.assertTrue(strat.getNetProfit() == 0)\n\n    def testShortPositionExitCanceledAndReSubmitted(self):\n        strat = self.createStrategy()\n        strat.getBroker().setCash(0)\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-24,23.31,24.25,23.12,24.12,22446100,23.58\n        # 2000-11-22,23.62,24.06,22.06,22.31,53317000,21.81 - exitShort that gets filled\n        # 2000-11-21,24.81,25.62,23.50,23.87,58651900,23.34\n        # 2000-11-20,24.31,25.87,24.00,24.75,89783100,24.20\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74 - exitShort that gets canceled\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterShort\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterShort, BaseTestCase.TestInstrument, 1)\n        strat.addPosExitMarket(datetime.datetime(2000, 11, 14))\n        strat.addPosExitMarket(datetime.datetime(2000, 11, 22))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 1)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(25.12 - 23.31, 2))\n\n    def testUnrealized(self):\n        strat = self.createStrategy(True)\n\n        # 3/Jan/2011 205300 - Enter long\n        # 3/Jan/2011 205400 - entry gets filled at 127.21\n        # 3/Jan/2011 210000 - last bar\n\n        strat.addPosEntry(dt.localize(datetime.datetime(2011, 1, 3, 20, 53), pytz.utc), strat.enterShort, BaseTestCase.TestInstrument, 1, True)\n        strat.run()\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n\n        entryPrice = 127.21\n        lastPrice = strat.getFeed().getCurrentBars()[BaseTestCase.TestInstrument].getClose()\n\n        self.assertEqual(strat.getActivePosition().getReturn(), (entryPrice - lastPrice) / entryPrice)\n        self.assertEqual(strat.getActivePosition().getPnL(), entryPrice - lastPrice)\n\n\nclass LimitPosTestCase(BaseTestCase):\n    def testLong(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - exit filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - exitPosition\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 - entry filled\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongLimit, BaseTestCase.TestInstrument, 25, 1)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 16), 29)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == 1004)\n\n    def testShort(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-24,23.31,24.25,23.12,24.12,22446100,23.58 - exit filled\n        # 2000-11-22,23.62,24.06,22.06,22.31,53317000,21.81 - exitPosition\n        # 2000-11-21,24.81,25.62,23.50,23.87,58651900,23.34\n        # 2000-11-20,24.31,25.87,24.00,24.75,89783100,24.20\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - entry filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - enterShortLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 16), strat.enterShortLimit, BaseTestCase.TestInstrument, 29, 1)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 22), 24)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (29 - 23.31), 2))\n\n    def testExitOnEntryNotFilled(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - entry canceled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - exitPosition\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongLimit, BaseTestCase.TestInstrument, 5, 1, True)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 16), 29)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == 1000)\n\n    def testExitTwice(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - exit filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - exitPosition using a market order (cancels the previous one).\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74 - exitPosition\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 - entry filled\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongLimit, BaseTestCase.TestInstrument, 25, 1)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 14), 100)\n        strat.addPosExitMarket(datetime.datetime(2000, 11, 16))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (26.94 - 25), 2))\n\n    def testExitCancelsEntry(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74 - exitPosition (cancels the entry).\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 -\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongLimit, BaseTestCase.TestInstrument, 5, 1, True)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 14), 100)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 0)\n        self.assertEqual(strat.enterCanceledCalls, 1)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == 1000)\n\n    def testEntryGTCExitNotGTC(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23 - GTC exitPosition (never filled)\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74 -\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 - entry filled\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongLimit, BaseTestCase.TestInstrument, 25, 1, True)\n        strat.addPosExitLimit(datetime.datetime(2000, 11, 15), 100, False)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertTrue(strat.exitCanceledCalls == 1)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 - 25, 2))\n\n\nclass StopPosTestCase(BaseTestCase):\n    def testLong(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - exit filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - exitPosition\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 - entry filled\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongStop\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongStop, BaseTestCase.TestInstrument, 25, 1)\n        strat.addPosExitStop(datetime.datetime(2000, 11, 16), 26)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (26 - 25.12), 2))\n\n    def testShort(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-24,23.31,24.25,23.12,24.12,22446100,23.58 - exit filled\n        # 2000-11-22,23.62,24.06,22.06,22.31,53317000,21.81 - exitPosition\n        # 2000-11-21,24.81,25.62,23.50,23.87,58651900,23.34\n        # 2000-11-20,24.31,25.87,24.00,24.75,89783100,24.20\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - entry filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - enterShortStop\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 16), strat.enterShortStop, BaseTestCase.TestInstrument, 27, 1)\n        strat.addPosExitStop(datetime.datetime(2000, 11, 22), 23)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (26.94 - 23.31), 2))\n\n    def testPartialFillGTC1(self):\n        # Open and close after entry has been fully filled.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 6), 15, 15, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLongStop, instrument, 12, 4, True)\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 4))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 12)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 3))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 15)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 6))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC2(self):\n        # Open and close after entry has been partially filled.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 6), 15, 15, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLongStop, instrument, 12, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 3))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 12)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 3))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 13)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 4))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isCanceled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 2)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC3(self):\n        class SkipCancelBroker(object):\n            def __init__(self, decorated):\n                self.__decorated = decorated\n\n            def __getattr__(self, name):\n                return getattr(self.__decorated, name)\n\n            def cancelOrder(self, order):\n                return\n\n        # Open and close after entry has been partially filled.\n        # Cancelations get skipped and the position is left open.\n        # The idea is to simulate a real scenario where cancelation gets submited but the order gets\n        # filled before the cancelation gets processed.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 6), 15, 15, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat._setBroker(SkipCancelBroker(strat.getBroker()))\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLongStop, instrument, 12, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 3))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 0)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 1)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 12)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 3))\n\n        self.assertEqual(strat.positions[0].isOpen(), True)\n        self.assertEqual(strat.positions[0].getShares(), 2)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n    def testPartialFillGTC4(self):\n        class SkipFirstCancelBroker(object):\n            def __init__(self, decorated):\n                self.__decorated = decorated\n                self.__cancelSkipped = False\n\n            def __getattr__(self, name):\n                return getattr(self.__decorated, name)\n\n            def cancelOrder(self, order):\n                if not self.__cancelSkipped:\n                    self.__cancelSkipped = True\n                    return\n                self.__decorated.cancelOrder(order)\n\n        # Open and close after entry has been partially filled.\n        # The first cancelation get skipped and a second exit has to be requested to close the position.\n        # The idea is to simulate a real scenario where cancelation gets submited but the order gets\n        # filled before the cancelation gets processed.\n        instrument = \"orcl\"\n        bf = TestBarFeed(bar.Frequency.DAY)\n        bars = [\n            bar.BasicBar(datetime.datetime(2000, 1, 1), 10, 10, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 2), 11, 11, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 3), 12, 12, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 4), 13, 13, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 5), 14, 14, 10, 10, 10, 10, bar.Frequency.DAY),\n            bar.BasicBar(datetime.datetime(2000, 1, 6), 15, 15, 10, 10, 10, 10, bar.Frequency.DAY),\n            ]\n        bf.addBarsFromSequence(instrument, bars)\n        strat = TestStrategy(bf, instrument, 1000)\n        strat._setBroker(SkipFirstCancelBroker(strat.getBroker()))\n        strat.addPosEntry(datetime.datetime(2000, 1, 1), strat.enterLongStop, instrument, 12, 4, True)\n        # Exit the position before the entry order gets completely filled.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 3))\n        # Retry exit.\n        strat.addPosExitMarket(datetime.datetime(2000, 1, 5))\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertEqual(strat.exitCanceledCalls, 0)\n\n        self.assertEqual(len(strat.posExecutionInfo), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getPrice(), 12)\n        self.assertEqual(strat.posExecutionInfo[0].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[0].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[0].getDateTime(), datetime.datetime(2000, 1, 3))\n        self.assertEqual(strat.posExecutionInfo[1].getPrice(), 15)\n        self.assertEqual(strat.posExecutionInfo[1].getQuantity(), 2)\n        self.assertEqual(strat.posExecutionInfo[1].getCommission(), 0)\n        self.assertEqual(strat.posExecutionInfo[1].getDateTime(), datetime.datetime(2000, 1, 6))\n\n        self.assertEqual(strat.positions[0].isOpen(), False)\n        self.assertEqual(strat.positions[0].getShares(), 0)\n        self.assertTrue(strat.positions[0].getEntryOrder().isFilled())\n        self.assertEqual(strat.positions[0].getEntryOrder().getFilled(), 4)\n        self.assertEqual(strat.positions[0].getEntryOrder().getRemaining(), 0)\n        self.assertTrue(strat.positions[0].getExitOrder().isFilled())\n        self.assertEqual(strat.positions[0].getExitOrder().getFilled(), 2)\n        self.assertEqual(strat.positions[0].getExitOrder().getRemaining(), 0)\n\n\nclass StopLimitPosTestCase(BaseTestCase):\n    def testLong(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - exit filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - exitPosition\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20 - entry filled\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87 - enterLongStopLimit\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 10), strat.enterLongStopLimit, BaseTestCase.TestInstrument, 25.5, 24, 1)\n        strat.addPosExitStopLimit(datetime.datetime(2000, 11, 16), 27, 28)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (28 - 24), 2))\n\n    def testShort(self):\n        strat = self.createStrategy()\n\n        # Date,Open,High,Low,Close,Volume,Adj Close\n        # 2000-11-24,23.31,24.25,23.12,24.12,22446100,23.58 - exit filled\n        # 2000-11-22,23.62,24.06,22.06,22.31,53317000,21.81 - exitPosition\n        # 2000-11-21,24.81,25.62,23.50,23.87,58651900,23.34\n        # 2000-11-20,24.31,25.87,24.00,24.75,89783100,24.20\n        # 2000-11-17,26.94,29.25,25.25,28.81,59639400,28.17 - entry filled\n        # 2000-11-16,28.75,29.81,27.25,27.37,37990000,26.76 - enterShortStopLimit\n        # 2000-11-15,28.81,29.44,27.70,28.87,50655200,28.23\n        # 2000-11-14,27.37,28.50,26.50,28.37,77496700,27.74\n        # 2000-11-13,25.12,25.87,23.50,24.75,61651900,24.20\n        # 2000-11-10,26.44,26.94,24.87,25.44,54614100,24.87\n\n        strat.addPosEntry(datetime.datetime(2000, 11, 16), strat.enterShortStopLimit, BaseTestCase.TestInstrument, 27, 29, 1)\n        strat.addPosExitStopLimit(datetime.datetime(2000, 11, 22), 24, 25)\n        strat.run()\n\n        self.assertEqual(strat.enterOkCalls, 1)\n        self.assertEqual(strat.enterCanceledCalls, 0)\n        self.assertEqual(strat.exitOkCalls, 1)\n        self.assertTrue(strat.exitCanceledCalls == 0)\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (29 - 24), 2))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\nimport datetime\n\nimport common\n\nfrom pyalgotrade.tools import quandl\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import quandlfeed\n\n\nauth_token = None\n\n\nclass ToolsTestCase(common.TestCase):\n    def testDownloadAndParseDaily(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"ORCL\"\n            path = os.path.join(tmpPath, \"quandl-daily-orcl-2010.csv\")\n            quandl.download_daily_bars(\"WIKI\", instrument, 2010, path, auth_token)\n            bf = quandlfeed.Feed()\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 31))\n            self.assertEquals(bf[instrument][-1].getOpen(), 31.22)\n            self.assertEquals(bf[instrument][-1].getHigh(), 31.33)\n            self.assertEquals(bf[instrument][-1].getLow(), 30.93)\n            self.assertEquals(bf[instrument][-1].getClose(), 31.3)\n            self.assertEquals(bf[instrument][-1].getVolume(), 11716300)\n            self.assertEquals(bf[instrument][-1].getPrice(), 31.3)\n            # Not checking against a specific value since this is going to change\n            # as time passes by.\n            self.assertNotEquals(bf[instrument][-1].getAdjClose(), None)\n\n    def testDownloadAndParseDaily_UseAdjClose(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"ORCL\"\n            path = os.path.join(tmpPath, \"quandl-daily-orcl-2010.csv\")\n            quandl.download_daily_bars(\"WIKI\", instrument, 2010, path, auth_token)\n            bf = quandlfeed.Feed()\n            bf.addBarsFromCSV(instrument, path)\n            # Need to setUseAdjustedValues(True) after loading the file because we\n            # can't tell in advance if adjusted values are there or not.\n            bf.setUseAdjustedValues(True)\n            bf.loadAll()\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 31))\n            self.assertEquals(bf[instrument][-1].getOpen(), 31.22)\n            self.assertEquals(bf[instrument][-1].getHigh(), 31.33)\n            self.assertEquals(bf[instrument][-1].getLow(), 30.93)\n            self.assertEquals(bf[instrument][-1].getClose(), 31.3)\n            self.assertEquals(bf[instrument][-1].getVolume(), 11716300)\n            self.assertEquals(bf[instrument][-1].getPrice(), bf[instrument][-1].getAdjClose())\n            # Not checking against a specific value since this is going to change\n            # as time passes by.\n            self.assertNotEquals(bf[instrument][-1].getAdjClose(), None)\n\n    def testDownloadAndParseDailyNoAdjClose(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"ORCL\"\n            path = os.path.join(tmpPath, \"quandl-daily-orcl-2013.csv\")\n            quandl.download_daily_bars(\"GOOG\", \"NASDAQ_ORCL\", 2013, path, auth_token)\n            bf = quandlfeed.Feed()\n            bf.setNoAdjClose()\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2013, 12, 31))\n            self.assertEquals(bf[instrument][-1].getOpen(), 37.94)\n            self.assertEquals(bf[instrument][-1].getHigh(), 38.34)\n            self.assertEquals(bf[instrument][-1].getLow(), 37.88)\n            self.assertEquals(bf[instrument][-1].getClose(), 38.26)\n            self.assertEquals(bf[instrument][-1].getVolume(), 11747517)\n            self.assertEquals(bf[instrument][-1].getAdjClose(), None)\n            self.assertEquals(bf[instrument][-1].getPrice(), 38.26)\n\n    def testDownloadAndParseWeekly(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"AAPL\"\n            path = os.path.join(tmpPath, \"quandl-aapl-weekly-2010.csv\")\n            quandl.download_weekly_bars(\"WIKI\", instrument, 2010, path, auth_token)\n            bf = quandlfeed.Feed(frequency=bar.Frequency.WEEK)\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            # Quandl used to report 2010-1-3 as the first week of 2010.\n            self.assertTrue(\n                bf[instrument][0].getDateTime() in [datetime.datetime(2010, 1, 3), datetime.datetime(2010, 1, 10)]\n            )\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 26))\n            self.assertEquals(bf[instrument][-1].getOpen(), 325.0)\n            self.assertEquals(bf[instrument][-1].getHigh(), 325.15)\n            self.assertEquals(bf[instrument][-1].getLow(), 323.17)\n            self.assertEquals(bf[instrument][-1].getClose(), 323.6)\n            self.assertEquals(bf[instrument][-1].getVolume(), 7969900)\n            self.assertEquals(bf[instrument][-1].getPrice(), 323.6)\n            # Not checking against a specific value since this is going to change\n            # as time passes by.\n            self.assertNotEquals(bf[instrument][-1].getAdjClose(), None)\n\n    def testInvalidFrequency(self):\n        with self.assertRaisesRegexp(Exception, \"Invalid frequency.*\"):\n            quandlfeed.Feed(frequency=bar.Frequency.MINUTE)\n\n    def testBuildFeedDaily(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"ORCL\"\n            bf = quandl.build_feed(\"WIKI\", [instrument], 2010, 2010, tmpPath, authToken=auth_token)\n            bf.loadAll()\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 31))\n            self.assertEquals(bf[instrument][-1].getOpen(), 31.22)\n            self.assertEquals(bf[instrument][-1].getHigh(), 31.33)\n            self.assertEquals(bf[instrument][-1].getLow(), 30.93)\n            self.assertEquals(bf[instrument][-1].getClose(), 31.3)\n            self.assertEquals(bf[instrument][-1].getVolume(), 11716300)\n            self.assertEquals(bf[instrument][-1].getPrice(), 31.3)\n            # Not checking against a specific value since this is going to change\n            # as time passes by.\n            self.assertNotEquals(bf[instrument][-1].getAdjClose(), None)\n\n    def testBuildFeedWeekly(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"AAPL\"\n            bf = quandl.build_feed(\"WIKI\", [instrument], 2010, 2010, tmpPath, bar.Frequency.WEEK, authToken=auth_token)\n            bf.loadAll()\n            # Quandl used to report 2010-1-3 as the first week of 2010.\n            self.assertTrue(\n                bf[instrument][0].getDateTime() in [datetime.datetime(2010, 1, 3), datetime.datetime(2010, 1, 10)]\n            )\n            self.assertEquals(bf[instrument][-1].getDateTime(), datetime.datetime(2010, 12, 26))\n            self.assertEquals(bf[instrument][-1].getOpen(), 325.0)\n            self.assertEquals(bf[instrument][-1].getHigh(), 325.15)\n            self.assertEquals(bf[instrument][-1].getLow(), 323.17)\n            self.assertEquals(bf[instrument][-1].getClose(), 323.6)\n            self.assertEquals(bf[instrument][-1].getVolume(), 7969900)\n            self.assertEquals(bf[instrument][-1].getPrice(), 323.6)\n            # Not checking against a specific value since this is going to change\n            # as time passes by.\n            self.assertNotEquals(bf[instrument][-1].getAdjClose(), None)\n\n    def testInvalidInstrument(self):\n        instrument = \"inexistent\"\n\n        # Don't skip errors.\n        with self.assertRaisesRegexp(Exception, \"404 Client Error: Not Found\"):\n            with common.TmpDir() as tmpPath:\n                quandl.build_feed(\n                    instrument, [instrument], 2010, 2010, tmpPath, bar.Frequency.WEEK, authToken=auth_token\n                )\n\n        # Skip errors.\n        with common.TmpDir() as tmpPath:\n            bf = quandl.build_feed(\n                instrument, [instrument], 2010, 2010, tmpPath, bar.Frequency.WEEK, authToken=auth_token, skipErrors=True\n            )\n            bf.loadAll()\n            self.assertNotIn(instrument, bf)\n\n    def testMapColumnNames(self):\n        with common.TmpDir() as tmpPath:\n            bf = quandl.build_feed(\"YAHOO\", [\"AAPL\"], 2010, 2010, tmpPath, columnNames={\"adj_close\": \"Adjusted Close\"})\n            bf.setUseAdjustedValues(True)\n            bf.loadAll()\n            self.assertEquals(bf[\"AAPL\"][-1].getClose(), 322.560013)\n            self.assertIsNotNone(bf[\"AAPL\"][-1].getAdjClose())\n            self.assertIsNotNone(bf[\"AAPL\"][-1].getPrice())\n\n    def testExtraColumns(self):\n        with common.TmpDir() as tmpPath:\n            columnNames = {\n                \"open\": \"Last\",\n                \"close\": \"Last\"\n            }\n            bf = quandl.build_feed(\"BITSTAMP\", [\"USD\"], 2014, 2014, tmpPath, columnNames=columnNames)\n            bf.loadAll()\n            self.assertEquals(bf[\"USD\"][-1].getExtraColumns()[\"Bid\"], 319.19)\n            self.assertEquals(bf[\"USD\"][-1].getExtraColumns()[\"Ask\"], 319.63)\n            bids = bf[\"USD\"].getExtraDataSeries(\"Bid\")\n            self.assertEquals(bids[-1], 319.19)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport os\n\nimport common\n\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade.tools import resample\nfrom pyalgotrade import marketsession\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.dataseries import resampled as resampled_ds\nfrom pyalgotrade.barfeed import resampled as resampled_bf\nfrom pyalgotrade.dataseries import bards\nfrom pyalgotrade import bar\nfrom pyalgotrade import dispatcher\nfrom pyalgotrade import resamplebase\n\n\nclass IntraDayRange(common.TestCase):\n    def __testMinuteRangeImpl(self, timezone=None):\n        freq = bar.Frequency.MINUTE\n\n        begin = datetime.datetime(2011, 1, 1, 1, 1)\n        end = datetime.datetime(2011, 1, 1, 1, 2)\n        if timezone is not None:\n            begin = dt.localize(begin, timezone)\n            end = dt.localize(end, timezone)\n\n        r = resamplebase.build_range(begin + datetime.timedelta(seconds=5), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), end)\n\n    def __testFiveMinuteRangeImpl(self, timezone=None):\n        freq = 60*5\n\n        begin = datetime.datetime(2011, 1, 1, 1)\n        end = datetime.datetime(2011, 1, 1, 1, 5)\n        if timezone is not None:\n            begin = dt.localize(begin, timezone)\n            end = dt.localize(end, timezone)\n\n        r = resamplebase.build_range(begin + datetime.timedelta(seconds=120), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), end)\n\n    def __testHourRangeImpl(self, timezone=None):\n        freq = bar.Frequency.HOUR\n\n        begin = datetime.datetime(2011, 1, 1, 16)\n        end = datetime.datetime(2011, 1, 1, 17)\n        if timezone is not None:\n            begin = dt.localize(begin, timezone)\n            end = dt.localize(end, timezone)\n\n        r = resamplebase.build_range(begin + datetime.timedelta(seconds=120), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), end)\n\n    def testMinuteRange(self):\n        self.__testMinuteRangeImpl()\n\n    def testMinuteRangeLocalized(self):\n        self.__testMinuteRangeImpl(marketsession.NASDAQ.timezone)\n\n    def testFiveMinuteRange(self):\n        self.__testFiveMinuteRangeImpl()\n\n    def testFiveMinuteRangeLocalized(self):\n        self.__testFiveMinuteRangeImpl(marketsession.NASDAQ.timezone)\n\n    def testHourRange(self):\n        self.__testHourRangeImpl()\n\n    def testHourRangeLocalized(self):\n        self.__testHourRangeImpl(marketsession.NASDAQ.timezone)\n\n\nclass DayRange(common.TestCase):\n    def __testImpl(self, timezone=None):\n        freq = bar.Frequency.DAY\n\n        begin = datetime.datetime(2011, 1, 1)\n        end = datetime.datetime(2011, 1, 2)\n        if timezone is not None:\n            begin = dt.localize(begin, timezone)\n            end = dt.localize(end, timezone)\n\n        r = resamplebase.build_range(begin + datetime.timedelta(hours=5, minutes=25), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), end)\n\n    def testOk(self):\n        self.__testImpl()\n\n    def testLocalizedOk(self):\n        self.__testImpl(marketsession.NASDAQ.timezone)\n\n\nclass MonthRange(common.TestCase):\n    def test31Days(self):\n        freq = bar.Frequency.MONTH\n        begin = datetime.datetime(2011, 1, 1)\n        r = resamplebase.build_range(begin + datetime.timedelta(hours=5, minutes=25), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), datetime.datetime(2011, 2, 1))\n\n    def test28Days(self):\n        freq = bar.Frequency.MONTH\n        begin = datetime.datetime(2011, 2, 1)\n        r = resamplebase.build_range(begin + datetime.timedelta(hours=5, minutes=25), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq - bar.Frequency.DAY*3):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), datetime.datetime(2011, 3, 1))\n\n    def testDecember(self):\n        freq = bar.Frequency.MONTH\n        begin = datetime.datetime(2011, 12, 1)\n        r = resamplebase.build_range(begin + datetime.timedelta(hours=5, minutes=25), freq)\n        self.assertEqual(r.getBeginning(), begin)\n        for i in range(freq):\n            self.assertTrue(r.belongs(begin + datetime.timedelta(seconds=i)))\n        self.assertFalse(r.belongs(begin + datetime.timedelta(seconds=freq+1)))\n        self.assertEqual(r.getEnding(), datetime.datetime(2012, 1, 1))\n\n\nclass DataSeriesTestCase(common.TestCase):\n\n    def testResample(self):\n        barDs = bards.BarDataSeries()\n        resampledDS = resampled_ds.ResampledDataSeries(barDs.getCloseDataSeries(), bar.Frequency.MINUTE, sum)\n        resampledBarDS = resampled_ds.ResampledBarDataSeries(barDs, bar.Frequency.MINUTE)\n\n        barDs.append(bar.BasicBar(datetime.datetime(2011, 1, 1, 1, 1, 1), 2.1, 3, 1, 2, 10, 1, bar.Frequency.SECOND))\n        barDs.append(bar.BasicBar(datetime.datetime(2011, 1, 1, 1, 1, 2), 2, 3, 1, 2.3, 10, 2, bar.Frequency.SECOND))\n        barDs.append(bar.BasicBar(datetime.datetime(2011, 1, 1, 1, 2, 1), 2, 3, 1, 2, 10, 2, bar.Frequency.SECOND))\n\n        self.assertEqual(len(resampledBarDS), 1)\n        self.assertEqual(resampledBarDS[0].getDateTime(), datetime.datetime(2011, 1, 1, 1, 1))\n        self.assertEqual(resampledBarDS[0].getOpen(), 2.1)\n        self.assertEqual(resampledBarDS[0].getHigh(), 3)\n        self.assertEqual(resampledBarDS[0].getLow(), 1)\n        self.assertEqual(resampledBarDS[0].getClose(), 2.3)\n        self.assertEqual(resampledBarDS[0].getVolume(), 20)\n        self.assertEqual(resampledBarDS[0].getAdjClose(), 2)\n        self.assertEqual(resampledDS[-1], 2 + 2.3)\n\n        resampledBarDS.pushLast()\n        self.assertEqual(len(resampledBarDS), 2)\n        self.assertEqual(resampledBarDS[1].getDateTime(), datetime.datetime(2011, 1, 1, 1, 2))\n        self.assertEqual(resampledBarDS[1].getOpen(), 2)\n        self.assertEqual(resampledBarDS[1].getHigh(), 3)\n        self.assertEqual(resampledBarDS[1].getLow(), 1)\n        self.assertEqual(resampledBarDS[1].getClose(), 2)\n        self.assertEqual(resampledBarDS[1].getVolume(), 10)\n        self.assertEqual(resampledBarDS[1].getAdjClose(), 2)\n\n        resampledDS.pushLast()\n        self.assertEqual(resampledDS[1], 2)\n\n    def testResampleNinjaTraderHour(self):\n        with common.TmpDir() as tmp_path:\n            # Resample.\n            feed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n            feed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n            resampledBarDS = resampled_ds.ResampledBarDataSeries(feed[\"spy\"], bar.Frequency.HOUR)\n            resampledFile = os.path.join(tmp_path, \"hour-nt-spy-minute-2011.csv\")\n            resample.resample_to_csv(feed, bar.Frequency.HOUR, resampledFile)\n            resampledBarDS.pushLast()  # Need to manually push the last stot since time didn't change.\n\n            # Load the resampled file.\n            feed = csvfeed.GenericBarFeed(bar.Frequency.HOUR, marketsession.USEquities.getTimezone())\n            feed.addBarsFromCSV(\"spy\", resampledFile)\n            feed.loadAll()\n\n        self.assertEqual(len(feed[\"spy\"]), 340)\n        self.assertEqual(feed[\"spy\"][0].getDateTime(), dt.localize(datetime.datetime(2011, 1, 3, 9), marketsession.USEquities.getTimezone()))\n        self.assertEqual(feed[\"spy\"][-1].getDateTime(), dt.localize(datetime.datetime(2011, 2, 1, 1), marketsession.USEquities.getTimezone()))\n        self.assertEqual(feed[\"spy\"][0].getOpen(), 126.35)\n        self.assertEqual(feed[\"spy\"][0].getHigh(), 126.45)\n        self.assertEqual(feed[\"spy\"][0].getLow(), 126.3)\n        self.assertEqual(feed[\"spy\"][0].getClose(), 126.4)\n        self.assertEqual(feed[\"spy\"][0].getVolume(), 3397.0)\n        self.assertEqual(feed[\"spy\"][0].getAdjClose(), None)\n\n        self.assertEqual(len(resampledBarDS), len(feed[\"spy\"]))\n        self.assertEqual(resampledBarDS[0].getDateTime(), dt.as_utc(datetime.datetime(2011, 1, 3, 9)))\n        self.assertEqual(resampledBarDS[-1].getDateTime(), dt.as_utc(datetime.datetime(2011, 2, 1, 1)))\n\n    def testResampleNinjaTraderDay(self):\n        with common.TmpDir() as tmp_path:\n            # Resample.\n            feed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n            feed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n            resampledBarDS = resampled_ds.ResampledBarDataSeries(feed[\"spy\"], bar.Frequency.DAY)\n            resampledFile = os.path.join(tmp_path, \"day-nt-spy-minute-2011.csv\")\n            resample.resample_to_csv(feed, bar.Frequency.DAY, resampledFile)\n            resampledBarDS.pushLast()  # Need to manually push the last stot since time didn't change.\n\n            # Load the resampled file.\n            feed = csvfeed.GenericBarFeed(bar.Frequency.DAY)\n            feed.addBarsFromCSV(\"spy\", resampledFile, marketsession.USEquities.getTimezone())\n            feed.loadAll()\n\n        self.assertEqual(len(feed[\"spy\"]), 25)\n        self.assertEqual(feed[\"spy\"][0].getDateTime(), dt.localize(datetime.datetime(2011, 1, 3), marketsession.USEquities.getTimezone()))\n        self.assertEqual(feed[\"spy\"][-1].getDateTime(), dt.localize(datetime.datetime(2011, 2, 1), marketsession.USEquities.getTimezone()))\n\n        self.assertEqual(len(resampledBarDS), len(feed[\"spy\"]))\n        self.assertEqual(resampledBarDS[0].getDateTime(), dt.as_utc(datetime.datetime(2011, 1, 3)))\n        self.assertEqual(resampledBarDS[-1].getDateTime(), dt.as_utc(datetime.datetime(2011, 2, 1)))\n\n    def testCheckNow(self):\n        barDs = bards.BarDataSeries()\n        resampledBarDS = resampled_ds.ResampledBarDataSeries(barDs, bar.Frequency.MINUTE)\n\n        barDateTime = datetime.datetime(2014, 07, 07, 22, 46, 28, 10000)\n        barDs.append(bar.BasicBar(barDateTime, 2.1, 3, 1, 2, 10, 1, bar.Frequency.MINUTE))\n        self.assertEqual(len(resampledBarDS), 0)\n\n        resampledBarDS.checkNow(barDateTime + datetime.timedelta(minutes=1))\n        self.assertEqual(len(resampledBarDS), 1)\n        self.assertEqual(barDs[0].getOpen(), resampledBarDS[0].getOpen())\n        self.assertEqual(barDs[0].getHigh(), resampledBarDS[0].getHigh())\n        self.assertEqual(barDs[0].getLow(), resampledBarDS[0].getLow())\n        self.assertEqual(barDs[0].getClose(), resampledBarDS[0].getClose())\n        self.assertEqual(barDs[0].getVolume(), resampledBarDS[0].getVolume())\n        self.assertEqual(barDs[0].getAdjClose(), resampledBarDS[0].getAdjClose())\n        self.assertEqual(resampledBarDS[0].getDateTime(), datetime.datetime(2014, 07, 07, 22, 46))\n\n\nclass BarFeedTestCase(common.TestCase):\n\n    def testResampledBarFeed(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"spy-2010-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(\"nikkei\", common.get_data_file_path(\"nikkei-2010-yahoofinance.csv\"))\n        resampledBarFeed = resampled_bf.ResampledBarFeed(barFeed, bar.Frequency.MONTH)\n\n        disp = dispatcher.Dispatcher()\n        disp.addSubject(barFeed)\n        disp.addSubject(resampledBarFeed)\n        disp.run()\n\n        weeklySpyBarDS = resampledBarFeed[\"spy\"]\n        weeklyNikkeiBarDS = resampledBarFeed[\"nikkei\"]\n\n        # Check first bar\n        self.assertEqual(weeklySpyBarDS[0].getDateTime().date(), datetime.date(2010, 1, 1))\n        self.assertEqual(weeklyNikkeiBarDS[0].getDateTime().date(), datetime.date(2010, 1, 1))\n\n        # Check last bar\n        self.assertEqual(weeklySpyBarDS[-1].getDateTime().date(), datetime.date(2010, 11, 1))\n        self.assertEqual(weeklyNikkeiBarDS[-1].getDateTime().date(), datetime.date(2010, 11, 1))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\nimport strategy_test\nimport position_test\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade.stratanalyzer import returns\nfrom pyalgotrade import broker\nfrom pyalgotrade import marketsession\n\n\nclass TimeWeightedReturnsTestCase(common.TestCase):\n    def testNullPortfolio(self):\n        retTracker = returns.TimeWeightedReturns(0)\n        self.assertEqual(retTracker.getCumulativeReturns(), 0)\n\n    def testNoUpdates(self):\n        retTracker = returns.TimeWeightedReturns(10)\n        self.assertEqual(retTracker.getCumulativeReturns(), 0)\n\n    def testInvestopedia(self):\n        # http://www.investopedia.com/exam-guide/cfa-level-1/quantitative-methods/discounted-cash-flow-time-weighted-return.asp\n        retTracker = returns.TimeWeightedReturns(200000)\n        retTracker.update(196500)  # March 31, 2004\n        self.assertEquals(round(retTracker.getLastPeriodReturns(), 4), -0.0175)\n        retTracker.update(200000)  # June 30, 2004\n        self.assertEquals(round(retTracker.getLastPeriodReturns(), 4), 0.0178)\n        retTracker.deposit(20000)\n        retTracker.update(222000)  # July 30, 2004\n        self.assertEquals(round(retTracker.getLastPeriodReturns(), 2), 0.01)\n        retTracker.update(243000)  # Sept. 30, 2004\n        self.assertEquals(round(retTracker.getLastPeriodReturns(), 4), 0.0946)\n        retTracker.deposit(2000)\n        retTracker.update(250000)  # Dec. 31, 2004\n        self.assertEquals(round(retTracker.getLastPeriodReturns(), 4), 0.0206)\n        self.assertEquals(round(retTracker.getCumulativeReturns(), 6),  0.128288)\n\n\nclass PosTrackerTestCase(common.TestCase):\n    def testBuyAndSellBreakEven(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        posTracker.sell(1, 10)\n        # self.assertEqual(posTracker.getCash(), 0)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getReturn(), 0)\n\n    def testBuyAndSellBreakEvenWithCommission(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        # self.assertEqual(posTracker.getCash(), 0)\n        posTracker.buy(1, 10, 0.01)\n        # self.assertEqual(posTracker.getCash(), -10.01)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        posTracker.sell(1, 10.02, 0.01)\n        # self.assertEqual(round(posTracker.getCash(), 2), 0)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        # We need to round to avoid floating point errors.\n        # The same issue can be reproduced with this piece of code:\n        # a = 10.02 - 10\n        # b = 0.02\n        # print a - b\n        # print a - b == 0\n        self.assertEqual(posTracker.getPosition(), 0)\n        self.assertEqual(round(posTracker.getPnL(), 2), 0)\n        self.assertEqual(round(posTracker.getReturn(), 2), 0)\n\n    def testBuyAndSellWin(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        posTracker.sell(1, 11)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 1)\n        self.assertTrue(posTracker.getReturn() == 0.1)\n\n    def testBuyAndSellInTwoTrades(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(2, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        posTracker.sell(1, 11)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        self.assertEqual(posTracker.getPnL(), 1)\n        self.assertEqual(posTracker.getReturn(), 0.05)\n        posTracker.sell(1, 12)\n        self.assertEqual(posTracker.getPnL(), 3)\n        self.assertEqual(posTracker.getReturn(), 3/20.0)\n\n    def testBuyAndSellMultipleEvals(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(2, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getPnL(price=9), -2)\n        self.assertEqual(posTracker.getPnL(price=10), 0)\n        self.assertEqual(posTracker.getPnL(price=11), 2)\n        self.assertEqual(posTracker.getReturn(10), 0)\n\n        self.assertEqual(posTracker.getPnL(price=11), 2)\n        self.assertEqual(round(posTracker.getReturn(11), 2), 0.1)\n\n        self.assertEqual(posTracker.getPnL(price=20), 20)\n        self.assertEqual(posTracker.getReturn(20), 1)\n\n        posTracker.sell(1, 11)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        self.assertEqual(posTracker.getPnL(price=11), 2)\n        self.assertEqual(posTracker.getReturn(11), 0.1)\n\n        posTracker.sell(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 1)\n        self.assertEqual(posTracker.getReturn(11), 0.05)\n\n    def testSellAndBuyWin(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.sell(1, 13)\n        self.assertEqual(posTracker.getAvgPrice(), 13)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getPnL(price=10), 3)\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 3)\n        self.assertEqual(round(posTracker.getReturn(), 9), round(0.23076923076923, 9))\n\n    def testSellAndBuyMultipleEvals(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.sell(2, 11)\n        self.assertEqual(posTracker.getAvgPrice(), 11)\n        self.assertEqual(posTracker.getPnL(price=10), 2)\n        self.assertEqual(posTracker.getPnL(price=11), 0)\n        self.assertEqual(posTracker.getPnL(price=12), -2)\n        self.assertEqual(posTracker.getReturn(11), 0)\n\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 11)\n        self.assertEqual(posTracker.getPnL(price=11), 1)\n        self.assertEqual(round(posTracker.getReturn(11), 9), round(0.045454545, 9))\n\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 2)\n        self.assertEqual(posTracker.getPnL(price=100), 2)\n        self.assertEqual(round(posTracker.getReturn(), 9), round(0.090909091, 9))\n\n    def testBuySellBuy(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        self.assertEqual(posTracker.getPnL(price=9), -1)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getPnL(price=10), 0)\n        self.assertEqual(posTracker.getPnL(price=11), 1)\n        self.assertEqual(posTracker.getReturn(), 0)\n        self.assertEqual(posTracker.getReturn(13), 0.3)\n\n        # Closing the long position and short selling 1 @ $13.\n        # The cost basis for the new position is $13.\n        posTracker.sell(2, 13)\n        self.assertEqual(posTracker.getAvgPrice(), 13)\n        self.assertEqual(posTracker.getPnL(), 3)\n        self.assertEqual(round(posTracker.getReturn(), 8), 0.23076923)\n\n        posTracker.buy(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), 6)\n        self.assertEqual(round(posTracker.getReturn(), 9), round(0.46153846153846, 9))\n\n    def testSellBuySell(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.sell(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getReturn(), 0)\n        self.assertEqual(posTracker.getPnL(price=13), -3)\n        self.assertEqual(posTracker.getReturn(13), -0.3)\n\n        # Closing the short position and going long 1 @ $13.\n        # The cost basis for the new position is $13.\n        posTracker.buy(2, 13)\n        self.assertEqual(posTracker.getAvgPrice(), 13)\n        self.assertEqual(posTracker.getPnL(), -3)\n        self.assertEqual(round(posTracker.getReturn(), 9), round(-0.23076923076923, 9))\n\n        posTracker.sell(1, 10)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), -6)\n        self.assertEqual(round(posTracker.getReturn(), 9), round(-0.46153846153846, 9))\n\n    def testBuyAndSellBreakEvenWithCommision(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(1, 10, 0.5)\n        self.assertEqual(posTracker.getAvgPrice(), 10)\n        posTracker.sell(1, 11, 0.5)\n        self.assertEqual(posTracker.getPnL(includeCommissions=False), 1)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getReturn(includeCommissions=False), 0.1)\n        self.assertEqual(posTracker.getReturn(), 0)\n\n    def testSeparateAndCombined(self):\n        posA = returns.PositionTracker(broker.IntegerTraits())\n        posA.buy(11, 10)\n        posA.sell(11, 30)\n        self.assertEqual(posA.getPnL(), 20*11)\n        self.assertEqual(posA.getReturn(), 2)\n\n        posB = returns.PositionTracker(broker.IntegerTraits())\n        posB.sell(100, 1.1)\n        posB.buy(100, 1)\n        self.assertEqual(round(posB.getPnL(), 2), 100*0.1)\n        self.assertEqual(round(posB.getReturn(), 2), 0.09)\n\n        combinedPos = returns.PositionTracker(broker.IntegerTraits())\n        combinedPos.buy(11, 10)\n        combinedPos.sell(11, 30)\n        combinedPos.sell(100, 1.1)\n        combinedPos.buy(100, 1)\n        self.assertEqual(round(combinedPos.getReturn(), 6), 2.090909)\n        # The return of the combined position is less than the two returns combined\n        # because when the second position gets opened the amount of cash not invested is greater\n        # than that of posB alone.\n        self.assertLess(round(combinedPos.getReturn(), 6), ((1+posA.getReturn())*(1+posB.getReturn())-1))\n\n    def testProfitReturnsAndCost(self):\n        posTracker = returns.PositionTracker(broker.IntegerTraits())\n        posTracker.buy(10, 1)\n        self.assertEqual(posTracker.getPnL(), 0)\n        self.assertEqual(posTracker.getAvgPrice(), 1)\n        self.assertEqual(posTracker.getCommissions(), 0)\n        # self.assertEqual(posTracker.getCash(), -10)\n\n        posTracker.buy(20, 1, 10)\n        self.assertEqual(posTracker.getPnL(), -10)\n        self.assertEqual(posTracker.getAvgPrice(), 1)\n        self.assertEqual(posTracker.getCommissions(), 10)\n        # self.assertEqual(posTracker.getCash(), -40)\n\n        posTracker.sell(30, 1)\n        self.assertEqual(posTracker.getAvgPrice(), 0)\n        self.assertEqual(posTracker.getPnL(), -10)\n        # self.assertEqual(posTracker.getCash(), -10)\n        self.assertEqual(posTracker.getCommissions(), 10)\n        self.assertEqual(posTracker.getReturn(), -10/30.0)\n\n        posTracker.buy(10, 1)\n        self.assertEqual(posTracker.getPnL(), -10)\n        self.assertEqual(posTracker.getAvgPrice(), 1)\n\n\nclass AnalyzerTestCase(common.TestCase):\n    TestInstrument = \"any\"\n\n    def testOneBarReturn(self):\n        initialCash = 1000\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(datetime.datetime(2001, 12, 07), datetime.datetime(2001, 12, 07)))\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        # 2001-12-07,15.74,15.95,15.55,15.91,42463200,15.56\n        # Manually place the orders to get them filled on the first (and only) bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, False)  # Open: 15.74\n        strat.getBroker().submitOrder(order)\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.SELL, AnalyzerTestCase.TestInstrument, 1, True)  # Close: 15.91\n        strat.getBroker().submitOrder(order)\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == initialCash + (15.91 - 15.74))\n\n        finalValue = 1000 - 15.74 + 15.91\n        rets = (finalValue - initialCash) / float(initialCash)\n        self.assertEqual(stratAnalyzer.getReturns()[-1], rets)\n\n    def testTwoBarReturns_OpenOpen(self):\n        initialCash = 15.61\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(datetime.datetime(2001, 12, 06), datetime.datetime(2001, 12, 07)))\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        # 2001-12-06,15.61,16.03,15.50,15.90,66944900,15.55\n        # 2001-12-07,15.74,15.95,15.55,15.91,42463200,15.56\n        # Manually place the entry order, to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, False)  # Open: 15.61\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2001, 12, 06), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, AnalyzerTestCase.TestInstrument, 1, False)  # Open: 15.74\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == initialCash + (15.74 - 15.61))\n        # First day returns: Open vs Close\n        self.assertTrue(stratAnalyzer.getReturns()[0] == (15.90 - 15.61) / 15.61)\n        # Second day returns: Open vs Prev. day's close\n        self.assertTrue(stratAnalyzer.getReturns()[1] == (15.74 - 15.90) / 15.90)\n\n    def testTwoBarReturns_OpenClose(self):\n        initialCash = 15.61\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(datetime.datetime(2001, 12, 06), datetime.datetime(2001, 12, 07)))\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        # 2001-12-06,15.61,16.03,15.50,15.90,66944900,15.55\n        # 2001-12-07,15.74,15.95,15.55,15.91,42463200,15.56\n        # Manually place the entry order, to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, False)  # Open: 15.61\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2001, 12, 06), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, AnalyzerTestCase.TestInstrument, 1, True)  # Close: 15.91\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == initialCash + (15.91 - 15.61))\n        # First day returns: Open vs Close\n        self.assertTrue(stratAnalyzer.getReturns()[0] == (15.90 - 15.61) / 15.61)\n        # Second day returns: Close vs Prev. day's close\n        self.assertTrue(stratAnalyzer.getReturns()[1] == (15.91 - 15.90) / 15.90)\n\n    def testTwoBarReturns_CloseOpen(self):\n        initialCash = 15.9\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(datetime.datetime(2001, 12, 06), datetime.datetime(2001, 12, 07)))\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        # 2001-12-06,15.61,16.03,15.50,15.90,66944900,15.55\n        # 2001-12-07,15.74,15.95,15.55,15.91,42463200,15.56\n        # Manually place the entry order, to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, True)  # Close: 15.90\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2001, 12, 06), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, AnalyzerTestCase.TestInstrument, 1, False)  # Open: 15.74\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == initialCash + (15.74 - 15.90))\n        # First day returns: 0\n        self.assertTrue(stratAnalyzer.getReturns()[0] == 0)\n        # Second day returns: Open vs Prev. day's close\n        self.assertTrue(stratAnalyzer.getReturns()[1] == (15.74 - 15.90) / 15.90)\n\n    def testTwoBarReturns_CloseClose(self):\n        initialCash = 15.90\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(datetime.datetime(2001, 12, 06), datetime.datetime(2001, 12, 07)))\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        # 2001-12-06,15.61,16.03,15.50,15.90,66944900,15.55\n        # 2001-12-07,15.74,15.95,15.55,15.91,42463200,15.56\n        # Manually place the entry order, to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, True)  # Close: 15.90\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2001, 12, 06), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, AnalyzerTestCase.TestInstrument, 1, True)  # Close: 15.91\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == initialCash + (15.91 - 15.90))\n        # First day returns: 0\n        self.assertTrue(stratAnalyzer.getReturns()[0] == 0)\n        # Second day returns: Open vs Prev. day's close\n        self.assertTrue(stratAnalyzer.getReturns()[1] == (15.91 - 15.90) / 15.90)\n\n    def testCumulativeReturn(self):\n        initialCash = 33.06\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = position_test.TestStrategy(barFeed, AnalyzerTestCase.TestInstrument, initialCash)\n\n        strat.addPosEntry(datetime.datetime(2001, 1, 12), strat.enterLong, AnalyzerTestCase.TestInstrument, 1)  # 33.06\n        strat.addPosExitMarket(datetime.datetime(2001, 11, 27))  # 14.32\n\n        stratAnalyzer = returns.Returns(maxLen=10)\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(initialCash + (14.32 - 33.06), 2))\n        self.assertTrue(round(33.06 * (1 + stratAnalyzer.getCumulativeReturns()[-1]), 2) == 14.32)\n        self.assertEqual(len(stratAnalyzer.getCumulativeReturns()), 10)\n        self.assertEqual(len(stratAnalyzer.getReturns()), 10)\n\n    def testGoogle2011(self):\n        initialValue = 1000000\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"goog-2011-yahoofinance.csv\"))\n\n        strat = strategy_test.TestStrategy(barFeed, initialValue)\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1654, True)  # 2011-01-03 close: 604.35\n        strat.getBroker().submitOrder(order)\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        finalValue = strat.getBroker().getEquity()\n\n        self.assertEqual(round(stratAnalyzer.getCumulativeReturns()[-1], 4), round((finalValue - initialValue) / float(initialValue), 4))\n\n    def testMultipleInstrumentsInterleaved(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"spy-2010-yahoofinance.csv\"), marketsession.NYSE.getTimezone())\n        barFeed.addBarsFromCSV(\"nikkei\", common.get_data_file_path(\"nikkei-2010-yahoofinance.csv\"), marketsession.TSE.getTimezone())\n\n        strat = strategy_test.TestStrategy(barFeed, 1000)\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        strat.marketOrder(\"spy\", 1)\n        strat.run()\n        # The cumulative return should be the same if we load nikkei or not.\n        self.assertEqual(round(stratAnalyzer.getCumulativeReturns()[-1], 5), 0.01338)\n\n    def testFirstBar(self):\n        initialCash = 1000\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(AnalyzerTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n\n        strat.addOrder(datetime.datetime(2001, 01, 02), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, AnalyzerTestCase.TestInstrument, 1, False)  # 2001-01-03 Open: 25.25 Close: 32.00\n\n        stratAnalyzer = returns.Returns()\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.run()\n        self.assertEqual(stratAnalyzer.getReturns()[0], 0)\n        self.assertEqual(stratAnalyzer.getReturns()[1], (32.00 - 25.25) / 1000)\n\n        # Check date times.\n        datetimes = barFeed[AnalyzerTestCase.TestInstrument].getDateTimes()\n        for i in [0, -1]:\n            self.assertEqual(stratAnalyzer.getReturns().getDateTimes()[i], datetimes[i])\n            self.assertEqual(stratAnalyzer.getCumulativeReturns().getDateTimes()[i], datetimes[i])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\nimport strategy_test\n\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade.stratanalyzer import sharpe\nfrom pyalgotrade.broker import backtesting\nfrom pyalgotrade import broker\nfrom pyalgotrade import marketsession\n\n\nclass SharpeRatioTestCase(common.TestCase):\n    def testDateTimeDiffs(self):\n        # sharpe.days_traded\n        self.assertEqual(sharpe.days_traded(datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 2)), 2)\n        self.assertEqual(sharpe.days_traded(datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 2, 12)), 2)\n        self.assertEqual(sharpe.days_traded(datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 1, 12)), 1)\n        self.assertEqual(sharpe.days_traded(datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 1, 6)), 1)\n        self.assertEqual(sharpe.days_traded(datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 2, 6)), 2)\n\n    def testNoTrades(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"ige\", common.get_data_file_path(\"sharpe-ratio-test-ige.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, 1000)\n        stratAnalyzer = sharpe.SharpeRatio()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        strat.run()\n        self.assertTrue(strat.getBroker().getCash() == 1000)\n        self.assertTrue(stratAnalyzer.getSharpeRatio(0.04, True) == 0)\n        self.assertTrue(stratAnalyzer.getSharpeRatio(0) == 0)\n        self.assertTrue(stratAnalyzer.getSharpeRatio(0, True) == 0)\n\n    def __testIGE_BrokerImpl(self, quantity):\n        initialCash = 42.09 * quantity\n        # This testcase is based on an example from Ernie Chan's book:\n        # 'Quantitative Trading: How to Build Your Own Algorithmic Trading Business'\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"ige\", common.get_data_file_path(\"sharpe-ratio-test-ige.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n        strat.setUseAdjustedValues(True)\n        strat.setBrokerOrdersGTC(True)\n        stratAnalyzer = sharpe.SharpeRatio()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Disable volume checks to match book results.\n        strat.getBroker().getFillStrategy().setVolumeLimit(None)\n\n        # Manually place the order to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, \"ige\", quantity, True)  # Adj. Close: 42.09\n        order.setGoodTillCanceled(True)\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2007, 11, 13), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, \"ige\", quantity, True)  # Adj. Close: 127.64\n        strat.run()\n        self.assertEqual(round(strat.getBroker().getCash(), 2), initialCash + (127.64 - 42.09) * quantity)\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        # The results are slightly different only because I'm taking into account the first bar as well.\n        self.assertEqual(round(stratAnalyzer.getSharpeRatio(0.04, True), 4), 0.7889)\n        self.assertEqual(round(stratAnalyzer.getSharpeRatio(0.04, False), 4), 0.0497)\n\n    def testIGE_Broker(self):\n        self.__testIGE_BrokerImpl(1)\n\n    def testIGE_Broker2(self):\n        self.__testIGE_BrokerImpl(2)\n\n    def testIGE_BrokerWithCommission(self):\n        commision = 0.5\n        initialCash = 42.09 + commision\n        # This testcase is based on an example from Ernie Chan's book:\n        # 'Quantitative Trading: How to Build Your Own Algorithmic Trading Business'\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(\"ige\", common.get_data_file_path(\"sharpe-ratio-test-ige.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, initialCash)\n        strat.getBroker().setCommission(backtesting.FixedPerTrade(commision))\n        strat.setUseAdjustedValues(True)\n        strat.setBrokerOrdersGTC(True)\n        stratAnalyzer = sharpe.SharpeRatio()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Disable volume checks to match book results.\n        strat.getBroker().getFillStrategy().setVolumeLimit(None)\n\n        # Manually place the order to get it filled on the first bar.\n        order = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, \"ige\", 1, True)  # Adj. Close: 42.09\n        order.setGoodTillCanceled(True)\n        strat.getBroker().submitOrder(order)\n        strat.addOrder(datetime.datetime(2007, 11, 13), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, \"ige\", 1, True)  # Adj. Close: 127.64\n\n        strat.run()\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == initialCash + (127.64 - 42.09 - commision*2))\n        self.assertEqual(strat.orderUpdatedCalls, 6)\n        # The results are slightly different only because I'm taking into account the first bar as well,\n        # and I'm also adding commissions.\n        self.assertEqual(round(stratAnalyzer.getSharpeRatio(0.04, True), 6), 0.776443)\n\n    def testIntraDay(self):\n        barFeed = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE, marketsession.USEquities.getTimezone())\n        barFeed.setBarFilter(csvfeed.USEquitiesRTH())\n        barFeed.addBarsFromCSV(\"spy\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        strat = strategy_test.TestStrategy(barFeed, 1000)\n        stratAnalyzer = sharpe.SharpeRatio(False)\n        strat.attachAnalyzer(stratAnalyzer)\n        strat.marketOrder(\"spy\", 1)\n\n        strat.run()\n\n        tradingPeriods = 252*6.5*60\n        manualAnnualized = sharpe.sharpe_ratio(stratAnalyzer.getReturns(), 0.04, tradingPeriods, True)\n        manualNotAnnualized = sharpe.sharpe_ratio(stratAnalyzer.getReturns(), 0.04, tradingPeriods, False)\n        analyzerAnnualized = stratAnalyzer.getSharpeRatio(0.04)\n        analyzerNotAnnualized = stratAnalyzer.getSharpeRatio(0.04, False)\n\n        self.assertEqual(round(analyzerAnnualized, 10), -1.1814830854)\n        self.assertEqual(round(analyzerNotAnnualized, 10), -0.0037659686)\n        # They should be similar, but not identical because the analyzer uses 365 days/year\n        # when useDailyReturns is set to False.\n        self.assertEqual(round(analyzerAnnualized, 1), round(manualAnnualized, 1))\n        self.assertEqual(round(analyzerNotAnnualized, 3), round(manualNotAnnualized, 3))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport unittest\n\nimport broker_backtesting_test\n\nfrom pyalgotrade import broker\nfrom pyalgotrade.broker import slippage\nfrom pyalgotrade.broker import backtesting\nfrom pyalgotrade import bar\n\n\nclass BaseTestCase(unittest.TestCase):\n    TestInstrument = \"orcl\"\n\n\nclass NoSlippageTestCase(BaseTestCase):\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.slippage = slippage.NoSlippage()\n        self.barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.DAY)\n\n    def __test_impl(self, action):\n        order = backtesting.MarketOrder(\n            action, BaseTestCase.TestInstrument, 5, False, broker.IntegerTraits()\n        )\n        price = 10\n\n        slippedPrice = self.slippage.calculatePrice(\n            order, price, order.getQuantity(), self.barsBuilder.nextBar(10, 11, 9, 10, volume=100), 0\n        )\n        self.assertEqual(price, slippedPrice)\n\n        slippedPrice = self.slippage.calculatePrice(\n            order, price, order.getQuantity(), self.barsBuilder.nextBar(10, 11, 9, 10, volume=100), 20\n        )\n        self.assertEqual(slippedPrice, price)\n\n    def test_buy_market_order(self):\n        self.__test_impl(broker.Order.Action.BUY)\n\n    def test_sell_market_order(self):\n        self.__test_impl(broker.Order.Action.SELL)\n\n\nclass VolumeShareSlippageTestCase(BaseTestCase):\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.priceImpact = 0.1\n        self.slippage = slippage.VolumeShareSlippage(self.priceImpact)\n        self.barsBuilder = broker_backtesting_test.BarsBuilder(BaseTestCase.TestInstrument, bar.Frequency.DAY)\n\n    def __test_impl(self, action):\n        order = backtesting.MarketOrder(\n            action, BaseTestCase.TestInstrument, 25, False, broker.IntegerTraits()\n        )\n        price = 10\n        volumeUsed = 0\n\n        # Try the order once.\n        slippedPrice = self.slippage.calculatePrice(\n            order, price, order.getQuantity(), self.barsBuilder.nextBar(10, 11, 9, 10, volume=100), volumeUsed\n        )\n        quantity = order.getQuantity()\n        expectedPriceImpactPct = quantity/100.0 * quantity/100.0 * self.priceImpact\n        self.assertEqual(expectedPriceImpactPct, 0.00625)\n        if action == broker.Order.Action.BUY:\n            self.assertEqual(slippedPrice, price * (1 + expectedPriceImpactPct))\n        else:\n            self.assertEqual(slippedPrice, price * (1 - expectedPriceImpactPct))\n\n        # Try the same order once again.\n        volumeUsed += quantity\n        quantity += order.getQuantity()\n\n        slippedPrice = self.slippage.calculatePrice(\n            order, price, order.getQuantity(), self.barsBuilder.nextBar(10, 11, 9, 10, volume=100), volumeUsed\n        )\n        expectedPriceImpactPct = quantity/100.0 * quantity/100.0 * self.priceImpact\n        self.assertEqual(expectedPriceImpactPct, 0.025)\n        if action == broker.Order.Action.BUY:\n            self.assertEqual(slippedPrice, price * (1 + expectedPriceImpactPct))\n        else:\n            self.assertEqual(slippedPrice, price * (1 - expectedPriceImpactPct))\n\n    def test_buy_market_order(self):\n        self.__test_impl(broker.Order.Action.BUY)\n\n    def test_sell_market_order(self):\n        self.__test_impl(broker.Order.Action.SELL)\n\n    def test_full_volume_used(self):\n        orderSize = 100\n        order = backtesting.MarketOrder(\n            broker.Order.Action.BUY, BaseTestCase.TestInstrument, orderSize, False, broker.IntegerTraits()\n        )\n        price = 10\n        volumeUsed = 0\n\n        # Try the order once.\n        slippedPrice = self.slippage.calculatePrice(\n            order,\n            price,\n            order.getQuantity(),\n            self.barsBuilder.nextBar(10, 11, 9, 10, volume=orderSize),\n            volumeUsed\n        )\n        self.assertEqual(slippedPrice, price*1.1)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade import strategy\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade.technical import cross\n\n\nclass SMACrossOverStrategy(strategy.BacktestingStrategy):\n    def __init__(self, feed, fastSMA, slowSMA):\n        strategy.BacktestingStrategy.__init__(self, feed, 1000)\n        ds = feed[\"orcl\"].getPriceDataSeries()\n        self.__fastSMADS = ma.SMA(ds, fastSMA)\n        self.__slowSMADS = ma.SMA(ds, slowSMA)\n        self.__longPos = None\n        self.__shortPos = None\n        self.__finalValue = None\n\n    def enterLongPosition(self, bars):\n        raise Exception(\"Not implemented\")\n\n    def enterShortPosition(self, bars):\n        raise Exception(\"Not implemented\")\n\n    def exitLongPosition(self, bars, position):\n        raise Exception(\"Not implemented\")\n\n    def exitShortPosition(self, bars, position):\n        raise Exception(\"Not implemented\")\n\n    def getFinalValue(self):\n        return self.__finalValue\n\n    def printDebug(self, *args):\n        args = [str(arg) for arg in args]\n        # print \" \".join(args)\n\n    def onEnterOk(self, position):\n        self.printDebug(\"enterOk: \", self.getCurrentDateTime(), position.getEntryOrder().getExecutionInfo().getPrice(), position)\n\n    def onEnterCanceled(self, position):\n        self.printDebug(\"enterCanceled: \", self.getCurrentDateTime(), position)\n        if position == self.__longPos:\n            self.__longPos = None\n        elif position == self.__shortPos:\n            self.__shortPos = None\n        else:\n            assert(False)\n\n    def onExitOk(self, position):\n        self.printDebug(\"exitOk: \", self.getCurrentDateTime(), position.getExitOrder().getExecutionInfo().getPrice(), position)\n        if position == self.__longPos:\n            self.__longPos = None\n        elif position == self.__shortPos:\n            self.__shortPos = None\n        else:\n            assert(False)\n\n    def onExitCanceled(self, position):\n        self.printDebug(\"exitCanceled: \", self.getCurrentDateTime(), position, \". Resubmitting as a Market order.\")\n        # If the exit was canceled, re-submit it.\n        position.exitMarket()\n\n    def onBars(self, bars):\n        bar = bars.getBar(\"orcl\")\n        self.printDebug(\"%s: O=%s H=%s L=%s C=%s\" % (bar.getDateTime(), bar.getOpen(), bar.getHigh(), bar.getLow(), bar.getClose()))\n\n        if cross.cross_above(self.__fastSMADS, self.__slowSMADS) == 1:\n            if self.__shortPos:\n                self.exitShortPosition(bars, self.__shortPos)\n            assert(self.__longPos is None)\n            self.__longPos = self.enterLongPosition(bars)\n        elif cross.cross_below(self.__fastSMADS, self.__slowSMADS) == 1:\n            if self.__longPos:\n                self.exitLongPosition(bars, self.__longPos)\n            assert(self.__shortPos is None)\n            self.__shortPos = self.enterShortPosition(bars)\n\n    def onFinish(self, bars):\n        self.__finalValue = self.getBroker().getEquity()\n\n\nclass MarketOrderStrategy(SMACrossOverStrategy):\n    def enterLongPosition(self, bars):\n        return self.enterLong(\"orcl\", 10)\n\n    def enterShortPosition(self, bars):\n        return self.enterShort(\"orcl\", 10)\n\n    def exitLongPosition(self, bars, position):\n        position.exitMarket()\n\n    def exitShortPosition(self, bars, position):\n        position.exitMarket()\n\n\nclass LimitOrderStrategy(SMACrossOverStrategy):\n    def __getMiddlePrice(self, bars):\n        bar = bars.getBar(\"orcl\")\n        ret = bar.getLow() + (bar.getHigh() - bar.getLow()) / 2.0\n        ret = round(ret, 2)\n        return ret\n\n    def enterLongPosition(self, bars):\n        price = self.__getMiddlePrice(bars)\n        ret = self.enterLongLimit(\"orcl\", price, 10)\n        self.printDebug(\"enterLong:\", self.getCurrentDateTime(), price, ret)\n        return ret\n\n    def enterShortPosition(self, bars):\n        price = self.__getMiddlePrice(bars)\n        ret = self.enterShortLimit(\"orcl\", price, 10)\n        self.printDebug(\"enterShort:\", self.getCurrentDateTime(), price, ret)\n        return ret\n\n    def exitLongPosition(self, bars, position):\n        price = self.__getMiddlePrice(bars)\n        self.printDebug(\"exitLong:\", self.getCurrentDateTime(), price, position)\n        position.exitLimit(price)\n\n    def exitShortPosition(self, bars, position):\n        price = self.__getMiddlePrice(bars)\n        self.printDebug(\"exitShort:\", self.getCurrentDateTime(), price, position)\n        position.exitLimit(price)\n\n\nclass TestSMACrossOver(common.TestCase):\n    def __test(self, strategyClass, finalValue):\n        feed = yahoofeed.Feed()\n        feed.addBarsFromCSV(\"orcl\", common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        myStrategy = strategyClass(feed, 10, 25)\n        myStrategy.run()\n        myStrategy.printDebug(\"Final result:\", round(myStrategy.getFinalValue(), 2))\n        self.assertTrue(round(myStrategy.getFinalValue(), 2) == finalValue)\n\n    def testWithMarketOrder(self):\n        # This is the exact same result that we get using NinjaTrader.\n        self.__test(MarketOrderStrategy, 1000 - 22.7)\n\n    def testWithLimitOrder(self):\n        # The result is different than the one we get using NinjaTrader. NinjaTrader processes Limit orders in a different way.\n        self.__test(LimitOrderStrategy, 1000 + 32.7)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import strategy\nfrom pyalgotrade import broker\nfrom pyalgotrade.barfeed import yahoofeed\n\n\ndef get_by_datetime_or_date(dict_, dateTimeOrDate):\n    ret = dict_.get(dateTimeOrDate, [])\n    if len(ret) == 0 and isinstance(dateTimeOrDate, datetime.datetime):\n        ret = dict_.get(dateTimeOrDate.date(), [])\n    return ret\n\n\nclass TestStrategy(strategy.BacktestingStrategy):\n    def __init__(self, barFeed, cash):\n        strategy.BacktestingStrategy.__init__(self, barFeed, cash)\n\n        # Maps dates to a tuple of (method, params)\n        self.__orderEntry = {}\n\n        self.__brokerOrdersGTC = False\n        self.orderUpdatedCalls = 0\n        self.onStartCalled = False\n        self.onIdleCalled = False\n        self.onFinishCalled = False\n\n    def addOrder(self, dateTime, method, *args, **kwargs):\n        self.__orderEntry.setdefault(dateTime, [])\n        self.__orderEntry[dateTime].append((method, args, kwargs))\n\n    def setBrokerOrdersGTC(self, gtc):\n        self.__brokerOrdersGTC = gtc\n\n    def onStart(self):\n        self.onStartCalled = True\n\n    def onIdle(self):\n        self.onIdleCalled = True\n\n    def onFinish(self, bars):\n        self.onFinishCalled = True\n\n    def onOrderUpdated(self, order):\n        self.orderUpdatedCalls += 1\n\n    def onBars(self, bars):\n        dateTime = bars.getDateTime()\n\n        # Check order entry.\n        for meth, args, kwargs in get_by_datetime_or_date(self.__orderEntry, dateTime):\n            order = meth(*args, **kwargs)\n            order.setGoodTillCanceled(self.__brokerOrdersGTC)\n            self.getBroker().submitOrder(order)\n\n\nclass StrategyTestCase(common.TestCase):\n    TestInstrument = \"doesntmatter\"\n\n    def loadDailyBarFeed(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(StrategyTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        return barFeed\n\n    def createStrategy(self):\n        barFeed = self.loadDailyBarFeed()\n        strat = TestStrategy(barFeed, 1000)\n        return strat\n\n\nclass BrokerOrderTestCase(StrategyTestCase):\n    def testMarketOrder(self):\n        strat = self.createStrategy()\n\n        o = strat.getBroker().createMarketOrder(broker.Order.Action.BUY, StrategyTestCase.TestInstrument, 1)\n        strat.getBroker().submitOrder(o)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n\nclass StrategyOrderTestCase(StrategyTestCase):\n    def testOrder(self):\n        strat = self.createStrategy()\n\n        o = strat.marketOrder(StrategyTestCase.TestInstrument, 1)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testMarketOrderBuy(self):\n        strat = self.createStrategy()\n\n        o = strat.marketOrder(StrategyTestCase.TestInstrument, 1)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.BUY)\n        self.assertEquals(o.getQuantity(), 1)\n        self.assertEquals(o.getFilled(), 1)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testMarketOrderSell(self):\n        strat = self.createStrategy()\n\n        o = strat.marketOrder(StrategyTestCase.TestInstrument, -2)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.SELL)\n        self.assertEquals(o.getQuantity(), 2)\n        self.assertEquals(o.getFilled(), 2)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testLimitOrderBuy(self):\n        strat = self.createStrategy()\n\n        o = strat.limitOrder(StrategyTestCase.TestInstrument, 60, 1, True)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.BUY)\n        self.assertEquals(o.getAvgFillPrice(), 56.13)\n        self.assertEquals(o.getQuantity(), 1)\n        self.assertEquals(o.getFilled(), 1)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testLimitOrderSell(self):\n        strat = self.createStrategy()\n\n        o = strat.limitOrder(StrategyTestCase.TestInstrument, 60, -3, False)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.SELL)\n        self.assertEquals(o.getAvgFillPrice(), 124.62)\n        self.assertEquals(o.getQuantity(), 3)\n        self.assertEquals(o.getFilled(), 3)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testStopOrderBuy(self):\n        strat = self.createStrategy()\n\n        o = strat.stopOrder(StrategyTestCase.TestInstrument, 100, 1, False)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.BUY)\n        self.assertEquals(o.getAvgFillPrice(), 124.62)\n        self.assertEquals(o.getQuantity(), 1)\n        self.assertEquals(o.getFilled(), 1)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n\n    def testStopOrderSell(self):\n        strat = self.createStrategy()\n\n        o = strat.stopOrder(StrategyTestCase.TestInstrument, 55, -2, True)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.SELL)\n        self.assertEquals(o.getAvgFillPrice(), 55)\n        self.assertEquals(o.getQuantity(), 2)\n        self.assertEquals(o.getFilled(), 2)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n        self.assertEqual(o.getExecutionInfo().getDateTime(), datetime.datetime(2000, 1, 19))\n\n    def testStopLimitOrderBuy(self):\n        strat = self.createStrategy()\n\n        o = strat.stopLimitOrder(StrategyTestCase.TestInstrument, 110, 100, 1, True)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.BUY)\n        self.assertEquals(o.getAvgFillPrice(), 100)\n        self.assertEquals(o.getQuantity(), 1)\n        self.assertEquals(o.getFilled(), 1)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n        self.assertEqual(o.getExecutionInfo().getDateTime(), datetime.datetime(2000, 1, 5))\n\n    def testStopLimitOrderSell(self):\n        strat = self.createStrategy()\n\n        o = strat.stopLimitOrder(StrategyTestCase.TestInstrument, 100, 110, -2, True)\n        strat.run()\n        self.assertTrue(o.isFilled())\n        self.assertEquals(o.getAction(), broker.Order.Action.SELL)\n        self.assertEquals(o.getAvgFillPrice(), 110)\n        self.assertEquals(o.getQuantity(), 2)\n        self.assertEquals(o.getFilled(), 2)\n        self.assertEquals(o.getRemaining(), 0)\n        self.assertEqual(strat.orderUpdatedCalls, 3)\n        self.assertEqual(o.getExecutionInfo().getDateTime(), datetime.datetime(2000, 1, 10))\n\n\nclass OptionalOverridesTestCase(StrategyTestCase):\n    def testOnStartIdleFinish(self):\n        strat = self.createStrategy()\n        strat.run()\n        self.assertTrue(strat.onStartCalled)\n        self.assertTrue(strat.onFinishCalled)\n        self.assertFalse(strat.onIdleCalled)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport talib\n\nimport common\n\nfrom pyalgotrade.talibext import indicator\nfrom pyalgotrade import bar\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.dataseries import bards\n\n\n# Market data used for regression tests (252 price bars) extracted from ta-lib/src/tools/ta_regtest/test_data.c\nOPEN_VALUES = [\n    92.500000, 91.500000, 95.155000, 93.970000, 95.500000, 94.500000, 95.000000, 91.500000, 91.815000, 91.125000, 93.875000,\n    97.500000, 98.815000, 92.000000, 91.125000, 91.875000, 93.405000, 89.750000, 89.345000, 92.250000, 89.780000,\n    87.940000, 87.595000, 85.220000, 83.500000, 83.500000, 81.250000, 85.125000, 88.125000, 87.500000, 85.250000,\n    86.000000, 87.190000, 86.125000, 89.000000, 88.625000, 86.000000, 85.500000, 84.750000, 85.250000, 84.250000,\n    86.750000, 86.940000, 89.315000, 89.940000, 90.815000, 91.190000, 91.345000, 89.595000, 91.000000, 89.750000,\n    88.750000, 88.315000, 84.345000, 83.500000, 84.000000, 86.000000, 85.530000, 87.500000, 88.500000, 90.000000,\n    88.655000, 89.500000, 91.565000, 92.000000, 93.000000, 92.815000, 91.750000, 92.000000, 91.375000, 89.750000,\n    88.750000, 85.440000, 83.500000, 84.875000, 98.625000, 96.690000, 102.375000, 106.000000, 104.625000, 102.500000,\n    104.250000, 104.000000, 106.125000, 106.065000, 105.940000, 105.625000, 108.625000, 110.250000, 110.565000, 117.000000,\n    120.750000, 118.000000, 119.125000, 119.125000, 117.815000, 116.375000, 115.155000, 111.250000, 111.500000, 116.690000,\n    116.000000, 113.620000, 111.750000, 114.560000, 113.620000, 118.120000, 119.870000, 116.620000, 115.870000, 115.060000,\n    115.870000, 117.500000, 119.870000, 119.250000, 120.190000, 122.870000, 123.870000, 122.250000, 123.120000, 123.310000,\n    124.000000, 123.000000, 124.810000, 130.000000, 130.880000, 132.500000, 131.000000, 132.500000, 134.000000, 137.440000,\n    135.750000, 138.310000, 138.000000, 136.380000, 136.500000, 132.000000, 127.500000, 127.620000, 124.000000, 123.620000,\n    125.000000, 126.370000, 126.250000, 125.940000, 124.000000, 122.750000, 120.000000, 120.000000, 122.000000, 123.620000,\n    121.500000, 120.120000, 123.750000, 122.750000, 125.000000, 128.500000, 128.380000, 123.870000, 124.370000, 122.750000,\n    123.370000, 122.000000, 122.620000, 125.000000, 124.250000, 124.370000, 125.620000, 126.500000, 128.380000, 128.880000,\n    131.500000, 132.500000, 137.500000, 134.630000, 132.000000, 134.000000, 132.000000, 131.380000, 126.500000, 128.750000,\n    127.190000, 127.500000, 120.500000, 126.620000, 123.000000, 122.060000, 121.000000, 121.000000, 118.000000, 122.000000,\n    122.250000, 119.120000, 115.000000, 113.500000, 114.000000, 110.810000, 106.500000, 106.440000, 108.000000, 107.000000,\n    108.620000, 93.000000, 93.750000, 94.250000, 94.870000, 95.500000, 94.500000, 97.000000, 98.500000, 96.750000,\n    95.870000, 94.440000, 92.750000, 90.500000, 95.060000, 94.620000, 97.500000, 96.000000, 96.000000, 94.620000,\n    94.870000, 94.000000, 99.000000, 105.500000, 108.810000, 105.000000, 105.940000, 104.940000, 103.690000, 102.560000,\n    103.440000, 109.810000, 113.000000, 117.000000, 116.250000, 120.500000, 111.620000, 108.120000, 110.190000, 107.750000,\n    108.000000, 110.690000, 109.060000, 108.500000, 109.870000, 109.120000, 109.690000, 109.560000, 110.440000, 109.690000,\n    109.190000]\n\nHIGH_VALUES = [\n    93.250000, 94.940000, 96.375000, 96.190000, 96.000000, 94.720000, 95.000000, 93.720000, 92.470000, 92.750000, 96.250000,\n    99.625000, 99.125000, 92.750000, 91.315000, 93.250000, 93.405000, 90.655000, 91.970000, 92.250000, 90.345000,\n    88.500000, 88.250000, 85.500000, 84.440000, 84.750000, 84.440000, 89.405000, 88.125000, 89.125000, 87.155000,\n    87.250000, 87.375000, 88.970000, 90.000000, 89.845000, 86.970000, 85.940000, 84.750000, 85.470000, 84.470000,\n    88.500000, 89.470000, 90.000000, 92.440000, 91.440000, 92.970000, 91.720000, 91.155000, 91.750000, 90.000000,\n    88.875000, 89.000000, 85.250000, 83.815000, 85.250000, 86.625000, 87.940000, 89.375000, 90.625000, 90.750000,\n    88.845000, 91.970000, 93.375000, 93.815000, 94.030000, 94.030000, 91.815000, 92.000000, 91.940000, 89.750000,\n    88.750000, 86.155000, 84.875000, 85.940000, 99.375000, 103.280000, 105.375000, 107.625000, 105.250000, 104.500000,\n    105.500000, 106.125000, 107.940000, 106.250000, 107.000000, 108.750000, 110.940000, 110.940000, 114.220000, 123.000000,\n    121.750000, 119.815000, 120.315000, 119.375000, 118.190000, 116.690000, 115.345000, 113.000000, 118.315000, 116.870000,\n    116.750000, 113.870000, 114.620000, 115.310000, 116.000000, 121.690000, 119.870000, 120.870000, 116.750000, 116.500000,\n    116.000000, 118.310000, 121.500000, 122.000000, 121.440000, 125.750000, 127.750000, 124.190000, 124.440000, 125.750000,\n    124.690000, 125.310000, 132.000000, 131.310000, 132.250000, 133.880000, 133.500000, 135.500000, 137.440000, 138.690000,\n    139.190000, 138.500000, 138.130000, 137.500000, 138.880000, 132.130000, 129.750000, 128.500000, 125.440000, 125.120000,\n    126.500000, 128.690000, 126.620000, 126.690000, 126.000000, 123.120000, 121.870000, 124.000000, 127.000000, 124.440000,\n    122.500000, 123.750000, 123.810000, 124.500000, 127.870000, 128.560000, 129.630000, 124.870000, 124.370000, 124.870000,\n    123.620000, 124.060000, 125.870000, 125.190000, 125.620000, 126.000000, 128.500000, 126.750000, 129.750000, 132.690000,\n    133.940000, 136.500000, 137.690000, 135.560000, 133.560000, 135.000000, 132.380000, 131.440000, 130.880000, 129.630000,\n    127.250000, 127.810000, 125.000000, 126.810000, 124.750000, 122.810000, 122.250000, 121.060000, 120.000000, 123.250000,\n    122.750000, 119.190000, 115.060000, 116.690000, 114.870000, 110.870000, 107.250000, 108.870000, 109.000000, 108.500000,\n    113.060000, 93.000000, 94.620000, 95.120000, 96.000000, 95.560000, 95.310000, 99.000000, 98.810000, 96.810000,\n    95.940000, 94.440000, 92.940000, 93.940000, 95.500000, 97.060000, 97.500000, 96.250000, 96.370000, 95.000000,\n    94.870000, 98.250000, 105.120000, 108.440000, 109.870000, 105.000000, 106.000000, 104.940000, 104.500000, 104.440000,\n    106.310000, 112.870000, 116.500000, 119.190000, 121.000000, 122.120000, 111.940000, 112.750000, 110.190000, 107.940000,\n    109.690000, 111.060000, 110.440000, 110.120000, 110.310000, 110.440000, 110.000000, 110.750000, 110.500000, 110.500000,\n    109.500000]\n\nLOW_VALUES = [\n    90.750000, 91.405000, 94.250000, 93.500000, 92.815000, 93.500000, 92.000000, 89.750000, 89.440000, 90.625000, 92.750000,\n    96.315000, 96.030000, 88.815000, 86.750000, 90.940000, 88.905000, 88.780000, 89.250000, 89.750000, 87.500000,\n    86.530000, 84.625000, 82.280000, 81.565000, 80.875000, 81.250000, 84.065000, 85.595000, 85.970000, 84.405000,\n    85.095000, 85.500000, 85.530000, 87.875000, 86.565000, 84.655000, 83.250000, 82.565000, 83.440000, 82.530000,\n    85.065000, 86.875000, 88.530000, 89.280000, 90.125000, 90.750000, 89.000000, 88.565000, 90.095000, 89.000000,\n    86.470000, 84.000000, 83.315000, 82.000000, 83.250000, 84.750000, 85.280000, 87.190000, 88.440000, 88.250000,\n    87.345000, 89.280000, 91.095000, 89.530000, 91.155000, 92.000000, 90.530000, 89.970000, 88.815000, 86.750000,\n    85.065000, 82.030000, 81.500000, 82.565000, 96.345000, 96.470000, 101.155000, 104.250000, 101.750000, 101.720000,\n    101.720000, 103.155000, 105.690000, 103.655000, 104.000000, 105.530000, 108.530000, 108.750000, 107.750000, 117.000000,\n    118.000000, 116.000000, 118.500000, 116.530000, 116.250000, 114.595000, 110.875000, 110.500000, 110.720000, 112.620000,\n    114.190000, 111.190000, 109.440000, 111.560000, 112.440000, 117.500000, 116.060000, 116.560000, 113.310000, 112.560000,\n    114.000000, 114.750000, 118.870000, 119.000000, 119.750000, 122.620000, 123.000000, 121.750000, 121.560000, 123.120000,\n    122.190000, 122.750000, 124.370000, 128.000000, 129.500000, 130.810000, 130.630000, 132.130000, 133.880000, 135.380000,\n    135.750000, 136.190000, 134.500000, 135.380000, 133.690000, 126.060000, 126.870000, 123.500000, 122.620000, 122.750000,\n    123.560000, 125.810000, 124.620000, 124.370000, 121.810000, 118.190000, 118.060000, 117.560000, 121.000000, 121.120000,\n    118.940000, 119.810000, 121.000000, 122.000000, 124.500000, 126.560000, 123.500000, 121.250000, 121.060000, 122.310000,\n    121.000000, 120.870000, 122.060000, 122.750000, 122.690000, 122.870000, 125.500000, 124.250000, 128.000000, 128.380000,\n    130.690000, 131.630000, 134.380000, 132.000000, 131.940000, 131.940000, 129.560000, 123.750000, 126.000000, 126.250000,\n    124.370000, 121.440000, 120.440000, 121.370000, 121.690000, 120.000000, 119.620000, 115.500000, 116.750000, 119.060000,\n    119.060000, 115.060000, 111.060000, 113.120000, 110.000000, 105.000000, 104.690000, 103.870000, 104.690000, 105.440000,\n    107.000000, 89.000000, 92.500000, 92.120000, 94.620000, 92.810000, 94.250000, 96.250000, 96.370000, 93.690000,\n    93.500000, 90.000000, 90.190000, 90.500000, 92.120000, 94.120000, 94.870000, 93.000000, 93.870000, 93.000000,\n    92.620000, 93.560000, 98.370000, 104.440000, 106.000000, 101.810000, 104.120000, 103.370000, 102.120000, 102.250000,\n    103.370000, 107.940000, 112.500000, 115.440000, 115.500000, 112.250000, 107.560000, 106.560000, 106.870000, 104.500000,\n    105.750000, 108.620000, 107.750000, 108.060000, 108.000000, 108.190000, 108.120000, 109.060000, 108.750000, 108.560000,\n    106.620000]\n\nCLOSE_VALUES = [\n    91.500000, 94.815000, 94.375000, 95.095000, 93.780000, 94.625000, 92.530000, 92.750000, 90.315000, 92.470000, 96.125000,\n    97.250000, 98.500000, 89.875000, 91.000000, 92.815000, 89.155000, 89.345000, 91.625000, 89.875000, 88.375000,\n    87.625000, 84.780000, 83.000000, 83.500000, 81.375000, 84.440000, 89.250000, 86.375000, 86.250000, 85.250000,\n    87.125000, 85.815000, 88.970000, 88.470000, 86.875000, 86.815000, 84.875000, 84.190000, 83.875000, 83.375000,\n    85.500000, 89.190000, 89.440000, 91.095000, 90.750000, 91.440000, 89.000000, 91.000000, 90.500000, 89.030000,\n    88.815000, 84.280000, 83.500000, 82.690000, 84.750000, 85.655000, 86.190000, 88.940000, 89.280000, 88.625000,\n    88.500000, 91.970000, 91.500000, 93.250000, 93.500000, 93.155000, 91.720000, 90.000000, 89.690000, 88.875000,\n    85.190000, 83.375000, 84.875000, 85.940000, 97.250000, 99.875000, 104.940000, 106.000000, 102.500000, 102.405000,\n    104.595000, 106.125000, 106.000000, 106.065000, 104.625000, 108.625000, 109.315000, 110.500000, 112.750000, 123.000000,\n    119.625000, 118.750000, 119.250000, 117.940000, 116.440000, 115.190000, 111.875000, 110.595000, 118.125000, 116.000000,\n    116.000000, 112.000000, 113.750000, 112.940000, 116.000000, 120.500000, 116.620000, 117.000000, 115.250000, 114.310000,\n    115.500000, 115.870000, 120.690000, 120.190000, 120.750000, 124.750000, 123.370000, 122.940000, 122.560000, 123.120000,\n    122.560000, 124.620000, 129.250000, 131.000000, 132.250000, 131.000000, 132.810000, 134.000000, 137.380000, 137.810000,\n    137.880000, 137.250000, 136.310000, 136.250000, 134.630000, 128.250000, 129.000000, 123.870000, 124.810000, 123.000000,\n    126.250000, 128.380000, 125.370000, 125.690000, 122.250000, 119.370000, 118.500000, 123.190000, 123.500000, 122.190000,\n    119.310000, 123.310000, 121.120000, 123.370000, 127.370000, 128.500000, 123.870000, 122.940000, 121.750000, 124.440000,\n    122.000000, 122.370000, 122.940000, 124.000000, 123.190000, 124.560000, 127.250000, 125.870000, 128.860000, 132.000000,\n    130.750000, 134.750000, 135.000000, 132.380000, 133.310000, 131.940000, 130.000000, 125.370000, 130.130000, 127.120000,\n    125.190000, 122.000000, 125.000000, 123.000000, 123.500000, 120.060000, 121.000000, 117.750000, 119.870000, 122.000000,\n    119.190000, 116.370000, 113.500000, 114.250000, 110.000000, 105.060000, 107.000000, 107.870000, 107.000000, 107.120000,\n    107.000000, 91.000000, 93.940000, 93.870000, 95.500000, 93.000000, 94.940000, 98.250000, 96.750000, 94.810000,\n    94.370000, 91.560000, 90.250000, 93.940000, 93.620000, 97.000000, 95.000000, 95.870000, 94.060000, 94.620000,\n    93.750000, 98.000000, 103.940000, 107.870000, 106.060000, 104.500000, 105.000000, 104.190000, 103.060000, 103.420000,\n    105.270000, 111.870000, 116.000000, 116.620000, 118.280000, 113.370000, 109.000000, 109.700000, 109.250000, 107.000000,\n    109.190000, 110.000000, 109.200000, 110.120000, 108.000000, 108.620000, 109.750000, 109.810000, 109.000000, 108.750000,\n    107.870000]\n\nVOLUME_VALUES = [\n    4077500, 4955900, 4775300, 4155300, 4593100, 3631300, 3382800, 4954200, 4500000, 3397500, 4204500,\n    6321400, 10203600, 19043900, 11692000, 9553300, 8920300, 5970900, 5062300, 3705600, 5865600,\n    5603000, 5811900, 8483800, 5995200, 5408800, 5430500, 6283800, 5834800, 4515500, 4493300,\n    4346100, 3700300, 4600200, 4557200, 4323600, 5237500, 7404100, 4798400, 4372800, 3872300,\n    10750800, 5804800, 3785500, 5014800, 3507700, 4298800, 4842500, 3952200, 3304700, 3462000,\n    7253900, 9753100, 5953000, 5011700, 5910800, 4916900, 4135000, 4054200, 3735300, 2921900,\n    2658400, 4624400, 4372200, 5831600, 4268600, 3059200, 4495500, 3425000, 3630800, 4168100,\n    5966900, 7692800, 7362500, 6581300, 19587700, 10378600, 9334700, 10467200, 5671400, 5645000,\n    4518600, 4519500, 5569700, 4239700, 4175300, 4995300, 4776600, 4190000, 6035300, 12168900,\n    9040800, 5780300, 4320800, 3899100, 3221400, 3455500, 4304200, 4703900, 8316300, 10553900,\n    6384800, 7163300, 7007800, 5114100, 5263800, 6666100, 7398400, 5575000, 4852300, 4298100,\n    4900500, 4887700, 6964800, 4679200, 9165000, 6469800, 6792000, 4423800, 5231900, 4565600,\n    6235200, 5225900, 8261400, 5912500, 3545600, 5714500, 6653900, 6094500, 4799200, 5050800,\n    5648900, 4726300, 5585600, 5124800, 7630200, 14311600, 8793600, 8874200, 6966600, 5525500,\n    6515500, 5291900, 5711700, 4327700, 4568000, 6859200, 5757500, 7367000, 6144100, 4052700,\n    5849700, 5544700, 5032200, 4400600, 4894100, 5140000, 6610900, 7585200, 5963100, 6045500,\n    8443300, 6464700, 6248300, 4357200, 4774700, 6216900, 6266900, 5584800, 5284500, 7554500,\n    7209500, 8424800, 5094500, 4443600, 4591100, 5658400, 6094100, 14862200, 7544700, 6985600,\n    8093000, 7590000, 7451300, 7078000, 7105300, 8778800, 6643900, 10563900, 7043100, 6438900,\n    8057700, 14240000, 17872300, 7831100, 8277700, 15017800, 14183300, 13921100, 9683000, 9187300,\n    11380500, 69447300, 26673600, 13768400, 11371600, 9872200, 9450500, 11083300, 9552800, 11108400,\n    10374200, 16701900, 13741900, 8523600, 9551900, 8680500, 7151700, 9673100, 6264700, 8541600,\n    8358000, 18720800, 19683100, 13682500, 10668100, 9710600, 3113100, 5682000, 5763600, 5340000,\n    6220800, 14680500, 9933000, 11329500, 8145300, 16644700, 12593800, 7138100, 7442300, 9442300,\n    7123600, 7680600, 4839800, 4775500, 4008800, 4533600, 3741100, 4084800, 2685200, 3438000,\n    2870500]\n\nSAR_HIGH = [51.12, 52.35, 52.1, 51.8, 52.1, 52.5, 52.8, 52.5, 53.5, 53.5, 53.8, 54.2, 53.4, 53.5, 54.4, 55.2, 55.7, 57, 57.5, 58, 57.7, 58, 57.5, 57, 56.7, 57.5, 56.70, 56.00, 56.20, 54.80, 55.50, 54.70, 54.00, 52.50, 51.00, 51.50, 51.70, 53.00]\nSAR_LOW = [50.0, 51.5, 51, 50.5, 51.25, 51.7, 51.85, 51.5, 52.3, 52.5, 53, 53.5, 52.5, 52.1, 53, 54, 55, 56, 56.5, 57, 56.5, 57.3, 56.7, 56.3, 56.2, 56, 55.50, 55.00, 54.90, 54.00, 54.50, 53.80, 53.00, 51.50, 50.00, 50.50, 50.20, 51.50]\n\n\ndef compare(obtained, expected, decimals=2):\n    obtained = round(obtained, decimals)\n    expected = round(expected, decimals)\n    return obtained == expected\n\n\nclass TestCase(common.TestCase):\n    TestInstrument = \"orcl\"\n\n    def __loadMedPriceDS(self):\n        ret = dataseries.SequenceDataSeries()\n        for i in xrange(len(OPEN_VALUES)):\n            ret.append(LOW_VALUES[i] + (HIGH_VALUES[i] - LOW_VALUES[i]) / 2.0)\n        return ret\n\n    def __loadBarDS(self):\n        seconds = 0\n\n        ret = bards.BarDataSeries()\n        for i in xrange(len(OPEN_VALUES)):\n            dateTime = datetime.datetime.now() + datetime.timedelta(seconds=seconds)\n            ret.append(bar.BasicBar(dateTime, OPEN_VALUES[i], HIGH_VALUES[i], LOW_VALUES[i], CLOSE_VALUES[i], VOLUME_VALUES[i], CLOSE_VALUES[i], bar.Frequency.DAY))\n            seconds += 1\n        return ret\n\n    def __loadSarTestBarDs(self):\n        seconds = 0\n\n        ret = bards.BarDataSeries()\n        for i in xrange(len(SAR_HIGH)):\n            dateTime = datetime.datetime.now() + datetime.timedelta(seconds=seconds)\n            ret.append(bar.BasicBar(dateTime, SAR_LOW[i], SAR_HIGH[i], SAR_LOW[i], SAR_HIGH[i], 0, SAR_LOW[i], bar.Frequency.DAY))\n            seconds += 1\n        return ret\n\n    def testAD(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.AD(barDs, 252)[0], -1631000.00))\n        self.assertTrue(compare(indicator.AD(barDs, 252)[1], 2974412.02))\n        self.assertTrue(compare(indicator.AD(barDs, 252)[-2], 8707691.07))\n        self.assertTrue(compare(indicator.AD(barDs, 252)[-1], 8328944.54))\n\n    def testADOSC(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ADOSC(barDs, 252, 3, 10)[9], 841238.33))  # Original value was 841238.32\n        self.assertTrue(compare(indicator.ADOSC(barDs, 252, 3, 10)[9+1], 2255663.07))\n        self.assertTrue(compare(indicator.ADOSC(barDs, 252, 3, 10)[-2], -526700.32))\n        self.assertTrue(compare(indicator.ADOSC(barDs, 252, 3, 10)[-1], -1139932.729))\n\n    def testADX(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ADX(barDs, 252, 14)[27], 23.0000))\n        self.assertTrue(compare(indicator.ADX(barDs, 252, 14)[28], 22.0802))\n        self.assertTrue(compare(indicator.ADX(barDs, 252, 14)[-2], 16.6840))\n        self.assertTrue(compare(indicator.ADX(barDs, 252, 14)[-1], 15.5260))\n\n    def testADXR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ADXR(barDs, 252, 14)[40], 19.8666))\n        self.assertTrue(compare(indicator.ADXR(barDs, 252, 14)[41], 18.9092))\n        self.assertTrue(compare(indicator.ADXR(barDs, 252, 14)[-2], 21.5972))\n        self.assertTrue(compare(indicator.ADXR(barDs, 252, 14)[-1], 20.4920))\n\n    def testAPO(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.APO(barDs.getCloseDataSeries(), 252, 26, 12, talib.MA_Type.SMA)[25], -3.3124))\n        self.assertTrue(compare(indicator.APO(barDs.getCloseDataSeries(), 252, 12, 26, talib.MA_Type.SMA)[25], -3.3124))\n        self.assertTrue(compare(indicator.APO(barDs.getCloseDataSeries(), 252, 12, 26, talib.MA_Type.SMA)[26], -3.5876))\n        self.assertTrue(compare(indicator.APO(barDs.getCloseDataSeries(), 252, 12, 26, talib.MA_Type.SMA)[-1], -0.1667))\n\n    def testAROON(self):\n        barDs = self.__loadBarDS()\n        # AROON DOWN TEST\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[0][14], 100))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[0][14+1], 92.857))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[0][-2], 28.571))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[0][-1], 21.429))\n        # AROON UP TEST\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[1][14], 78.571))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[1][14+1], 71.429))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[1][-2], 0))\n        self.assertTrue(compare(indicator.AROON(barDs, 252, 14)[1][-1], 7.1429))\n\n    def testAROONOSC(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.AROONOSC(barDs, 252, 14)[14], -21.4285))\n        self.assertTrue(compare(indicator.AROONOSC(barDs, 252, 14)[14+6], -21.4285))\n        self.assertTrue(compare(indicator.AROONOSC(barDs, 252, 14)[14+7], -71.4285))\n        self.assertTrue(compare(indicator.AROONOSC(barDs, 252, 14)[-2], -28.5714))\n        self.assertTrue(compare(indicator.AROONOSC(barDs, 252, 14)[-1], -14.28571))\n\n    def testATR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ATR(barDs, 252, 1)[1], 3.535, 3))\n        self.assertTrue(compare(indicator.ATR(barDs, 252, 1)[13], 9.685, 3))\n        self.assertTrue(compare(indicator.ATR(barDs, 252, 1)[41], 5.125, 3))\n        self.assertTrue(compare(indicator.ATR(barDs, 252, 1)[-1], 2.88, 3))\n\n    def testAVGPRICE(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.AVGPRICE(barDs, 252)[0], 92.0))\n        self.assertTrue(compare(indicator.AVGPRICE(barDs, 252)[1], 93.16))  # Original value was 93.17\n\n    def testBBANDS(self):\n        barDs = self.__loadBarDS()\n        # EMA\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.EMA)[0][19+13], 93.674))\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.EMA)[1][19+13], 87.679))\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.EMA)[2][19+13], 81.685))\n        # SMA\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.SMA)[0][19], 98.0734))\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.SMA)[1][19], 92.8910))\n        self.assertTrue(compare(indicator.BBANDS(barDs.getCloseDataSeries(), 252, 20, 2.0, 2.0, talib.MA_Type.SMA)[2][19], 87.7086))\n\n    def testBETA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.BETA(barDs.getHighDataSeries(), barDs.getLowDataSeries(), 252, 5)[5], 0.62907))\n        self.assertTrue(compare(indicator.BETA(barDs.getHighDataSeries(), barDs.getLowDataSeries(), 252, 5)[6], 0.83604))\n\n    def testBOP(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.BOP(barDs, 252)[0], -0.40))\n        self.assertTrue(compare(indicator.BOP(barDs, 252)[1], 0.94))\n\n    def testCCI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.CCI(barDs, 252, 2)[1], 66.666))\n        self.assertTrue(compare(indicator.CCI(barDs, 252, 5)[4], 18.857))\n        self.assertTrue(compare(indicator.CCI(barDs, 252, 11)[10], 87.927))\n        self.assertTrue(compare(indicator.CCI(barDs, 252, 11)[11], 180.005, 3))\n\n    def testCMO(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.CMO(barDs.getCloseDataSeries(), 252, 14)[14], -1.70, 1))\n\n    def testCORREL(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.CORREL(barDs.getHighDataSeries(), barDs.getLowDataSeries(), 252, 20)[19], 0.9401569))\n        self.assertTrue(compare(indicator.CORREL(barDs.getHighDataSeries(), barDs.getLowDataSeries(), 252, 20)[20], 0.9471812))\n        self.assertTrue(compare(indicator.CORREL(barDs.getHighDataSeries(), barDs.getLowDataSeries(), 252, 20)[-1], 0.8866901))\n\n    def testDX(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.DX(barDs, 252, 14)[14], 19.3689))\n        self.assertTrue(compare(indicator.DX(barDs, 252, 14)[15], 9.7131))\n        self.assertTrue(compare(indicator.DX(barDs, 252, 14)[16], 17.2905))\n        self.assertTrue(compare(indicator.DX(barDs, 252, 14)[-2], 10.6731))\n        self.assertTrue(compare(indicator.DX(barDs, 252, 14)[-1], 0.4722))\n\n    def testEMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.EMA(barDs.getCloseDataSeries(), 252, 2)[1], 93.16))  # Original value 93.15\n        self.assertTrue(compare(indicator.EMA(barDs.getCloseDataSeries(), 252, 2)[2], 93.97))  # Original value 93.96\n        self.assertTrue(compare(indicator.EMA(barDs.getCloseDataSeries(), 252, 2)[-1], 108.22))  # Original value 108.21\n        self.assertTrue(compare(indicator.EMA(barDs.getCloseDataSeries(), 252, 10)[9], 93.23))  # Original value 93.22\n\n    def testHT_DCPERIOD(self):\n        ds = self.__loadMedPriceDS()\n        self.assertTrue(compare(indicator.HT_DCPERIOD(ds, 252)[32], 15.5527, 4))\n        self.assertTrue(compare(indicator.HT_DCPERIOD(ds, 252)[-1], 18.6140, 4))\n\n    def testHT_DCPHASE(self):\n        ds = self.__loadMedPriceDS()\n        self.assertTrue(compare(indicator.HT_DCPHASE(ds, 252)[63], 22.1496, 4))  # Original value 22.1495\n        self.assertTrue(compare(indicator.HT_DCPHASE(ds, 252)[-3], -31.182, 3))\n        self.assertTrue(compare(indicator.HT_DCPHASE(ds, 252)[-2], 23.2691, 4))\n        self.assertTrue(compare(indicator.HT_DCPHASE(ds, 252)[-1], 47.2765, 4))\n\n    def testHT_TRENDLINE(self):\n        ds = self.__loadMedPriceDS()\n        self.assertTrue(compare(indicator.HT_TRENDLINE(ds, 252)[63], 88.257))\n        self.assertTrue(compare(indicator.HT_TRENDLINE(ds, 252)[-3], 109.69))\n        self.assertTrue(compare(indicator.HT_TRENDLINE(ds, 252)[-2], 110.18))\n        self.assertTrue(compare(indicator.HT_TRENDLINE(ds, 252)[-1], 110.46))\n\n    def testHT_TRENDMODE(self):\n        ds = self.__loadMedPriceDS()\n        self.assertTrue(compare(indicator.HT_TRENDMODE(ds, 252)[63], 1.0))\n\n    def testKAMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.KAMA(barDs.getCloseDataSeries(), 252, 10)[10], 92.6575))\n        self.assertTrue(compare(indicator.KAMA(barDs.getCloseDataSeries(), 252, 10)[11], 92.7783))\n        self.assertTrue(compare(indicator.KAMA(barDs.getCloseDataSeries(), 252, 10)[-1], 109.294))\n\n    def testMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MA(barDs.getCloseDataSeries(), 252, 2, talib.MA_Type.SMA)[1], 93.16))  # Original value 93.15\n        self.assertTrue(compare(indicator.MA(barDs.getCloseDataSeries(), 252, 2, talib.MA_Type.SMA)[2], 94.59))\n        self.assertTrue(compare(indicator.MA(barDs.getCloseDataSeries(), 252, 2, talib.MA_Type.SMA)[3], 94.73))\n        self.assertTrue(compare(indicator.MA(barDs.getCloseDataSeries(), 252, 2, talib.MA_Type.SMA)[-1], 108.31))\n\n    def testMACD(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 12, 26, 9)[0][33], -1.9738))\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 12, 26, 9)[1][33], -2.7071))\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 12, 26, 9)[2][33], (-1.9738)-(-2.7071)))\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 26, 12, 9)[0][33], -1.9738))\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 26, 12, 9)[1][33], -2.7071))\n        self.assertTrue(compare(indicator.MACD(barDs.getCloseDataSeries(), 252, 26, 12, 9)[2][33], (-1.9738)-(-2.7071)))\n\n    def testMACDEXT(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MACDEXT(barDs.getCloseDataSeries(), 252, 12, talib.MA_Type.EMA, 26, talib.MA_Type.EMA, 9, talib.MA_Type.EMA)[0][33], -1.9738))\n        self.assertTrue(compare(indicator.MACDEXT(barDs.getCloseDataSeries(), 252, 12, talib.MA_Type.EMA, 26, talib.MA_Type.EMA, 9, talib.MA_Type.EMA)[1][33], -2.7071))\n        self.assertTrue(compare(indicator.MACDEXT(barDs.getCloseDataSeries(), 252, 12, talib.MA_Type.EMA, 26, talib.MA_Type.EMA, 9, talib.MA_Type.EMA)[2][33], (-1.9738)-(-2.7071)))\n\n    def testMAMA(self):\n        ds = self.__loadMedPriceDS()\n        self.assertTrue(compare(indicator.MAMA(ds, 252, 0.5, 0.05)[0][32], 85.3643))\n        self.assertTrue(compare(indicator.MAMA(ds, 252, 0.5, 0.05)[0][-1], 110.1116))\n\n    def testMAX(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MAX(barDs.getOpenDataSeries(), 252, 14)[13], 98.815))\n        self.assertTrue(compare(indicator.MAX(barDs.getOpenDataSeries(), 252, 14)[14], 98.815))\n        self.assertTrue(compare(indicator.MAX(barDs.getOpenDataSeries(), 252, 14)[-1], 110.69))\n\n    def testMFI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MFI(barDs, 252, 14)[14], 42.8923))\n        self.assertTrue(compare(indicator.MFI(barDs, 252, 14)[15], 45.6072))\n        self.assertTrue(compare(indicator.MFI(barDs, 252, 14)[-1], 53.1997))\n\n    def testMIN(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MIN(barDs.getOpenDataSeries(), 252, 14)[13], 91.125))\n        self.assertTrue(compare(indicator.MIN(barDs.getOpenDataSeries(), 252, 14)[14], 91.125))\n        self.assertTrue(compare(indicator.MIN(barDs.getOpenDataSeries(), 252, 14)[-1], 107.75))\n\n    def testMINUS_DI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MINUS_DI(barDs, 252, 14)[14], 30.1684))\n        self.assertTrue(compare(indicator.MINUS_DI(barDs, 252, 14)[28], 24.969182))\n        self.assertTrue(compare(indicator.MINUS_DI(barDs, 252, 14)[-1], 21.1988))\n\n    def testMINUS_DM(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MINUS_DM(barDs, 252, 14)[13], 12.995, 3))\n        self.assertTrue(compare(indicator.MINUS_DM(barDs, 252, 14)[-2], 8.33))\n        self.assertTrue(compare(indicator.MINUS_DM(barDs, 252, 14)[-1], 9.68))  # Original value 9.672\n\n    def testMOM(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.MOM(barDs.getCloseDataSeries(), 252, 14)[14], -0.50))\n        self.assertTrue(compare(indicator.MOM(barDs.getCloseDataSeries(), 252, 14)[15], -2.00))\n        self.assertTrue(compare(indicator.MOM(barDs.getCloseDataSeries(), 252, 14)[16], -5.22))\n        self.assertTrue(compare(indicator.MOM(barDs.getCloseDataSeries(), 252, 14)[-1], -1.13))\n\n    def testNATR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.NATR(barDs, 252, 14)[14], 3.9321))\n        self.assertTrue(compare(indicator.NATR(barDs, 252, 14)[15], 3.7576))\n        self.assertTrue(compare(indicator.NATR(barDs, 252, 14)[-1], 3.0229))\n\n    def testPLUS_DI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.PLUS_DI(barDs, 252, 14)[14], 20.3781))\n        self.assertTrue(compare(indicator.PLUS_DI(barDs, 252, 14)[14+13], 22.1073))\n        self.assertTrue(compare(indicator.PLUS_DI(barDs, 252, 14)[14+14], 20.3746))\n        self.assertTrue(compare(indicator.PLUS_DI(barDs, 252, 14)[-1], 21.0000))\n\n    def testPLUS_DM(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.PLUS_DM(barDs, 252, 14)[13], 10.28))\n        self.assertTrue(compare(indicator.PLUS_DM(barDs, 252, 14)[-2], 10.317))\n        self.assertTrue(compare(indicator.PLUS_DM(barDs, 252, 14)[-1], 9.59))  # Original value 9.58\n\n    def testPPO(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.PPO(barDs.getCloseDataSeries(), 252, 2, 3, talib.MA_Type.SMA)[2], 1.10264))\n        self.assertTrue(compare(indicator.PPO(barDs.getCloseDataSeries(), 252, 2, 3, talib.MA_Type.SMA)[3], -0.02813))\n        self.assertTrue(compare(indicator.PPO(barDs.getCloseDataSeries(), 252, 2, 3, talib.MA_Type.SMA)[-1], -0.21191))\n\n    def testROC(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ROC(barDs.getCloseDataSeries(), 252, 14)[14], -0.546))\n        self.assertTrue(compare(indicator.ROC(barDs.getCloseDataSeries(), 252, 14)[15], -2.109))\n        self.assertTrue(compare(indicator.ROC(barDs.getCloseDataSeries(), 252, 14)[16], -5.53))\n        self.assertTrue(compare(indicator.ROC(barDs.getCloseDataSeries(), 252, 14)[-1], -1.0367))\n\n    def testROCR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ROCR(barDs.getCloseDataSeries(), 252, 14)[14], 0.994536, 4))\n        self.assertTrue(compare(indicator.ROCR(barDs.getCloseDataSeries(), 252, 14)[15], 0.978906, 4))\n        self.assertTrue(compare(indicator.ROCR(barDs.getCloseDataSeries(), 252, 14)[16], 0.944689, 4))\n        self.assertTrue(compare(indicator.ROCR(barDs.getCloseDataSeries(), 252, 14)[-1], 0.989633, 4))\n\n    def testROCR100(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ROCR100(barDs.getCloseDataSeries(), 252, 14)[14], 99.4536, 4))\n        self.assertTrue(compare(indicator.ROCR100(barDs.getCloseDataSeries(), 252, 14)[15], 97.8906, 4))\n        self.assertTrue(compare(indicator.ROCR100(barDs.getCloseDataSeries(), 252, 14)[16], 94.4689, 4))\n        self.assertTrue(compare(indicator.ROCR100(barDs.getCloseDataSeries(), 252, 14)[-1], 98.9633, 4))\n\n    def testRSI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.RSI(barDs.getCloseDataSeries(), 252, 14)[14], 49.15))  # Original value 49.14\n        self.assertTrue(compare(indicator.RSI(barDs.getCloseDataSeries(), 252, 14)[15], 52.33))  # Original value 52.32\n        self.assertTrue(compare(indicator.RSI(barDs.getCloseDataSeries(), 252, 14)[16], 46.07))\n        self.assertTrue(compare(indicator.RSI(barDs.getCloseDataSeries(), 252, 14)[-1], 49.63))\n\n    def testSAR(self):\n        barDs = self.__loadSarTestBarDs()\n        self.assertTrue(compare(indicator.SAR(barDs, len(SAR_HIGH), 0.02, 0.20)[1], 50.00))\n        self.assertTrue(compare(indicator.SAR(barDs, len(SAR_HIGH), 0.02, 0.20)[2], 50.047))\n        self.assertTrue(compare(indicator.SAR(barDs, len(SAR_HIGH), 0.02, 0.20)[5], 50.182))\n        self.assertTrue(compare(indicator.SAR(barDs, len(SAR_HIGH), 0.02, 0.20)[-2], 52.93))\n        self.assertTrue(compare(indicator.SAR(barDs, len(SAR_HIGH), 0.02, 0.20)[-1], 50.00))\n\n    def testSMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.SMA(barDs.getCloseDataSeries(), 252, 2)[1], 93.16))  # Original value 93.15\n        self.assertTrue(compare(indicator.SMA(barDs.getCloseDataSeries(), 252, 2)[2], 94.59))\n        self.assertTrue(compare(indicator.SMA(barDs.getCloseDataSeries(), 252, 2)[3], 94.73))\n        self.assertTrue(compare(indicator.SMA(barDs.getCloseDataSeries(), 252, 2)[-1], 108.31))\n\n    def testSTDDEV(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1)[4], 1.2856))\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1)[5], 0.4462))\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1)[-1], 0.7144))\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1.5)[4], 1.9285))\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1.5)[5], 0.66937))\n        self.assertTrue(compare(indicator.STDDEV(barDs.getCloseDataSeries(), 252, 5.0, 1.5)[-1], 1.075))\n\n    def testSTOCH(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.STOCH(barDs, 252, 5, 3, talib.MA_Type.SMA, 3, talib.MA_Type.SMA)[0][8], 24.0128))\n        self.assertTrue(compare(indicator.STOCH(barDs, 252, 5, 3, talib.MA_Type.SMA, 3, talib.MA_Type.SMA)[1][8], 36.254))\n        self.assertTrue(compare(indicator.STOCH(barDs, 252, 5, 3, talib.MA_Type.SMA, 4, talib.MA_Type.SMA)[0][-1], 30.194))\n        self.assertTrue(compare(indicator.STOCH(barDs, 252, 5, 3, talib.MA_Type.SMA, 4, talib.MA_Type.SMA)[1][-1], 46.641))\n\n    def testSTOCHRSI(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 14, 1, talib.MA_Type.SMA)[0][27], 94.156709))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 14, 1, talib.MA_Type.SMA)[1][27], 94.156709))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 14, 1, talib.MA_Type.SMA)[0][-1], 0))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 14, 1, talib.MA_Type.SMA)[1][-1], 0))\n\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 45, 1, talib.MA_Type.SMA)[0][58], 79.729186))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 45, 1, talib.MA_Type.SMA)[1][58], 79.729186))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 45, 1, talib.MA_Type.SMA)[0][-1], 48.1550743))\n        self.assertTrue(compare(indicator.STOCHRSI(barDs.getCloseDataSeries(), 252, 14, 45, 1, talib.MA_Type.SMA)[1][-1], 48.1550743))\n\n    def testT3(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.T3(barDs.getCloseDataSeries(), 252, 5, 0.7)[24],  85.73))\n        self.assertTrue(compare(indicator.T3(barDs.getCloseDataSeries(), 252, 5, 0.7)[25],  84.37))\n        self.assertTrue(compare(indicator.T3(barDs.getCloseDataSeries(), 252, 5, 0.7)[-2], 109.03))\n        self.assertTrue(compare(indicator.T3(barDs.getCloseDataSeries(), 252, 5, 0.7)[-1], 108.88))\n\n    def testTRANGE(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.TRANGE(barDs, 252)[1], 3.535, 3))\n        self.assertTrue(compare(indicator.TRANGE(barDs, 252)[13], 9.685, 3))\n        self.assertTrue(compare(indicator.TRANGE(barDs, 252)[41], 5.125, 3))\n        self.assertTrue(compare(indicator.TRANGE(barDs, 252)[-1], 2.88))\n\n    def testTRIMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.TRIMA(barDs.getCloseDataSeries(), 252, 10)[9], 93.6043))\n        self.assertTrue(compare(indicator.TRIMA(barDs.getCloseDataSeries(), 252, 10)[10], 93.4252))\n        self.assertTrue(compare(indicator.TRIMA(barDs.getCloseDataSeries(), 252, 10)[-2], 109.1850, 3))\n        self.assertTrue(compare(indicator.TRIMA(barDs.getCloseDataSeries(), 252, 10)[-1], 109.1407))\n\n    def testTRIX(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.TRIX(barDs.getCloseDataSeries(), 252, 5)[13], 0.2589))\n        self.assertTrue(compare(indicator.TRIX(barDs.getCloseDataSeries(), 252, 5)[14], 0.010495))\n        self.assertTrue(compare(indicator.TRIX(barDs.getCloseDataSeries(), 252, 5)[-2], -0.058))\n        self.assertTrue(compare(indicator.TRIX(barDs.getCloseDataSeries(), 252, 5)[-1], -0.095))\n\n    def testULTOSC(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.ULTOSC(barDs, 252, 7, 14, 28)[28], 47.1713))\n        self.assertTrue(compare(indicator.ULTOSC(barDs, 252, 7, 14, 28)[29], 46.2802))\n        self.assertTrue(compare(indicator.ULTOSC(barDs, 252, 7, 14, 28)[-1], 40.0854))\n\n    def testVAR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.VAR(barDs.getCloseDataSeries(), 252, 5.0, 1)[4], 1.2856**2))\n        self.assertTrue(compare(indicator.VAR(barDs.getCloseDataSeries(), 252, 5.0, 1)[5], 0.4462**2))\n        self.assertTrue(compare(indicator.VAR(barDs.getCloseDataSeries(), 252, 5.0, 1)[-1], 0.7144**2))\n\n    def testWILLR(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.WILLR(barDs, 252, 14)[13], -90.1943))\n        self.assertTrue(compare(indicator.WILLR(barDs, 252, 14)[13+112], 0))\n\n    def testWMA(self):\n        barDs = self.__loadBarDS()\n        self.assertTrue(compare(indicator.WMA(barDs.getCloseDataSeries(), 252, 2)[1], 93.71))\n        self.assertTrue(compare(indicator.WMA(barDs.getCloseDataSeries(), 252, 2)[2], 94.52))\n        self.assertTrue(compare(indicator.WMA(barDs.getCloseDataSeries(), 252, 2)[3], 94.86))  # Original value 94.85\n        self.assertTrue(compare(indicator.WMA(barDs.getCloseDataSeries(), 252, 2)[-1], 108.16))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade.technical import atr\nfrom pyalgotrade import bar\nfrom pyalgotrade.dataseries import bards\n\n\nclass TestCase(common.TestCase):\n    def testStockChartsATR(self):\n        # Test data from http://stockcharts.com/help/doku.php?id=chart_school:technical_indicators:average_true_range_a\n        high = [48.70, 48.72, 48.90, 48.87, 48.82, 49.05, 49.20, 49.35, 49.92, 50.19, 50.12, 49.66, 49.88, 50.19, 50.36, 50.57, 50.65, 50.43, 49.63, 50.33, 50.29, 50.17, 49.32, 48.50, 48.32, 46.80, 47.80, 48.39, 48.66, 48.79]\n        low = [47.790, 48.140, 48.390, 48.370, 48.240, 48.635, 48.940, 48.860, 49.500, 49.870, 49.200, 48.900, 49.430, 49.725, 49.260, 50.090, 50.300, 49.210, 48.980, 49.610, 49.200, 49.430, 48.080, 47.640, 41.550, 44.283, 47.310, 47.200, 47.900, 47.730]\n        close = [48.160, 48.610, 48.750, 48.630, 48.740, 49.030, 49.070, 49.320, 49.910, 50.130, 49.530, 49.500, 49.750, 50.030, 50.310, 50.520, 50.410, 49.340, 49.370, 50.230, 49.238, 49.930, 48.430, 48.180, 46.570, 45.410, 47.770, 47.720, 48.620, 47.850]\n        expected = [None, None, None, None, None, None, None, None, None, None, None, None, None, 0.56, 0.59, 0.59, 0.57, 0.62, 0.62, 0.64, 0.67, 0.69, 0.78, 0.78, 1.21, 1.30, 1.38, 1.37, 1.34, 1.32]\n\n        # Build a bar dataseries using the test data.\n        barDataSeries = bards.BarDataSeries()\n        atrDS = atr.ATR(barDataSeries, 14)\n        now = datetime.datetime(2000, 1, 1)\n        for i in xrange(len(high)):\n            b = bar.BasicBar(now + datetime.timedelta(days=i), close[i], high[i], low[i], close[i], 100, close[i], bar.Frequency.DAY)\n            barDataSeries.append(b)\n            self.assertEqual(common.safe_round(atrDS[-1], 2), expected[i])\n\n    def testStockChartsATRAdjusted(self):\n        # Test data from http://stockcharts.com/help/doku.php?id=chart_school:technical_indicators:average_true_range_a\n        high = [48.70, 48.72, 48.90, 48.87, 48.82, 49.05, 49.20, 49.35, 49.92, 50.19, 50.12, 49.66, 49.88, 50.19, 50.36, 50.57, 50.65, 50.43, 49.63, 50.33, 50.29, 50.17, 49.32, 48.50, 48.32, 46.80, 47.80, 48.39, 48.66, 48.79]\n        low = [47.790, 48.140, 48.390, 48.370, 48.240, 48.635, 48.940, 48.860, 49.500, 49.870, 49.200, 48.900, 49.430, 49.725, 49.260, 50.090, 50.300, 49.210, 48.980, 49.610, 49.200, 49.430, 48.080, 47.640, 41.550, 44.283, 47.310, 47.200, 47.900, 47.730]\n        close = [48.160, 48.610, 48.750, 48.630, 48.740, 49.030, 49.070, 49.320, 49.910, 50.130, 49.530, 49.500, 49.750, 50.030, 50.310, 50.520, 50.410, 49.340, 49.370, 50.230, 49.238, 49.930, 48.430, 48.180, 46.570, 45.410, 47.770, 47.720, 48.620, 47.850]\n        expected = [None, None, None, None, None, None, None, None, None, None, None, None, None, 0.555000, 0.593929, 0.585791, 0.568949, 0.615452, 0.617920, 0.642354, 0.674329, 0.692770, 0.775429, 0.781470, 1.209229, 1.302620, 1.380290, 1.366698, 1.336219, 1.316482]\n\n        # Build a bar dataseries using the test data.\n        barDataSeries = bards.BarDataSeries()\n        atrDS = atr.ATR(barDataSeries, 14, True)\n        now = datetime.datetime(2000, 1, 1)\n        for i in xrange(len(high)):\n            b = bar.BasicBar(now + datetime.timedelta(days=i), close[i], high[i], low[i], close[i], 100, close[i]/2, bar.Frequency.DAY)\n            barDataSeries.append(b)\n            if expected[i] is None:\n                self.assertEqual(atrDS[-1], None)\n            else:\n                self.assertEqual(common.safe_round(atrDS[-1], 2), round(expected[i]/2, 2))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import bollinger\nfrom pyalgotrade import dataseries\n\n\nclass TestCase(common.TestCase):\n    def testStockChartsBollinger(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:bollinger_bands\n        prices = [86.1557, 89.0867, 88.7829, 90.3228, 89.0671, 91.1453, 89.4397, 89.1750, 86.9302, 87.6752, 86.9596, 89.4299, 89.3221, 88.7241, 87.4497, 87.2634, 89.4985, 87.9006, 89.1260, 90.7043, 92.9001, 92.9784, 91.8021, 92.6647, 92.6843, 92.3021, 92.7725, 92.5373, 92.9490, 93.2039, 91.0669, 89.8318, 89.7435, 90.3994, 90.7387, 88.0177, 88.0867, 88.8439, 90.7781, 90.5416, 91.3894, 90.6500]\n        expectedMiddle = [88.71, 89.05, 89.24, 89.39, 89.51, 89.69, 89.75, 89.91, 90.08, 90.38, 90.66, 90.86, 90.88, 90.91, 90.99, 91.15, 91.19, 91.12, 91.17, 91.25, 91.24, 91.17, 91.05]\n        expectedUpper = [91.29, 91.95, 92.61, 92.93, 93.31, 93.73, 93.90, 94.27, 94.57, 94.79, 95.04, 94.91, 94.90, 94.90, 94.86, 94.67, 94.56, 94.68, 94.58, 94.53, 94.53, 94.37, 94.15]\n        expectedLower = [86.12, 86.14, 85.87, 85.85, 85.70, 85.65, 85.59, 85.56, 85.60, 85.98, 86.27, 86.82, 86.87, 86.91, 87.12, 87.63, 87.83, 87.56, 87.76, 87.97, 87.95, 87.96, 87.95]\n\n        seqDS = dataseries.SequenceDataSeries()\n        bBands = bollinger.BollingerBands(seqDS, 20, 2)\n        for value in prices:\n            seqDS.append(value)\n\n        for i in xrange(19):\n            self.assertEqual(bBands.getMiddleBand()[i], None)\n            self.assertEqual(bBands.getUpperBand()[i], None)\n            self.assertEqual(bBands.getLowerBand()[i], None)\n\n        for i in xrange(19, len(seqDS)):\n            self.assertEqual(round(bBands.getMiddleBand()[i], 2), expectedMiddle[i-19])\n            self.assertEqual(round(bBands.getUpperBand()[i], 2), expectedUpper[i-19])\n            self.assertEqual(round(bBands.getLowerBand()[i], 2), expectedLower[i-19])\n\n    def testStockChartsBollinger_Bounded(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:bollinger_bands\n        prices = [86.1557, 89.0867, 88.7829, 90.3228, 89.0671, 91.1453, 89.4397, 89.1750, 86.9302, 87.6752, 86.9596, 89.4299, 89.3221, 88.7241, 87.4497, 87.2634, 89.4985, 87.9006, 89.1260, 90.7043, 92.9001, 92.9784, 91.8021, 92.6647, 92.6843, 92.3021, 92.7725, 92.5373, 92.9490, 93.2039, 91.0669, 89.8318, 89.7435, 90.3994, 90.7387, 88.0177, 88.0867, 88.8439, 90.7781, 90.5416, 91.3894, 90.6500]\n        expectedMiddle = [91.24, 91.17, 91.05]\n        expectedUpper = [94.53, 94.37, 94.15]\n        expectedLower = [87.95, 87.96, 87.95]\n\n        seqDS = dataseries.SequenceDataSeries()\n        bBands = bollinger.BollingerBands(seqDS, 20, 2, 3)\n        for value in prices:\n            seqDS.append(value)\n\n        for i in xrange(3):\n            self.assertEqual(round(bBands.getMiddleBand()[i], 2), expectedMiddle[i])\n            self.assertEqual(round(bBands.getUpperBand()[i], 2), expectedUpper[i])\n            self.assertEqual(round(bBands.getLowerBand()[i], 2), expectedLower[i])\n\n        self.assertEqual(len(bBands.getMiddleBand()), 3)\n        self.assertEqual(len(bBands.getMiddleBand()[:]), 3)\n        self.assertEqual(len(bBands.getMiddleBand().getDateTimes()), 3)\n        self.assertEqual(len(bBands.getUpperBand()), 3)\n        self.assertEqual(len(bBands.getUpperBand()[:]), 3)\n        self.assertEqual(len(bBands.getUpperBand().getDateTimes()), 3)\n        self.assertEqual(len(bBands.getLowerBand()), 3)\n        self.assertEqual(len(bBands.getLowerBand()[:]), 3)\n        self.assertEqual(len(bBands.getLowerBand().getDateTimes()), 3)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import cross\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade import dataseries\n\n\nclass HelpersTestCase(common.TestCase):\n    def test_get_stripped_left(self):\n        v1, v2 = cross._get_stripped([1, 2, 3], [1], True)\n        self.assertEqual(v1, [1])\n        self.assertEqual(v2, [1])\n\n        v1, v2 = cross._get_stripped([1], [1, 2, 3], True)\n        self.assertEqual(v1, [1])\n        self.assertEqual(v2, [1])\n\n        v1, v2 = cross._get_stripped([1, 2, 3], [1, 2], True)\n        self.assertEqual(v1, [1, 2])\n        self.assertEqual(v2, [1, 2])\n\n        v1, v2 = cross._get_stripped([1, 2], [1, 2, 3], True)\n        self.assertEqual(v1, [1, 2])\n        self.assertEqual(v2, [1, 2])\n\n    def test_get_stripped_right(self):\n        v1, v2 = cross._get_stripped([1, 2, 3], [1], False)\n        self.assertEqual(v1, [3])\n        self.assertEqual(v2, [1])\n\n        v1, v2 = cross._get_stripped([1], [1, 2, 3], False)\n        self.assertEqual(v1, [1])\n        self.assertEqual(v2, [3])\n\n        v1, v2 = cross._get_stripped([1, 2, 3], [1, 2], False)\n        self.assertEqual(v1, [2, 3])\n        self.assertEqual(v2, [1, 2])\n\n        v1, v2 = cross._get_stripped([1, 2], [1, 2, 3], False)\n        self.assertEqual(v1, [1, 2])\n        self.assertEqual(v2, [2, 3])\n\n    def test_compute_diff(self):\n        self.assertEqual(cross.compute_diff([1, 1, 1], [0, 1, 2]), [1, 0, -1])\n        self.assertEqual(cross.compute_diff([0, 1, 2], [1, 1, 1]), [-1, 0, 1])\n\n\nclass TestCase(common.TestCase):\n    def __buildSeqDS(self, values):\n        ret = dataseries.SequenceDataSeries()\n        for value in values:\n            ret.append(value)\n        return ret\n\n    def testCrossAboveOnce(self):\n        values1 = self.__buildSeqDS([1, 1, 1, 10, 1, 1, 1])\n        values2 = self.__buildSeqDS([2, 2, 2,  2, 2, 2, 2])\n\n        # Check every 2 values.\n        self.assertEqual(cross.cross_above(values1, values2, 0, 2), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 1, 3), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 2, 4), 1)\n        self.assertEqual(cross.cross_above(values1, values2, 3, 5), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 4, 6), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 5, 7), 0)\n\n        # Check every 3 values.\n        self.assertEqual(cross.cross_above(values1, values2, 0, 3), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 1, 4), 1)\n        self.assertEqual(cross.cross_above(values1, values2, 2, 5), 1)\n        self.assertEqual(cross.cross_above(values1, values2, 3, 6), 0)\n        self.assertEqual(cross.cross_above(values1, values2, 4, 7), 0)\n\n        # Check for all values.\n        self.assertEqual(cross.cross_above(values1, values2, 0, 7), 1)\n        self.assertEqual(cross.cross_above(values1, values2, 0, -1), 1)\n\n    def testCrossAboveMany(self):\n        count = 100\n        values1 = [-1 if i % 2 == 0 else 1 for i in range(count)]\n        values2 = [0 for i in range(count)]\n\n        # Check first value\n        self.assertEqual(cross.cross_above(values1, values2, 0, 0), 0)\n\n        # Check every 2 values.\n        period = 2\n        for i in range(1, count):\n            if i % 2 == 0:\n                self.assertEqual(cross.cross_above(values1, values2, i - period + 1, i + 1), 0)\n            else:\n                self.assertEqual(cross.cross_above(values1, values2, i - period + 1, i + 1), 1)\n\n        # Check every 4 values.\n        period = 4\n        for i in range(3, count):\n            if i % 2 == 0:\n                self.assertEqual(cross.cross_above(values1, values2, i - period + 1, i + 1), 1)\n            else:\n                self.assertEqual(cross.cross_above(values1, values2, i - period + 1, i + 1), 2)\n\n        # Check for all values.\n        self.assertEqual(cross.cross_above(values1, values2, 0, count), count / 2)\n\n    def testCrossBelowOnce(self):\n        values1 = self.__buildSeqDS([2, 2, 2,  2, 2, 2, 2])\n        values2 = self.__buildSeqDS([1, 1, 1, 10, 1, 1, 1])\n\n        # Check every 2 values.\n        self.assertEqual(cross.cross_below(values1, values2, 0, 2), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 1, 3), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 2, 4), 1)\n        self.assertEqual(cross.cross_below(values1, values2, 3, 5), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 4, 6), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 5, 7), 0)\n\n        # Check every 3 values.\n        self.assertEqual(cross.cross_below(values1, values2, 0, 3), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 1, 4), 1)\n        self.assertEqual(cross.cross_below(values1, values2, 2, 5), 1)\n        self.assertEqual(cross.cross_below(values1, values2, 3, 6), 0)\n        self.assertEqual(cross.cross_below(values1, values2, 4, 7), 0)\n\n        # Check for all values.\n        self.assertEqual(cross.cross_below(values1, values2, 0, 7), 1)\n        self.assertEqual(cross.cross_below(values1, values2, 0, -1), 1)\n\n    def testCrossBelowMany(self):\n        count = 100\n        values1 = [0 for i in range(count)]\n        values2 = [-1 if i % 2 == 0 else 1 for i in range(count)]\n\n        # Check first value\n        self.assertEqual(cross.cross_below(values1, values2, 0, 0), 0)\n\n        # Check every 2 values.\n        period = 2\n        for i in range(1, count):\n            if i % 2 == 0:\n                self.assertEqual(cross.cross_below(values1, values2, i - period + 1, i + 1), 0)\n            else:\n                self.assertEqual(cross.cross_below(values1, values2, i - period + 1, i + 1), 1)\n\n        # Check every 4 values.\n        period = 4\n        for i in range(3, count):\n            if i % 2 == 0:\n                self.assertEqual(cross.cross_below(values1, values2, i - period + 1, i + 1), 1)\n            else:\n                self.assertEqual(cross.cross_below(values1, values2, i - period + 1, i + 1), 2)\n\n        # Check for all values.\n        self.assertEqual(cross.cross_below(values1, values2, 0, count), count / 2)\n\n    def testCrossAboveWithSMA(self):\n        ds1 = dataseries.SequenceDataSeries()\n        ds2 = dataseries.SequenceDataSeries()\n        sma1 = ma.SMA(ds1, 15)\n        sma2 = ma.SMA(ds2, 25)\n        for i in range(100):\n            ds1.append(i)\n            ds2.append(50)\n            if i == 58:\n                self.assertEqual(cross.cross_above(sma1[:], sma2[:], -2, None), 1)\n            else:\n                self.assertEqual(cross.cross_above(sma1[:], sma2[:], -2, None), 0)\n\n    def testWithLists(self):\n        self.assertEqual(cross.cross_above([1, 2], [1, 1], -2), 0)\n        self.assertEqual(cross.cross_above([0, 1, 2], [1, 1, 1], -3), 1)\n        self.assertEqual(cross.cross_above([0, 0, 0, 1, 2], [1, 1, 1], -3), 1)\n        self.assertEqual(cross.cross_above([0, 0, 0, 1, 2], [1, 1], -3), 0)\n        self.assertEqual(cross.cross_above([0, 0, 0, 0, 2], [1, 1], -3), 1)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.technical import cumret\n\n\nclass CumRetTestCase(common.TestCase):\n    def testCumRet(self):\n        values = dataseries.SequenceDataSeries()\n        rets = cumret.CumulativeReturn(values)\n        for value in [1, 2, 3, 4, 4, 3, 1, 1.2]:\n            values.append(value)\n        self.assertEqual(rets[0], None)\n        self.assertEqual(rets[1], 1)\n        self.assertEqual(rets[2], 2)\n        self.assertEqual(rets[3], 3)\n        self.assertEqual(rets[4], 3)\n        self.assertEqual(rets[5], 2)\n        self.assertEqual(rets[6], 0)\n        self.assertEqual(round(rets[7], 1), 0.2)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.technical import highlow\n\n\nclass HighLowTestCase(common.TestCase):\n    def testHighLow(self):\n        values = dataseries.SequenceDataSeries()\n        high = highlow.High(values, 5)\n        low = highlow.Low(values, 3)\n        for value in [1, 2, 3, 4, 5]:\n            values.append(value)\n        self.assertEqual(high[-1], 5)\n        self.assertEqual(low[-1], 3)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport numpy as np\n\nimport common\n\nfrom pyalgotrade.technical import hurst\nfrom pyalgotrade import dataseries\n\n\ndef build_hurst(values, period, minLags, maxLags):\n    ds = dataseries.SequenceDataSeries()\n    ret = hurst.HurstExponent(ds, period, minLags, maxLags)\n    for value in values:\n        ds.append(value)\n    return ret\n\n\nclass TestCase(common.TestCase):\n    def testHurstExpFunRandomWalk(self):\n        values = np.cumsum(np.random.randn(50000)) + 1000\n        h = hurst.hurst_exp(np.log10(values), 2, 20)\n        self.assertEquals(round(h, 1), 0.5)\n\n    def testHurstExpFunTrending(self):\n        values = np.cumsum(np.random.randn(50000)+1) + 1000\n        h = hurst.hurst_exp(np.log10(values), 2, 20)\n        self.assertEquals(round(h), 1)\n\n    def testHurstExpFunMeanRev(self):\n        values = (np.random.randn(50000)) + 1000\n        h = hurst.hurst_exp(np.log10(values), 2, 20)\n        self.assertEquals(round(h), 0)\n\n    def testRandomWalk(self):\n        num_values = 10000\n        values = np.cumsum(np.random.randn(num_values)) + 1000\n        hds = build_hurst(values, num_values - 10, 2, 20)\n        self.assertEquals(round(hds[-1], 1), 0.5)\n        self.assertEquals(round(hds[-2], 1), 0.5)\n\n    def testTrending(self):\n        num_values = 10000\n        values = np.cumsum(np.random.randn(num_values) + 10) + 1000\n        hds = build_hurst(values, num_values - 10, 2, 20)\n        self.assertEquals(round(hds[-1], 1), 1)\n        self.assertEquals(round(hds[-2], 1), 1)\n\n    def testMeanRev(self):\n        num_values = 10000\n        values = np.random.randn(num_values) + 100\n        hds = build_hurst(values, num_values - 10, 2, 20)\n        self.assertEquals(round(hds[-1], 1), 0)\n        self.assertEquals(round(hds[-2], 1), 0)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import linebreak\nfrom pyalgotrade.barfeed import yahoofeed\n\n\nclass LineBreakTestCase(common.TestCase):\n    Instrument = \"orcl\"\n\n    def __getFeed(self):\n        # Load the feed and process all bars.\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(LineBreakTestCase.Instrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        return barFeed\n\n    def test2LineBreak(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[LineBreakTestCase.Instrument]\n        lineBreak = linebreak.LineBreak(bars, 2)\n        barFeed.loadAll()\n\n        self.assertEqual(len(lineBreak), 77)\n        self.assertEqual(bars[0].getLow(), lineBreak[0].getLow())\n        self.assertEqual(bars[0].getHigh(), lineBreak[0].getHigh())\n        self.assertEqual(bars[0].getClose() > bars[0].getOpen(), lineBreak[0].isWhite())\n        self.assertEqual(lineBreak[76].getLow(), 13.81)\n        self.assertEqual(lineBreak[76].getHigh(), 13.99)\n        self.assertEqual(lineBreak[76].isWhite(), False)\n        self.assertEqual(lineBreak[76].isBlack(), True)\n\n    def test3LineBreak(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[LineBreakTestCase.Instrument]\n        lineBreak = linebreak.LineBreak(bars, 3)\n        barFeed.loadAll()\n\n        self.assertEqual(len(lineBreak), 33)\n        self.assertEqual(bars[0].getLow(), lineBreak[0].getLow())\n        self.assertEqual(bars[0].getHigh(), lineBreak[0].getHigh())\n        self.assertEqual(bars[0].getClose() > bars[0].getOpen(), lineBreak[0].isWhite())\n        self.assertEqual(lineBreak[32].getLow(), 10.76)\n        self.assertEqual(lineBreak[32].getHigh(), 10.92)\n        self.assertEqual(lineBreak[32].isWhite(), False)\n        self.assertEqual(lineBreak[32].isBlack(), True)\n\n    def testLineBreakBounded(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[LineBreakTestCase.Instrument]\n\n        # Invalid maxLen, smaller than reversalLines.\n        with self.assertRaises(Exception):\n            lineBreak = linebreak.LineBreak(bars, 3, maxLen=2)\n\n        lineBreak = linebreak.LineBreak(bars, 3, maxLen=4)\n        # Invalid maxLen, smaller than reversalLines.\n        with self.assertRaises(Exception):\n            lineBreak.setMaxLen(2)\n        barFeed.loadAll()\n\n        self.assertEqual(len(lineBreak), 4)\n        self.assertEqual(len(lineBreak[:]), 4)\n        self.assertEqual(len(lineBreak.getDateTimes()), 4)\n        self.assertEqual(lineBreak[-1].getLow(), 10.76)\n        self.assertEqual(lineBreak[-1].getHigh(), 10.92)\n        self.assertEqual(lineBreak[-1].isWhite(), False)\n        self.assertEqual(lineBreak[-1].isBlack(), True)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade.technical import linreg\nfrom pyalgotrade import dataseries\n\n\nclass LeastSquaresRegressionTestCase(common.TestCase):\n    def testLsreg1(self):\n        x = [0, 1, 2]\n        y = [1, 2, 3]\n        a, b = linreg.lsreg(x, y)\n        self.assertEqual(round(a, 2), 1.0)\n        self.assertEqual(round(b, 2), 1.0)\n\n    def testLsreg2(self):\n        x = [0, 1, 2]\n        y = [4, 5, 6]\n        a, b = linreg.lsreg(x, y)\n        self.assertEqual(round(a, 2), 1.0)\n        self.assertEqual(round(b, 2), 4.0)\n\n    def testLsreg3(self):\n        x = [1, 2, 3]\n        y = [1, 2, 3]\n        a, b = linreg.lsreg(x, y)\n        self.assertEqual(round(a, 2), 1.0)\n        self.assertEqual(round(b, 2), 0)\n\n    def testStraightLine(self):\n        seqDS = dataseries.SequenceDataSeries()\n        lsReg = linreg.LeastSquaresRegression(seqDS, 3)\n\n        nextDateTime = datetime.datetime(2012, 1, 1)\n        seqDS.appendWithDateTime(nextDateTime, 1)\n        self.assertEqual(lsReg[-1], None)\n\n        nextDateTime = nextDateTime + datetime.timedelta(hours=1)\n        seqDS.appendWithDateTime(nextDateTime, 2)\n        self.assertEqual(lsReg[-1], None)\n\n        # Check current value.\n        nextDateTime = nextDateTime + datetime.timedelta(hours=1)\n        seqDS.appendWithDateTime(nextDateTime, 3)\n        self.assertEqual(round(lsReg[-1], 2), 3)\n\n        # Check future values.\n        futureDateTime = nextDateTime + datetime.timedelta(hours=1)\n        self.assertEqual(round(lsReg.getValueAt(futureDateTime), 2), 4)\n        futureDateTime = futureDateTime + datetime.timedelta(minutes=30)\n        self.assertEqual(round(lsReg.getValueAt(futureDateTime), 2), 4.5)\n        futureDateTime = futureDateTime + datetime.timedelta(minutes=30)\n        self.assertEqual(round(lsReg.getValueAt(futureDateTime), 2), 5)\n\n        # Move forward in sub-second increments.\n        nextDateTime = nextDateTime + datetime.timedelta(milliseconds=50)\n        seqDS.appendWithDateTime(nextDateTime, 4)\n        nextDateTime = nextDateTime + datetime.timedelta(milliseconds=50)\n        seqDS.appendWithDateTime(nextDateTime, 5)\n        self.assertEqual(round(lsReg[-1], 2), 5)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import ma\nfrom pyalgotrade import dataseries\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade import bar\n\n\ndef safe_round(number, ndigits):\n    ret = None\n    if number is not None:\n        ret = round(number, ndigits)\n    return ret\n\n\nclass SMATestCase(common.TestCase):\n    def __buildSMA(self, period, values, smaMaxLen=None):\n        seqDs = dataseries.SequenceDataSeries()\n        ret = ma.SMA(seqDs, period, smaMaxLen)\n        for value in values:\n            seqDs.append(value)\n        return ret\n\n    def testPeriod1(self):\n        sma = self.__buildSMA(1, [10, 20])\n\n        self.assertTrue(sma[0] == 10)\n        self.assertTrue(sma[1] == 20)\n        self.assertTrue(sma[-1] == 20)\n        self.assertTrue(sma[-2] == 10)\n        with self.assertRaises(IndexError):\n            sma[2]\n\n        with self.assertRaises(IndexError):\n            sma[-3]\n\n        self.assertEqual(len(sma.getDateTimes()), 2)\n        for i in range(len(sma)):\n            self.assertEqual(sma.getDateTimes()[i], None)\n\n    def testPeriod2(self):\n        sma = self.__buildSMA(2, [0, 1, 2])\n        self.assertEqual(sma[0], None)\n        self.assertEqual(sma[1], (0+1) / float(2))\n        self.assertEqual(sma[2], (1+2) / float(2))\n        with self.assertRaises(IndexError):\n            sma[3]\n\n        self.assertEqual(len(sma.getDateTimes()), 3)\n        for i in range(len(sma)):\n            self.assertEqual(sma.getDateTimes()[i], None)\n\n    def testPeriod2_BoundedFilter(self):\n        sma = self.__buildSMA(2, [0, 1, 2, 3, 4], 2)\n        self.assertEqual(sma[0], (2+3) / float(2))\n        self.assertEqual(sma[1], (3+4) / float(2))\n        self.assertEqual(sma[1], sma[-1])\n        self.assertEqual(len(sma.getDateTimes()), 2)\n\n    def testMultipleValues(self):\n        period = 5\n        values = range(1, 10)\n        sma = self.__buildSMA(period, values)\n        for i in xrange(period-1, len(values)):\n            expected = sum(values[i-(period-1):i+1]) / float(period)\n            self.assertTrue(sma[i] == expected)\n\n    def testMultipleValuesSkippingOne(self):\n        # Test SMA invalidating fast sma calculation.\n        period = 5\n        values = range(1, 10)\n        sma = self.__buildSMA(period, values)\n        for i in xrange(period-1, len(values), 2):\n            expected = sum(values[i-(period-1):i+1]) / float(period)\n            self.assertTrue(sma[i] == expected)\n\n    def testStockChartsSMA(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages\n        common.test_from_csv(self, \"sc-sma-10.csv\", lambda inputDS: ma.SMA(inputDS, 10))\n\n    def testNinjaTraderSMA(self):\n        common.test_from_csv(self, \"nt-sma-15.csv\", lambda inputDS: ma.SMA(inputDS, 15), 3)\n\n    def testSeqLikeOps(self):\n        # ds and seq should be the same.\n        seq = [1.0 for i in xrange(10)]\n        ds = self.__buildSMA(1, seq)\n\n        # Test length and every item.\n        self.assertEqual(len(ds), len(seq))\n        for i in xrange(len(seq)):\n            self.assertEqual(ds[i], seq[i])\n\n        # Test negative indices\n        self.assertEqual(ds[-1], seq[-1])\n        self.assertEqual(ds[-2], seq[-2])\n        self.assertEqual(ds[-9], seq[-9])\n\n        # Test slices\n        sl = slice(0, 1, 2)\n        self.assertEqual(ds[sl], seq[sl])\n        sl = slice(0, 9, 2)\n        self.assertEqual(ds[sl], seq[sl])\n        sl = slice(0, -1, 1)\n        self.assertEqual(ds[sl], seq[sl])\n\n        for i in xrange(-100, 100):\n            self.assertEqual(ds[i:], seq[i:])\n\n        for step in xrange(1, 10):\n            for i in xrange(-100, 100):\n                self.assertEqual(ds[i::step], seq[i::step])\n\n    def testEventWindow(self):\n        ds = dataseries.SequenceDataSeries()\n        smaEW = ma.SMAEventWindow(10)\n        sma = ma.SMA(ds, 10)\n        smaEW.onNewValue(None, None)  # This value should get skipped\n        for i in xrange(100):\n            ds.append(i)\n            smaEW.onNewValue(None, i)\n            self.assertEqual(sma[-1], smaEW.getValue())\n            smaEW.onNewValue(None, None)  # This value should get skipped\n\n    def testStockChartsSMA_BoundedSeq(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages\n        common.test_from_csv(self, \"sc-sma-10.csv\", lambda inputDS: ma.SMA(inputDS, 10), maxLen=1)\n        common.test_from_csv(self, \"sc-sma-10.csv\", lambda inputDS: ma.SMA(inputDS, 10), maxLen=2)\n        common.test_from_csv(self, \"sc-sma-10.csv\", lambda inputDS: ma.SMA(inputDS, 10), maxLen=4)\n        common.test_from_csv(self, \"sc-sma-10.csv\", lambda inputDS: ma.SMA(inputDS, 10), maxLen=1000)\n\n\nclass WMATestCase(common.TestCase):\n    def __buildWMA(self, weights, values, seqMaxLen=None, wmaMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries(maxLen=seqMaxLen)\n        ret = ma.WMA(seqDS, weights, wmaMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testPeriod1(self):\n        wma = self.__buildWMA([2], [10, 20])\n        self.assertTrue(wma[0] == 10)\n        self.assertTrue(wma[1] == 20)\n\n        self.assertEqual(len(wma.getDateTimes()), 2)\n        for i in range(len(wma)):\n            self.assertEqual(wma.getDateTimes()[i], None)\n\n    def __testPeriod2Impl(self, maxLen):\n        weights = [3, 2, 1]\n        values = [1, 2, 3]\n\n        wma = self.__buildWMA(weights, values, maxLen)\n        self.assertEqual(wma[0], None)\n        self.assertEqual(wma[1], None)\n        self.assertEqual(wma[2], (1*3 + 2*2 + 3*1) / float(3+2+1))\n\n        self.assertEqual(len(wma.getDateTimes()), 3)\n        for i in range(len(wma)):\n            self.assertEqual(wma.getDateTimes()[i], None)\n\n    def testPeriod2_BoundedSeq(self):\n        self.__testPeriod2Impl(1)\n        self.__testPeriod2Impl(2)\n        self.__testPeriod2Impl(100)\n\n    def testPeriod2_BoundedFilter(self):\n        weights = [3, 2, 1]\n        values = [1, 2, 3]\n\n        wma = self.__buildWMA(weights, values, wmaMaxLen=2)\n        self.assertEqual(wma[0], None)\n        self.assertEqual(wma[1], (1*3 + 2*2 + 3*1) / float(3+2+1))\n        self.assertEqual(len(wma), 2)\n        self.assertEqual(len(wma[:]), 2)\n        self.assertEqual(len(wma.getDateTimes()), 2)\n\n\nclass EMATestCase(common.TestCase):\n    def testStockChartsEMA(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages\n        common.test_from_csv(self, \"sc-ema-10.csv\", lambda inputDS: ma.EMA(inputDS, 10), 3)\n\n    def testMaxRecursion(self):\n        barFeed = ninjatraderfeed.Feed(bar.Frequency.MINUTE)\n        barFeed.addBarsFromCSV(\"any\", common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        ema = ma.EMA(barFeed[\"any\"].getPriceDataSeries(), 10)\n        # Load all the feed.\n        barFeed.loadAll()\n\n        # Check that the max recursion limit bug is not hit when generating the last value first.\n        self.assertEqual(round(ema[-1], 2), 128.81)\n\n    def testBoundedFilter(self):\n        values = [22.2734, 22.1940, 22.0847, 22.1741, 22.1840, 22.1344, 22.2337, 22.4323, 22.2436, 22.2933, 22.1542, 22.3926, 22.3816, 22.6109, 23.3558, 24.0519, 23.7530, 23.8324, 23.9516, 23.6338, 23.8225, 23.8722, 23.6537, 23.1870, 23.0976, 23.3260, 22.6805, 23.0976, 22.4025, 22.1725]\n\n        seqDS = dataseries.SequenceDataSeries()\n        ema = ma.EMA(seqDS, 10, 2)\n        for value in values:\n            seqDS.append(value)\n\n        self.assertEqual(round(ema[0], 5), 23.08068)\n        self.assertEqual(round(ema[1], 5), 22.91556)\n        self.assertEqual(len(ema), 2)\n        self.assertEqual(len(ema[:]), 2)\n        self.assertEqual(len(ema.getDateTimes()), 2)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import macd\nfrom pyalgotrade import dataseries\n\n\nclass MACDTestCase(common.TestCase):\n    def testMACD(self):\n        values = [16.39, 16.4999, 16.45, 16.43, 16.52, 16.51, 16.423, 16.41, 16.47, 16.45, 16.32, 16.36, 16.34, 16.59, 16.54, 16.52, 16.44, 16.47, 16.5, 16.45, 16.28, 16.07, 16.08, 16.1, 16.1, 16.09, 16.43, 16.4899, 16.59, 16.65, 16.78, 16.86, 16.86, 16.76]\n        # These expected values were generated using TA-Lib\n        macdValues = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.0067, 0.0106, 0.0028, -0.0342, -0.0937, -0.1214, -0.1276, -0.125, -0.1195, -0.0459, 0.0097, 0.0601, 0.0975, 0.139, 0.1713, 0.1816, 0.1598]\n        signalValues = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.0036, 0.0056, 0.0048, -0.0064, -0.0313, -0.057, -0.0772, -0.0909, -0.0991, -0.0839, -0.0571, -0.0236, 0.011, 0.0475, 0.0829, 0.1111, 0.125]\n        histogramValues = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.0031, 0.005, -0.002, -0.0279, -0.0624, -0.0643, -0.0504, -0.0342, -0.0205, 0.0379, 0.0668, 0.0838, 0.0865, 0.0914, 0.0884, 0.0705, 0.0348]\n        ds = dataseries.SequenceDataSeries()\n\n        macdDs = macd.MACD(ds, 5, 13, 6)\n        for i, value in enumerate(values):\n            ds.append(value)\n            self.assertEqual(common.safe_round(macdDs[i], 4), macdValues[i])\n            self.assertEqual(common.safe_round(macdDs.getSignal()[i], 4), signalValues[i])\n            self.assertEqual(common.safe_round(macdDs.getHistogram()[i], 4), histogramValues[i])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import ratio\nfrom pyalgotrade import dataseries\n\n\nclass TestCase(common.TestCase):\n    def __buildRatio(self, values, ratioMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries()\n        ret = ratio.Ratio(seqDS, ratioMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testSimple(self):\n        ratio = self.__buildRatio([1, 2, 1])\n        self.assertEqual(ratio[0], None)\n        self.assertEqual(ratio[1], 1)\n        self.assertEqual(ratio[2], -0.5)\n        self.assertEqual(ratio[-1], -0.5)\n        with self.assertRaises(IndexError):\n            ratio[3]\n\n        self.assertEqual(ratio[-2], ratio[1])\n        self.assertEqual(ratio[-1], ratio[2])\n\n        self.assertEqual(len(ratio.getDateTimes()), 3)\n        for i in range(len(ratio)):\n            self.assertEqual(ratio.getDateTimes()[i], None)\n\n    def testNegativeValues(self):\n        ratio = self.__buildRatio([-1, -2, -1])\n        self.assertEqual(ratio[0], None)\n        self.assertEqual(ratio[1], -1)\n        self.assertEqual(ratio[2], 0.5)\n        self.assertEqual(ratio[-1], 0.5)\n        with self.assertRaises(IndexError):\n            ratio[3]\n\n        self.assertEqual(ratio[-2], ratio[1])\n        self.assertEqual(ratio[-1], ratio[2])\n\n        self.assertEqual(len(ratio.getDateTimes()), 3)\n        for i in range(len(ratio)):\n            self.assertEqual(ratio.getDateTimes()[i], None)\n\n    def testBounded(self):\n        ratio = self.__buildRatio([-1, -2, -1], 2)\n        self.assertEqual(ratio[0], -1)\n        self.assertEqual(ratio[1], 0.5)\n        self.assertEqual(len(ratio), 2)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import roc\nfrom pyalgotrade import dataseries\n\n\nclass ROCTestCase(common.TestCase):\n    def __buildROC(self, values, period, rocMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries()\n        ret = roc.RateOfChange(seqDS, period, rocMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testPeriod12(self):\n        # http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:rate_of_change\n        inputValues = [11045.27, 11167.32, 11008.61, 11151.83, 10926.77, 10868.12, 10520.32, 10380.43, 10785.14, 10748.26, 10896.91, 10782.95, 10620.16, 10625.83, 10510.95, 10444.37, 10068.01, 10193.39, 10066.57, 10043.75]\n        roc_ = self.__buildROC(inputValues, 12)\n        outputValues = [-3.85, -4.85, -4.52, -6.34, -7.86, -6.21, -4.31, -3.24]\n        for i in range(len(outputValues)):\n            outputValue = roc_[12 + i] * 100\n            self.assertTrue(round(outputValue, 2) == outputValues[i])\n\n        self.assertEqual(len(roc_.getDateTimes()), len(inputValues))\n        for i in range(len(roc_)):\n            self.assertEqual(roc_.getDateTimes()[i], None)\n\n    def testPeriod1(self):\n        def simple_roc(value1, value2):\n            return self.__buildROC([value1, value2], 1)[1]\n\n        self.assertTrue(simple_roc(1, 2) == 1)\n        self.assertTrue(simple_roc(1, 2) == simple_roc(50, 100))\n        self.assertTrue(simple_roc(2, 1) == -0.5)\n        self.assertTrue(simple_roc(2, 1) == simple_roc(100, 50))\n\n    def testBounded(self):\n        # http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:rate_of_change\n        inputValues = [11045.27, 11167.32, 11008.61, 11151.83, 10926.77, 10868.12, 10520.32, 10380.43, 10785.14, 10748.26, 10896.91, 10782.95, 10620.16, 10625.83, 10510.95, 10444.37, 10068.01, 10193.39, 10066.57, 10043.75]\n        outputValues = [-4.31, -3.24]\n        roc_ = self.__buildROC(inputValues, 12, 2)\n        for i in xrange(2):\n            self.assertEqual(round(roc_[i], 4), round(outputValues[i] / 100, 4))\n\n    def testZeroes(self):\n        inputValues = [0, 0, 0]\n        outputValues = [None, 0, 0]\n        roc_ = self.__buildROC(inputValues, 1)\n        for i in xrange(len(inputValues)):\n            self.assertEqual(roc_[i], outputValues[i])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import rsi\nfrom pyalgotrade import dataseries\n\n\nclass TestCase(common.TestCase):\n    def testAvgGainLoss(self):\n        # We divide by 2 because N samples yield N-1 averages.\n\n        # Gain only\n        avgGain, avgLoss = rsi.avg_gain_loss([1, 2, 3], 0, 3)\n        self.assertTrue(avgGain == 2 / float(2))\n        self.assertTrue(avgLoss == 0)\n\n        # Loss only\n        avgGain, avgLoss = rsi.avg_gain_loss([3, 2, 1], 0, 3)\n        self.assertTrue(avgGain == 0)\n        self.assertTrue(avgLoss == 2 / float(2))\n\n        # Gain and Loss equal\n        avgGain, avgLoss = rsi.avg_gain_loss([1, 0, 1], 0, 3)\n        self.assertTrue(avgGain == 1 / float(2))\n        self.assertTrue(avgLoss == 1 / float(2))\n\n        # Gain and Loss different\n        avgGain, avgLoss = rsi.avg_gain_loss([1, 3, 2], 0, 3)\n        self.assertTrue(avgGain == 2 / float(2))\n        self.assertTrue(avgLoss == 1 / float(2))\n\n        # Error\n        self.assertEqual(rsi.avg_gain_loss([1, 1.5, 2], 0, 1), None)\n        self.assertEqual(rsi.avg_gain_loss([1, 1.5, 2], 1, 2), None)\n        with self.assertRaises(IndexError):\n            rsi.avg_gain_loss([1, 1.5, 2], 2, 4)\n\n    def __buildRSI(self, values, period, rsiMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries()\n        ret = rsi.RSI(seqDS, period, rsiMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testStockChartsRSI(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_in\n        common.test_from_csv(self, \"rsi-test.csv\", lambda inputDS: rsi.RSI(inputDS, 14), 3)\n\n    def testDateTimes(self):\n        rsi = self.__buildRSI(range(10), 3)\n\n        self.assertEqual(len(rsi.getDateTimes()), 10)\n        for i in range(len(rsi)):\n            self.assertEqual(rsi.getDateTimes()[i], None)\n\n    def testRSIFunc(self):\n        values = [44.3389, 44.0902, 44.1497, 43.6124, 44.3278, 44.8264, 45.0955, 45.4245, 45.8433, 46.0826, 45.8931, 46.0328, 45.6140, 46.2820, 46.2820]\n        self.assertEqual(round(rsi.rsi(values, 14), 8), 70.53278948)\n        values = [44.3389, 44.0902, 44.1497, 43.6124, 44.3278, 44.8264, 45.0955, 45.4245, 45.8433, 46.0826, 45.8931, 46.0328, 45.6140, 46.2820, 46.2820, 46.0028, 46.0328, 46.4116, 46.2222, 45.6439, 46.2122, 46.2521, 45.7137, 46.4515, 45.7835, 45.3548, 44.0288, 44.1783, 44.2181, 44.5672, 43.4205, 42.6628, 43.1314]\n        self.assertEqual(round(rsi.rsi(values, 14), 8), 37.77295211)\n\n    def testRSI_Bounded(self):\n        values = [44.3389, 44.0902, 44.1497, 43.6124, 44.3278, 44.8264, 45.0955, 45.4245, 45.8433, 46.0826, 45.8931, 46.0328, 45.6140, 46.2820, 46.2820]\n        rsi = self.__buildRSI(values, 14, 1)\n        self.assertEqual(round(rsi[0], 8), 70.53278948)\n        self.assertEqual(len(rsi), 1)\n        self.assertEqual(len(rsi[:]), 1)\n        self.assertEqual(len(rsi.getDateTimes()), 1)\n        self.assertEqual(round(rsi[-1], 8), 70.53278948)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport numpy\n\nimport common\n\nfrom pyalgotrade.technical import stats\nfrom pyalgotrade import dataseries\n\n\nclass TestCase(common.TestCase):\n    def testStdDev_1(self):\n        values = [1, 1, 2, 3, 5]\n        seqDS = dataseries.SequenceDataSeries()\n        stdDev = stats.StdDev(seqDS, 1)\n        for value in values:\n            seqDS.append(value)\n        for i in stdDev:\n            self.assertEqual(i, 0)\n\n    def testStdDev(self):\n        values = [1, 1, 2, 3, 5]\n        seqDS = dataseries.SequenceDataSeries()\n        stdDev = stats.StdDev(seqDS, 2)\n        for value in values:\n            seqDS.append(value)\n\n        self.assertEqual(stdDev[0], None)\n        self.assertEqual(stdDev[1], numpy.array([1, 1]).std())\n        self.assertEqual(stdDev[2], numpy.array([1, 2]).std())\n        self.assertEqual(stdDev[3], numpy.array([2, 3]).std())\n        self.assertEqual(stdDev[4], numpy.array([3, 5]).std())\n\n    def testStdDev_Bounded(self):\n        values = [1, 1, 2, 3, 5]\n        seqDS = dataseries.SequenceDataSeries()\n        stdDev = stats.StdDev(seqDS, 2, maxLen=2)\n        for value in values:\n            seqDS.append(value)\n\n        self.assertEqual(stdDev[0], numpy.array([2, 3]).std())\n        self.assertEqual(stdDev[1], numpy.array([3, 5]).std())\n\n    def testZScore(self):\n        values = [1.10, 2.20, 4.00, 5.10, 6.00, 7.10, 8.20, 9.00, 10.10, 3.00, 4.10, 5.20, 7.00, 8.10, 9.20, 16.00, 17.10, 18.20, 19.30, 20.40]\n        expected = [None, None, None, None, 1.283041407, 1.317884611, 1.440611043, 1.355748299, 1.4123457, -1.831763202, -0.990484842, -0.388358578, 0.449889908, 1.408195169, 1.332948099, 1.867732104, 1.334258333, 1.063608066, 0.939656572, 1.414213562]\n        seqDS = dataseries.SequenceDataSeries()\n        zscore = stats.ZScore(seqDS, 5)\n        i = 0\n        for value in values:\n            seqDS.append(value)\n            if i >= 4:\n                self.assertEqual(round(zscore[-1], 4), round(expected[i], 4))\n            i += 1\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade.technical import stoch\nfrom pyalgotrade.dataseries import bards\nfrom pyalgotrade import bar\n\n\ndef values_equal(v1, v2):\n    if v1 is not None and v2 is not None:\n        return round(v1, 3) == round(v2, 3)\n    elif v1 is None and v2 is None:\n        return True\n    return False\n\n\nclass TestCase(common.TestCase):\n    def setUp(self):\n        self.__currSeconds = 0\n\n    def __buildBar(self, openPrice, highPrice, lowPrice, closePrice):\n        dateTime = datetime.datetime.now() + datetime.timedelta(seconds=self.__currSeconds)\n        self.__currSeconds += 1\n        return bar.BasicBar(dateTime, openPrice, highPrice, lowPrice, closePrice, closePrice*10, closePrice, bar.Frequency.DAY)\n\n    def __fillBarDataSeries(self, barDS, closePrices, highPrices, lowPrices):\n        assert(len(closePrices) == len(highPrices) == len(lowPrices))\n        for i in range(len(highPrices)):\n            barDS.append(self.__buildBar(closePrices[i], highPrices[i], lowPrices[i], closePrices[i]))\n\n    def testShortPeriod(self):\n        highPrices = [3, 3, 3]\n        lowPrices = [1, 1, 1]\n        closePrices = [2, 2, 3]\n\n        barDS = bards.BarDataSeries()\n        stochFilter = stoch.StochasticOscillator(barDS, 2, 2)\n        self.__fillBarDataSeries(barDS, closePrices, highPrices, lowPrices)\n\n        self.assertTrue(values_equal(stochFilter[0], None))\n        self.assertTrue(values_equal(stochFilter[1], 50))\n        self.assertTrue(values_equal(stochFilter[2], 100))\n\n        self.assertTrue(values_equal(stochFilter.getD()[0], None))\n        self.assertTrue(values_equal(stochFilter.getD()[1], None))\n        self.assertTrue(values_equal(stochFilter.getD()[2], 75))\n\n        self.assertEqual(len(stochFilter.getDateTimes()), len(closePrices))\n        for i in range(len(stochFilter)):\n            self.assertNotEqual(stochFilter.getDateTimes()[i], None)\n\n    def testStockChartsStoch(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:stochastic_oscillato\n        highPrices = [127.0090, 127.6159, 126.5911, 127.3472, 128.1730, 128.4317, 127.3671, 126.4220, 126.8995, 126.8498, 125.6460, 125.7156, 127.1582, 127.7154, 127.6855, 128.2228, 128.2725, 128.0934, 128.2725, 127.7353, 128.7700, 129.2873, 130.0633, 129.1182, 129.2873, 128.4715, 128.0934, 128.6506, 129.1381, 128.6406]\n        lowPrices = [125.3574, 126.1633, 124.9296, 126.0937, 126.8199, 126.4817, 126.0340, 124.8301, 126.3921, 125.7156, 124.5615, 124.5715, 125.0689, 126.8597, 126.6309, 126.8001, 126.7105, 126.8001, 126.1335, 125.9245, 126.9891, 127.8148, 128.4715, 128.0641, 127.6059, 127.5960, 126.9990, 126.8995, 127.4865, 127.3970]\n        closePrices = lowPrices[:13]  # To keep initial close prince between low/high\n        closePrices.extend([127.2876, 127.1781, 128.0138, 127.1085, 127.7253, 127.0587, 127.3273, 128.7103, 127.8745, 128.5809, 128.6008, 127.9342, 128.1133, 127.5960, 127.5960, 128.6904, 128.2725])\n        kValues = [None, None, None, None, None, None, None, None, None, None, None, None, None, 70.4382, 67.6089, 89.2021, 65.8106, 81.7477, 64.5238, 74.5298, 98.5814, 70.1045, 73.0561, 73.4178, 61.2313, 60.9563, 40.3861, 40.3861, 66.8285, 56.7314]\n        dValues = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 75.7497, 74.2072, 78.9201, 70.6940, 73.6004, 79.2117, 81.0719, 80.5807, 72.1928, 69.2351, 65.2018, 54.1912, 47.2428, 49.2003, 54.6487]\n\n        barDS = bards.BarDataSeries()\n        stochFilter = stoch.StochasticOscillator(barDS, 14)\n        self.__fillBarDataSeries(barDS, closePrices, highPrices, lowPrices)\n\n        for i in range(len(kValues)):\n            self.assertTrue(values_equal(stochFilter[i], kValues[i]))\n            self.assertTrue(values_equal(stochFilter.getD()[i], dValues[i]))\n\n        self.assertEqual(len(stochFilter.getDateTimes()), len(closePrices))\n        for i in range(len(stochFilter)):\n            self.assertNotEqual(stochFilter.getDateTimes()[i], None)\n\n    def testStockChartsStoch_Bounded(self):\n        # Test data from http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:stochastic_oscillato\n        highPrices = [127.0090, 127.6159, 126.5911, 127.3472, 128.1730, 128.4317, 127.3671, 126.4220, 126.8995, 126.8498, 125.6460, 125.7156, 127.1582, 127.7154, 127.6855, 128.2228, 128.2725, 128.0934, 128.2725, 127.7353, 128.7700, 129.2873, 130.0633, 129.1182, 129.2873, 128.4715, 128.0934, 128.6506, 129.1381, 128.6406]\n        lowPrices = [125.3574, 126.1633, 124.9296, 126.0937, 126.8199, 126.4817, 126.0340, 124.8301, 126.3921, 125.7156, 124.5615, 124.5715, 125.0689, 126.8597, 126.6309, 126.8001, 126.7105, 126.8001, 126.1335, 125.9245, 126.9891, 127.8148, 128.4715, 128.0641, 127.6059, 127.5960, 126.9990, 126.8995, 127.4865, 127.3970]\n        closePrices = lowPrices[:13]  # To keep initial close prince between low/high\n        closePrices.extend([127.2876, 127.1781, 128.0138, 127.1085, 127.7253, 127.0587, 127.3273, 128.7103, 127.8745, 128.5809, 128.6008, 127.9342, 128.1133, 127.5960, 127.5960, 128.6904, 128.2725])\n        kValues = [40.3861, 66.8285, 56.7314]\n        dValues = [47.2428, 49.2003, 54.6487]\n\n        barDS = bards.BarDataSeries()\n        stochFilter = stoch.StochasticOscillator(barDS, 14, maxLen=3)\n        self.__fillBarDataSeries(barDS, closePrices, highPrices, lowPrices)\n\n        self.assertEqual(len(stochFilter), 3)\n        self.assertEqual(len(stochFilter[:]), 3)\n        self.assertEqual(len(stochFilter.getDateTimes()), 3)\n        self.assertEqual(len(stochFilter.getD()), 3)\n        self.assertEqual(len(stochFilter.getD().getDateTimes()), 3)\n\n        for i in xrange(3):\n            self.assertEqual(round(stochFilter[i], 4), kValues[i])\n            self.assertEqual(round(stochFilter.getD()[i], 4), dValues[i])\n\n    def testZeroDivision(self):\n        highPrices = [1, 1, 1]\n        lowPrices = [1, 1, 1]\n        closePrices = [1, 1, 1]\n\n        barDS = bards.BarDataSeries()\n        stochFilter = stoch.StochasticOscillator(barDS, 2, 2)\n        self.__fillBarDataSeries(barDS, closePrices, highPrices, lowPrices)\n        self.assertEqual(stochFilter[-1], 0)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade import technical\nfrom pyalgotrade import dataseries\n\n\nclass TestEventWindow(technical.EventWindow):\n    def __init__(self):\n        technical.EventWindow.__init__(self, 1, skipNone=False, dtype=object)\n\n    def getValue(self):\n        return self.getValues()[-1]\n\n\nclass TestFilter(technical.EventBasedFilter):\n    def __init__(self, dataSeries):\n        technical.EventBasedFilter.__init__(self, dataSeries, TestEventWindow())\n\n\nclass DataSeriesFilterTest(common.TestCase):\n    def testInvalidPosNotCached(self):\n        ds = dataseries.SequenceDataSeries()\n        testFilter = TestFilter(ds)\n        for i in range(10):\n            ds.append(i)\n            ds.append(None)  # Interleave Nones.\n\n        self.assertEqual(testFilter[-1], None)\n        self.assertEqual(testFilter[-2], 9)\n        self.assertEqual(testFilter[-4], 8)  # We go 3 instead of 2 because we need to skip the interleaved None values.\n\n        self.assertEqual(testFilter[18], 9)\n        self.assertEqual(testFilter[19], None)\n        # Absolut pos 20 should have the next value once we insert it, but right now it should be invalid.\n        with self.assertRaises(IndexError):\n            testFilter[20]\n        ds.append(10)\n        self.assertEqual(testFilter[20], 10)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import linreg\nfrom pyalgotrade import dataseries\n\n\nclass SlopeTest(common.TestCase):\n    def __buildSlope(self, values, period, slopeMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries()\n        ret = linreg.Slope(seqDS, period, slopeMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testSlope(self):\n        slope = self.__buildSlope([1, 2, 3, 2, 1], 3)\n        self.assertEqual(slope[0], None)\n        self.assertEqual(slope[1], None)\n        self.assertEqual(round(slope[2], 2), 1.0)\n        self.assertEqual(slope[3], 0.0)\n        self.assertEqual(slope[4], -1.0)\n\n    def testSlopeBounded(self):\n        slope = self.__buildSlope([1, 2, 3, 2, 1], 3, 2)\n        self.assertEqual(slope[0], 0.0)\n        self.assertEqual(slope[1], -1.0)\n\n\nclass TrendTest(common.TestCase):\n    def __buildTrend(self, values, trendDays, positiveThreshold, negativeThreshold, trendMaxLen=None):\n        seqDS = dataseries.SequenceDataSeries()\n        ret = linreg.Trend(seqDS, trendDays, positiveThreshold, negativeThreshold, trendMaxLen)\n        for value in values:\n            seqDS.append(value)\n        return ret\n\n    def testTrend(self):\n        trend = self.__buildTrend([1, 2, 3, 2, 1], 3, 0, 0)\n        self.assertEqual(trend[0], None)\n        self.assertEqual(trend[1], None)\n        self.assertEqual(trend[2], True)\n        self.assertEqual(trend[3], None)\n        self.assertEqual(trend[4], False)\n\n        self.assertEqual(len(trend.getDateTimes()), 5)\n        for i in range(len(trend)):\n            self.assertEqual(trend.getDateTimes()[i], None)\n\n    def testTrendWithCustomThresholds(self):\n        trend = self.__buildTrend([1, 2, 3, 5, -10], 3, 1, -1)\n        self.assertEqual(trend[0], None)\n        self.assertEqual(trend[1], None)\n        self.assertEqual(trend[2], None)\n        self.assertEqual(trend[3], True)\n        self.assertEqual(trend[4], False)\n\n        self.assertEqual(len(trend.getDateTimes()), 5)\n        for i in range(len(trend)):\n            self.assertEqual(trend.getDateTimes()[i], None)\n\n    def testTrendWithCustomThresholds_Bounded(self):\n        trend = self.__buildTrend([1, 2, 3, 5, -10], 3, 1, -1, 2)\n        self.assertEqual(trend[0], True)\n        self.assertEqual(trend[1], False)\n        self.assertEqual(len(trend), 2)\n\n    def testInvalidThreshold(self):\n        seqDS = dataseries.SequenceDataSeries()\n        with self.assertRaisesRegexp(Exception, \"Invalid thresholds\"):\n            linreg.Trend(seqDS, 10, 0.2, 0.5, 5)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport common\n\nfrom pyalgotrade.technical import vwap\nfrom pyalgotrade.barfeed import yahoofeed\n\n\nclass VWAPTestCase(common.TestCase):\n    Instrument = \"orcl\"\n\n    def __getFeed(self):\n        # Load the feed and process all bars.\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(VWAPTestCase.Instrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        return barFeed\n\n    def testPeriod1_ClosingPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 1)\n        barFeed.loadAll()\n        for i in xrange(len(bars)):\n            self.assertEqual(round(bars[i].getClose(), 5), round(vwap_[i], 5))\n\n    def testPeriod1_TypicalPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 1, True)\n        barFeed.loadAll()\n        for i in xrange(len(bars)):\n            self.assertEqual(round(bars[i].getTypicalPrice(), 5), round(vwap_[i], 5))\n\n    def testPeriod2_ClosingPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 2)\n        barFeed.loadAll()\n        self.assertEqual(vwap_[0], None)\n        for i in xrange(1, len(vwap_)):\n            self.assertNotEqual(vwap_[i], None)\n\n    def testPeriod2_TypicalPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 2, True)\n        barFeed.loadAll()\n        self.assertEqual(vwap_[0], None)\n        for i in xrange(1, len(vwap_)):\n            self.assertNotEqual(vwap_[i], None)\n\n    def testPeriod50_ClosingPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 50)\n        barFeed.loadAll()\n        for i in xrange(49):\n            self.assertEqual(vwap_[i], None)\n        for i in xrange(49, len(vwap_)):\n            self.assertNotEqual(vwap_[i], None)\n\n    def testPeriod50_TypicalPrice(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 50, True)\n        barFeed.loadAll()\n        for i in xrange(49):\n            self.assertEqual(vwap_[i], None)\n        for i in xrange(49, len(vwap_)):\n            self.assertNotEqual(vwap_[i], None)\n\n    def testBounded(self):\n        barFeed = self.__getFeed()\n        bars = barFeed[VWAPTestCase.Instrument]\n        vwap_ = vwap.VWAP(bars, 50, True, 2)\n        barFeed.loadAll()\n\n        outputValues = [14.605005665747331, 14.605416923506045]\n        for i in xrange(2):\n            self.assertEqual(round(vwap_[i], 4), round(outputValues[i], 4))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\nimport math\nfrom distutils import version\nimport pytz\nimport numpy\n\nimport common\nimport strategy_test\nimport position_test\n\nfrom pyalgotrade.barfeed import ninjatraderfeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade.stratanalyzer import trades\nfrom pyalgotrade import broker\nfrom pyalgotrade.broker import backtesting\n\n\ndef buildUTCDateTime(year, month, day, hour, minute):\n    ret = datetime.datetime(year, month, day, hour, minute)\n    ret = pytz.utc.localize(ret)\n    return ret\n\n\nclass TradesAnalyzerTestCase(common.TestCase):\n    TestInstrument = \"spy\"\n\n    def __loadBarFeed(self):\n        ret = ninjatraderfeed.Feed(ninjatraderfeed.Frequency.MINUTE)\n        barFilter = csvfeed.USEquitiesRTH()\n        ret.setBarFilter(barFilter)\n        ret.addBarsFromCSV(TradesAnalyzerTestCase.TestInstrument, common.get_data_file_path(\"nt-spy-minute-2011.csv\"))\n        return ret\n\n    def __createStrategy(self):\n        barFeed = self.__loadBarFeed()\n        return strategy_test.TestStrategy(barFeed, 1000)\n\n    def __createPositionStrategy(self):\n        barFeed = self.__loadBarFeed()\n        return position_test.TestStrategy(barFeed, TradesAnalyzerTestCase.TestInstrument, 1000)\n\n    def testNoTrades(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        strat.run()\n\n        self.assertTrue(strat.getBroker().getCash() == 1000)\n\n        self.assertTrue(stratAnalyzer.getCount() == 0)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 0)\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 0)\n\n    def testSomeTrades_Position(self):\n        strat = self.__createPositionStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Winning trade\n        strat.addPosEntry(buildUTCDateTime(2011, 1, 3, 15, 0), strat.enterLong, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        strat.addPosExitMarket(buildUTCDateTime(2011, 1, 3, 15, 16))  # 127.16\n        # Losing trade\n        strat.addPosEntry(buildUTCDateTime(2011, 1, 3, 15, 30), strat.enterLong, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.addPosExitMarket(buildUTCDateTime(2011, 1, 3, 15, 31))  # 127.16\n        # Winning trade\n        strat.addPosEntry(buildUTCDateTime(2011, 1, 3, 15, 38), strat.enterLong, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        strat.addPosExitMarket(buildUTCDateTime(2011, 1, 3, 15, 42))  # 127.26\n        # Unfinished trade not closed\n        strat.addPosEntry(buildUTCDateTime(2011, 1, 3, 15, 47), strat.enterLong, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.34\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.16 - 127.14) + (127.16 - 127.2) + (127.26 - 127.16) - 127.34, 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 3)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.03)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 2) == 0.07)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=0), 2) == 0.06)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 2)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.06)\n        self.assertTrue(round(stratAnalyzer.getProfits().std(ddof=1), 2) == 0.06)\n        self.assertTrue(round(stratAnalyzer.getProfits().std(ddof=0), 2) == 0.04)\n        self.assertEqual(stratAnalyzer.getPositiveReturns()[0], (127.16 - 127.14) / 127.14)\n        self.assertEqual(stratAnalyzer.getPositiveReturns()[1], (127.26 - 127.16) / 127.16)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.04)\n        if version.LooseVersion(numpy.__version__) >= version.LooseVersion(\"1.6.2\"):\n            self.assertTrue(math.isnan(stratAnalyzer.getLosses().std(ddof=1)))\n        else:\n            self.assertTrue(stratAnalyzer.getLosses().std(ddof=1) == 0)\n        self.assertTrue(stratAnalyzer.getLosses().std(ddof=0) == 0)\n        self.assertEqual(stratAnalyzer.getNegativeReturns()[0], (127.16 - 127.2) / 127.2)\n\n    def testSomeTrades(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Winning trade\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Losing trade\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 31), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Winning trade\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 38), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 42), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.26\n        # Open trade.\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 47), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.34\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.16 - 127.14) + (127.16 - 127.2) + (127.26 - 127.16) - 127.34, 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 3)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.03)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 2) == 0.07)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=0), 2) == 0.06)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 2)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.06)\n        self.assertTrue(round(stratAnalyzer.getProfits().std(ddof=1), 2) == 0.06)\n        self.assertTrue(round(stratAnalyzer.getProfits().std(ddof=0), 2) == 0.04)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.04)\n        if version.LooseVersion(numpy.__version__) >= version.LooseVersion(\"1.6.2\"):\n            self.assertTrue(math.isnan(stratAnalyzer.getLosses().std(ddof=1)))\n        else:\n            self.assertTrue(stratAnalyzer.getLosses().std(ddof=1) == 0)\n        self.assertTrue(stratAnalyzer.getLosses().std(ddof=0) == 0)\n\n    def testSomeTradesWithCommissions(self):\n        strat = self.__createStrategy()\n        strat.getBroker().setCommission(backtesting.FixedPerTrade(0.01))\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Losing trade\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 31), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Winning trade\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 38), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 42), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.26\n        # Open trade.\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 47), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.34\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.16 - 127.2) + (127.26 - 127.16) - 127.34 - 0.01*5, 2))\n        self.assertTrue(numpy.array_equal(stratAnalyzer.getCommissionsForAllTrades(), numpy.array([0.02, 0.02])))\n        self.assertTrue(numpy.array_equal(stratAnalyzer.getCommissionsForProfitableTrades(), numpy.array([0.02])))\n        self.assertTrue(numpy.array_equal(stratAnalyzer.getCommissionsForUnprofitableTrades(), numpy.array([0.02])))\n        self.assertTrue(numpy.array_equal(stratAnalyzer.getCommissionsForEvenTrades(), numpy.array([])))\n\n    def testProportionalCommissionBug(self):\n        # Regression test for a bug reported by 'Jackson Sam' on 30/Aug/2013.\n        strat = self.__createStrategy()\n        strat.getBroker().setCommission(backtesting.FixedPerTrade(0.01))\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # There are 3 trades here:\n        # Trade 1 (Long)\n        #   Buy 1 @ 127.16 Commission: 0.01\n        #   Sell 1 @ 127.26 Commission: 0.005\n        # Trade 2 (Short)\n        #   Sell 1 @ 127.26 Commission: 0.005\n        #   Buy 1 @ 127.37 Commission: 0.005\n        # Trade 3 (Long)\n        #   Buy 1 @ 127.37 Commission: 0.005\n        #   Sell 1 @ 127.4 Commission: 0.01\n\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 38), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # Fill at 127.16\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 42), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 2)  # Fill at 127.26\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 53), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 2)  # Fill at 127.37\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 58), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # Fill at 127.4\n\n        strat.run()\n        allReturns = stratAnalyzer.getAllReturns()\n        self.assertEqual(round(allReturns[0], 6), 0.000668)\n        self.assertEqual(round(allReturns[1], 6), -0.000943)\n        self.assertEqual(round(allReturns[2], 6), 0.000118)\n\n    def testLongShort(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Exit long and enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.16\n        # Exit short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.16 - 127.14) + (127.16 - 127.2), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 2)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == -0.01)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 4) == 0.0424)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.02)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.04)\n\n    def testLongShort2(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Exit long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.16 - 127.14) + (127.16 - 127.2), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 2)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == -0.01)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 4) == 0.0424)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.02)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.04)\n\n    def testShortLong(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Exit short and enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.16\n        # Exit long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.14 - 127.16) + (127.2 - 127.16), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 2)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.01)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 4) == 0.0424)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.04)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.02)\n\n    def testShortLong2(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Exit short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.14 - 127.16) + (127.2 - 127.16), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 2)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.01)\n        self.assertTrue(round(stratAnalyzer.getAll().std(ddof=1), 4) == 0.0424)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.04)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.02)\n\n    def testLong2(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Extend long position\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.2 - 127.14) + (127.2 - 127.16), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 1)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.1)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.1)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 0)\n\n    def testLong3(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.BUY, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.14\n        # Decrease long position\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit long\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.SELL, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.2 - 127.14) + (127.16 - 127.14), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 1)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == 0.08)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getProfits().mean(), 2) == 0.08)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 0)\n\n    def testShort2(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.14\n        # Extend short position\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.14 - 127.2) + (127.16 - 127.2), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 1)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertEqual(round(stratAnalyzer.getAll().mean(), 2), -0.1)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.1)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 0)\n\n    def testShort3(self):\n        strat = self.__createStrategy()\n        stratAnalyzer = trades.Trades()\n        strat.attachAnalyzer(stratAnalyzer)\n\n        # Enter short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 0), strat.getBroker().createMarketOrder, broker.Order.Action.SELL_SHORT, TradesAnalyzerTestCase.TestInstrument, 2)  # 127.14\n        # Decrease short position\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 16), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.16\n        # Exit short\n        strat.addOrder(buildUTCDateTime(2011, 1, 3, 15, 30), strat.getBroker().createMarketOrder, broker.Order.Action.BUY_TO_COVER, TradesAnalyzerTestCase.TestInstrument, 1)  # 127.2\n        strat.run()\n\n        self.assertTrue(round(strat.getBroker().getCash(), 2) == round(1000 + (127.14 - 127.16) + (127.14 - 127.2), 2))\n\n        self.assertTrue(stratAnalyzer.getCount() == 1)\n        self.assertTrue(stratAnalyzer.getEvenCount() == 0)\n\n        self.assertTrue(round(stratAnalyzer.getAll().mean(), 2) == -0.08)\n\n        self.assertTrue(stratAnalyzer.getUnprofitableCount() == 1)\n        self.assertTrue(round(stratAnalyzer.getLosses().mean(), 2) == -0.08)\n\n        self.assertTrue(stratAnalyzer.getProfitableCount() == 0)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\nimport datetime\n\nimport common\ntry:\n    # This will get environment variables set.\n    import credentials\nexcept:\n    pass\n\nfrom pyalgotrade import dispatcher\nfrom pyalgotrade.twitter import feed as twitterfeed\n\n\nclass TwitterFeedTestCase(common.TestCase):\n    def testTwitterFeed(self):\n        events = {\n            \"on_tweet\": False,\n            \"start\": datetime.datetime.now()\n        }\n        disp = dispatcher.Dispatcher()\n\n        def on_tweet(data):\n            events[\"on_tweet\"] = True\n            disp.stop()\n\n        def on_idle():\n            # Stop after 5 minutes.\n            if (datetime.datetime.now() - events[\"start\"]).seconds > 60*5:\n                disp.stop()\n\n        # Create a twitter feed to track BitCoin related events.\n        consumer_key = os.getenv(\"TWITTER_CONSUMER_KEY\")\n        consumer_secret = os.getenv(\"TWITTER_CONSUMER_SECRET\")\n        access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n        access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n        track = [\"bitcoin\", \"btc\"]\n        follow = []\n        languages = [\"en\"]\n        twitterFeed = twitterfeed.TwitterFeed(\n            consumer_key,\n            consumer_secret,\n            access_token,\n            access_token_secret,\n            track,\n            follow,\n            languages\n        )\n\n        disp.addSubject(twitterFeed)\n        twitterFeed.subscribe(on_tweet)\n        disp.getIdleEvent().subscribe(on_idle)\n        disp.run()\n\n        # Check that we received both events.\n        self.assertTrue(events[\"on_tweet\"])\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\n\nfrom pyalgotrade import utils\nfrom pyalgotrade.utils import collections\nfrom pyalgotrade.utils import dt\n\n\nclass UtilsTestCase(common.TestCase):\n    def testChangePercentage(self):\n        self.assertEqual(utils.get_change_percentage(1, 1), 0)\n        self.assertEqual(round(utils.get_change_percentage(1.1, 1), 2), 0.1)\n        self.assertEqual(round(utils.get_change_percentage(2, 1), 2), 1)\n        self.assertEqual(utils.get_change_percentage(1, 2), -0.5)\n        self.assertEqual(utils.get_change_percentage(0, -1), 1)\n        self.assertEqual(utils.get_change_percentage(1, -1), 2)\n        self.assertEqual(utils.get_change_percentage(-2, -1), -1)\n        self.assertEqual(utils.get_change_percentage(-1.5, -1), -0.5)\n        with self.assertRaisesRegexp(Exception, \"Invalid values\"):\n            utils.get_change_percentage(-1.5, 0)\n\n    def testSafeMin(self):\n        self.assertEqual(utils.safe_min(None, 0), 0)\n        self.assertEqual(utils.safe_min(0, None), 0)\n        self.assertEqual(utils.safe_min(None, None), None)\n        self.assertEqual(utils.safe_min(0, 0), 0)\n        self.assertEqual(utils.safe_min(1, 0), 0)\n        self.assertEqual(utils.safe_min(0, 1), 0)\n        self.assertEqual(utils.safe_min(-1, 1), -1)\n        self.assertEqual(utils.safe_min(1, -1), -1)\n        self.assertEqual(utils.safe_min(-1, -2), -2)\n        self.assertEqual(utils.safe_min(-2, -1), -2)\n\n    def testSafeMax(self):\n        self.assertEqual(utils.safe_max(None, 0), 0)\n        self.assertEqual(utils.safe_max(None, 1), 1)\n        self.assertEqual(utils.safe_max(2, None), 2)\n        self.assertEqual(utils.safe_max(None, None), None)\n        self.assertEqual(utils.safe_max(1, 100), 100)\n        self.assertEqual(utils.safe_max(-1, 1), 1)\n        self.assertEqual(utils.safe_max(-1, 1.1), 1.1)\n        self.assertEqual(utils.safe_max(2, 1.1), 2)\n\n\nclass IntersectTestCase(common.TestCase):\n    def testEmptyIntersection(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 3], [4, 5, 6])\n        self.assertEqual(len(values), 0)\n        self.assertEqual(len(ix1), 0)\n        self.assertEqual(len(ix2), 0)\n\n        values, ix1, ix2 = collections.intersect([], [])\n        self.assertEqual(len(values), 0)\n        self.assertEqual(len(ix1), 0)\n        self.assertEqual(len(ix2), 0)\n\n    def testFullIntersection(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 3], [1, 2, 3])\n        self.assertEqual(len(values), 3)\n        self.assertEqual(len(ix1), 3)\n        self.assertEqual(len(ix2), 3)\n        self.assertEqual(ix1, ix2)\n\n    def testPartialIntersection1(self):\n        values, ix1, ix2 = collections.intersect([0, 2, 4], [1, 2, 3])\n        self.assertEqual(len(values), 1)\n        self.assertEqual(values[0], 2)\n        self.assertEqual(ix1[0], 1)\n        self.assertEqual(ix2[0], 1)\n\n    def testPartialIntersection2(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 4], [1, 2, 3])\n        self.assertEqual(len(values), 2)\n        self.assertEqual(values[0], 1)\n        self.assertEqual(values[1], 2)\n        self.assertEqual(ix1[0], 0)\n        self.assertEqual(ix1[1], 1)\n        self.assertEqual(ix2[0], 0)\n        self.assertEqual(ix2[1], 1)\n\n    def testPartialIntersection3(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 5], [1, 3, 5])\n        self.assertEqual(len(values), 2)\n        self.assertEqual(values[0], 1)\n        self.assertEqual(values[1], 5)\n        self.assertEqual(ix1[0], 0)\n        self.assertEqual(ix1[1], 2)\n        self.assertEqual(ix2[0], 0)\n        self.assertEqual(ix2[1], 2)\n\n    def testPartialIntersection4(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 3], [2, 4, 6])\n        self.assertEqual(len(values), 1)\n        self.assertEqual(values[0], 2)\n        self.assertEqual(ix1[0], 1)\n        self.assertEqual(ix2[0], 0)\n\n    def testPartialIntersection5(self):\n        values, ix1, ix2 = collections.intersect([1, 2, 3], [3, 6])\n        self.assertEqual(len(values), 1)\n        self.assertEqual(values[0], 3)\n        self.assertEqual(ix1[0], 2)\n        self.assertEqual(ix2[0], 0)\n\n    def testPartialIntersection6(self):\n        v1 = [1, 1, 2, 2, 3, 3]\n        v2 = [1, 2, 3]\n\n        values, ix1, ix2 = collections.intersect(v1, v2)\n        self.assertEqual(values, [1, 2, 3])\n        self.assertEqual(ix1, [0, 2, 4])\n        self.assertEqual(ix2, [0, 1, 2])\n\n        values, ix2, ix1 = collections.intersect(v2, v1)\n        self.assertEqual(values, [1, 2, 3])\n        self.assertEqual(ix1, [0, 2, 4])\n        self.assertEqual(ix2, [0, 1, 2])\n\n    def testPartialIntersectionIncludeNones(self):\n        v1 = [None, 1, None, None, 2, None, 3, None, 4]\n        v2 = [1, None, 2, None, 3, 4]\n\n        values, ix1, ix2 = collections.intersect(v1, v2)\n        self.assertEqual(values, [1, None, 2, None, 3, 4])\n        self.assertEqual(ix1, [1, 2, 4, 5, 6, 8])\n        self.assertEqual(ix2, [0, 1, 2, 3, 4, 5])\n\n        values, ix2, ix1 = collections.intersect(v2, v1)\n        self.assertEqual(values, [1, None, 2, None, 3, 4])\n        self.assertEqual(ix1, [1, 2, 4, 5, 6, 8])\n        self.assertEqual(ix2, [0, 1, 2, 3, 4, 5])\n\n    def testPartialIntersectionSkipNones(self):\n        v1 = [None, 1, None, None, 2, None, 3, None, 4]\n        v2 = [1, None, 2, None, 3, 4]\n\n        values, ix1, ix2 = collections.intersect(v1, v2, True)\n        self.assertEqual(values, [1, 2, 3, 4])\n        self.assertEqual(ix1, [1, 4, 6, 8])\n        self.assertEqual(ix2, [0, 2, 4, 5])\n\n        values, ix2, ix1 = collections.intersect(v2, v1, True)\n        self.assertEqual(values, [1, 2, 3, 4])\n        self.assertEqual(ix1, [1, 4, 6, 8])\n        self.assertEqual(ix2, [0, 2, 4, 5])\n\n    def testFullIntersectionWithDateTimes(self):\n        size = 1000\n        dateTimes1 = []\n        dateTimes2 = []\n        now = datetime.datetime.now()\n        for i in xrange(size):\n            dateTimes1.append(now + datetime.timedelta(seconds=i))\n            dateTimes2.append(now + datetime.timedelta(seconds=i))\n\n        self.assertEqual(dateTimes1, dateTimes2)\n\n        values, ix1, ix2 = collections.intersect(dateTimes1, dateTimes2)\n        self.assertEqual(values, dateTimes1)\n        self.assertEqual(values, dateTimes2)\n        self.assertEqual(ix1, range(size))\n        self.assertEqual(ix1, ix2)\n\n\nclass CollectionTestCaseBase(common.TestCase):\n    def buildCollection(self, maxLen):\n        raise NotImplementedError()\n\n    def _testBasicOpsImpl(self):\n        d = self.buildCollection(10)\n        self.assertEqual(len(d), 0)\n        self.assertEqual(d.getMaxLen(), 10)\n\n        for i in range(10):\n            d.append(i)\n        self.assertEqual(d[0], 0)\n        self.assertEqual(d[9], 9)\n        self.assertEqual(d[-1], 9)\n        self.assertEqual(d[-2], 8)\n\n        for i in range(3):\n            d.append(i)\n        self.assertEqual(len(d), 10)\n        self.assertEqual(d[0], 3)\n        self.assertEqual(d[9], 2)\n        self.assertEqual(d[-1], 2)\n        self.assertEqual(d[-2], 1)\n\n    def _testResizeImpl(self):\n        d = self.buildCollection(10)\n\n        # Fill the array.\n        self.assertEqual(len(d), 0)\n        for i in range(20):\n            d.append(i)\n        self.assertEqual(d[0], 10)\n        self.assertEqual(d[9], 19)\n        self.assertEqual(d[-1], 19)\n        self.assertEqual(len(d), 10)\n\n        # Shrink the array.\n        d.resize(5)\n        self.assertEqual(len(d), 5)\n        self.assertEqual(d[0], 15)\n        self.assertEqual(d[4], 19)\n        self.assertEqual(d[-1], 19)\n\n        # Grow the array.\n        d.resize(10)\n        self.assertEqual(len(d), 5)\n        self.assertEqual(d[0], 15)\n        self.assertEqual(d[4], 19)\n        self.assertEqual(d[-1], 19)\n\n        # Add one element.\n        d.append(20)\n        self.assertEqual(len(d), 6)\n        self.assertEqual(d[0], 15)\n        self.assertEqual(d[1], 16)\n        self.assertEqual(d[2], 17)\n        self.assertEqual(d[3], 18)\n        self.assertEqual(d[4], 19)\n        self.assertEqual(d[5], 20)\n        self.assertEqual(d[-1], 20)\n\n        # No resize\n        d.resize(10)\n        self.assertEqual(d[0], 15)\n        self.assertEqual(d[5], 20)\n        self.assertEqual(d[-1], 20)\n\n        # Shrink it back.\n        d.resize(4)\n        self.assertEqual(len(d), 4)\n        self.assertEqual(d[0], 17)\n        self.assertEqual(d[3], 20)\n        self.assertEqual(d[-1], 20)\n\n    def _testResizeEmptyImpl(self):\n        d = self.buildCollection(10)\n        self.assertEqual(len(d), 0)\n\n        # No resize\n        d.resize(10)\n\n        # Shrink the array.\n        d.resize(5)\n        self.assertEqual(len(d), 0)\n\n        # Grow the array.\n        d.resize(10)\n        self.assertEqual(len(d), 0)\n\n        # Add one element.\n        d.append(20)\n        self.assertEqual(len(d), 1)\n        self.assertEqual(d[0], 20)\n        self.assertEqual(d[-1], 20)\n\n\nclass NumPyDequeTestCase(CollectionTestCaseBase):\n    def buildCollection(self, maxLen):\n        return collections.NumPyDeque(maxLen)\n\n    def testBasicOps(self):\n        CollectionTestCaseBase._testBasicOpsImpl(self)\n\n    def testResize(self):\n        CollectionTestCaseBase._testResizeImpl(self)\n\n    def testResizeEmpty(self):\n        CollectionTestCaseBase._testResizeEmptyImpl(self)\n\n    def testSum(self):\n        d = collections.NumPyDeque(10)\n\n        for i in range(10):\n            d.append(i)\n        self.assertEqual(d[0:3].sum(), 3)\n\n\nclass ListDequeTestCase(CollectionTestCaseBase):\n    def buildCollection(self, maxLen):\n        return collections.ListDeque(maxLen)\n\n    def testBasicOps(self):\n        CollectionTestCaseBase._testBasicOpsImpl(self)\n\n    def testResize(self):\n        CollectionTestCaseBase._testResizeImpl(self)\n\n    def testResizeEmpty(self):\n        CollectionTestCaseBase._testResizeEmptyImpl(self)\n\n\nclass DateTimeTestCase(common.TestCase):\n    def testTimeStampConversions(self):\n        dateTime = datetime.datetime(2000, 1, 1)\n        self.assertEqual(dt.timestamp_to_datetime(dt.datetime_to_timestamp(dateTime), False), dateTime)\n\n        dateTime = dt.as_utc(datetime.datetime(2000, 1, 1, 1, 1))\n        self.assertEqual(dt.timestamp_to_datetime(dt.datetime_to_timestamp(dateTime), True), dateTime)\n\n    def testTimeStampConversionsWithMicroseconds(self):\n        dateTime = datetime.datetime(2000, 1, 1, 1, 1, 1, microsecond=10)\n        self.assertEqual(dt.timestamp_to_datetime(dt.datetime_to_timestamp(dateTime), False), dateTime)\n\n        dateTime = dt.as_utc(datetime.datetime(2000, 1, 1, 1, 1, 1, microsecond=10))\n        self.assertEqual(dt.timestamp_to_datetime(dt.datetime_to_timestamp(dateTime), True), dateTime)\n\n    def testGetFirstMonday(self):\n        self.assertEquals(dt.get_first_monday(2010), datetime.date(2010, 1, 4))\n        self.assertEquals(dt.get_first_monday(2011), datetime.date(2011, 1, 3))\n\n    def testGetLastMonday(self):\n        self.assertEquals(dt.get_last_monday(2010), datetime.date(2010, 12, 27))\n        self.assertEquals(dt.get_last_monday(2011), datetime.date(2011, 12, 26))\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\n\nimport common\n\nfrom pyalgotrade.tools import yahoofinance\nfrom pyalgotrade import bar\nfrom pyalgotrade.barfeed import yahoofeed\n\n\nclass ToolsTestCase(common.TestCase):\n    def testDownloadAndParseDaily(self):\n        instrument = \"orcl\"\n\n        with common.TmpDir() as tmp_path:\n            path = os.path.join(tmp_path, \"orcl-2010.csv\")\n            yahoofinance.download_daily_bars(instrument, 2010, path)\n            bf = yahoofeed.Feed()\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            self.assertEqual(round(bf[instrument][-1].getOpen(), 2), 31.22)\n            self.assertEqual(round(bf[instrument][-1].getClose(), 2), 31.30)\n\n    def testDownloadAndParseWeekly(self):\n        instrument = \"aapl\"\n\n        with common.TmpDir() as tmp_path:\n            path = os.path.join(tmp_path, \"aapl-weekly-2013.csv\")\n            yahoofinance.download_weekly_bars(instrument, 2013, path)\n            bf = yahoofeed.Feed(frequency=bar.Frequency.WEEK)\n            bf.addBarsFromCSV(instrument, path)\n            bf.loadAll()\n            self.assertEqual(round(bf[instrument][-1].getOpen(), 2), 557.46)\n            self.assertEqual(round(bf[instrument][-1].getHigh(), 2), 561.28)\n            self.assertEqual(round(bf[instrument][-1].getLow(), 2), 540.43)\n            self.assertEqual(round(bf[instrument][-1].getClose(), 2), 540.98)\n            self.assertTrue(bf[instrument][-1].getVolume() in (9852500, 9855900, 68991600))\n\n    def testBuildDailyFeed(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"orcl\"\n            bf = yahoofinance.build_feed([instrument], 2010, 2010, storage=tmpPath)\n            bf.loadAll()\n            self.assertEqual(round(bf[instrument][-1].getOpen(), 2), 31.22)\n            self.assertEqual(round(bf[instrument][-1].getClose(), 2), 31.30)\n\n    def testBuildWeeklyFeed(self):\n        with common.TmpDir() as tmpPath:\n            instrument = \"aapl\"\n            bf = yahoofinance.build_feed([instrument], 2013, 2013, storage=tmpPath, frequency=bar.Frequency.WEEK)\n            bf.loadAll()\n            self.assertEqual(round(bf[instrument][-1].getOpen(), 2), 557.46)\n            self.assertEqual(round(bf[instrument][-1].getHigh(), 2), 561.28)\n            self.assertEqual(round(bf[instrument][-1].getLow(), 2), 540.43)\n            self.assertEqual(round(bf[instrument][-1].getClose(), 2), 540.98)\n            self.assertTrue(bf[instrument][-1].getVolume() in (9852500, 9855900, 68991600))\n\n    def testInvalidDates(self):\n        instrument = \"orcl\"\n\n        # Don't skip errors.\n        with self.assertRaisesRegexp(Exception, \"404 Client Error: Not Found\"):\n            with common.TmpDir() as tmpPath:\n                bf = yahoofinance.build_feed([instrument], 2100, 2101, storage=tmpPath, frequency=bar.Frequency.DAY)\n\n        # Skip errors.\n        with common.TmpDir() as tmpPath:\n            bf = yahoofinance.build_feed(\n                [instrument], 2100, 2101, storage=tmpPath, frequency=bar.Frequency.DAY, skipErrors=True\n            )\n            bf.loadAll()\n            self.assertNotIn(instrument, bf)\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport datetime\n\nimport common\nimport barfeed_test\nimport feed_test\n\nfrom pyalgotrade.utils import dt\nfrom pyalgotrade.barfeed import yahoofeed\nfrom pyalgotrade.barfeed import csvfeed\nfrom pyalgotrade import bar\nfrom pyalgotrade import marketsession\n\n\nclass BarFeedEventHandler_TestLoadOrder:\n    def __init__(self, testcase, barFeed, instrument):\n        self.__testcase = testcase\n        self.__count = 0\n        self.__prevDateTime = None\n        self.__barFeed = barFeed\n        self.__instrument = instrument\n\n    def onBars(self, dateTime, bars):\n        self.__count += 1\n        dateTime = bars.getBar(self.__instrument).getDateTime()\n        if self.__prevDateTime is not None:\n            # Check that bars are loaded in order\n            self.__testcase.assertTrue(self.__prevDateTime < dateTime)\n            # Check that the last value in the dataseries match the current datetime.\n            self.__testcase.assertTrue(self.__barFeed.getDataSeries()[-1].getDateTime() == dateTime)\n            # Check that the datetime for the last value matches that last datetime in the dataseries.\n            self.__testcase.assertEqual(self.__barFeed.getDataSeries()[-1].getDateTime(), self.__barFeed.getDataSeries().getDateTimes()[-1])\n        self.__prevDateTime = dateTime\n\n    def getEventCount(self):\n            return self.__count\n\n\nclass BarFeedEventHandler_TestFilterRange:\n    def __init__(self, testcase, instrument, fromDate, toDate):\n        self.__testcase = testcase\n        self.__count = 0\n        self.__instrument = instrument\n        self.__fromDate = fromDate\n        self.__toDate = toDate\n\n    def onBars(self, dateTime, bars):\n        self.__count += 1\n\n        if self.__fromDate is not None:\n            self.__testcase.assertTrue(bars.getBar(self.__instrument).getDateTime() >= self.__fromDate)\n        if self.__toDate is not None:\n            self.__testcase.assertTrue(bars.getBar(self.__instrument).getDateTime() <= self.__toDate)\n\n    def getEventCount(self):\n            return self.__count\n\n\nclass FeedTestCase(common.TestCase):\n    TestInstrument = \"orcl\"\n\n    def __parseDate(self, date):\n        parser = yahoofeed.RowParser(datetime.time(23, 59), bar.Frequency.DAY)\n        row = {\n            \"Date\": date,\n            \"Close\": 0,\n            \"Open\": 0,\n            \"High\": 0,\n            \"Low\": 0,\n            \"Volume\": 0,\n            \"Adj Close\": 0}\n        return parser.parseBar(row).getDateTime()\n\n    def testInvalidConstruction(self):\n        with self.assertRaises(Exception):\n            yahoofeed.Feed(maxLen=0)\n\n    def testDefaultInstrument(self):\n        barFeed = yahoofeed.Feed()\n        self.assertEquals(barFeed.getDefaultInstrument(), None)\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        self.assertEquals(barFeed.getDefaultInstrument(), FeedTestCase.TestInstrument)\n\n    def testDuplicateBars(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        with self.assertRaisesRegexp(Exception, \"Duplicate bars found for.*\"):\n            barFeed.loadAll()\n\n    def testBaseBarFeed(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.sanitizeBars(True)\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barfeed_test.check_base_barfeed(self, barFeed, True)\n\n    def testInvalidFrequency(self):\n        with self.assertRaisesRegexp(Exception, \"Invalid frequency.*\"):\n            yahoofeed.Feed(frequency=bar.Frequency.MINUTE)\n\n    def testBaseFeedInterface(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        feed_test.tstBaseFeedInterface(self, barFeed)\n\n    def testParseDate_1(self):\n        date = self.__parseDate(\"1950-01-01\")\n        self.assertTrue(date.day == 1)\n        self.assertTrue(date.month == 1)\n        self.assertTrue(date.year == 1950)\n\n    def testParseDate_2(self):\n        date = self.__parseDate(\"2000-01-01\")\n        self.assertTrue(date.day == 1)\n        self.assertTrue(date.month == 1)\n        self.assertTrue(date.year == 2000)\n\n    def testDateCompare(self):\n        self.assertTrue(self.__parseDate(\"2000-01-01\") == self.__parseDate(\"2000-01-01\"))\n        self.assertTrue(self.__parseDate(\"2000-01-01\") != self.__parseDate(\"2001-01-01\"))\n        self.assertTrue(self.__parseDate(\"1999-01-01\") < self.__parseDate(\"2001-01-01\"))\n        self.assertTrue(self.__parseDate(\"2011-01-01\") > self.__parseDate(\"2001-02-02\"))\n\n    def testCSVFeedLoadOrder(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n\n        # Dispatch and handle events.\n        handler = BarFeedEventHandler_TestLoadOrder(self, barFeed, FeedTestCase.TestInstrument)\n        barFeed.getNewValuesEvent().subscribe(handler.onBars)\n        while not barFeed.eof():\n            barFeed.dispatch()\n        self.assertTrue(handler.getEventCount() > 0)\n\n    def __testFilteredRangeImpl(self, fromDate, toDate):\n        barFeed = yahoofeed.Feed()\n        barFeed.setBarFilter(csvfeed.DateRangeFilter(fromDate, toDate))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n\n        # Dispatch and handle events.\n        handler = BarFeedEventHandler_TestFilterRange(self, FeedTestCase.TestInstrument, fromDate, toDate)\n        barFeed.getNewValuesEvent().subscribe(handler.onBars)\n        while not barFeed.eof():\n            barFeed.dispatch()\n        self.assertTrue(handler.getEventCount() > 0)\n\n    def testFilteredRangeFrom(self):\n        # Only load bars from year 2001.\n        self.__testFilteredRangeImpl(datetime.datetime(2001, 1, 1, 00, 00), None)\n\n    def testFilteredRangeTo(self):\n        # Only load bars up to year 2000.\n        self.__testFilteredRangeImpl(None, datetime.datetime(2000, 12, 31, 23, 55))\n\n    def testFilteredRangeFromTo(self):\n        # Only load bars in year 2000.\n        self.__testFilteredRangeImpl(datetime.datetime(2000, 1, 1, 00, 00), datetime.datetime(2000, 12, 31, 23, 55))\n\n    def testWithoutTimezone(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        for dateTime, bars in barFeed:\n            bar = bars.getBar(FeedTestCase.TestInstrument)\n            self.assertTrue(dt.datetime_is_naive(bar.getDateTime()))\n\n    def testWithDefaultTimezone(self):\n        barFeed = yahoofeed.Feed(timezone=marketsession.USEquities.getTimezone())\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"))\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"))\n        for dateTime, bars in barFeed:\n            bar = bars.getBar(FeedTestCase.TestInstrument)\n            self.assertFalse(dt.datetime_is_naive(bar.getDateTime()))\n\n    def testWithPerFileTimezone(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.getTimezone())\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2001-yahoofinance.csv\"), marketsession.USEquities.getTimezone())\n        for dateTime, bars in barFeed:\n            bar = bars.getBar(FeedTestCase.TestInstrument)\n            self.assertFalse(dt.datetime_is_naive(bar.getDateTime()))\n\n    def testWithIntegerTimezone(self):\n        try:\n            barFeed = yahoofeed.Feed(timezone=-5)\n            self.assertTrue(False, \"Exception expected\")\n        except Exception, e:\n            self.assertTrue(str(e).find(\"timezone as an int parameter is not supported anymore\") == 0)\n\n        try:\n            barFeed = yahoofeed.Feed()\n            barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), -3)\n            self.assertTrue(False, \"Exception expected\")\n        except Exception, e:\n            self.assertTrue(str(e).find(\"timezone as an int parameter is not supported anymore\") == 0)\n\n    def testMapTypeOperations(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.getTimezone())\n        for dateTime, bars in barFeed:\n            self.assertTrue(FeedTestCase.TestInstrument in bars)\n            self.assertFalse(FeedTestCase.TestInstrument not in bars)\n            bars[FeedTestCase.TestInstrument]\n            with self.assertRaises(KeyError):\n                bars[\"pirulo\"]\n\n    def testBounded(self):\n        barFeed = yahoofeed.Feed(maxLen=2)\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.getTimezone())\n        for dateTime, bars in barFeed:\n            pass\n\n        barDS = barFeed[FeedTestCase.TestInstrument]\n        self.assertEqual(len(barDS), 2)\n        self.assertEqual(len(barDS.getDateTimes()), 2)\n        self.assertEqual(len(barDS.getCloseDataSeries()), 2)\n        self.assertEqual(len(barDS.getCloseDataSeries().getDateTimes()), 2)\n        self.assertEqual(len(barDS.getOpenDataSeries()), 2)\n        self.assertEqual(len(barDS.getHighDataSeries()), 2)\n        self.assertEqual(len(barDS.getLowDataSeries()), 2)\n        self.assertEqual(len(barDS.getAdjCloseDataSeries()), 2)\n\n    def testReset(self):\n        barFeed = yahoofeed.Feed()\n        barFeed.addBarsFromCSV(FeedTestCase.TestInstrument, common.get_data_file_path(\"orcl-2000-yahoofinance.csv\"), marketsession.USEquities.getTimezone())\n        barFeed.loadAll()\n        instruments = barFeed.getRegisteredInstruments()\n        ds = barFeed[FeedTestCase.TestInstrument]\n\n        barFeed.reset()\n        barFeed.loadAll()\n        reloadedDs = barFeed[FeedTestCase.TestInstrument]\n\n        self.assertEqual(len(reloadedDs), len(ds))\n        self.assertNotEqual(reloadedDs, ds)\n        self.assertEqual(instruments, barFeed.getRegisteredInstruments())\n        for i in range(len(ds)):\n            self.assertEqual(ds[i].getDateTime(), reloadedDs[i].getDateTime())\n            self.assertEqual(ds[i].getClose(), reloadedDs[i].getClose())\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nsys.path.append(\"../..\")\n\nimport pyalgotrade.logger\nimport lxml.html\nimport symbolsxml\n\nlogger = pyalgotrade.logger.getLogger(\"get_merval_symbols\")\n\n\ndef find_company(htmlTree, ticker):\n    ret = None\n    anchor = htmlTree.xpath(\"//td[1]/a[@href='/q/pr?s=%s']/text()\" % (ticker))\n    if anchor:\n        ret = anchor[0]\n    return ret\n\n\ndef find_sector(htmlTree):\n    ret = None\n    anchor = htmlTree.xpath(\"//th[1][text() = 'Sector:']/../td/a[1]/text()\")\n    if anchor:\n        ret = anchor[0]\n    return ret\n\n\ndef find_industry(htmlTree):\n    ret = None\n    anchor = htmlTree.xpath(\"//th[1][text() = 'Industry:']/../td/a[1]/text()\")\n    if anchor:\n        ret = anchor[0]\n    return ret\n\n\ndef process_symbol(writer, symbol):\n    try:\n        logger.info(\"Getting info for %s\" % (symbol))\n        url = \"http://finance.yahoo.com/q/in?s=%s+Industry\" % (symbol)\n        htmlTree = lxml.html.parse(url)\n\n        company = find_company(htmlTree, symbol)\n        if not company:\n            raise Exception(\"Company name not found\")\n\n        sector = find_sector(htmlTree)\n        if not sector:\n            sector = \"\"\n            logger.warning(\"Sector not found\")\n\n        industry = find_industry(htmlTree)\n        if not industry:\n            industry = \"\"\n            logger.warning(\"Industry not found\")\n\n        writer.addStock(symbol, company, sector, industry)\n    except Exception, e:\n        logger.error(str(e))\n\n\ndef main():\n    try:\n        writer = symbolsxml.Writer()\n        for symbol in open(\"merval-symbols.txt\", \"r\"):\n            symbol = symbol.strip()\n            process_symbol(writer, symbol)\n\n        # Index\n        writer.addIndex(\"^MERV\", \"Merval\")\n\n        logger.info(\"Writing merval.xml\")\n        writer.write(\"merval.xml\")\n    except Exception, e:\n        logger.error(str(e))\n\nif __name__ == \"__main__\":\n    main()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nsys.path.append(\"../..\")\n\nimport pyalgotrade.logger\nimport tempfile\nimport urllib2\nimport csv\nimport symbolsxml\n\n\nlogger = pyalgotrade.logger.getLogger(\"get_nasdaq_symbols\")\n\n\ndef main():\n    try:\n        logger.info(\"Getting NASDAQ symbols from http://www.nasdaq.com/\")\n        url = \"http://www.nasdaq.com/screening/companies-by-name.aspx?exchange=NASDAQ&render=download\"\n        buff = urllib2.urlopen(url).read()\n\n        tmpFile = tempfile.NamedTemporaryFile()\n        tmpFile.write(buff)\n        tmpFile.flush()\n        with open(tmpFile.name, 'rb') as csvfile:\n            symbolsXML = symbolsxml.Writer()\n            for row in csv.DictReader(csvfile):\n                symbolsXML.addStock(row[\"Symbol\"], row[\"Name\"], row[\"Sector\"], row[\"industry\"])\n\n        logger.info(\"Writing nasdaq.xml\")\n        symbolsXML.write(\"nasdaq.xml\")\n    except Exception, e:\n        logger.error(str(e))\n\nif __name__ == \"__main__\":\n    main()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nsys.path.append(\"../..\")\n\nimport pyalgotrade.logger\nimport tempfile\nimport urllib2\nimport csv\nimport symbolsxml\n\n\nlogger = pyalgotrade.logger.getLogger(\"get_nyse_symbols\")\n\n\ndef main():\n    try:\n        logger.info(\"Getting NYSE symbols from http://www.nasdaq.com/\")\n        url = \"http://www.nasdaq.com/screening/companies-by-name.aspx?exchange=NYSE&render=download\"\n        buff = urllib2.urlopen(url).read()\n\n        tmpFile = tempfile.NamedTemporaryFile()\n        tmpFile.write(buff)\n        tmpFile.flush()\n        with open(tmpFile.name, 'rb') as csvfile:\n            symbolsXML = symbolsxml.Writer()\n            for row in csv.DictReader(csvfile):\n                symbolsXML.addStock(row[\"Symbol\"], row[\"Name\"], row[\"Sector\"], row[\"industry\"])\n\n        logger.info(\"Writing nyse.xml\")\n        symbolsXML.write(\"nyse.xml\")\n    except Exception, e:\n        logger.error(str(e))\n\nif __name__ == \"__main__\":\n    main()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nsys.path.append(\"../..\")\n\nimport pyalgotrade.logger\nimport lxml.html\nimport symbolsxml\n\n# pyalgotrade.logger.file_log = \"get_sp500_symbols.log\"\nlogger = pyalgotrade.logger.getLogger(\"get_sp500_symbols\")\n\nTICKER_SYMBOL_COL = 0\nCOMPANY_COL = 1\nGICS_COL = 3\nGICS_SUB_INDUSTRY_COL = 4\n\n\ndef get_html():\n    logger.info(\"Getting S&P 500 Component Stocks from Wikipedia\")\n    ret = lxml.html.parse(\"http://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")\n    return ret\n\n\ndef find_table(htmlTree):\n    logger.info(\"Finding the right table\")\n    ret = None\n    tables = htmlTree.xpath(\"//table[@class='wikitable sortable']\")\n    for table in tables:\n        headers = table.xpath(\"tr[1]/th\")\n        if len(headers) > 5:\n            if headers[TICKER_SYMBOL_COL].xpath(\"a[1]\")[0].text != \"Ticker symbol\":\n                continue\n            if headers[COMPANY_COL].text != \"Company\":\n                continue\n            if headers[GICS_COL].xpath(\"a[1]\")[0].text != \"GICS\":\n                continue\n            if headers[GICS_SUB_INDUSTRY_COL].text != \"GICS Sub Industry\":\n                continue\n            ret = table\n            break\n    return ret\n\n\ndef parse_results(table):\n    ret = symbolsxml.Writer()\n    logger.info(\"Parsing table\")\n    rows = table.xpath(\"tr\")\n    for row in rows[1:]:\n        cols = row.xpath(\"td\")\n        tickerSymbol = cols[TICKER_SYMBOL_COL].xpath(\"a[1]\")[0].text\n        company = cols[COMPANY_COL].xpath(\"a[1]\")[0].text\n        gics = cols[GICS_COL].text\n        gicsSubIndustry = cols[GICS_SUB_INDUSTRY_COL].text\n        if gicsSubIndustry is None:\n            gicsSubIndustry = \"\"\n\n        ret.addStock(tickerSymbol, company, gics, gicsSubIndustry)\n    return ret\n\n\ndef main():\n    try:\n        htmlTree = get_html()\n        table = find_table(htmlTree)\n        if table is None:\n            raise Exception(\"S&P 500 Component Stocks table not found\")\n        symbolsXML = parse_results(table)\n\n        logger.info(\"Writing sp500.xml\")\n        symbolsXML.write(\"sp500.xml\")\n    except Exception, e:\n        logger.error(str(e))\n\nif __name__ == \"__main__\":\n    main()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport sys\nimport os\nimport datetime\n\nsys.path.append(os.path.join(\"..\", \"symbols\"))\nsys.path.append(os.path.join(\"..\", \"..\"))  # For pyalgotrade\n\nimport symbolsxml\nimport merval_calendar\nimport pyalgotrade.logger\n\npyalgotrade.logger.file_log = \"analyze_gaps.log\"\nlogger = pyalgotrade.logger.getLogger(\"analyze_gaps\")\n\nfrom pyalgotrade.barfeed import yahoofeed\n\n\nstorage = \"data\"\n\n\nclass MissingDataVerifier:\n    def __init__(self, barDataSeries, threshold):\n        self.__barDataSeries = barDataSeries\n        self.__threshold = threshold\n\n    def isTradingDay(self, dateTime):\n        raise NotImplementedError()\n\n    def getDatesInBetween(self, prevDateTime, currentDateTime):\n        assert((currentDateTime - prevDateTime).days > 1)\n        ret = []\n        dateTime = prevDateTime + datetime.timedelta(days=1)\n        while dateTime < currentDateTime:\n            # Skip weekends.\n            if dateTime.weekday() not in [5, 6]:\n                ret.append(dateTime.date())\n            dateTime = dateTime + datetime.timedelta(days=1)\n        return ret\n\n    def __processGap(self, prevDateTime, currentDateTime):\n        dates = self.getDatesInBetween(prevDateTime, currentDateTime)\n        dates = filter(lambda x: not self.isTradingDay(x), dates)\n        if len(dates) >= self.__threshold:\n            logger.warning(\"%d day gap between %s and %s\" % (len(dates), prevDateTime, currentDateTime))\n\n    def run(self):\n        prevDateTime = None\n        for bar in self.__barDataSeries:\n            currentDateTime = bar.getDateTime()\n            if prevDateTime is not None:\n                if (currentDateTime - prevDateTime).days > 1:\n                    self.__processGap(prevDateTime, currentDateTime)\n            prevDateTime = currentDateTime\n\n\ndef get_csv_filename(symbol, year):\n    return os.path.join(storage, \"%s-%d-yahoofinance.csv\" % (symbol, year))\n\n\ndef process_symbol(symbol, fromYear, toYear, missingDataVerifierClass):\n    logger.info(\"Processing %s from %d to %d\" % (symbol, fromYear, toYear))\n\n    filesFound = 0\n    # Load the bars from the CSV files.\n    feed = yahoofeed.Feed(maxLen=1000000)\n    feed.sanitizeBars(True)\n    for year in range(fromYear, toYear+1):\n        fileName = get_csv_filename(symbol, year)\n        if os.path.exists(fileName):\n            filesFound += 1\n            feed.addBarsFromCSV(symbol, fileName)\n\n    if filesFound > 0:\n        # Process all items.\n        for dateTime, bars in feed:\n            pass\n\n        missingDataVerifier = missingDataVerifierClass(feed[symbol])\n        missingDataVerifier.run()\n    else:\n        logger.error(\"No files found\")\n\n\nclass MervalMissingDataVerifier(MissingDataVerifier):\n    def __init__(self, barDataSeries):\n        MissingDataVerifier.__init__(self, barDataSeries, 5)\n\n    def isTradingDay(self, dateTime):\n        return not merval_calendar.is_trading_day(dateTime)\n\n\ndef main():\n    fromYear = 2000\n    toYear = 2012\n\n    try:\n        # MERVAL config.\n        symbolsFile = os.path.join(\"..\", \"symbols\", \"merval.xml\")\n        missingDataVerifierClass = MervalMissingDataVerifier\n\n        stockCallback = lambda stock: process_symbol(stock.getTicker(), fromYear, toYear, missingDataVerifierClass)\n        indexCallback = stockCallback\n        symbolsxml.parse(symbolsFile, stockCallback, indexCallback)\n    except Exception, e:\n        logger.error(str(e))\n\nmain()\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(\"..\", \"symbols\"))\nsys.path.append(os.path.join(\"..\", \"..\"))  # For pyalgotrade\n\nimport pyalgotrade.logger\n\npyalgotrade.logger.file_log = \"download_data.log\"\nlogger = pyalgotrade.logger.getLogger(\"download_data\")\n\nfrom pyalgotrade.tools import yahoofinance\nimport symbolsxml\n\n\nstorage = \"data\"\n\n\ndef get_csv_filename(symbol, year):\n    return os.path.join(storage, \"%s-%d-yahoofinance.csv\" % (symbol, year))\n\n\ndef download_files_for_symbol(symbol, fromYear, toYear):\n    if not os.path.exists(storage):\n        logger.info(\"Creating %s directory\" % (storage))\n        os.mkdir(storage)\n\n    status = \"\"\n    for year in range(fromYear, toYear+1):\n        fileName = get_csv_filename(symbol, year)\n        if not os.path.exists(fileName):\n            logger.info(\"Downloading %s %d to %s\" % (symbol, year, fileName))\n            try:\n                yahoofinance.download_daily_bars(symbol, year, fileName)\n                status += \"1\"\n            except Exception, e:\n                logger.error(str(e))\n                status += \"0\"\n        else:\n            status += \"1\"\n\n    if status.find(\"1\") == -1:\n        logger.fatal(\"No data found for %s\" % (symbol))\n    elif status.lstrip(\"0\").find(\"0\") != -1:\n        logger.fatal(\"Some bars are missing for %s\" % (symbol))\n\n\ndef main():\n    fromYear = 2000\n    toYear = 2013\n\n    try:\n        symbolsFile = os.path.join(\"..\", \"symbols\", \"merval.xml\")\n        callback = lambda stock: download_files_for_symbol(stock.getTicker(), fromYear, toYear)\n        symbolsxml.parse(symbolsFile, callback, callback)\n    except Exception, e:\n        logger.error(str(e))\n\nmain()\n",
          "env:\n  global:\n    - secure: \"YgngWTEKD6bUVgExTN8vWYCq7mrMKsVVqZaWIsepRziU++K1e9iBuQrSspQbyrdsKMqKMyMnsM3ru7M6+D0F+p/BC4gyWg6N+wn3RoqrWidyutejibsobDJnS3EsSfo4cJUFcoAOzzuXK+Du0yUZPNBxjJdF/NA1C8vDg0YAmzs=\"\n    - secure: \"GUuZEQSA9x96bIJiC/Ab/RxQ+Dv2eqwhRVaZYVlmuSDQXouDJdFAF9dEcUplwt3TCKg7CbOa3gr2c1rJ7bclk1ueUSXdwzlgEfEdcu7Xm+lf6LcfZESDtWUvPgRzvVqspp3hwx+SCpSl2TzXwgCRNOX8ybDC5ZvRZRQaagT6maA=\"\n    - secure: \"oAUaWICTHks6N/RNFZbF/NJE+1mB45DRZu1uvzKN5EhEp+Qys6LGHm1yZLI1sMAfwYrLro1oxNGpxD26hoMDjy2bDyBNZj59i2M7Eeo9juY3wR2sz0tKuvEdMdbuhMGyDgvXV6xlqO10hHeUUwbJOyxcQxmcp0c33B0Jse3m+pI=\"\n    - secure: \"OrR5+l3HzjKWQtVRUIHB1uLCFtRt1wNAQxJjoL6mufBMIigcpMnE2QzsoP9ywSgew8St4nFMGaL/xxR/RYTi59c6fVRh8zGg/JYMSYtr4UM8M2Gk0vSYmo3JO9OihOxn3JWbF4606vzEgqtjMT9qojo3Xx/d0Hp9br5s/wwe1oQ=\"\n\nsudo: required\n\nlanguage: python\npython:\n  - \"2.7\"\n\nservices:\n  - docker\n\nbefore_install:\n  - docker pull gbecedillas/pyalgotrade:0.18\n  - cp travis/Dockerfile .\n  - docker build -t testcases .\n  - sudo pip install coveralls\n\nscript:\n  - docker run --name testcases --env TWITTER_CONSUMER_KEY=$TWITTER_CONSUMER_KEY --env TWITTER_CONSUMER_SECRET=$TWITTER_CONSUMER_SECRET --env TWITTER_ACCESS_TOKEN=$TWITTER_ACCESS_TOKEN --env TWITTER_ACCESS_TOKEN_SECRET=$TWITTER_ACCESS_TOKEN_SECRET testcases /bin/bash -c \"cd /tmp/pyalgotrade; ./run_tests.sh\"\n\nafter_success:\n  - docker cp testcases:/tmp/pyalgotrade/.coverage .\n  # .coverage file has absolute paths in it, so we need to put the source in the right path for coveralls to find it.\n  - mkdir /tmp/pyalgotrade\n  - docker cp testcases:/tmp/pyalgotrade/pyalgotrade /tmp/pyalgotrade/pyalgotrade\n  - coveralls\n",
          "FROM gbecedillas/pyalgotrade:0.18\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN pip install nose\nRUN pip install nose-cov\n\nRUN mkdir /tmp/pyalgotrade\n\n# Files needed to execute testcases.\nCOPY travis/run_tests.sh /tmp/pyalgotrade/\nCOPY coverage.cfg /tmp/pyalgotrade/\nCOPY setup.cfg /tmp/pyalgotrade/\nCOPY testcases /tmp/pyalgotrade/testcases\nCOPY samples /tmp/pyalgotrade/samples\n\n# We need to upgrade the installed version with the one checked out from GIT.\nCOPY setup.py /tmp/pyalgotrade/\nCOPY pyalgotrade /tmp/pyalgotrade/pyalgotrade\nRUN pip install -U /tmp/pyalgotrade\n",
          "env:\n  global:\n    - secure: \"YgngWTEKD6bUVgExTN8vWYCq7mrMKsVVqZaWIsepRziU++K1e9iBuQrSspQbyrdsKMqKMyMnsM3ru7M6+D0F+p/BC4gyWg6N+wn3RoqrWidyutejibsobDJnS3EsSfo4cJUFcoAOzzuXK+Du0yUZPNBxjJdF/NA1C8vDg0YAmzs=\"\n    - secure: \"GUuZEQSA9x96bIJiC/Ab/RxQ+Dv2eqwhRVaZYVlmuSDQXouDJdFAF9dEcUplwt3TCKg7CbOa3gr2c1rJ7bclk1ueUSXdwzlgEfEdcu7Xm+lf6LcfZESDtWUvPgRzvVqspp3hwx+SCpSl2TzXwgCRNOX8ybDC5ZvRZRQaagT6maA=\"\n    - secure: \"oAUaWICTHks6N/RNFZbF/NJE+1mB45DRZu1uvzKN5EhEp+Qys6LGHm1yZLI1sMAfwYrLro1oxNGpxD26hoMDjy2bDyBNZj59i2M7Eeo9juY3wR2sz0tKuvEdMdbuhMGyDgvXV6xlqO10hHeUUwbJOyxcQxmcp0c33B0Jse3m+pI=\"\n    - secure: \"OrR5+l3HzjKWQtVRUIHB1uLCFtRt1wNAQxJjoL6mufBMIigcpMnE2QzsoP9ywSgew8St4nFMGaL/xxR/RYTi59c6fVRh8zGg/JYMSYtr4UM8M2Gk0vSYmo3JO9OihOxn3JWbF4606vzEgqtjMT9qojo3Xx/d0Hp9br5s/wwe1oQ=\"\n\nsudo: required\n\nlanguage: python\npython:\n  - \"2.7\"\n\nservices:\n  - docker\n\nbefore_install:\n  - docker pull gbecedillas/pyalgotrade:0.18\n  - cp travis/Dockerfile .\n  - docker build -t testcases .\n  - sudo pip install coveralls\n\nscript:\n  - docker run --name testcases --env TWITTER_CONSUMER_KEY=$TWITTER_CONSUMER_KEY --env TWITTER_CONSUMER_SECRET=$TWITTER_CONSUMER_SECRET --env TWITTER_ACCESS_TOKEN=$TWITTER_ACCESS_TOKEN --env TWITTER_ACCESS_TOKEN_SECRET=$TWITTER_ACCESS_TOKEN_SECRET testcases /bin/bash -c \"cd /tmp/pyalgotrade; ./run_tests.sh\"\n\nafter_success:\n  - docker cp testcases:/tmp/pyalgotrade/.coverage .\n  # .coverage file has absolute paths in it, so we need to put the source in the right path for coveralls to find it.\n  - mkdir /tmp/pyalgotrade\n  - docker cp testcases:/tmp/pyalgotrade/pyalgotrade /tmp/pyalgotrade/pyalgotrade\n  - coveralls\n",
          "FROM ubuntu:latest\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN apt-get update\n\nRUN apt-get install -y build-essential\nRUN apt-get install -y python-setuptools python-dev\nRUN apt-get install -y python-pip\n\nRUN apt-get install -y gfortran libopenblas-dev liblapack-dev\nRUN apt-get install -y pkg-config\nRUN apt-get install -y wget\n\nRUN pip install numpy\nRUN pip install scipy\nRUN pip install pandas\nRUN pip install patsy\nRUN pip install statsmodels\nRUN apt-get install -y libfreetype6-dev; \\\n\tpip install matplotlib\nRUN pip install ws4py\nRUN pip install tornado\nRUN pip install tweepy\nRUN pip install cython\n\n# TA-Lib\nRUN cd /tmp; \\\n\twget http://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz; \\\n\ttar -xzf ta-lib-0.4.0-src.tar.gz; \\\n\tcd ta-lib; \\\n\t./configure ; make; make install; \\\n\tcd ..; \\\n\trm ta-lib-0.4.0-src.tar.gz; \\\n\trm -rf ta-lib\nRUN pip install TA-Lib\n\nRUN pip install pyalgotrade==0.18\n",
          "FROM gbecedillas/pyalgotrade:0.18\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN pip install nose\nRUN pip install nose-cov\n\nRUN mkdir /tmp/pyalgotrade\n\n# Files needed to execute testcases.\nCOPY travis/run_tests.sh /tmp/pyalgotrade/\nCOPY coverage.cfg /tmp/pyalgotrade/\nCOPY setup.cfg /tmp/pyalgotrade/\nCOPY testcases /tmp/pyalgotrade/testcases\nCOPY samples /tmp/pyalgotrade/samples\n\n# We need to upgrade the installed version with the one checked out from GIT.\nCOPY setup.py /tmp/pyalgotrade/\nCOPY pyalgotrade /tmp/pyalgotrade/pyalgotrade\nRUN pip install -U /tmp/pyalgotrade\n",
          "FROM ubuntu:latest\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN apt-get update\n\nRUN apt-get install -y build-essential\nRUN apt-get install -y python-setuptools python-dev\nRUN apt-get install -y python-pip\n\nRUN apt-get install -y gfortran libopenblas-dev liblapack-dev\nRUN apt-get install -y pkg-config\nRUN apt-get install -y wget\n\nRUN pip install numpy\nRUN pip install scipy\nRUN pip install pandas\nRUN pip install patsy\nRUN pip install statsmodels\nRUN apt-get install -y libfreetype6-dev; \\\n\tpip install matplotlib\nRUN pip install ws4py\nRUN pip install tornado\nRUN pip install tweepy\nRUN pip install cython\n\n# TA-Lib\nRUN cd /tmp; \\\n\twget http://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz; \\\n\ttar -xzf ta-lib-0.4.0-src.tar.gz; \\\n\tcd ta-lib; \\\n\t./configure ; make; make install; \\\n\tcd ..; \\\n\trm ta-lib-0.4.0-src.tar.gz; \\\n\trm -rf ta-lib\nRUN pip install TA-Lib\n\nRUN pip install pyalgotrade==0.18\n",
          "",
          "env:\n  global:\n    - secure: \"YgngWTEKD6bUVgExTN8vWYCq7mrMKsVVqZaWIsepRziU++K1e9iBuQrSspQbyrdsKMqKMyMnsM3ru7M6+D0F+p/BC4gyWg6N+wn3RoqrWidyutejibsobDJnS3EsSfo4cJUFcoAOzzuXK+Du0yUZPNBxjJdF/NA1C8vDg0YAmzs=\"\n    - secure: \"GUuZEQSA9x96bIJiC/Ab/RxQ+Dv2eqwhRVaZYVlmuSDQXouDJdFAF9dEcUplwt3TCKg7CbOa3gr2c1rJ7bclk1ueUSXdwzlgEfEdcu7Xm+lf6LcfZESDtWUvPgRzvVqspp3hwx+SCpSl2TzXwgCRNOX8ybDC5ZvRZRQaagT6maA=\"\n    - secure: \"oAUaWICTHks6N/RNFZbF/NJE+1mB45DRZu1uvzKN5EhEp+Qys6LGHm1yZLI1sMAfwYrLro1oxNGpxD26hoMDjy2bDyBNZj59i2M7Eeo9juY3wR2sz0tKuvEdMdbuhMGyDgvXV6xlqO10hHeUUwbJOyxcQxmcp0c33B0Jse3m+pI=\"\n    - secure: \"OrR5+l3HzjKWQtVRUIHB1uLCFtRt1wNAQxJjoL6mufBMIigcpMnE2QzsoP9ywSgew8St4nFMGaL/xxR/RYTi59c6fVRh8zGg/JYMSYtr4UM8M2Gk0vSYmo3JO9OihOxn3JWbF4606vzEgqtjMT9qojo3Xx/d0Hp9br5s/wwe1oQ=\"\n\nsudo: required\n\nlanguage: python\npython:\n  - \"2.7\"\n\nservices:\n  - docker\n\nbefore_install:\n  - docker pull gbecedillas/pyalgotrade:0.18\n  - cp travis/Dockerfile .\n  - docker build -t testcases .\n  - sudo pip install coveralls\n\nscript:\n  - docker run --name testcases --env TWITTER_CONSUMER_KEY=$TWITTER_CONSUMER_KEY --env TWITTER_CONSUMER_SECRET=$TWITTER_CONSUMER_SECRET --env TWITTER_ACCESS_TOKEN=$TWITTER_ACCESS_TOKEN --env TWITTER_ACCESS_TOKEN_SECRET=$TWITTER_ACCESS_TOKEN_SECRET testcases /bin/bash -c \"cd /tmp/pyalgotrade; ./run_tests.sh\"\n\nafter_success:\n  - docker cp testcases:/tmp/pyalgotrade/.coverage .\n  # .coverage file has absolute paths in it, so we need to put the source in the right path for coveralls to find it.\n  - mkdir /tmp/pyalgotrade\n  - docker cp testcases:/tmp/pyalgotrade/pyalgotrade /tmp/pyalgotrade/pyalgotrade\n  - coveralls\n",
          "FROM ubuntu:latest\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN apt-get update\n\nRUN apt-get install -y build-essential\nRUN apt-get install -y python-setuptools python-dev\nRUN apt-get install -y python-pip\n\nRUN apt-get install -y gfortran libopenblas-dev liblapack-dev\nRUN apt-get install -y pkg-config\nRUN apt-get install -y wget\n\nRUN pip install numpy\nRUN pip install scipy\nRUN pip install pandas\nRUN pip install patsy\nRUN pip install statsmodels\nRUN apt-get install -y libfreetype6-dev; \\\n\tpip install matplotlib\nRUN pip install ws4py\nRUN pip install tornado\nRUN pip install tweepy\nRUN pip install cython\n\n# TA-Lib\nRUN cd /tmp; \\\n\twget http://sourceforge.net/projects/ta-lib/files/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz; \\\n\ttar -xzf ta-lib-0.4.0-src.tar.gz; \\\n\tcd ta-lib; \\\n\t./configure ; make; make install; \\\n\tcd ..; \\\n\trm ta-lib-0.4.0-src.tar.gz; \\\n\trm -rf ta-lib\nRUN pip install TA-Lib\n\nRUN pip install pyalgotrade==0.18\n",
          "FROM gbecedillas/pyalgotrade:0.18\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN pip install nose\nRUN pip install nose-cov\n\nRUN mkdir /tmp/pyalgotrade\n\n# Files needed to execute testcases.\nCOPY travis/run_tests.sh /tmp/pyalgotrade/\nCOPY coverage.cfg /tmp/pyalgotrade/\nCOPY setup.cfg /tmp/pyalgotrade/\nCOPY testcases /tmp/pyalgotrade/testcases\nCOPY samples /tmp/pyalgotrade/samples\n\n# We need to upgrade the installed version with the one checked out from GIT.\nCOPY setup.py /tmp/pyalgotrade/\nCOPY pyalgotrade /tmp/pyalgotrade/pyalgotrade\nRUN pip install -U /tmp/pyalgotrade\n",
          "*pyc\nMANIFEST\ndoc/_build\n.idea\n.coverage\n.ipynb_checkpoints\n.ropeproject\n\ndata\ndist\nbuild\nPyAlgoTrade.egg-info\n\ntestcases/twitter_credentials.py\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import barfeed\nfrom pyalgotrade import bar\nfrom pyalgotrade import utils\n\n\n# A non real-time BarFeed responsible for:\n# - Holding bars in memory.\n# - Aligning them with respect to time.\n#\n# Subclasses should:\n# - Forward the call to start() if they override it.\n\nclass BarFeed(barfeed.BaseBarFeed):\n    def __init__(self, frequency, maxLen=None):\n        super(BarFeed, self).__init__(frequency, maxLen)\n\n        self.__bars = {}\n        self.__nextPos = {}\n        self.__started = False\n        self.__currDateTime = None\n\n    def reset(self):\n        self.__nextPos = {}\n        for instrument in self.__bars.keys():\n            self.__nextPos.setdefault(instrument, 0)\n        self.__currDateTime = None\n        super(BarFeed, self).reset()\n\n    def getCurrentDateTime(self):\n        return self.__currDateTime\n\n    def start(self):\n        super(BarFeed, self).start()\n        self.__started = True\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def addBarsFromSequence(self, instrument, bars):\n        if self.__started:\n            raise Exception(\"Can't add more bars once you started consuming bars\")\n\n        self.__bars.setdefault(instrument, [])\n        self.__nextPos.setdefault(instrument, 0)\n\n        # Add and sort the bars\n        self.__bars[instrument].extend(bars)\n        barCmp = lambda x, y: cmp(x.getDateTime(), y.getDateTime())\n        self.__bars[instrument].sort(barCmp)\n\n        self.registerInstrument(instrument)\n\n    def eof(self):\n        ret = True\n        # Check if there is at least one more bar to return.\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars):\n                ret = False\n                break\n        return ret\n\n    def peekDateTime(self):\n        ret = None\n\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars):\n                ret = utils.safe_min(ret, bars[nextPos].getDateTime())\n        return ret\n\n    def getNextBars(self):\n        # All bars must have the same datetime. We will return all the ones with the smallest datetime.\n        smallestDateTime = self.peekDateTime()\n\n        if smallestDateTime is None:\n            return None\n\n        # Make a second pass to get all the bars that had the smallest datetime.\n        ret = {}\n        for instrument, bars in self.__bars.iteritems():\n            nextPos = self.__nextPos[instrument]\n            if nextPos < len(bars) and bars[nextPos].getDateTime() == smallestDateTime:\n                ret[instrument] = bars[nextPos]\n                self.__nextPos[instrument] += 1\n\n        if self.__currDateTime == smallestDateTime:\n            raise Exception(\"Duplicate bars found for %s on %s\" % (ret.keys(), smallestDateTime))\n\n        self.__currDateTime = smallestDateTime\n        return bar.Bars(ret)\n\n    def loadAll(self):\n        for dateTime, bars in self:\n            pass\n",
          "# PyAlgoTrade\n#\n# Copyright 2011-2015 Gabriel Martin Becedillas Ruiz\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\n.. moduleauthor:: Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\"\"\"\n\nfrom pyalgotrade import feed\nfrom pyalgotrade import dataseries\n\n\nclass MemFeed(feed.BaseFeed):\n    def __init__(self, maxLen=None):\n        super(MemFeed, self).__init__(maxLen)\n\n        self.__values = []\n        self.__nextIdx = 0\n\n    def reset(self):\n        self.__nextIdx = 0\n        feed.BaseFeed.reset(self)\n\n    def start(self):\n        super(MemFeed, self).start()\n        # Now that all the data is in place, sort it to dispatch it in order.\n        cmpFun = lambda x, y: cmp(x[0], y[0])\n        self.__values.sort(cmpFun)\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def eof(self):\n        if self.__nextIdx < len(self.__values):\n            return False\n        else:\n            return True\n\n    def peekDateTime(self):\n        ret = None\n        if self.__nextIdx < len(self.__values):\n            ret = self.__values[self.__nextIdx][0]\n        return ret\n\n    def createDataSeries(self, key, maxLen):\n        return dataseries.SequenceDataSeries(maxLen)\n\n    def getNextValues(self):\n        ret = (None, None)\n        if self.__nextIdx < len(self.__values):\n            ret = self.__values[self.__nextIdx]\n            self.__nextIdx += 1\n        return ret\n\n    # Add values to the feed. values should be a sequence of tuples. The tuples should have two elements:\n    # 1: datetime.datetime.\n    # 2: dictionary or dict-like object.\n    def addValues(self, values):\n        # Register a dataseries for each item.\n        for key in values[0][1].keys():\n            self.registerDataSeries(key)\n\n        self.__values.extend(values)\n",
          "FROM gbecedillas/pyalgotrade:0.18\nMAINTAINER Gabriel Martin Becedillas Ruiz <gabriel.becedillas@gmail.com>\n\nRUN pip install nose\nRUN pip install nose-cov\n\nRUN mkdir /tmp/pyalgotrade\n\n# Files needed to execute testcases.\nCOPY travis/run_tests.sh /tmp/pyalgotrade/\nCOPY coverage.cfg /tmp/pyalgotrade/\nCOPY setup.cfg /tmp/pyalgotrade/\nCOPY testcases /tmp/pyalgotrade/testcases\nCOPY samples /tmp/pyalgotrade/samples\n\n# We need to upgrade the installed version with the one checked out from GIT.\nCOPY setup.py /tmp/pyalgotrade/\nCOPY pyalgotrade /tmp/pyalgotrade/pyalgotrade\nRUN pip install -U /tmp/pyalgotrade\n",
          "#!/bin/sh\n\nexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n# This is needed to avoid \"Coverage.py warning: No data was collected\" from cov plugin.\nexport PYTHONPATH=.\n\nnosetests --with-cov --cov=pyalgotrade --cov-config=coverage.cfg\n",
          "",
          "*pyc\nMANIFEST\ndoc/_build\n.idea\n.coverage\n.ipynb_checkpoints\n.ropeproject\n\ndata\ndist\nbuild\nPyAlgoTrade.egg-info\n\ntestcases/twitter_credentials.py\n",
          "#!/bin/sh\n\nexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n# This is needed to avoid \"Coverage.py warning: No data was collected\" from cov plugin.\nexport PYTHONPATH=.\n\nnosetests --with-cov --cov=pyalgotrade --cov-config=coverage.cfg\n"
        ],
        "test_patch": "",
        "patch_preview": "From 245c8d05df66b9f9d7d07fec5e11f1cdb7b6d342 Mon Sep 17 00:00:00 2001\nFrom: Glenn <glenn+snow_flake@mercury-wireless.com>\nDate: Wed, 11 Oct 2017 20:43:54 -0400\nSubject: [PATCH 1/8] Upgraded to python 3.6\n\n---\n .gitignore                                  |   1 +\n doc/conf.py                                 |  16 +-\n pyalgotrade/bar.py                          |  12 +-\n pyalgotrade/barfeed/csvfeed.py              |   2 +-\n pyalgotrade/barfeed/membf.py                |  10 +-\n pyalgotrade/barfeed/"
      },
      "patch": {
        "length": 245619,
        "files_changed": 133,
        "lines_added": 857,
        "lines_deleted": 892,
        "net_change": -35,
        "changed_files": [
          {
            "file": ".gitignore",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "doc/conf.py",
            "added": 8,
            "deleted": 8
          },
          {
            "file": "pyalgotrade/bar.py",
            "added": 5,
            "deleted": 7
          },
          {
            "file": "pyalgotrade/barfeed/csvfeed.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "pyalgotrade/barfeed/membf.py",
            "added": 5,
            "deleted": 5
          },
          {
            "file": "pyalgotrade/barfeed/resampled.py",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "pyalgotrade/bitstamp/httpclient.py",
            "added": 1,
            "deleted": 3
          },
          {
            "file": "pyalgotrade/bitstamp/livebroker.py",
            "added": 6,
            "deleted": 6
          },
          {
            "file": "pyalgotrade/bitstamp/livefeed.py",
            "added": 4,
            "deleted": 4
          },
          {
            "file": "pyalgotrade/bitstamp/wsclient.py",
            "added": 4,
            "deleted": 4
          },
          {
            "file": "pyalgotrade/broker/__init__.py",
            "added": 2,
            "deleted": 7
          },
          {
            "file": "pyalgotrade/broker/backtesting.py",
            "added": 7,
            "deleted": 9
          },
          {
            "file": "pyalgotrade/broker/fillstrategy.py",
            "added": 2,
            "deleted": 4
          },
          {
            "file": "pyalgotrade/broker/slippage.py",
            "added": 1,
            "deleted": 3
          },
          {
            "file": "pyalgotrade/dataseries/__init__.py",
            "added": 2,
            "deleted": 4
          },
          {
            "file": "pyalgotrade/dataseries/bards.py",
            "added": 1,
            "deleted": 1
          },
          {
            "file": "pyalgotrade/dataseries/resampled.py",
            "added": 1,
            "deleted": 3
          },
          {
            "file": "pyalgotrade/eventprofiler.py",
            "added": 4,
            "deleted": 4
          },
          {
            "file": "pyalgotrade/feed/__init__.py",
            "added": 3,
            "deleted": 3
          },
          {
            "file": "pyalgotrade/feed/csvfeed.py",
            "added": 3,
            "deleted": 10
          }
        ]
      },
      "issue_comments": [
        {
          "id": 406931001,
          "body": "I'm working on supporting both Python2.7 and Python3 in feature/python3.",
          "user": "gbeced",
          "created_at": "2018-07-23T03:45:10Z",
          "html_url": "https://github.com/gbeced/pyalgotrade/pull/114#issuecomment-406931001"
        },
        {
          "id": 336030940,
          "body": "Related issue https://github.com/gbeced/pyalgotrade/issues/45",
          "user": "femtotrader",
          "created_at": "2017-10-12T06:12:49Z",
          "html_url": "https://github.com/gbeced/pyalgotrade/pull/114#issuecomment-336030940"
        }
      ],
      "issue_comments_count": 2,
      "code_statistics": {
        "total_files": 384,
        "total_lines": 647926,
        "total_bytes": 27318364,
        "python_files": 190,
        "python_lines": 28756,
        "file_extensions": {
          ".txt": 3,
          "": 5,
          ".sh": 4,
          ".cfg": 2,
          ".in": 1,
          ".py": 190,
          ".md": 1,
          ".png": 13,
          ".output": 18,
          ".csv": 104,
          ".sqlite": 1,
          ".rst": 37,
          ".html": 1,
          ".xml": 4
        },
        "largest_files": [
          {
            "path": "samples/data/trades-mtgox-usd-2013-03.csv",
            "size": 11750164,
            "lines": 324878,
            "extension": ".csv"
          },
          {
            "path": "testcases/data/bitstampUSD-2.csv",
            "size": 8604620,
            "lines": 200000,
            "extension": ".csv"
          },
          {
            "path": "testcases/data/nt-spy-minute-2011-03.csv",
            "size": 871830,
            "lines": 17577,
            "extension": ".csv"
          },
          {
            "path": "testcases/data/nt-spy-minute-2011.csv",
            "size": 717440,
            "lines": 14180,
            "extension": ".csv"
          },
          {
            "path": "testcases/data/bitstampUSD.csv",
            "size": 411399,
            "lines": 9999,
            "extension": ".csv"
          },
          {
            "path": "tools/symbols/nyse.xml",
            "size": 395222,
            "lines": 3274,
            "extension": ".xml"
          },
          {
            "path": "testcases/data/multiinstrument.sqlite",
            "size": 118784,
            "lines": 3025,
            "extension": ".sqlite"
          },
          {
            "path": "tools/symbols/nasdaq.xml",
            "size": 347418,
            "lines": 2758,
            "extension": ".xml"
          },
          {
            "path": "testcases/broker_backtesting_test.py",
            "size": 127243,
            "lines": 2599,
            "extension": ".py"
          },
          {
            "path": "samples/data/quandl_gold_2.csv",
            "size": 66661,
            "lines": 2375,
            "extension": ".csv"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 384,
        "files_changed_count": 133,
        "files_changed_ratio": 0.3463541666666667,
        "total_lines_in_repo": 647926,
        "lines_added": 857,
        "lines_deleted": 892,
        "net_lines_changed": -35,
        "lines_changed_ratio": 0.002699382336871803,
        "pr_body_length": 0,
        "commit_message_length": 22,
        "python_file_count": 190,
        "python_line_count": 28756
      }
    },
    {
      "tar_file_name": "havakv#pycox#pull#71",
      "repo_name": "havakv#pycox#pull#71",
      "success": true,
      "error": null,
      "commit": {
        "sha": "7ed3e02faaeeced885ac15a1ba2dd255c9b6f8ec",
        "message": "Torch lightning example for MLP logistic hazard model (#66)\n\n* torch lightning example for MLP logistic hazard model\r\n\r\n* Suggested PEP8 style edits, cleanup for PR #66. Added line to requirements.txt",
        "author": {
          "name": "Rohan Shad",
          "email": "rohan.shad@gmail.com",
          "date": "2021-02-01T14:51:38Z"
        },
        "html_url": "https://github.com/havakv/pycox/commit/7ed3e02faaeeced885ac15a1ba2dd255c9b6f8ec",
        "api_url": "https://api.github.com/repos/havakv/pycox/commits/7ed3e02faaeeced885ac15a1ba2dd255c9b6f8ec"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/havakv#pycox#pull#71",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/havakv#pycox#pull#71.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/havakv#pycox#pull#71/source_code"
      },
      "pr": {
        "number": 71,
        "title": "New coxph with less emphasis on torchtuples",
        "body": "This versions enables fitting models without using the CoxPH method.",
        "state": "closed",
        "created_at": "2021-02-06T20:00:28Z",
        "updated_at": "2021-02-06T20:23:10Z",
        "merged_at": "2021-02-06T20:23:02Z",
        "html_url": "https://github.com/havakv/pycox/pull/71",
        "user": "havakv",
        "additions": 194,
        "deletions": 0,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "havakv_pycox-71",
        "repo": "/havakv/pycox",
        "base_commit": "7ed3e02faaeeced885ac15a1ba2dd255c9b6f8ec",
        "problem_statement": {},
        "edit_files": [
          "pycox/models/coxph.py"
        ],
        "oracle_files": [
          ""
        ],
        "test_patch": "",
        "patch_preview": "From 4a2c6862790d230424bcbd08b03c4ef826306285 Mon Sep 17 00:00:00 2001\nFrom: Haavard Kvamme <haavard.kvamme@abelee.com>\nDate: Sat, 6 Feb 2021 20:58:57 +0100\nSubject: [PATCH] New coxph with less emphasis on torchtuples\n\n---\n pycox/models/coxph.py | 194 ++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 194 insertions(+)\n create mode 100644 pycox/models/coxph.py\n\ndiff --git a/pycox/models/coxph.py b/pycox/models/coxph.py\nnew file mode 100644\nindex 0000000..62a179c\n--- /dev/null\n+++ b/pyco"
      },
      "patch": {
        "length": 8101,
        "files_changed": 1,
        "lines_added": 194,
        "lines_deleted": 0,
        "net_change": 194,
        "changed_files": [
          {
            "file": "pycox/models/coxph.py",
            "added": 194,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 75,
        "total_lines": 19019,
        "total_bytes": 2113335,
        "python_files": 56,
        "python_lines": 7003,
        "file_extensions": {
          ".txt": 1,
          "": 1,
          ".cfg": 1,
          ".py": 56,
          ".md": 2,
          ".svg": 1,
          ".ipynb": 13
        },
        "largest_files": [
          {
            "path": "examples/02_introduction.ipynb",
            "size": 208568,
            "lines": 1278,
            "extension": ".ipynb"
          },
          {
            "path": "examples/03_network_architectures.ipynb",
            "size": 138109,
            "lines": 1218,
            "extension": ".ipynb"
          },
          {
            "path": "examples/04_mnist_dataloaders_cnn.ipynb",
            "size": 208801,
            "lines": 1084,
            "extension": ".ipynb"
          },
          {
            "path": "examples/01_introduction.ipynb",
            "size": 106601,
            "lines": 971,
            "extension": ".ipynb"
          },
          {
            "path": "examples/administrative_brier_score.ipynb",
            "size": 263641,
            "lines": 917,
            "extension": ".ipynb"
          },
          {
            "path": "examples/pc-hazard.ipynb",
            "size": 118152,
            "lines": 822,
            "extension": ".ipynb"
          },
          {
            "path": "examples/deephit.ipynb",
            "size": 120749,
            "lines": 813,
            "extension": ".ipynb"
          },
          {
            "path": "pycox/datasets/from_kkbox.py",
            "size": 41717,
            "lines": 806,
            "extension": ".py"
          },
          {
            "path": "examples/mtlr.ipynb",
            "size": 113636,
            "lines": 803,
            "extension": ".ipynb"
          },
          {
            "path": "examples/pmf.ipynb",
            "size": 119563,
            "lines": 799,
            "extension": ".ipynb"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 75,
        "files_changed_count": 1,
        "files_changed_ratio": 0.013333333333333334,
        "total_lines_in_repo": 19019,
        "lines_added": 194,
        "lines_deleted": 0,
        "net_lines_changed": 194,
        "lines_changed_ratio": 0.010200325989799674,
        "pr_body_length": 68,
        "commit_message_length": 200,
        "python_file_count": 56,
        "python_line_count": 7003
      }
    },
    {
      "tar_file_name": "hiyouga#LLaMA-Factory#pull#8432",
      "repo_name": "hiyouga#LLaMA-Factory#pull#8432",
      "success": true,
      "error": null,
      "commit": {
        "sha": "1221533542f1aedde5af9df04f8754ba2759b6ab",
        "message": "[model] unsloth resume from checkpoint bug (#8423)\n\nCo-authored-by: viyer <vivek_iyer2@apple.com>",
        "author": {
          "name": "Vivek Iyer",
          "email": "vivekbalasundaram@gmail.com",
          "date": "2025-06-23T08:43:54Z"
        },
        "html_url": "https://github.com/hiyouga/LLaMA-Factory/commit/1221533542f1aedde5af9df04f8754ba2759b6ab",
        "api_url": "https://api.github.com/repos/hiyouga/LLaMA-Factory/commits/1221533542f1aedde5af9df04f8754ba2759b6ab"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/hiyouga#LLaMA-Factory#pull#8432",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/hiyouga#LLaMA-Factory#pull#8432.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/hiyouga#LLaMA-Factory#pull#8432/source_code"
      },
      "pr": {
        "number": 8432,
        "title": "[model] add kimi vl 2506",
        "body": "# What does this PR do?\r\n\r\nFixes # (issue)\r\n\r\n## Before submitting\r\n\r\n- [x] Did you read the [contributor guideline](https://github.com/hiyouga/LLaMA-Factory/blob/main/.github/CONTRIBUTING.md)?\r\n- [ ] Did you write any new necessary tests?\r\n",
        "state": "closed",
        "created_at": "2025-06-23T09:45:26Z",
        "updated_at": "2025-06-23T10:20:30Z",
        "merged_at": "2025-06-23T09:56:49Z",
        "html_url": "https://github.com/hiyouga/LLaMA-Factory/pull/8432",
        "user": "hiyouga",
        "additions": 36,
        "deletions": 30,
        "changed_files": 4,
        "commits": 1
      },
      "swebench": {
        "instance_id": "hiyouga_LLaMA-Factory-8432",
        "repo": "/hiyouga/LLaMA-Factory",
        "base_commit": "1221533542f1aedde5af9df04f8754ba2759b6ab",
        "problem_statement": {},
        "edit_files": [
          "README_zh.md",
          "src/llamafactory/data/template.py",
          "src/llamafactory/extras/constants.py",
          "src/llamafactory/model/model_utils/unsloth.py"
        ],
        "oracle_files": [
          "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-614-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Alaya](assets/alaya_new.svg)](https://docs.alayanew.com/docs/documents/newActivities/llamafactory/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ðŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### èŽ·å¾—[äºšé©¬é€Š](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)ã€[è‹±ä¼Ÿè¾¾](https://developer.nvidia.cn/rtx/ai-toolkit)ã€[é˜¿é‡Œäº‘](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory)ç­‰çš„åº”ç”¨ã€‚\n\n<div align=\"center\" markdown=\"1\">\n\n### èµžåŠ©å•† â¤ï¸\n\n<a href=\"https://warp.dev/llama-factory\">\n    <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae\">\n</a>\n\n#### [Warpï¼Œé¢å‘å¼€å‘è€…çš„æ™ºèƒ½ç»ˆç«¯](https://warp.dev/llama-factory)\n\n[é€‚ç”¨äºŽ MacOSã€Linux å’Œ Windows](https://warp.dev/llama-factory)\n\n----\n\n### ä½¿ç”¨é›¶ä»£ç [å‘½ä»¤è¡Œ](#å¿«é€Ÿå¼€å§‹)ä¸Ž [Web UI](#llama-board-å¯è§†åŒ–å¾®è°ƒç”±-gradio-é©±åŠ¨) è½»æ¾å¾®è°ƒç™¾ä½™ç§å¤§æ¨¡åž‹\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nðŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤](assets/wechat.jpg)ã€[NPU ç”¨æˆ·ç¾¤](assets/wechat_npu.jpg)æˆ– [ä¹ç« æ™ºç®—äº‘ç®—åŠ›ä¼˜æƒ ç¾¤](assets/wechat_alaya.png)ã€‚\n\n\\[ [English](README.md) | ä¸­æ–‡ \\]\n\n**å¾®è°ƒå¤§æ¨¡åž‹å¯ä»¥åƒè¿™æ ·è½»æ¾â€¦**\n\nhttps://github.com/user-attachments/assets/43b700c6-a178-41db-b1f8-8190a5d3fcfc\n\né€‰æ‹©ä½ çš„æ‰“å¼€æ–¹å¼ï¼š\n\n- **å…¥é—¨æ•™ç¨‹**ï¼šhttps://zhuanlan.zhihu.com/p/695287607\n- **æ¡†æž¶æ–‡æ¡£**ï¼šhttps://llamafactory.readthedocs.io/zh-cn/latest/\n- **æ¡†æž¶æ–‡æ¡£ï¼ˆæ˜‡è…¾ NPUï¼‰**ï¼šhttps://ascend.github.io/docs/sources/llamafactory/\n- **Colabï¼ˆå…è´¹ï¼‰**ï¼šhttps://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing\n- **æœ¬åœ°æœºå™¨**ï¼šè¯·è§[å¦‚ä½•ä½¿ç”¨](#å¦‚ä½•ä½¿ç”¨)\n- **PAI-DSWï¼ˆå…è´¹è¯•ç”¨ï¼‰**ï¼šhttps://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **ä¹ç« æ™ºç®—äº‘ï¼ˆç®—åŠ›ä¼˜æƒ æ´»åŠ¨ï¼‰**ï¼šhttps://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\n> [!NOTE]\n> é™¤ä¸Šè¿°é“¾æŽ¥ä»¥å¤–çš„å…¶ä»–ç½‘ç«™å‡ä¸ºæœªç»è®¸å¯çš„ç¬¬ä¸‰æ–¹ç½‘ç«™ï¼Œè¯·å°å¿ƒç”„åˆ«ã€‚\n\n## ç›®å½•\n\n- [é¡¹ç›®ç‰¹è‰²](#é¡¹ç›®ç‰¹è‰²)\n- [å®˜æ–¹åšå®¢](#å®˜æ–¹åšå®¢)\n- [æ›´æ–°æ—¥å¿—](#æ›´æ–°æ—¥å¿—)\n- [æ¨¡åž‹](#æ¨¡åž‹)\n- [è®­ç»ƒæ–¹æ³•](#è®­ç»ƒæ–¹æ³•)\n- [æ•°æ®é›†](#æ•°æ®é›†)\n- [è½¯ç¡¬ä»¶ä¾èµ–](#è½¯ç¡¬ä»¶ä¾èµ–)\n- [å¦‚ä½•ä½¿ç”¨](#å¦‚ä½•ä½¿ç”¨)\n  - [å®‰è£… LLaMA Factory](#å®‰è£…-llama-factory)\n  - [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)\n  - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\n  - [LLaMA Board å¯è§†åŒ–å¾®è°ƒ](#llama-board-å¯è§†åŒ–å¾®è°ƒç”±-gradio-é©±åŠ¨)\n  - [æž„å»º Docker](#æž„å»º-docker)\n  - [åˆ©ç”¨ vLLM éƒ¨ç½² OpenAI API](#åˆ©ç”¨-vllm-éƒ¨ç½²-openai-api)\n  - [ä»Žé­”æ­ç¤¾åŒºä¸‹è½½](#ä»Žé­”æ­ç¤¾åŒºä¸‹è½½)\n  - [ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½](#ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½)\n  - [ä½¿ç”¨ W&B é¢æ¿](#ä½¿ç”¨-wb-é¢æ¿)\n  - [ä½¿ç”¨ SwanLab é¢æ¿](#ä½¿ç”¨-swanlab-é¢æ¿)\n- [ä½¿ç”¨äº† LLaMA Factory çš„é¡¹ç›®](#ä½¿ç”¨äº†-llama-factory-çš„é¡¹ç›®)\n- [åè®®](#åè®®)\n- [å¼•ç”¨](#å¼•ç”¨)\n- [è‡´è°¢](#è‡´è°¢)\n\n## é¡¹ç›®ç‰¹è‰²\n\n- **å¤šç§æ¨¡åž‹**ï¼šLLaMAã€LLaVAã€Mistralã€Mixtral-MoEã€Qwenã€Qwen2-VLã€DeepSeekã€Yiã€Gemmaã€ChatGLMã€Phi ç­‰ç­‰ã€‚\n- **é›†æˆæ–¹æ³•**ï¼šï¼ˆå¢žé‡ï¼‰é¢„è®­ç»ƒã€ï¼ˆå¤šæ¨¡æ€ï¼‰æŒ‡ä»¤ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡åž‹è®­ç»ƒã€PPO è®­ç»ƒã€DPO è®­ç»ƒã€KTO è®­ç»ƒã€ORPO è®­ç»ƒç­‰ç­‰ã€‚\n- **å¤šç§ç²¾åº¦**ï¼š16 æ¯”ç‰¹å…¨å‚æ•°å¾®è°ƒã€å†»ç»“å¾®è°ƒã€LoRA å¾®è°ƒå’ŒåŸºäºŽ AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ çš„ 2/3/4/5/6/8 æ¯”ç‰¹ QLoRA å¾®è°ƒã€‚\n- **å…ˆè¿›ç®—æ³•**ï¼š[GaLore](https://github.com/jiaweizzhao/GaLore)ã€[BAdam](https://github.com/Ledzy/BAdam)ã€[APOLLO](https://github.com/zhuhanqing/APOLLO)ã€[Adam-mini](https://github.com/zyushun/Adam-mini)ã€[Muon](https://github.com/KellerJordan/Muon)ã€DoRAã€LongLoRAã€LLaMA Proã€Mixture-of-Depthsã€LoRA+ã€LoftQ å’Œ PiSSAã€‚\n- **å®žç”¨æŠ€å·§**ï¼š[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)ã€[Unsloth](https://github.com/unslothai/unsloth)ã€[Liger Kernel](https://github.com/linkedin/Liger-Kernel)ã€RoPE scalingã€NEFTune å’Œ rsLoRAã€‚\n- **å¹¿æ³›ä»»åŠ¡**ï¼šå¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ã€å›¾åƒç†è§£ã€è§†è§‰å®šä½ã€è§†é¢‘è¯†åˆ«å’Œè¯­éŸ³ç†è§£ç­‰ç­‰ã€‚\n- **å®žéªŒç›‘æŽ§**ï¼šLlamaBoardã€TensorBoardã€Wandbã€MLflowã€[SwanLab](https://github.com/SwanHubX/SwanLab) ç­‰ç­‰ã€‚\n- **æžé€ŸæŽ¨ç†**ï¼šåŸºäºŽ [vLLM](https://github.com/vllm-project/vllm) æˆ– [SGLang](https://github.com/sgl-project/sglang) çš„ OpenAI é£Žæ ¼ APIã€æµè§ˆå™¨ç•Œé¢å’Œå‘½ä»¤è¡ŒæŽ¥å£ã€‚\n\n### æœ€æ–°æ¨¡åž‹çš„ Day-N å¾®è°ƒé€‚é…\n\n| é€‚é…æ—¶é—´      | æ¨¡åž‹åç§°                                                       |\n| ------------ | ------------------------------------------------------------ |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6    |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4       |\n\n## å®˜æ–¹åšå®¢\n\n- [åŸºäºŽ LLaMA-Factory å’Œ EasyR1 æ‰“é€ ä¸€ç«™å¼æ— ä»£ç å¤§æ¨¡åž‹å¼ºåŒ–å­¦ä¹ å’Œéƒ¨ç½²å¹³å° LLM Model Hub](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/)ï¼ˆä¸­æ–‡ï¼‰\n- [ä½¿ç”¨ LLaMA-Factory å¾®è°ƒ Qwen2.5-VL å®žçŽ°è‡ªåŠ¨é©¾é©¶åœºæ™¯å¾®è°ƒ](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory)ï¼ˆä¸­æ–‡ï¼‰\n- [é€šè¿‡äºšé©¬é€Š SageMaker HyperPod ä¸Šçš„ LLaMA-Factory å¢žå¼ºå¤šæ¨¡æ€æ¨¡åž‹é“¶è¡Œæ–‡æ¡£çš„è§†è§‰ä¿¡æ¯æå–](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/)ï¼ˆè‹±æ–‡ï¼‰\n- [Easy Dataset Ã— LLaMA Factory: è®©å¤§æ¨¡åž‹é«˜æ•ˆå­¦ä¹ é¢†åŸŸçŸ¥è¯†](https://buaa-act.feishu.cn/wiki/KY9xwTGs1iqHrRkjXBwcZP9WnL9)ï¼ˆä¸­æ–‡ï¼‰\n\n<details><summary>å…¨éƒ¨åšå®¢</summary>\n\n- [LLaMA Factoryï¼šå¾®è°ƒ DeepSeek-R1-Distill-Qwen-7B æ¨¡åž‹å®žçŽ°æ–°é—»æ ‡é¢˜åˆ†ç±»å™¨](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b)ï¼ˆä¸­æ–‡ï¼‰\n- [åŸºäºŽ Amazon SageMaker å’Œ LLaMA-Factory æ‰“é€ ä¸€ç«™å¼æ— ä»£ç æ¨¡åž‹å¾®è°ƒéƒ¨ç½²å¹³å° Model Hub](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)ï¼ˆä¸­æ–‡ï¼‰\n- [LLaMA Factory å¤šæ¨¡æ€å¾®è°ƒå®žè·µï¼šå¾®è°ƒ Qwen2-VL æž„å»ºæ–‡æ—…å¤§æ¨¡åž‹](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl)ï¼ˆä¸­æ–‡ï¼‰\n- [LLaMA Factoryï¼šå¾®è°ƒLLaMA3æ¨¡åž‹å®žçŽ°è§’è‰²æ‰®æ¼”](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)ï¼ˆä¸­æ–‡ï¼‰\n\n</details>\n\n## æ›´æ–°æ—¥å¿—\n\n[25/04/28] æˆ‘ä»¬æ”¯æŒäº† **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** ç³»åˆ—æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[25/04/21] æˆ‘ä»¬æ”¯æŒäº† **[Muon](https://github.com/KellerJordan/Muon)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚æ„Ÿè°¢ [@tianshijing](https://github.com/tianshijing) çš„ PRã€‚\n\n[25/04/16] æˆ‘ä»¬æ”¯æŒäº† **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** æ¨¡åž‹çš„å¾®è°ƒã€‚æŸ¥çœ‹ [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) ä»¥ä½¿ç”¨ã€‚\n\n[25/04/14] æˆ‘ä»¬æ”¯æŒäº† **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** å’Œ **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[25/04/06] æˆ‘ä»¬æ”¯æŒäº† **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** æ¨¡åž‹çš„å¾®è°ƒã€‚æŸ¥çœ‹ [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) ä»¥ä½¿ç”¨ã€‚\n\n<details><summary>å±•å¼€æ—¥å¿—</summary>\n\n[25/03/31] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** æ¨¡åž‹çš„å¾®è°ƒã€‚æŸ¥çœ‹ [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) ä»¥ä½¿ç”¨ã€‚\n\n[25/03/15] æˆ‘ä»¬æ”¯æŒäº† **[SGLang](https://github.com/sgl-project/sglang)** æŽ¨ç†åŽç«¯ï¼Œè¯·ä½¿ç”¨ `infer_backend: sglang` å¯ç”¨ã€‚\n\n[25/03/12] æˆ‘ä»¬æ”¯æŒäº† **[Gemma 3](https://huggingface.co/blog/gemma3)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[25/02/24] æˆ‘ä»¬å®£å¸ƒå¼€æº **[EasyR1](https://github.com/hiyouga/EasyR1)**ï¼Œä¸€ä¸ªé«˜æ•ˆå¯æ‰©å±•çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œæ”¯æŒé«˜æ•ˆçš„ GRPO è®­ç»ƒã€‚\n\n[25/02/11] æˆ‘ä»¬æ”¯æŒäº†åœ¨å¯¼å‡ºæ¨¡åž‹æ—¶ä¿å­˜ **[Ollama](https://github.com/ollama/ollama)** é…ç½®æ–‡ä»¶ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[25/02/05] æˆ‘ä»¬æ”¯æŒäº†åœ¨è¯­éŸ³ç†è§£ä»»åŠ¡ä¸Šå¾®è°ƒ **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** å’Œ **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** æ¨¡åž‹ã€‚\n\n[25/01/31] æˆ‘ä»¬æ”¯æŒäº† **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** å’Œ **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[25/01/15] æˆ‘ä»¬æ”¯æŒäº† **[APOLLO](https://arxiv.org/abs/2412.05270)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[25/01/14] æˆ‘ä»¬æ”¯æŒäº† **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** å’Œ **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** æ¨¡åž‹çš„å¾®è°ƒã€‚ æ„Ÿè°¢ [@BUAADreamer](https://github.com/BUAADreamer) çš„ PR.\n\n[25/01/14] æˆ‘ä»¬æ”¯æŒäº† **[InternLM 3](https://huggingface.co/collections/internlm/)** æ¨¡åž‹çš„å¾®è°ƒã€‚æ„Ÿè°¢ [@hhaAndroid](https://github.com/hhaAndroid) çš„ PRã€‚\n\n[25/01/10] æˆ‘ä»¬æ”¯æŒäº† **[Phi-4](https://huggingface.co/microsoft/phi-4)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/12/21] æˆ‘ä»¬æ”¯æŒäº†ä½¿ç”¨ **[SwanLab](https://github.com/SwanHubX/SwanLab)** è·Ÿè¸ªä¸Žå¯è§†åŒ–å®žéªŒã€‚è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [æ­¤éƒ¨åˆ†](#ä½¿ç”¨-swanlab-é¢æ¿)ã€‚\n\n[24/11/27] æˆ‘ä»¬æ”¯æŒäº† **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** æ¨¡åž‹çš„å¾®è°ƒå’Œ **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** æ•°æ®é›†ã€‚\n\n[24/10/09] æˆ‘ä»¬æ”¯æŒäº†ä»Ž **[é­”ä¹ç¤¾åŒº](https://modelers.cn/models)** ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹å’Œæ•°æ®é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [æ­¤æ•™ç¨‹](#ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½)ã€‚\n\n[24/09/19] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/08/30] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** æ¨¡åž‹çš„å¾®è°ƒã€‚æ„Ÿè°¢ [@simonJJJ](https://github.com/simonJJJ) çš„ PRã€‚\n\n[24/08/27] æˆ‘ä»¬æ”¯æŒäº† **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**ã€‚è¯·ä½¿ç”¨ `enable_liger_kernel: true` æ¥åŠ é€Ÿè®­ç»ƒã€‚\n\n[24/08/09] æˆ‘ä»¬æ”¯æŒäº† **[Adam-mini](https://github.com/zyushun/Adam-mini)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚æ„Ÿè°¢ [@relic-yuexi](https://github.com/relic-yuexi) çš„ PRã€‚\n\n[24/07/04] æˆ‘ä»¬æ”¯æŒäº†[æ— æ±¡æŸ“æ‰“åŒ…è®­ç»ƒ](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)ã€‚è¯·ä½¿ç”¨ `neat_packing: true` å‚æ•°ã€‚æ„Ÿè°¢ [@chuan298](https://github.com/chuan298) çš„ PRã€‚\n\n[24/06/16] æˆ‘ä»¬æ”¯æŒäº† **[PiSSA](https://arxiv.org/abs/2404.02948)** ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/06/07] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** å’Œ **[GLM-4](https://github.com/THUDM/GLM-4)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/05/26] æˆ‘ä»¬æ”¯æŒäº† **[SimPO](https://arxiv.org/abs/2405.14734)** åå¥½å¯¹é½ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/05/20] æˆ‘ä»¬æ”¯æŒäº† **PaliGemma** ç³»åˆ—æ¨¡åž‹çš„å¾®è°ƒã€‚æ³¨æ„ PaliGemma æ˜¯é¢„è®­ç»ƒæ¨¡åž‹ï¼Œä½ éœ€è¦ä½¿ç”¨ `paligemma` æ¨¡æ¿è¿›è¡Œå¾®è°ƒä½¿å…¶èŽ·å¾—å¯¹è¯èƒ½åŠ›ã€‚\n\n[24/05/18] æˆ‘ä»¬æ”¯æŒäº† **[KTO](https://arxiv.org/abs/2402.01306)** åå¥½å¯¹é½ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/05/14] æˆ‘ä»¬æ”¯æŒäº†æ˜‡è…¾ NPU è®¾å¤‡çš„è®­ç»ƒå’ŒæŽ¨ç†ã€‚è¯¦æƒ…è¯·æŸ¥é˜…[å®‰è£…](#å®‰è£…-llama-factory)éƒ¨åˆ†ã€‚\n\n[24/04/26] æˆ‘ä»¬æ”¯æŒäº†å¤šæ¨¡æ€æ¨¡åž‹ **LLaVA-1.5** çš„å¾®è°ƒã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/22] æˆ‘ä»¬æä¾›äº†åœ¨å…è´¹ T4 GPU ä¸Šå¾®è°ƒ Llama-3 æ¨¡åž‹çš„ **[Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)**ã€‚Hugging Face ç¤¾åŒºå…¬å¼€äº†ä¸¤ä¸ªåˆ©ç”¨ LLaMA Factory å¾®è°ƒçš„ Llama-3 æ¨¡åž‹ï¼Œè¯¦æƒ…è¯·è§ [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) å’Œ [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese)ã€‚\n\n[24/04/21] æˆ‘ä»¬åŸºäºŽ [AstraMindAI çš„ä»“åº“](https://github.com/astramind-ai/Mixture-of-depths)æ”¯æŒäº† **[æ··åˆæ·±åº¦è®­ç»ƒ](https://arxiv.org/abs/2404.02258)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/16] æˆ‘ä»¬æ”¯æŒäº† **[BAdam](https://arxiv.org/abs/2404.02827)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/16] æˆ‘ä»¬æ”¯æŒäº† **[unsloth](https://github.com/unslothai/unsloth)** çš„é•¿åºåˆ—è®­ç»ƒï¼ˆ24GB å¯è®­ç»ƒ Llama-2-7B-56kï¼‰ã€‚è¯¥æ–¹æ³•ç›¸æ¯” FlashAttention-2 æä¾›äº† **117%** çš„è®­ç»ƒé€Ÿåº¦å’Œ **50%** çš„æ˜¾å­˜èŠ‚çº¦ã€‚æ›´å¤šæ•°æ®è¯·è§[æ­¤é¡µé¢](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã€‚\n\n[24/03/31] æˆ‘ä»¬æ”¯æŒäº† **[ORPO](https://arxiv.org/abs/2403.07691)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/21] æˆ‘ä»¬çš„è®ºæ–‡ \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" å¯åœ¨ arXiv ä¸ŠæŸ¥çœ‹ï¼\n\n[24/03/20] æˆ‘ä»¬æ”¯æŒäº†èƒ½åœ¨ 2x24GB GPU ä¸Šå¾®è°ƒ 70B æ¨¡åž‹çš„ **FSDP+QLoRA**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/13] æˆ‘ä»¬æ”¯æŒäº† **[LoRA+](https://arxiv.org/abs/2402.12354)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/07] æˆ‘ä»¬æ”¯æŒäº† **[GaLore](https://arxiv.org/abs/2403.03507)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/07] æˆ‘ä»¬é›†æˆäº† **[vLLM](https://github.com/vllm-project/vllm)** ä»¥å®žçŽ°æžé€Ÿå¹¶å‘æŽ¨ç†ã€‚è¯·ä½¿ç”¨ `infer_backend: vllm` æ¥èŽ·å¾— **270%** çš„æŽ¨ç†é€Ÿåº¦ã€‚\n\n[24/02/28] æˆ‘ä»¬æ”¯æŒäº† **[DoRA](https://arxiv.org/abs/2402.09353)** å¾®è°ƒã€‚è¯·ä½¿ç”¨ `use_dora: true` å‚æ•°è¿›è¡Œ DoRA å¾®è°ƒã€‚\n\n[24/02/15] æˆ‘ä»¬æ”¯æŒäº† [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro) æå‡ºçš„**å—æ‰©å±•**æ–¹æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/02/05] Qwen1.5ï¼ˆQwen2 æµ‹è¯•ç‰ˆï¼‰ç³»åˆ—æ¨¡åž‹å·²åœ¨ LLaMA-Factory ä¸­å®žçŽ°å¾®è°ƒæ”¯æŒã€‚è¯¦æƒ…è¯·æŸ¥é˜…è¯¥[åšå®¢é¡µé¢](https://qwenlm.github.io/zh/blog/qwen1.5/)ã€‚\n\n[24/01/18] æˆ‘ä»¬é’ˆå¯¹ç»å¤§å¤šæ•°æ¨¡åž‹å®žçŽ°äº† **Agent å¾®è°ƒ**ï¼Œå¾®è°ƒæ—¶æŒ‡å®š `dataset: glaive_toolcall_zh` å³å¯ä½¿æ¨¡åž‹èŽ·å¾—å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚\n\n[23/12/23] æˆ‘ä»¬é’ˆå¯¹ LLaMA, Mistral å’Œ Yi æ¨¡åž‹æ”¯æŒäº† **[unsloth](https://github.com/unslothai/unsloth)** çš„ LoRA è®­ç»ƒåŠ é€Ÿã€‚è¯·ä½¿ç”¨ `use_unsloth: true` å‚æ•°å¯ç”¨ unsloth ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å¯æä¾› **170%** çš„è®­ç»ƒé€Ÿåº¦ï¼Œè¯¦æƒ…è¯·æŸ¥é˜…[æ­¤é¡µé¢](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã€‚\n\n[23/12/12] æˆ‘ä»¬æ”¯æŒäº†å¾®è°ƒæœ€æ–°çš„æ··åˆä¸“å®¶æ¨¡åž‹ **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**ã€‚ç¡¬ä»¶éœ€æ±‚è¯·æŸ¥é˜…[æ­¤å¤„](#ç¡¬ä»¶ä¾èµ–)ã€‚\n\n[23/12/01] æˆ‘ä»¬æ”¯æŒäº†ä»Ž **[é­”æ­ç¤¾åŒº](https://modelscope.cn/models)** ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹å’Œæ•°æ®é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [æ­¤æ•™ç¨‹](#ä»Žé­”æ­ç¤¾åŒºä¸‹è½½)ã€‚\n\n[23/10/21] æˆ‘ä»¬æ”¯æŒäº† **[NEFTune](https://arxiv.org/abs/2310.05914)** è®­ç»ƒæŠ€å·§ã€‚è¯·ä½¿ç”¨ `neftune_noise_alpha: 5` å‚æ•°å¯ç”¨ NEFTuneã€‚\n\n[23/09/27] æˆ‘ä»¬é’ˆå¯¹ LLaMA æ¨¡åž‹æ”¯æŒäº† [LongLoRA](https://github.com/dvlab-research/LongLoRA) æå‡ºçš„ **$S^2$-Attn**ã€‚è¯·ä½¿ç”¨ `shift_attn: true` å‚æ•°ä»¥å¯ç”¨è¯¥åŠŸèƒ½ã€‚\n\n[23/09/23] æˆ‘ä»¬åœ¨é¡¹ç›®ä¸­é›†æˆäº† MMLUã€C-Eval å’Œ CMMLU è¯„ä¼°é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[23/09/10] æˆ‘ä»¬æ”¯æŒäº† **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**ã€‚å¦‚æžœæ‚¨ä½¿ç”¨çš„æ˜¯ RTX4090ã€A100 æˆ– H100 GPUï¼Œè¯·ä½¿ç”¨ `flash_attn: fa2` å‚æ•°ä»¥å¯ç”¨ FlashAttention-2ã€‚\n\n[23/08/12] æˆ‘ä»¬æ”¯æŒäº† **RoPE æ’å€¼**æ¥æ‰©å±• LLaMA æ¨¡åž‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¯·ä½¿ç”¨ `rope_scaling: linear` å‚æ•°è®­ç»ƒæ¨¡åž‹æˆ–ä½¿ç”¨ `rope_scaling: dynamic` å‚æ•°è¯„ä¼°æ¨¡åž‹ã€‚\n\n[23/08/11] æˆ‘ä»¬æ”¯æŒäº†æŒ‡ä»¤æ¨¡åž‹çš„ **[DPO è®­ç»ƒ](https://arxiv.org/abs/2305.18290)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[23/07/31] æˆ‘ä»¬æ”¯æŒäº†**æ•°æ®æµå¼åŠ è½½**ã€‚è¯·ä½¿ç”¨ `streaming: true` å’Œ `max_steps: 10000` å‚æ•°æ¥æµå¼åŠ è½½æ•°æ®é›†ã€‚\n\n[23/07/29] æˆ‘ä»¬åœ¨ Hugging Face å‘å¸ƒäº†ä¸¤ä¸ª 13B æŒ‡ä»¤å¾®è°ƒæ¨¡åž‹ã€‚è¯¦ç»†å†…å®¹è¯·æŸ¥é˜…æˆ‘ä»¬çš„ Hugging Face é¡¹ç›®ï¼ˆ[LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)ï¼‰ã€‚\n\n[23/07/18] æˆ‘ä»¬å¼€å‘äº†æ”¯æŒè®­ç»ƒå’Œæµ‹è¯•çš„**æµè§ˆå™¨ä¸€ä½“åŒ–ç•Œé¢**ã€‚è¯·ä½¿ç”¨ `train_web.py` åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­å¾®è°ƒæ¨¡åž‹ã€‚æ„Ÿè°¢ [@KanadeSiina](https://github.com/KanadeSiina) å’Œ [@codemayq](https://github.com/codemayq) åœ¨è¯¥åŠŸèƒ½å¼€å‘ä¸­ä»˜å‡ºçš„åŠªåŠ›ã€‚\n\n[23/07/09] æˆ‘ä»¬å¼€æºäº† **[FastEdit](https://github.com/hiyouga/FastEdit)** âš¡ðŸ©¹ï¼Œä¸€ä¸ªç®€å•æ˜“ç”¨çš„ã€èƒ½è¿…é€Ÿç¼–è¾‘å¤§æ¨¡åž‹äº‹å®žè®°å¿†çš„å·¥å…·åŒ…ã€‚å¦‚æžœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [FastEdit](https://github.com/hiyouga/FastEdit) é¡¹ç›®ã€‚\n\n[23/06/29] æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª**å¯å¤çŽ°çš„**æŒ‡ä»¤æ¨¡åž‹å¾®è°ƒç¤ºä¾‹ï¼Œè¯¦ç»†å†…å®¹è¯·æŸ¥é˜… [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft)ã€‚\n\n[23/06/22] æˆ‘ä»¬å¯¹é½äº†[ç¤ºä¾‹ API](src/api_demo.py) ä¸Ž [OpenAI API](https://platform.openai.com/docs/api-reference/chat) çš„æ ¼å¼ï¼Œæ‚¨å¯ä»¥å°†å¾®è°ƒæ¨¡åž‹æŽ¥å…¥**ä»»æ„åŸºäºŽ ChatGPT çš„åº”ç”¨**ä¸­ã€‚\n\n[23/06/03] æˆ‘ä»¬å®žçŽ°äº† 4 æ¯”ç‰¹çš„ LoRA è®­ç»ƒï¼ˆä¹Ÿç§° **[QLoRA](https://github.com/artidoro/qlora)**ï¼‰ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n</details>\n\n> [!TIP]\n> å¦‚æžœæ‚¨æ— æ³•ä½¿ç”¨æœ€æ–°çš„åŠŸèƒ½ï¼Œè¯·å°è¯•é‡æ–°æ‹‰å–ä»£ç å¹¶å†æ¬¡å®‰è£… LLaMA-Factoryã€‚\n\n## æ¨¡åž‹\n\n| æ¨¡åž‹å                                                             | å‚æ•°é‡                            | Template            |\n| ----------------------------------------------------------------- | -------------------------------- | ------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2           |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                   |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3            |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere              |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek            |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3           |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1          |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon              |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma               |\n| [Gemma 3](https://huggingface.co/google)                          | 1B/4B/12B/27B                    | gemma3/gemma (1B)   |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/THUDM)           | 9B/32B                           | glm4/glmz1          |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                   |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3            |\n| [Hunyuan](https://huggingface.co/tencent/)                        | 7B                               | hunyuan             |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index               |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2             |\n| [InternVL 2.5-3](https://huggingface.co/OpenGVLab)                | 1B/2B/8B/14B/38B/78B             | intern_vl           |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl             |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                   |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2              |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3              |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4              |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama              |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava               |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next          |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video    |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                |\n| [MiniCPM](https://huggingface.co/openbmb)                         | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4       |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral           |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral             |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small       |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                   |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma           |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                   |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                 |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small           |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral             |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                |\n| [Qwen3 (MoE)](https://huggingface.co/Qwen)                        | 0.6B/1.7B/4B/8B/14B/32B/235B     | qwen3               |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio         |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni          |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl            |\n| [Seed Coder](https://huggingface.co/ByteDance-Seed)               | 8B                               | seed_coder          |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1          |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                   |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2           |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse              |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                  |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl               |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                |\n\n> [!NOTE]\n> å¯¹äºŽæ‰€æœ‰â€œåŸºåº§â€ï¼ˆBaseï¼‰æ¨¡åž‹ï¼Œ`template` å‚æ•°å¯ä»¥æ˜¯ `default`, `alpaca`, `vicuna` ç­‰ä»»æ„å€¼ã€‚ä½†â€œå¯¹è¯â€ï¼ˆInstruct/Chatï¼‰æ¨¡åž‹è¯·åŠ¡å¿…ä½¿ç”¨**å¯¹åº”çš„æ¨¡æ¿**ã€‚\n>\n> è¯·åŠ¡å¿…åœ¨è®­ç»ƒå’ŒæŽ¨ç†æ—¶é‡‡ç”¨**å®Œå…¨ä¸€è‡´**çš„æ¨¡æ¿ã€‚\n>\n> \\*ï¼šæ‚¨éœ€è¦ä»Ž main åˆ†æ”¯å®‰è£… `transformers` å¹¶ä½¿ç”¨ `DISABLE_VERSION_CHECK=1` æ¥è·³è¿‡ç‰ˆæœ¬æ£€æŸ¥ã€‚\n>\n> \\*\\*ï¼šæ‚¨éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ `transformers` ä»¥ä½¿ç”¨è¯¥æ¨¡åž‹ã€‚\n\né¡¹ç›®æ‰€æ”¯æŒæ¨¡åž‹çš„å®Œæ•´åˆ—è¡¨è¯·å‚é˜… [constants.py](src/llamafactory/extras/constants.py)ã€‚\n\næ‚¨ä¹Ÿå¯ä»¥åœ¨ [template.py](src/llamafactory/data/template.py) ä¸­æ·»åŠ è‡ªå·±çš„å¯¹è¯æ¨¡æ¿ã€‚\n\n## è®­ç»ƒæ–¹æ³•\n\n| æ–¹æ³•                   |     å…¨å‚æ•°è®­ç»ƒ      |    éƒ¨åˆ†å‚æ•°è®­ç»ƒ     |       LoRA         |       QLoRA        |\n| --------------------- | ------------------ | ------------------ | ------------------ | ------------------ |\n| é¢„è®­ç»ƒ                 | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| æŒ‡ä»¤ç›‘ç£å¾®è°ƒ            | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| å¥–åŠ±æ¨¡åž‹è®­ç»ƒ            | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO è®­ç»ƒ              | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO è®­ç»ƒ             | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> æœ‰å…³ PPO çš„å®žçŽ°ç»†èŠ‚ï¼Œè¯·å‚è€ƒ[æ­¤åšå®¢](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html)ã€‚\n\n## æ•°æ®é›†\n\n<details><summary>é¢„è®­ç»ƒæ•°æ®é›†</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>åå¥½æ•°æ®é›†</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\néƒ¨åˆ†æ•°æ®é›†çš„ä½¿ç”¨éœ€è¦ç¡®è®¤ï¼Œæˆ‘ä»¬æŽ¨èä½¿ç”¨ä¸‹è¿°å‘½ä»¤ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ã€‚\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## è½¯ç¡¬ä»¶ä¾èµ–\n\n| å¿…éœ€é¡¹        | è‡³å°‘     | æŽ¨è      |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.45.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| å¯é€‰é¡¹        | è‡³å°‘     | æŽ¨è      |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### ç¡¬ä»¶ä¾èµ–\n\n\\* *ä¼°ç®—å€¼*\n\n| æ–¹æ³•                             | ç²¾åº¦ |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## å¦‚ä½•ä½¿ç”¨\n\n### å®‰è£… LLaMA Factory\n\n> [!IMPORTANT]\n> æ­¤æ­¥éª¤ä¸ºå¿…éœ€ã€‚\n\n#### ä»Žæºç å®‰è£…\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nå¯é€‰çš„é¢å¤–ä¾èµ–é¡¹ï¼štorchã€torch-npuã€metricsã€deepspeedã€liger-kernelã€bitsandbytesã€hqqã€eetqã€gptqã€aqlmã€vllmã€sglangã€galoreã€apolloã€badamã€adam-miniã€qwenã€minicpm_vã€modelscopeã€openmindã€swanlabã€dev\n\n#### ä»Žé•œåƒå®‰è£…\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nè¯¥é•œåƒåŸºäºŽ Ubuntu 22.04ï¼ˆx86\\_64ï¼‰ã€CUDA 12.4ã€Python 3.11ã€PyTorch 2.6.0 å’Œ Flash-attn 2.7.4 æž„å»ºã€‚\n\næŸ¥çœ‹å…¨éƒ¨é•œåƒï¼šhttps://hub.docker.com/r/hiyouga/llamafactory/tags\n\nè¯·å‚é˜…[æž„å»º Docker](#æž„å»º-docker) æ¥é‡æ–°æž„å»ºé•œåƒã€‚\n\n<details><summary>ä½¿ç”¨ <b>uv</b> æž„å»ºè™šæ‹ŸçŽ¯å¢ƒ</summary>\n\nä½¿ç”¨ [uv](https://github.com/astral-sh/uv) åˆ›å»ºéš”ç¦»çš„ Python çŽ¯å¢ƒï¼š\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nåœ¨çŽ¯å¢ƒä¸­è¿è¡Œ LLaMA-Factoryï¼š\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>Windows ç”¨æˆ·æŒ‡å—</summary>\n\n#### å®‰è£… PyTorch\n\nWindows å¹³å°éœ€è¦é¢å¤–æ‰‹åŠ¨å®‰è£… GPU ç‰ˆæœ¬çš„ PyTorch ä¾èµ–åŒ…ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[å®˜æ–¹ç½‘ç«™](https://pytorch.org/get-started/locally/)å’Œä»¥ä¸‹å‘½ä»¤å®‰è£…å¹¶æµ‹è¯• PyTorch æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nå¦‚æžœçœ‹åˆ° `True` åˆ™è¯´æ˜Žå®‰è£…æˆåŠŸã€‚\n\nè‹¥é‡åˆ°ç±»ä¼¼ `Can't pickle local object` çš„æŠ¥é”™ï¼Œè¯·è®¾ç½® `dataloader_num_workers: 0`ã€‚\n\n#### å®‰è£… BitsAndBytes\n\nå¦‚æžœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯é‡åŒ– LoRAï¼ˆQLoRAï¼‰ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `bitsandbytes` åº“, æ”¯æŒ CUDA 11.1 åˆ° 12.2, è¯·æ ¹æ®æ‚¨çš„ CUDA ç‰ˆæœ¬æƒ…å†µé€‰æ‹©é€‚åˆçš„[å‘å¸ƒç‰ˆæœ¬](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels)ã€‚\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### å®‰è£… Flash Attention-2\n\nå¦‚æžœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯ FlashAttention-2ï¼Œè¯·ä½¿ç”¨ [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) ä¸­çš„è„šæœ¬è‡ªè¡Œç¼–è¯‘ä¸Žå®‰è£…ã€‚\n\n</details>\n\n<details><summary>æ˜‡è…¾ NPU ç”¨æˆ·æŒ‡å—</summary>\n\nåœ¨æ˜‡è…¾ NPU è®¾å¤‡ä¸Šå®‰è£… LLaMA Factory æ—¶ï¼Œè¯·å‡çº§ Python åˆ° 3.10 åŠä»¥ä¸Šï¼Œå¹¶éœ€è¦æŒ‡å®šé¢å¤–ä¾èµ–é¡¹ï¼Œä½¿ç”¨ `pip install -e \".[torch-npu,metrics]\"` å‘½ä»¤å®‰è£…ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å®‰è£… **[Ascend CANN Toolkit ä¸Ž Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**ï¼Œå®‰è£…æ–¹æ³•è¯·å‚è€ƒ[å®‰è£…æ•™ç¨‹](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html)æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n\n```bash\n# è¯·æ›¿æ¢ URL ä¸º CANN ç‰ˆæœ¬å’Œè®¾å¤‡åž‹å·å¯¹åº”çš„ URL\n# å®‰è£… CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run --install\n\n# å®‰è£… CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run\nbash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install\n\n# è®¾ç½®çŽ¯å¢ƒå˜é‡\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| ä¾èµ–é¡¹        | è‡³å°‘     | æŽ¨è           |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nè¯·ä½¿ç”¨ `ASCEND_RT_VISIBLE_DEVICES` è€Œéž `CUDA_VISIBLE_DEVICES` æ¥æŒ‡å®šè¿ç®—è®¾å¤‡ã€‚\n\nå¦‚æžœé‡åˆ°æ— æ³•æ­£å¸¸æŽ¨ç†çš„æƒ…å†µï¼Œè¯·å°è¯•è®¾ç½® `do_sample: false`ã€‚\n\nä¸‹è½½é¢„æž„å»º Docker é•œåƒï¼š[32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### å®‰è£… BitsAndBytes\n\nå¦‚æžœè¦åœ¨ Ascend NPU ä¸Šè¿›è¡ŒåŸºäºŽ bitsandbytes çš„ QLoRA é‡åŒ–å¾®è°ƒï¼Œè¯·æ‰§è¡Œå¦‚ä¸‹æ­¥éª¤ï¼š\n\n1. æ‰‹åŠ¨ç¼–è¯‘ bitsandbytesï¼šè¯·å‚è€ƒ[å®‰è£…æ–‡æ¡£](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU)å®Œæˆ NPU ç‰ˆçš„ bitsandbytes å®‰è£…ï¼Œç¼–è¯‘è¦æ±‚çŽ¯å¢ƒ cmake ç‰ˆæœ¬ä¸ä½ŽäºŽ 3.22.1ï¼Œg++ ç‰ˆæœ¬ä¸ä½ŽäºŽ 12.xã€‚\n\n```bash\n# ä»Žæºç å®‰è£… bitsandbytes\n# å…‹éš† bitsandbytes ä»“åº“, Ascend NPU ç›®å‰åœ¨ multi-backend-refactor ä¸­æ”¯æŒ\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# å®‰è£…ä¾èµ–\npip install -r requirements-dev.txt\n\n# å®‰è£…ç¼–è¯‘å·¥å…·ä¾èµ–ï¼Œè¯¥æ­¥éª¤åœ¨ä¸åŒç³»ç»Ÿä¸Šå‘½ä»¤æœ‰æ‰€ä¸åŒï¼Œä¾›å‚è€ƒ\napt-get install -y build-essential cmake\n\n# ç¼–è¯‘ & å®‰è£…\ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. å®‰è£… transformers çš„ main åˆ†æ”¯ç‰ˆæœ¬ã€‚\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. åœ¨è®­ç»ƒå‚æ•°ä¸­è®¾ç½® `double_quantization: false`ï¼Œå¯å‚è€ƒ[ç¤ºä¾‹](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml)ã€‚\n\n</details>\n\n### æ•°æ®å‡†å¤‡\n\nå…³äºŽæ•°æ®é›†æ–‡ä»¶çš„æ ¼å¼ï¼Œè¯·å‚è€ƒ [data/README_zh.md](data/README_zh.md) çš„å†…å®¹ã€‚ä½ å¯ä»¥ä½¿ç”¨ HuggingFace / ModelScope / Modelers ä¸Šçš„æ•°æ®é›†æˆ–åŠ è½½æœ¬åœ°æ•°æ®é›†ã€‚\n\n> [!NOTE]\n> ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œè¯·æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ã€‚\n\næ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ **[Easy Dataset](https://github.com/ConardLi/easy-dataset)** æˆ– **[GraphGen](https://github.com/open-sciencelab/GraphGen)** æž„å»ºç”¨äºŽå¾®è°ƒçš„åˆæˆæ•°æ®ã€‚\n\n### å¿«é€Ÿå¼€å§‹\n\nä¸‹é¢ä¸‰è¡Œå‘½ä»¤åˆ†åˆ«å¯¹ Llama3-8B-Instruct æ¨¡åž‹è¿›è¡Œ LoRA **å¾®è°ƒ**ã€**æŽ¨ç†**å’Œ**åˆå¹¶**ã€‚\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\né«˜çº§ç”¨æ³•è¯·å‚è€ƒ [examples/README_zh.md](examples/README_zh.md)ï¼ˆåŒ…æ‹¬å¤š GPU å¾®è°ƒï¼‰ã€‚\n\n> [!TIP]\n> ä½¿ç”¨ `llamafactory-cli help` æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ã€‚\n>\n> é‡åˆ°æŠ¥é”™è¯·å…ˆçœ‹[å¸¸è§é—®é¢˜](https://github.com/hiyouga/LLaMA-Factory/issues/4614)ã€‚\n\n### LLaMA Board å¯è§†åŒ–å¾®è°ƒï¼ˆç”± [Gradio](https://github.com/gradio-app/gradio) é©±åŠ¨ï¼‰\n\n```bash\nllamafactory-cli webui\n```\n\n### æž„å»º Docker\n\nCUDA ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\næ˜‡è…¾ NPU ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nAMD ROCm ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>ä¸ä½¿ç”¨ Docker Compose æž„å»º</summary>\n\nCUDA ç”¨æˆ·ï¼š\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\næ˜‡è…¾ NPU ç”¨æˆ·ï¼š\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nAMD ROCm ç”¨æˆ·ï¼š\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>ä½¿ç”¨æ•°æ®å·</summary>\n\næ‚¨å¯ä»¥é€šè¿‡ç§»é™¤ Dockerfile ä¸­ `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` çš„æ³¨é‡Šæ¥ä½¿ç”¨æ•°æ®å·ã€‚\n\nåœ¨æž„å»º Docker æ—¶ä½¿ç”¨å‚æ•° `-v ./hf_cache:/root/.cache/huggingface` æ¥æŒ‚è½½æ•°æ®å·ã€‚å„ä¸ªæ•°æ®å·çš„å«ä¹‰è¡¨ç¤ºå¦‚ä¸‹ã€‚\n\n- `hf_cache`ï¼šä½¿ç”¨å®¿ä¸»æœºçš„ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ã€‚\n- `shared_data`ï¼šå®¿ä¸»æœºä¸­å­˜æ”¾æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n- `output`ï¼šå°†å¯¼å‡ºç›®å½•è®¾ç½®ä¸ºè¯¥è·¯å¾„åŽï¼Œå³å¯åœ¨å®¿ä¸»æœºä¸­è®¿é—®å¯¼å‡ºåŽçš„æ¨¡åž‹ã€‚\n\n</details>\n\n### åˆ©ç”¨ vLLM éƒ¨ç½² OpenAI API\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> API æ–‡æ¡£è¯·æŸ¥é˜…[è¿™é‡Œ](https://platform.openai.com/docs/api-reference/chat/create)ã€‚\n>\n> ç¤ºä¾‹ï¼š[å›¾åƒç†è§£](scripts/api_example/test_image.py) | [å·¥å…·è°ƒç”¨](scripts/api_example/test_toolcall.py)\n\n### ä»Žé­”æ­ç¤¾åŒºä¸‹è½½\n\nå¦‚æžœæ‚¨åœ¨ Hugging Face æ¨¡åž‹å’Œæ•°æ®é›†çš„ä¸‹è½½ä¸­é‡åˆ°äº†é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä¸‹è¿°æ–¹æ³•ä½¿ç”¨é­”æ­ç¤¾åŒºã€‚\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # Windows ä½¿ç”¨ `set USE_MODELSCOPE_HUB=1`\n```\n\nå°† `model_name_or_path` è®¾ç½®ä¸ºæ¨¡åž‹ ID æ¥åŠ è½½å¯¹åº”çš„æ¨¡åž‹ã€‚åœ¨[é­”æ­ç¤¾åŒº](https://modelscope.cn/models)æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ `LLM-Research/Meta-Llama-3-8B-Instruct`ã€‚\n\n### ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½\n\næ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ä¸‹è¿°æ–¹æ³•ï¼Œä½¿ç”¨é­”ä¹ç¤¾åŒºä¸‹è½½æ•°æ®é›†å’Œæ¨¡åž‹ã€‚\n\n```bash\nexport USE_OPENMIND_HUB=1 # Windows ä½¿ç”¨ `set USE_OPENMIND_HUB=1`\n```\n\nå°† `model_name_or_path` è®¾ç½®ä¸ºæ¨¡åž‹ ID æ¥åŠ è½½å¯¹åº”çš„æ¨¡åž‹ã€‚åœ¨[é­”ä¹ç¤¾åŒº](https://modelers.cn/models)æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ `TeleAI/TeleChat-7B-pt`ã€‚\n\n### ä½¿ç”¨ W&B é¢æ¿\n\nè‹¥è¦ä½¿ç”¨ [Weights & Biases](https://wandb.ai) è®°å½•å®žéªŒæ•°æ®ï¼Œè¯·åœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ ä¸‹é¢çš„å‚æ•°ã€‚\n\n```yaml\nreport_to: wandb\nrun_name: test_run # å¯é€‰\n```\n\nåœ¨å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶ï¼Œå°† `WANDB_API_KEY` è®¾ç½®ä¸º[å¯†é’¥](https://wandb.ai/authorize)æ¥ç™»å½• W&B è´¦æˆ·ã€‚\n\n### ä½¿ç”¨ SwanLab é¢æ¿\n\nè‹¥è¦ä½¿ç”¨ [SwanLab](https://github.com/SwanHubX/SwanLab) è®°å½•å®žéªŒæ•°æ®ï¼Œè¯·åœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ ä¸‹é¢çš„å‚æ•°ã€‚\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # å¯é€‰\n```\n\nåœ¨å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶ï¼Œç™»å½•SwanLabè´¦æˆ·æœ‰ä»¥ä¸‹ä¸‰ç§æ–¹å¼ï¼š\n\næ–¹å¼ä¸€ï¼šåœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ  `swanlab_api_key=<your_api_key>` ï¼Œå¹¶è®¾ç½®ä¸ºä½ çš„ [API å¯†é’¥](https://swanlab.cn/settings)ã€‚\næ–¹å¼äºŒï¼šå°†çŽ¯å¢ƒå˜é‡ `SWANLAB_API_KEY` è®¾ç½®ä¸ºä½ çš„ [API å¯†é’¥](https://swanlab.cn/settings)ã€‚\næ–¹å¼ä¸‰ï¼šå¯åŠ¨å‰ä½¿ç”¨ `swanlab login` å‘½ä»¤å®Œæˆç™»å½•ã€‚\n\n## ä½¿ç”¨äº† LLaMA Factory çš„é¡¹ç›®\n\nå¦‚æžœæ‚¨æœ‰é¡¹ç›®å¸Œæœ›æ·»åŠ è‡³ä¸‹è¿°åˆ—è¡¨ï¼Œè¯·é€šè¿‡é‚®ä»¶è”ç³»æˆ–è€…åˆ›å»ºä¸€ä¸ª PRã€‚\n\n<details><summary>ç‚¹å‡»æ˜¾ç¤º</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: å¤©æ–‡å¤§æ¨¡åž‹ StarWhisperï¼ŒåŸºäºŽ ChatGLM2-6B å’Œ Qwen-14B åœ¨å¤©æ–‡æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: ä¸­æ–‡æ³•å¾‹é¢†åŸŸå¤§æ¨¡åž‹ DISC-LawLLMï¼ŒåŸºäºŽ Baichuan-13B å¾®è°ƒè€Œå¾—ï¼Œå…·æœ‰æ³•å¾‹æŽ¨ç†å’ŒçŸ¥è¯†æ£€ç´¢èƒ½åŠ›ã€‚\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: å­™æ€é‚ˆä¸­æ–‡åŒ»ç–—å¤§æ¨¡åž‹ Sumsimiaoï¼ŒåŸºäºŽ Baichuan-7B å’Œ ChatGLM-6B åœ¨ä¸­æ–‡åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: åŒ»ç–—å¤§æ¨¡åž‹é¡¹ç›® CareGPTï¼ŒåŸºäºŽ LLaMA2-7B å’Œ Baichuan-13B åœ¨ä¸­æ–‡åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**ï¼šMBTIæ€§æ ¼å¤§æ¨¡åž‹é¡¹ç›®ï¼Œæ ¹æ®æ•°æ®é›†ä¸Žè®­ç»ƒæ–¹å¼è®©ä»»æ„ LLM æ‹¥æœ‰ 16 ä¸ªä¸åŒçš„æ€§æ ¼ç±»åž‹ã€‚\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**ï¼šä¸€ä¸ªç”¨äºŽç”Ÿæˆ Stable Diffusion æç¤ºè¯çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ã€‚[[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**ï¼šä¸­æ–‡å¤šæ¨¡æ€åŒ»å­¦å¤§æ¨¡åž‹ï¼ŒåŸºäºŽ LLaVA-1.5-7B åœ¨ä¸­æ–‡å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**ï¼šåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„æ–‡æ¡£çº§å…³ç³»æŠ½å–ç³»ç»Ÿã€‚\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**ï¼šåœ¨ Windows ä¸»æœºä¸Šåˆ©ç”¨è‹±ä¼Ÿè¾¾ RTX è®¾å¤‡è¿›è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹å¾®è°ƒçš„å¼€å‘åŒ…ã€‚\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**ï¼šä¸€ä¸ªä½Žä»£ç æž„å»ºå¤š Agent å¤§æ¨¡åž‹åº”ç”¨çš„å¼€å‘å·¥å…·ï¼Œæ”¯æŒåŸºäºŽ LLaMA Factory çš„æ¨¡åž‹å¾®è°ƒ.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**ï¼šä¸€ä¸ªå…¨é“¾è·¯ RAG æ£€ç´¢æ¨¡åž‹å¾®è°ƒã€æŽ¨ç†å’Œè’¸é¦ä»£ç åº“ã€‚[[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**ï¼šä¸€ä¸ªé­”æ”¹åŽçš„ä»£ç åº“ï¼Œé€šè¿‡ Ring Attention æ”¯æŒé•¿åºåˆ—çš„ SFT å’Œ DPO è®­ç»ƒã€‚\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**ï¼šç”± NovaSky AI å¾®è°ƒçš„ä½Žæˆæœ¬ç±» o1 é•¿æŽ¨ç†æ¨¡åž‹ã€‚\n1. **[WeClone](https://github.com/xming521/WeClone)**ï¼šä»ŽèŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆã€‚\n\n</details>\n\n## åè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚\n\nä½¿ç”¨æ¨¡åž‹æƒé‡æ—¶ï¼Œè¯·éµå¾ªå¯¹åº”çš„æ¨¡åž‹åè®®ï¼š[Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## å¼•ç”¨\n\nå¦‚æžœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## è‡´è°¢\n\næœ¬é¡¹ç›®å—ç›ŠäºŽ [PEFT](https://github.com/huggingface/peft)ã€[TRL](https://github.com/huggingface/trl)ã€[QLoRA](https://github.com/artidoro/qlora) å’Œ [FastChat](https://github.com/lm-sys/FastChat)ï¼Œæ„Ÿè°¢ä»¥ä¸Šè¯¸ä½ä½œè€…çš„ä»˜å‡ºã€‚\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
          "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Optional, Union\n\nfrom typing_extensions import override\n\nfrom ..extras import logging\nfrom .data_utils import Role\nfrom .formatter import EmptyFormatter, FunctionFormatter, StringFormatter, ToolFormatter\nfrom .mm_plugin import get_mm_plugin\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer\n\n    from ..hparams import DataArguments\n    from .formatter import SLOTS, Formatter\n    from .mm_plugin import BasePlugin\n    from .tool_utils import FunctionCall\n\n\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\nclass Template:\n    format_user: \"Formatter\"\n    format_assistant: \"Formatter\"\n    format_system: \"Formatter\"\n    format_function: \"Formatter\"\n    format_observation: \"Formatter\"\n    format_tools: \"Formatter\"\n    format_prefix: \"Formatter\"\n    default_system: str\n    stop_words: list[str]\n    thought_words: tuple[str, str]\n    efficient_eos: bool\n    replace_eos: bool\n    replace_jinja_template: bool\n    enable_thinking: Optional[bool]\n    mm_plugin: \"BasePlugin\"\n\n    def encode_oneturn(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n    ) -> tuple[list[int], list[int]]:\n        r\"\"\"Return a single pair of token ids representing prompt and response respectively.\"\"\"\n        encoded_messages = self._encode(tokenizer, messages, system, tools)\n        prompt_ids = []\n        for encoded_ids in encoded_messages[:-1]:\n            prompt_ids += encoded_ids\n\n        response_ids = encoded_messages[-1]\n        return prompt_ids, response_ids\n\n    def encode_multiturn(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n    ) -> list[tuple[list[int], list[int]]]:\n        r\"\"\"Return multiple pairs of token ids representing prompts and responses respectively.\"\"\"\n        encoded_messages = self._encode(tokenizer, messages, system, tools)\n        return [(encoded_messages[i], encoded_messages[i + 1]) for i in range(0, len(encoded_messages), 2)]\n\n    def extract_tool(self, content: str) -> Union[str, list[\"FunctionCall\"]]:\n        r\"\"\"Extract tool message.\"\"\"\n        return self.format_tools.extract(content)\n\n    def get_stop_token_ids(self, tokenizer: \"PreTrainedTokenizer\") -> list[int]:\n        r\"\"\"Return stop token ids.\"\"\"\n        stop_token_ids = {tokenizer.eos_token_id}\n        for token in self.stop_words:\n            stop_token_ids.add(tokenizer.convert_tokens_to_ids(token))\n\n        return list(stop_token_ids)\n\n    def add_thought(self, content: str = \"\") -> str:\n        r\"\"\"Add empty thought to assistant message.\"\"\"\n        return f\"{self.thought_words[0]}\\n\\n{self.thought_words[1]}\\n\\n\" + content\n\n    def remove_thought(self, content: str) -> str:\n        r\"\"\"Remove thought from assistant message.\"\"\"\n        pattern = re.compile(f\"{re.escape(self.thought_words[0])}(.*?){re.escape(self.thought_words[1])}\", re.DOTALL)\n        return re.sub(pattern, \"\", content).lstrip(\"\\n\")\n\n    def get_thought_word_ids(self, tokenizer: \"PreTrainedTokenizer\") -> list[int]:\n        r\"\"\"Get the token ids of thought words.\"\"\"\n        return tokenizer.encode(self.add_thought(), add_special_tokens=False)\n\n    def _convert_elements_to_ids(self, tokenizer: \"PreTrainedTokenizer\", elements: \"SLOTS\") -> list[int]:\n        r\"\"\"Convert elements to token ids.\"\"\"\n        token_ids = []\n        for elem in elements:\n            if isinstance(elem, str):\n                if len(elem) != 0:\n                    token_ids += tokenizer.encode(elem, add_special_tokens=False)\n            elif isinstance(elem, dict):\n                token_ids += [tokenizer.convert_tokens_to_ids(elem.get(\"token\"))]\n            elif isinstance(elem, set):\n                if \"bos_token\" in elem and tokenizer.bos_token_id is not None:\n                    token_ids += [tokenizer.bos_token_id]\n                elif \"eos_token\" in elem and tokenizer.eos_token_id is not None:\n                    token_ids += [tokenizer.eos_token_id]\n            else:\n                raise ValueError(f\"Input must be string, set[str] or dict[str, str], got {type(elem)}\")\n\n        return token_ids\n\n    def _encode(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: Optional[str],\n        tools: Optional[str],\n    ) -> list[list[int]]:\n        r\"\"\"Encode formatted inputs to pairs of token ids.\n\n        Turn 0: prefix + system + query        resp\n        Turn t: query                          resp.\n        \"\"\"\n        system = system or self.default_system\n        encoded_messages = []\n        for i, message in enumerate(messages):\n            elements = []\n\n            if i == 0:\n                elements += self.format_prefix.apply()\n                if system or tools:\n                    tool_text = self.format_tools.apply(content=tools)[0] if tools else \"\"\n                    elements += self.format_system.apply(content=(system + tool_text))\n\n            if message[\"role\"] == Role.USER:\n                elements += self.format_user.apply(content=message[\"content\"], idx=str(i // 2))\n            elif message[\"role\"] == Role.ASSISTANT:\n                elements += self.format_assistant.apply(content=message[\"content\"])\n            elif message[\"role\"] == Role.OBSERVATION:\n                elements += self.format_observation.apply(content=message[\"content\"])\n            elif message[\"role\"] == Role.FUNCTION:\n                elements += self.format_function.apply(content=message[\"content\"])\n            else:\n                raise NotImplementedError(\"Unexpected role: {}\".format(message[\"role\"]))\n\n            encoded_messages.append(self._convert_elements_to_ids(tokenizer, elements))\n\n        return encoded_messages\n\n    @staticmethod\n    def _add_or_replace_eos_token(tokenizer: \"PreTrainedTokenizer\", eos_token: str) -> None:\n        r\"\"\"Add or replace eos token to the tokenizer.\"\"\"\n        if tokenizer.eos_token == eos_token:\n            return\n\n        is_added = tokenizer.eos_token_id is None\n        num_added_tokens = tokenizer.add_special_tokens({\"eos_token\": eos_token})\n\n        if is_added:\n            logger.info_rank0(f\"Add eos token: {tokenizer.eos_token}.\")\n        else:\n            logger.info_rank0(f\"Replace eos token: {tokenizer.eos_token}.\")\n\n        if num_added_tokens > 0:\n            logger.warning_rank0(\"New tokens have been added, make sure `resize_vocab` is True.\")\n\n    def fix_special_tokens(self, tokenizer: \"PreTrainedTokenizer\") -> None:\n        r\"\"\"Add eos token and pad token to the tokenizer.\"\"\"\n        stop_words = self.stop_words\n        if self.replace_eos:\n            if not stop_words:\n                raise ValueError(\"Stop words are required to replace the EOS token.\")\n\n            self._add_or_replace_eos_token(tokenizer, eos_token=stop_words[0])\n            stop_words = stop_words[1:]\n\n        if tokenizer.eos_token_id is None:\n            self._add_or_replace_eos_token(tokenizer, eos_token=\"<|endoftext|>\")\n\n        if tokenizer.pad_token_id is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            logger.info_rank0(f\"Add pad token: {tokenizer.pad_token}\")\n\n        if stop_words:\n            num_added_tokens = tokenizer.add_special_tokens(\n                dict(additional_special_tokens=stop_words), replace_additional_special_tokens=False\n            )\n            logger.info_rank0(\"Add {} to stop words.\".format(\",\".join(stop_words)))\n            if num_added_tokens > 0:\n                logger.warning_rank0(\"New tokens have been added, make sure `resize_vocab` is True.\")\n\n    @staticmethod\n    def _jinja_escape(content: str) -> str:\n        r\"\"\"Escape single quotes in content.\"\"\"\n        return content.replace(\"'\", r\"\\'\")\n\n    @staticmethod\n    def _convert_slots_to_jinja(slots: \"SLOTS\", tokenizer: \"PreTrainedTokenizer\", placeholder: str = \"content\") -> str:\n        r\"\"\"Convert slots to jinja template.\"\"\"\n        slot_items = []\n        for slot in slots:\n            if isinstance(slot, str):\n                slot_pieces = slot.split(\"{{content}}\")\n                if slot_pieces[0]:\n                    slot_items.append(\"'\" + Template._jinja_escape(slot_pieces[0]) + \"'\")\n                if len(slot_pieces) > 1:\n                    slot_items.append(placeholder)\n                    if slot_pieces[1]:\n                        slot_items.append(\"'\" + Template._jinja_escape(slot_pieces[1]) + \"'\")\n            elif isinstance(slot, set):  # do not use {{ eos_token }} since it may be replaced\n                if \"bos_token\" in slot and tokenizer.bos_token_id is not None:\n                    slot_items.append(\"'\" + tokenizer.bos_token + \"'\")\n                elif \"eos_token\" in slot and tokenizer.eos_token_id is not None:\n                    slot_items.append(\"'\" + tokenizer.eos_token + \"'\")\n            elif isinstance(slot, dict):\n                raise ValueError(\"Dict is not supported.\")\n\n        return \" + \".join(slot_items)\n\n    def _get_jinja_template(self, tokenizer: \"PreTrainedTokenizer\") -> str:\n        r\"\"\"Return the jinja template.\"\"\"\n        prefix = self._convert_slots_to_jinja(self.format_prefix.apply(), tokenizer)\n        system = self._convert_slots_to_jinja(self.format_system.apply(), tokenizer, placeholder=\"system_message\")\n        user = self._convert_slots_to_jinja(self.format_user.apply(), tokenizer)\n        assistant = self._convert_slots_to_jinja(self.format_assistant.apply(), tokenizer)\n        jinja_template = \"\"\n        if prefix:\n            jinja_template += \"{{ \" + prefix + \" }}\"\n\n        if self.default_system:\n            jinja_template += \"{% set system_message = '\" + self._jinja_escape(self.default_system) + \"' %}\"\n\n        jinja_template += (\n            \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}\"\n            \"{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}\"\n            \"{% if system_message is defined %}{{ \" + system + \" }}{% endif %}\"\n            \"{% for message in loop_messages %}\"\n            \"{% set content = message['content'] %}\"\n            \"{% if message['role'] == 'user' %}\"\n            \"{{ \" + user + \" }}\"\n            \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ \" + assistant + \" }}\"\n            \"{% endif %}\"\n            \"{% endfor %}\"\n        )\n        return jinja_template\n\n    def fix_jinja_template(self, tokenizer: \"PreTrainedTokenizer\") -> None:\n        r\"\"\"Replace the jinja template in the tokenizer.\"\"\"\n        if tokenizer.chat_template is None or self.replace_jinja_template:\n            try:\n                tokenizer.chat_template = self._get_jinja_template(tokenizer)\n            except ValueError as e:\n                logger.info_rank0(f\"Cannot add this chat template to tokenizer: {e}.\")\n\n    @staticmethod\n    def _convert_slots_to_ollama(\n        slots: \"SLOTS\", tokenizer: \"PreTrainedTokenizer\", placeholder: str = \"content\"\n    ) -> str:\n        r\"\"\"Convert slots to ollama template.\"\"\"\n        slot_items = []\n        for slot in slots:\n            if isinstance(slot, str):\n                slot_pieces = slot.split(\"{{content}}\")\n                if slot_pieces[0]:\n                    slot_items.append(slot_pieces[0])\n                if len(slot_pieces) > 1:\n                    slot_items.append(\"{{ \" + placeholder + \" }}\")\n                    if slot_pieces[1]:\n                        slot_items.append(slot_pieces[1])\n            elif isinstance(slot, set):  # do not use {{ eos_token }} since it may be replaced\n                if \"bos_token\" in slot and tokenizer.bos_token_id is not None:\n                    slot_items.append(tokenizer.bos_token)\n                elif \"eos_token\" in slot and tokenizer.eos_token_id is not None:\n                    slot_items.append(tokenizer.eos_token)\n            elif isinstance(slot, dict):\n                raise ValueError(\"Dict is not supported.\")\n\n        return \"\".join(slot_items)\n\n    def _get_ollama_template(self, tokenizer: \"PreTrainedTokenizer\") -> str:\n        r\"\"\"Return the ollama template.\"\"\"\n        prefix = self._convert_slots_to_ollama(self.format_prefix.apply(), tokenizer)\n        system = self._convert_slots_to_ollama(self.format_system.apply(), tokenizer, placeholder=\".System\")\n        user = self._convert_slots_to_ollama(self.format_user.apply(), tokenizer, placeholder=\".Content\")\n        assistant = self._convert_slots_to_ollama(self.format_assistant.apply(), tokenizer, placeholder=\".Content\")\n        return (\n            f\"{prefix}{{{{ if .System }}}}{system}{{{{ end }}}}\"\n            f\"\"\"{{{{ range .Messages }}}}{{{{ if eq .Role \"user\" }}}}{user}\"\"\"\n            f\"\"\"{{{{ else if eq .Role \"assistant\" }}}}{assistant}{{{{ end }}}}{{{{ end }}}}\"\"\"\n        )\n\n    def get_ollama_modelfile(self, tokenizer: \"PreTrainedTokenizer\") -> str:\n        r\"\"\"Return the ollama modelfile.\n\n        TODO: support function calling.\n        \"\"\"\n        modelfile = \"# ollama modelfile auto-generated by llamafactory\\n\\n\"\n        modelfile += f'FROM .\\n\\nTEMPLATE \"\"\"{self._get_ollama_template(tokenizer)}\"\"\"\\n\\n'\n\n        if self.default_system:\n            modelfile += f'SYSTEM \"\"\"{self.default_system}\"\"\"\\n\\n'\n\n        for stop_token_id in self.get_stop_token_ids(tokenizer):\n            modelfile += f'PARAMETER stop \"{tokenizer.convert_ids_to_tokens(stop_token_id)}\"\\n'\n\n        modelfile += \"PARAMETER num_ctx 4096\\n\"\n        return modelfile\n\n\n@dataclass\nclass Llama2Template(Template):\n    r\"\"\"A template that fuse the system message to first user message.\"\"\"\n\n    @override\n    def _encode(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: str,\n        tools: str,\n    ) -> list[list[int]]:\n        system = system or self.default_system\n        encoded_messages = []\n        for i, message in enumerate(messages):\n            elements = []\n\n            system_text = \"\"\n            if i == 0:\n                elements += self.format_prefix.apply()\n                if system or tools:\n                    tool_text = self.format_tools.apply(content=tools)[0] if tools else \"\"\n                    system_text = self.format_system.apply(content=(system + tool_text))[0]\n\n            if message[\"role\"] == Role.USER:\n                elements += self.format_user.apply(content=system_text + message[\"content\"])\n            elif message[\"role\"] == Role.ASSISTANT:\n                elements += self.format_assistant.apply(content=message[\"content\"])\n            elif message[\"role\"] == Role.OBSERVATION:\n                elements += self.format_observation.apply(content=message[\"content\"])\n            elif message[\"role\"] == Role.FUNCTION:\n                elements += self.format_function.apply(content=message[\"content\"])\n            else:\n                raise NotImplementedError(\"Unexpected role: {}\".format(message[\"role\"]))\n\n            encoded_messages.append(self._convert_elements_to_ids(tokenizer, elements))\n\n        return encoded_messages\n\n    def _get_jinja_template(self, tokenizer: \"PreTrainedTokenizer\") -> str:\n        prefix = self._convert_slots_to_jinja(self.format_prefix.apply(), tokenizer)\n        system_message = self._convert_slots_to_jinja(\n            self.format_system.apply(), tokenizer, placeholder=\"system_message\"\n        )\n        user_message = self._convert_slots_to_jinja(self.format_user.apply(), tokenizer)\n        assistant_message = self._convert_slots_to_jinja(self.format_assistant.apply(), tokenizer)\n        jinja_template = \"\"\n        if prefix:\n            jinja_template += \"{{ \" + prefix + \" }}\"\n\n        if self.default_system:\n            jinja_template += \"{% set system_message = '\" + self._jinja_escape(self.default_system) + \"' %}\"\n\n        jinja_template += (\n            \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}\"\n            \"{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}\"\n            \"{% for message in loop_messages %}\"\n            \"{% if loop.index0 == 0 and system_message is defined %}\"\n            \"{% set content = \" + system_message + \" + message['content'] %}\"\n            \"{% else %}{% set content = message['content'] %}{% endif %}\"\n            \"{% if message['role'] == 'user' %}\"\n            \"{{ \" + user_message + \" }}\"\n            \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ \" + assistant_message + \" }}\"\n            \"{% endif %}\"\n            \"{% endfor %}\"\n        )\n        return jinja_template\n\n\n@dataclass\nclass ReasoningTemplate(Template):\n    r\"\"\"A template that add thought to assistant message.\"\"\"\n\n    @override\n    def encode_oneturn(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n    ) -> tuple[list[int], list[int]]:\n        messages = deepcopy(messages)\n        for i in range(1, len(messages) - 2, 2):\n            messages[i][\"content\"] = self.remove_thought(messages[i][\"content\"])\n\n        if self.enable_thinking is False:  # remove all cot\n            messages[-1][\"content\"] = self.remove_thought(messages[-1][\"content\"])\n\n        prompt_ids, response_ids = super().encode_oneturn(tokenizer, messages, system, tools)\n        if (\n            self.thought_words[0] not in messages[-1][\"content\"]\n            and self.thought_words[1] not in messages[-1][\"content\"]\n        ):  # add empty cot\n            if not self.enable_thinking:  # do not compute loss\n                prompt_ids += self.get_thought_word_ids(tokenizer)\n            else:  # do compute loss\n                response_ids = self.get_thought_word_ids(tokenizer) + response_ids\n\n        return prompt_ids, response_ids\n\n    @override\n    def encode_multiturn(\n        self,\n        tokenizer: \"PreTrainedTokenizer\",\n        messages: list[dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n    ) -> list[tuple[list[int], list[int]]]:\n        messages = deepcopy(messages)\n        if self.enable_thinking is False:  # remove all cot\n            for i in range(1, len(messages), 2):\n                messages[i][\"content\"] = self.remove_thought(messages[i][\"content\"])\n\n        encoded_messages = self._encode(tokenizer, messages, system, tools)\n        for i in range(0, len(messages), 2):\n            if (\n                self.thought_words[0] not in messages[i + 1][\"content\"]\n                and self.thought_words[1] not in messages[i + 1][\"content\"]\n            ):  # add empty cot\n                if not self.enable_thinking:  # do not compute loss\n                    encoded_messages[i] += self.get_thought_word_ids(tokenizer)\n                else:  # do compute loss\n                    encoded_messages[i + 1] = self.get_thought_word_ids(tokenizer) + encoded_messages[i + 1]\n\n        return [(encoded_messages[i], encoded_messages[i + 1]) for i in range(0, len(encoded_messages), 2)]\n\n\nTEMPLATES: dict[str, \"Template\"] = {}\n\n\ndef register_template(\n    name: str,\n    format_user: Optional[\"Formatter\"] = None,\n    format_assistant: Optional[\"Formatter\"] = None,\n    format_system: Optional[\"Formatter\"] = None,\n    format_function: Optional[\"Formatter\"] = None,\n    format_observation: Optional[\"Formatter\"] = None,\n    format_tools: Optional[\"Formatter\"] = None,\n    format_prefix: Optional[\"Formatter\"] = None,\n    default_system: str = \"\",\n    stop_words: Optional[list[str]] = None,\n    thought_words: Optional[tuple[str, str]] = None,\n    efficient_eos: bool = False,\n    replace_eos: bool = False,\n    replace_jinja_template: bool = False,\n    enable_thinking: Optional[bool] = True,\n    mm_plugin: \"BasePlugin\" = get_mm_plugin(name=\"base\"),\n    template_class: type[\"Template\"] = Template,\n) -> None:\n    r\"\"\"Register a chat template.\n\n    To add the following chat template:\n    ```\n    <s><user>user prompt here\n    <model>model response here</s>\n    <user>user prompt here\n    <model>model response here</s>\n    ```\n\n    The corresponding code should be:\n    ```\n    register_template(\n        name=\"custom\",\n        format_user=StringFormatter(slots=[\"<user>{{content}}\\n<model>\"]),\n        format_assistant=StringFormatter(slots=[\"{{content}}</s>\\n\"]),\n        format_prefix=EmptyFormatter(\"<s>\"),\n    )\n    ```\n    \"\"\"\n    if name in TEMPLATES:\n        raise ValueError(f\"Template {name} already exists.\")\n\n    default_slots = [\"{{content}}\"] if efficient_eos else [\"{{content}}\", {\"eos_token\"}]\n    default_user_formatter = StringFormatter(slots=[\"{{content}}\"])\n    default_assistant_formatter = StringFormatter(slots=default_slots)\n    if format_assistant is not None:\n        default_function_formatter = FunctionFormatter(slots=format_assistant.slots, tool_format=\"default\")\n    else:\n        default_function_formatter = FunctionFormatter(slots=default_slots, tool_format=\"default\")\n\n    default_tool_formatter = ToolFormatter(tool_format=\"default\")\n    default_prefix_formatter = EmptyFormatter()\n    TEMPLATES[name] = template_class(\n        format_user=format_user or default_user_formatter,\n        format_assistant=format_assistant or default_assistant_formatter,\n        format_system=format_system or default_user_formatter,\n        format_function=format_function or default_function_formatter,\n        format_observation=format_observation or format_user or default_user_formatter,\n        format_tools=format_tools or default_tool_formatter,\n        format_prefix=format_prefix or default_prefix_formatter,\n        default_system=default_system,\n        stop_words=stop_words or [],\n        thought_words=thought_words or (\"<think>\", \"</think>\"),\n        efficient_eos=efficient_eos,\n        replace_eos=replace_eos,\n        replace_jinja_template=replace_jinja_template,\n        enable_thinking=enable_thinking,\n        mm_plugin=mm_plugin,\n    )\n\n\ndef parse_template(tokenizer: \"PreTrainedTokenizer\") -> \"Template\":\n    r\"\"\"Extract a chat template from the tokenizer.\"\"\"\n\n    def find_diff(short_str: str, long_str: str) -> str:\n        i, j = 0, 0\n        diff = \"\"\n        while i < len(short_str) and j < len(long_str):\n            if short_str[i] == long_str[j]:\n                i += 1\n                j += 1\n            else:\n                diff += long_str[j]\n                j += 1\n\n        return diff\n\n    prefix = tokenizer.decode(tokenizer.encode(\"\"))\n\n    messages = [{\"role\": \"system\", \"content\": \"{{content}}\"}]\n    system_slot = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)[len(prefix) :]\n\n    messages = [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"{{content}}\"}]\n    user_slot_empty_system = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    user_slot_empty_system = user_slot_empty_system[len(prefix) :]\n\n    messages = [{\"role\": \"user\", \"content\": \"{{content}}\"}]\n    user_slot = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    user_slot = user_slot[len(prefix) :]\n\n    messages = [{\"role\": \"user\", \"content\": \"{{content}}\"}, {\"role\": \"assistant\", \"content\": \"{{content}}\"}]\n    assistant_slot = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n    assistant_slot = assistant_slot[len(prefix) + len(user_slot) :]\n    template_class = ReasoningTemplate if \"<think>\" in assistant_slot else Template\n    assistant_slot = assistant_slot.replace(\"<think>\", \"\").replace(\"</think>\", \"\").lstrip(\"\\n\")  # remove thought tags\n\n    if len(user_slot) > len(user_slot_empty_system):\n        default_system = find_diff(user_slot_empty_system, user_slot)\n        sole_system = system_slot.replace(\"{{content}}\", default_system, 1)\n        user_slot = user_slot[len(sole_system) :]\n    else:  # if defaut_system is empty, user_slot_empty_system will be longer than user_slot\n        default_system = \"\"\n\n    return template_class(\n        format_user=StringFormatter(slots=[user_slot]),\n        format_assistant=StringFormatter(slots=[assistant_slot]),\n        format_system=StringFormatter(slots=[system_slot]),\n        format_function=FunctionFormatter(slots=[assistant_slot], tool_format=\"default\"),\n        format_observation=StringFormatter(slots=[user_slot]),\n        format_tools=ToolFormatter(tool_format=\"default\"),\n        format_prefix=EmptyFormatter(slots=[prefix]) if prefix else EmptyFormatter(),\n        default_system=default_system,\n        stop_words=[],\n        thought_words=(\"<think>\", \"</think>\"),\n        efficient_eos=False,\n        replace_eos=False,\n        replace_jinja_template=False,\n        enable_thinking=True,\n        mm_plugin=get_mm_plugin(name=\"base\"),\n    )\n\n\ndef get_template_and_fix_tokenizer(tokenizer: \"PreTrainedTokenizer\", data_args: \"DataArguments\") -> \"Template\":\n    r\"\"\"Get chat template and fixes the tokenizer.\"\"\"\n    if data_args.template is None:\n        if isinstance(tokenizer.chat_template, str):\n            logger.warning_rank0(\"`template` was not specified, try parsing the chat template from the tokenizer.\")\n            template = parse_template(tokenizer)\n        else:\n            logger.warning_rank0(\"`template` was not specified, use `empty` template.\")\n            template = TEMPLATES[\"empty\"]  # placeholder\n    else:\n        if data_args.template not in TEMPLATES:\n            raise ValueError(f\"Template {data_args.template} does not exist.\")\n\n        template = TEMPLATES[data_args.template]\n\n    if data_args.train_on_prompt and template.efficient_eos:\n        raise ValueError(\"Current template does not support `train_on_prompt`.\")\n\n    if data_args.tool_format is not None:\n        logger.info_rank0(f\"Using tool format: {data_args.tool_format}.\")\n        default_slots = [\"{{content}}\"] if template.efficient_eos else [\"{{content}}\", {\"eos_token\"}]\n        template.format_function = FunctionFormatter(slots=default_slots, tool_format=data_args.tool_format)\n        template.format_tools = ToolFormatter(tool_format=data_args.tool_format)\n\n    if data_args.default_system is not None:\n        logger.info_rank0(f\"Using default system message: {data_args.default_system}.\")\n        template.default_system = data_args.default_system\n\n    template.enable_thinking = data_args.enable_thinking\n    template.fix_special_tokens(tokenizer)\n    template.fix_jinja_template(tokenizer)\n    return template\n\n\nregister_template(\n    name=\"alpaca\",\n    format_user=StringFormatter(slots=[\"### Instruction:\\n{{content}}\\n\\n### Response:\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\", {\"eos_token\"}, \"\\n\\n\"]),\n    default_system=(\n        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n    ),\n    replace_jinja_template=True,\n)\n\n\nregister_template(\n    name=\"aquila\",\n    format_user=StringFormatter(slots=[\"Human: {{content}}###Assistant:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}###\"]),\n    format_system=StringFormatter(slots=[\"System: {{content}}###\"]),\n    default_system=(\n        \"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n    ),\n    stop_words=[\"</s>\"],\n)\n\n\nregister_template(\n    name=\"atom\",\n    format_user=StringFormatter(\n        slots=[{\"bos_token\"}, \"Human: {{content}}\\n\", {\"eos_token\"}, {\"bos_token\"}, \"Assistant:\"]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\", {\"eos_token\"}]),\n)\n\n\nregister_template(\n    name=\"baichuan\",\n    format_user=StringFormatter(slots=[{\"token\": \"<reserved_102>\"}, \"{{content}}\", {\"token\": \"<reserved_103>\"}]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"baichuan2\",\n    format_user=StringFormatter(slots=[\"<reserved_106>{{content}}<reserved_107>\"]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"bailing\",\n    format_user=StringFormatter(slots=[\"<role>HUMAN</role>{{content}}<role>ASSISTANT</role>\"]),\n    format_system=StringFormatter(slots=[\"<role>SYSTEM</role>{{content}}\"]),\n    format_observation=StringFormatter(slots=[\"<role>OBSERVATION</role>{{content}}<role>ASSISTANT</role>\"]),\n    stop_words=[\"<|endoftext|>\"],\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"belle\",\n    format_user=StringFormatter(slots=[\"Human: {{content}}\\n\\nBelle: \"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\", {\"eos_token\"}, \"\\n\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\nregister_template(\n    name=\"bluelm\",\n    format_user=StringFormatter(slots=[{\"token\": \"[|Human|]:\"}, \"{{content}}\", {\"token\": \"[|AI|]:\"}]),\n)\n\n\nregister_template(\n    name=\"breeze\",\n    format_user=StringFormatter(slots=[\"[INST] {{content}} [/INST] \"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"chatglm2\",\n    format_user=StringFormatter(slots=[\"[Round {{idx}}]\\n\\né—®ï¼š{{content}}\\n\\nç­”ï¼š\"]),\n    format_prefix=EmptyFormatter(slots=[{\"token\": \"[gMASK]\"}, {\"token\": \"sop\"}]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"chatglm3\",\n    format_user=StringFormatter(slots=[{\"token\": \"<|user|>\"}, \"\\n\", \"{{content}}\", {\"token\": \"<|assistant|>\"}]),\n    format_assistant=StringFormatter(slots=[\"\\n\", \"{{content}}\"]),\n    format_system=StringFormatter(slots=[{\"token\": \"<|system|>\"}, \"\\n\", \"{{content}}\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}\"], tool_format=\"glm4\"),\n    format_observation=StringFormatter(\n        slots=[{\"token\": \"<|observation|>\"}, \"\\n\", \"{{content}}\", {\"token\": \"<|assistant|>\"}]\n    ),\n    format_tools=ToolFormatter(tool_format=\"glm4\"),\n    format_prefix=EmptyFormatter(slots=[{\"token\": \"[gMASK]\"}, {\"token\": \"sop\"}]),\n    stop_words=[\"<|user|>\", \"<|observation|>\"],\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"chatml\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    stop_words=[\"<|im_end|>\", \"<|im_start|>\"],\n    replace_eos=True,\n    replace_jinja_template=True,\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"chatml_de\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    default_system=\"Du bist ein freundlicher und hilfsbereiter KI-Assistent.\",\n    stop_words=[\"<|im_end|>\", \"<|im_start|>\"],\n    replace_eos=True,\n    replace_jinja_template=True,\n)\n\n\nregister_template(\n    name=\"codegeex2\",\n    format_prefix=EmptyFormatter(slots=[{\"token\": \"[gMASK]\"}, {\"token\": \"sop\"}]),\n)\n\n\nregister_template(\n    name=\"codegeex4\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|assistant|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}\"], tool_format=\"glm4\"),\n    format_observation=StringFormatter(slots=[\"<|observation|>\\n{{content}}<|assistant|>\\n\"]),\n    format_tools=ToolFormatter(tool_format=\"glm4\"),\n    format_prefix=EmptyFormatter(slots=[\"[gMASK]<sop>\"]),\n    default_system=(\n        \"ä½ æ˜¯ä¸€ä½æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ï¼Œä½ å«CodeGeeXã€‚ä½ ä¼šä¸ºç”¨æˆ·å›žç­”å…³äºŽç¼–ç¨‹ã€ä»£ç ã€è®¡ç®—æœºæ–¹é¢çš„ä»»ä½•é—®é¢˜ï¼Œ\"\n        \"å¹¶æä¾›æ ¼å¼è§„èŒƒã€å¯ä»¥æ‰§è¡Œã€å‡†ç¡®å®‰å…¨çš„ä»£ç ï¼Œå¹¶åœ¨å¿…è¦æ—¶æä¾›è¯¦ç»†çš„è§£é‡Šã€‚\"\n    ),\n    stop_words=[\"<|user|>\", \"<|observation|>\"],\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"cohere\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{content}}<|END_OF_TURN_TOKEN|>\"\n                \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n            )\n        ]\n    ),\n    format_system=StringFormatter(slots=[\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{content}}<|END_OF_TURN_TOKEN|>\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\nregister_template(\n    name=\"cpm\",\n    format_user=StringFormatter(slots=[\"<ç”¨æˆ·>{{content}}<AI>\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"cpm3\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"cpm4\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"dbrx\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    default_system=(\n        \"You are DBRX, created by Databricks. You were last updated in December 2023. \"\n        \"You answer questions based on information available up to that point.\\n\"\n        \"YOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough \"\n        \"responses to more complex and open-ended questions.\\nYou assist with various tasks, \"\n        \"from writing to coding (using markdown for code blocks â€” remember to use ``` with \"\n        \"code, JSON, and tables).\\n(You do not have real-time data access or code execution \"\n        \"capabilities. You avoid stereotyping and provide balanced perspectives on \"\n        \"controversial topics. You do not provide song lyrics, poems, or news articles and \"\n        \"do not divulge details of your training data.)\\nThis is your system prompt, \"\n        \"guiding your responses. Do not reference it, just respond to the user. If you find \"\n        \"yourself talking about this message, stop. You should be responding appropriately \"\n        \"and usually that means not mentioning this.\\nYOU DO NOT MENTION ANY OF THIS INFORMATION \"\n        \"ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY.\"\n    ),\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n)\n\n\nregister_template(\n    name=\"deepseek\",\n    format_user=StringFormatter(slots=[\"User: {{content}}\\n\\nAssistant:\"]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\nregister_template(\n    name=\"deepseek3\",\n    format_user=StringFormatter(slots=[\"<ï½œUserï½œ>{{content}}<ï½œAssistantï½œ>\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\n# copied from deepseek3 template\nregister_template(\n    name=\"deepseekr1\",\n    format_user=StringFormatter(slots=[\"<ï½œUserï½œ>{{content}}<ï½œAssistantï½œ>\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    template_class=ReasoningTemplate,\n)\n\n\nregister_template(\n    name=\"deepseekcoder\",\n    format_user=StringFormatter(slots=[\"### Instruction:\\n{{content}}\\n### Response:\"]),\n    format_assistant=StringFormatter(slots=[\"\\n{{content}}\\n<|EOT|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    default_system=(\n        \"You are an AI programming assistant, utilizing the DeepSeek Coder model, \"\n        \"developed by DeepSeek Company, and you only answer questions related to computer science. \"\n        \"For politically sensitive questions, security and privacy issues, \"\n        \"and other non-computer science questions, you will refuse to answer.\\n\"\n    ),\n)\n\n\nregister_template(\n    name=\"default\",\n    format_user=StringFormatter(slots=[\"Human: {{content}}\", {\"eos_token\"}, \"\\nAssistant:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\", {\"eos_token\"}, \"\\n\"]),\n    format_system=StringFormatter(slots=[\"System: {{content}}\", {\"eos_token\"}, \"\\n\"]),\n    replace_jinja_template=True,\n)\n\n\nregister_template(\n    name=\"empty\",\n    format_assistant=StringFormatter(slots=[\"{{content}}\"]),\n)\n\n\nregister_template(\n    name=\"exaone\",\n    format_user=StringFormatter(slots=[\"[|user|]{{content}}\\n[|assistant|]\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\", {\"eos_token\"}, \"\\n\"]),\n    format_system=StringFormatter(slots=[\"[|system|]{{content}}[|endofturn|]\\n\"]),\n)\n\n\nregister_template(\n    name=\"falcon\",\n    format_user=StringFormatter(slots=[\"User: {{content}}\\nFalcon:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\"]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"falcon_h1\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"default\"),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n\"]),\n    format_tools=ToolFormatter(tool_format=\"default\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|im_end|>\", \"<|end_of_text|>\"],\n)\n\n\nregister_template(\n    name=\"fewshot\",\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    efficient_eos=True,\n    replace_jinja_template=True,\n)\n\n\nregister_template(\n    name=\"gemma\",\n    format_user=StringFormatter(slots=[\"<start_of_turn>user\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<end_of_turn>\\n\"]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_observation=StringFormatter(\n        slots=[\"<start_of_turn>tool\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]\n    ),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<end_of_turn>\"],\n    replace_eos=True,\n    template_class=Llama2Template,\n)\n\n\n# copied from gemma template\nregister_template(\n    name=\"gemma3\",\n    format_user=StringFormatter(slots=[\"<start_of_turn>user\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<end_of_turn>\\n\"]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_observation=StringFormatter(\n        slots=[\"<start_of_turn>tool\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]\n    ),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<end_of_turn>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(\"gemma3\", image_token=\"<image_soft_token>\"),\n    template_class=Llama2Template,\n)\n\n\nregister_template(\n    name=\"glm4\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|assistant|>\"]),\n    format_assistant=StringFormatter(slots=[\"\\n{{content}}\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}\"], tool_format=\"glm4\"),\n    format_observation=StringFormatter(slots=[\"<|observation|>\\n{{content}}<|assistant|>\"]),\n    format_tools=ToolFormatter(tool_format=\"glm4\"),\n    format_prefix=EmptyFormatter(slots=[\"[gMASK]<sop>\"]),\n    stop_words=[\"<|user|>\", \"<|observation|>\"],\n    efficient_eos=True,\n)\n\n\n# copied from glm4 template\nregister_template(\n    name=\"glmz1\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|assistant|>\"]),\n    format_assistant=StringFormatter(slots=[\"\\n{{content}}\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}\"], tool_format=\"glm4\"),\n    format_observation=StringFormatter(slots=[\"<|observation|>\\n{{content}}<|assistant|>\"]),\n    format_tools=ToolFormatter(tool_format=\"glm4\"),\n    format_prefix=EmptyFormatter(slots=[\"[gMASK]<sop>\"]),\n    stop_words=[\"<|user|>\", \"<|observation|>\"],\n    efficient_eos=True,\n    template_class=ReasoningTemplate,\n)\n\n\nregister_template(\n    name=\"granite3\",\n    format_user=StringFormatter(\n        slots=[\n            \"<|start_of_role|>user<|end_of_role|>{{content}}<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\"\n        ]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|end_of_text|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|start_of_role|>system<|end_of_role|>{{content}}<|end_of_text|>\\n\"]),\n)\n\n\nregister_template(\n    name=\"granite3_vision\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}\\n<|assistant|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}\\n\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n)\n\n\nregister_template(\n    name=\"index\",\n    format_user=StringFormatter(slots=[\"reserved_0{{content}}reserved_1\"]),\n    format_system=StringFormatter(slots=[\"<unk>{{content}}\"]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"hunyuan\",\n    format_user=StringFormatter(slots=[\"<|bos|>user\\n{{content}}<|eos|>\\n<|bos|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eos|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|bos|>system\\n{{content}}<|eos|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[\"<|bos|>\"]),\n    stop_words=[\"<|eos|>\"],\n)\n\n\nregister_template(\n    name=\"intern\",\n    format_user=StringFormatter(slots=[\"<|User|>:{{content}}\\n<|Bot|>:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<eoa>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|System|>:{{content}}\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    default_system=(\n        \"You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).\\n\"\n        \"- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory \"\n        \"(ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤). It is designed to be helpful, honest, and harmless.\\n\"\n        \"- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language \"\n        \"chosen by the user such as English and ä¸­æ–‡.\"\n    ),\n    stop_words=[\"<eoa>\"],\n)\n\n\nregister_template(\n    name=\"intern2\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    default_system=(\n        \"You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).\\n\"\n        \"- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory \"\n        \"(ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤). It is designed to be helpful, honest, and harmless.\\n\"\n        \"- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language \"\n        \"chosen by the user such as English and ä¸­æ–‡.\"\n    ),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\nregister_template(\n    name=\"intern_vl\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    default_system=(\n        \"ä½ æ˜¯ä¹¦ç”ŸÂ·ä¸‡è±¡ï¼Œè‹±æ–‡åæ˜¯InternVLï¼Œæ˜¯ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤ã€æ¸…åŽå¤§å­¦åŠå¤šå®¶åˆä½œå•ä½è”åˆå¼€å‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ã€‚\"\n    ),\n    stop_words=[\"<|im_end|>\"],\n    mm_plugin=get_mm_plugin(name=\"intern_vl\", image_token=\"<image>\", video_token=\"<video>\"),\n)\n\n\nregister_template(\n    name=\"kimi_vl\",\n    format_user=StringFormatter(\n        slots=[\"<|im_user|>user<|im_middle|>{{content}}<|im_end|><|im_assistant|>assistant<|im_middle|>\"]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\"]),\n    format_system=StringFormatter(slots=[\"<|im_system|>system<|im_middle|>{{content}}<|im_end|>\"]),\n    default_system=\"You are a helpful assistant\",\n    stop_words=[\"<|im_end|>\"],\n    thought_words=(\"â—thinkâ–·\", \"â—/thinkâ–·\"),\n    mm_plugin=get_mm_plugin(\"kimi_vl\", image_token=\"<|media_pad|>\"),\n    template_class=ReasoningTemplate,\n)\n\n\nregister_template(\n    name=\"llama2\",\n    format_user=StringFormatter(slots=[{\"bos_token\"}, \"[INST] {{content}} [/INST]\"]),\n    format_system=StringFormatter(slots=[\"<<SYS>>\\n{{content}}\\n<</SYS>>\\n\\n\"]),\n    template_class=Llama2Template,\n)\n\n\n# copied from llama2 template\nregister_template(\n    name=\"llama2_zh\",\n    format_user=StringFormatter(slots=[{\"bos_token\"}, \"[INST] {{content}} [/INST]\"]),\n    format_system=StringFormatter(slots=[\"<<SYS>>\\n{{content}}\\n<</SYS>>\\n\\n\"]),\n    default_system=\"You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºŽåŠ©äººçš„åŠ©æ‰‹ã€‚\",\n    template_class=Llama2Template,\n)\n\n\nregister_template(\n    name=\"llama3\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eot_id|>\"]),\n    format_system=StringFormatter(slots=[\"<|start_header_id|>system<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|eot_id|>\"], tool_format=\"llama3\"),\n    format_observation=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>ipython<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_tools=ToolFormatter(tool_format=\"llama3\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|eot_id|>\", \"<|eom_id|>\"],\n    replace_eos=True,\n)\n\n\nregister_template(\n    name=\"llama4\",\n    format_user=StringFormatter(\n        slots=[\"<|header_start|>user<|header_end|>\\n\\n{{content}}<|eot|><|header_start|>assistant<|header_end|>\\n\\n\"]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eot|>\"]),\n    format_system=StringFormatter(slots=[\"<|header_start|>system<|header_end|>\\n\\n{{content}}<|eot|>\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|eot|>\"], tool_format=\"llama3\"),\n    format_observation=StringFormatter(\n        slots=[\n            \"<|header_start|>ipython<|header_end|>\\n\\n{{content}}<|eot|><|header_start|>assistant<|header_end|>\\n\\n\"\n        ]\n    ),\n    format_tools=ToolFormatter(tool_format=\"llama3\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|eot|>\", \"<|eom|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"llama4\", image_token=\"<|image|>\"),\n)\n\n\n# copied from llama3 template\nregister_template(\n    name=\"mllama\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eot_id|>\"]),\n    format_system=StringFormatter(slots=[\"<|start_header_id|>system<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|eot_id|>\"], tool_format=\"llama3\"),\n    format_observation=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>ipython<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_tools=ToolFormatter(tool_format=\"llama3\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|eot_id|>\", \"<|eom_id|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"mllama\", image_token=\"<|image|>\"),\n)\n\n\nregister_template(\n    name=\"moonlight\",\n    format_user=StringFormatter(\n        slots=[\"<|im_user|>user<|im_middle|>{{content}}<|im_end|><|im_assistant|>assistant<|im_middle|>\"]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\"]),\n    format_system=StringFormatter(slots=[\"<|im_system|>system<|im_middle|>{{content}}<|im_end|>\"]),\n    default_system=\"You are a helpful assistant provided by Moonshot-AI.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n)\n\n\n# copied from vicuna template\nregister_template(\n    name=\"llava\",\n    format_user=StringFormatter(slots=[\"USER: {{content}} ASSISTANT:\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    mm_plugin=get_mm_plugin(name=\"llava\", image_token=\"<image>\"),\n)\n\n\n# copied from vicuna template\nregister_template(\n    name=\"llava_next\",\n    format_user=StringFormatter(slots=[\"USER: {{content}} ASSISTANT:\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n)\n\n\n# copied from llama3 template\nregister_template(\n    name=\"llava_next_llama3\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eot_id|>\"]),\n    format_system=StringFormatter(slots=[\"<|start_header_id|>system<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|eot_id|>\"], tool_format=\"llama3\"),\n    format_observation=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>ipython<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_tools=ToolFormatter(tool_format=\"llama3\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|eot_id|>\", \"<|eom_id|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n)\n\n\n# copied from mistral template\nregister_template(\n    name=\"llava_next_mistral\",\n    format_user=StringFormatter(slots=[\"[INST] {{content}}[/INST]\"]),\n    format_assistant=StringFormatter(slots=[\" {{content}}\", {\"eos_token\"}]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS] {{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS] {\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n    template_class=Llama2Template,\n)\n\n\n# copied from qwen template\nregister_template(\n    name=\"llava_next_qwen\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"llava_next_yi\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n    mm_plugin=get_mm_plugin(name=\"llava_next\", image_token=\"<image>\"),\n)\n\n\n# copied from vicuna template\nregister_template(\n    name=\"llava_next_video\",\n    format_user=StringFormatter(slots=[\"USER: {{content}} ASSISTANT:\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    mm_plugin=get_mm_plugin(name=\"llava_next_video\", image_token=\"<image>\", video_token=\"<video>\"),\n)\n\n\n# copied from mistral template\nregister_template(\n    name=\"llava_next_video_mistral\",\n    format_user=StringFormatter(slots=[\"[INST] {{content}}[/INST]\"]),\n    format_assistant=StringFormatter(slots=[\" {{content}}\", {\"eos_token\"}]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS] {{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS] {\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    mm_plugin=get_mm_plugin(name=\"llava_next_video\", image_token=\"<image>\", video_token=\"<video>\"),\n    template_class=Llama2Template,\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"llava_next_video_yi\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n    mm_plugin=get_mm_plugin(name=\"llava_next_video\", image_token=\"<image>\", video_token=\"<video>\"),\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"marco\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    default_system=(\n        \"ä½ æ˜¯ä¸€ä¸ªç»è¿‡è‰¯å¥½è®­ç»ƒçš„AIåŠ©æ‰‹ï¼Œä½ çš„åå­—æ˜¯Marco-o1.\"\n        \"ç”±é˜¿é‡Œå›½é™…æ•°å­—å•†ä¸šé›†å›¢çš„AI Businessåˆ›é€ .\\n## é‡è¦ï¼ï¼ï¼ï¼ï¼\\n\"\n        \"å½“ä½ å›žç­”é—®é¢˜æ—¶ï¼Œä½ çš„æ€è€ƒåº”è¯¥åœ¨<Thought>å†…å®Œæˆï¼Œ<Output>å†…è¾“å‡ºä½ çš„ç»“æžœã€‚\\n\"\n        \"<Thought>åº”è¯¥å°½å¯èƒ½æ˜¯è‹±æ–‡ï¼Œä½†æ˜¯æœ‰2ä¸ªç‰¹ä¾‹ï¼Œä¸€ä¸ªæ˜¯å¯¹åŽŸæ–‡ä¸­çš„å¼•ç”¨ï¼Œå¦ä¸€ä¸ªæ˜¯æ˜¯æ•°å­¦åº”è¯¥ä½¿ç”¨markdownæ ¼å¼ï¼Œ<Output>å†…çš„è¾“å‡ºéœ€è¦éµå¾ªç”¨æˆ·è¾“å…¥çš„è¯­è¨€ã€‚\\n\"\n    ),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\n# copied from qwen template\nregister_template(\n    name=\"mimo\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    template_class=ReasoningTemplate,\n)\n\n# copied from qwen2vl\nregister_template(\n    name=\"mimo_vl\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are MiMo, an AI assistant developed by Xiaomi.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"qwen2_vl\", image_token=\"<|image_pad|>\", video_token=\"<|video_pad|>\"),\n    template_class=ReasoningTemplate,\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"minicpm_v\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n    default_system=\"You are a helpful assistant.\",\n    mm_plugin=get_mm_plugin(name=\"minicpm_v\", image_token=\"<image>\", video_token=\"<video>\"),\n)\n\n\n# copied from minicpm_v template\nregister_template(\n    name=\"minicpm_o\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n    default_system=\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n    mm_plugin=get_mm_plugin(name=\"minicpm_v\", image_token=\"<image>\", video_token=\"<video>\", audio_token=\"<audio>\"),\n)\n\n\n# mistral tokenizer v3 tekken\nregister_template(\n    name=\"ministral\",\n    format_user=StringFormatter(slots=[\"[INST]{{content}}[/INST]\"]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS]{{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS]{\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    template_class=Llama2Template,\n)\n\n\n# mistral tokenizer v3\nregister_template(\n    name=\"mistral\",\n    format_user=StringFormatter(slots=[\"[INST] {{content}}[/INST]\"]),\n    format_assistant=StringFormatter(slots=[\" {{content}}\", {\"eos_token\"}]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS] {{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS] {\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    template_class=Llama2Template,\n)\n\n\n# mistral tokenizer v7 tekken (copied from ministral)\nregister_template(\n    name=\"mistral_small\",\n    format_user=StringFormatter(slots=[\"[INST]{{content}}[/INST]\"]),\n    format_system=StringFormatter(slots=[\"[SYSTEM_PROMPT]{{content}}[/SYSTEM_PROMPT]\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS]{{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS]{\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    mm_plugin=get_mm_plugin(name=\"pixtral\", image_token=\"[IMG]\"),\n)\n\n\nregister_template(\n    name=\"olmo\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|assistant|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"eos_token\"}]),\n)\n\n\nregister_template(\n    name=\"openchat\",\n    format_user=StringFormatter(slots=[\"GPT4 Correct User: {{content}}\", {\"eos_token\"}, \"GPT4 Correct Assistant:\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\nregister_template(\n    name=\"openchat-3.6\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>GPT4 Correct User<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>GPT4 Correct Assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<|eot_id|>\"],\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"opencoder\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    default_system=\"You are OpenCoder, created by OpenCoder Team.\",\n    stop_words=[\"<|im_end|>\"],\n)\n\n\nregister_template(\n    name=\"orion\",\n    format_user=StringFormatter(slots=[\"Human: {{content}}\\n\\nAssistant: \", {\"eos_token\"}]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n)\n\n\nregister_template(\n    name=\"paligemma\",\n    format_user=StringFormatter(slots=[\"{{content}}\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    mm_plugin=get_mm_plugin(name=\"paligemma\", image_token=\"<image>\"),\n    template_class=Llama2Template,\n)\n\n\n# copied from gemma template\nregister_template(\n    name=\"paligemma_chat\",\n    format_user=StringFormatter(slots=[\"<start_of_turn>user\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<end_of_turn>\\n\"]),\n    format_observation=StringFormatter(\n        slots=[\"<start_of_turn>tool\\n{{content}}<end_of_turn>\\n<start_of_turn>model\\n\"]\n    ),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    stop_words=[\"<end_of_turn>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"paligemma\", image_token=\"<image>\"),\n    template_class=Llama2Template,\n)\n\n\nregister_template(\n    name=\"phi\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|end|>\\n<|assistant|>\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}<|end|>\\n\"]),\n    stop_words=[\"<|end|>\"],\n    replace_eos=True,\n)\n\n\nregister_template(\n    name=\"phi_small\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|end|>\\n<|assistant|>\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}<|end|>\\n\"]),\n    format_prefix=EmptyFormatter(slots=[{\"<|endoftext|>\"}]),\n    stop_words=[\"<|end|>\"],\n    replace_eos=True,\n)\n\n\nregister_template(\n    name=\"phi4\",\n    format_user=StringFormatter(\n        slots=[\"<|im_start|>user<|im_sep|>{{content}}<|im_end|><|im_start|>assistant<|im_sep|>\"]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system<|im_sep|>{{content}}<|im_end|>\"]),\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n)\n\n\n# copied from ministral template\nregister_template(\n    name=\"pixtral\",\n    format_user=StringFormatter(slots=[\"[INST]{{content}}[/INST]\"]),\n    format_system=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_function=FunctionFormatter(slots=[\"[TOOL_CALLS]{{content}}\", {\"eos_token\"}], tool_format=\"mistral\"),\n    format_observation=StringFormatter(slots=[\"\"\"[TOOL_RESULTS]{\"content\": {{content}}}[/TOOL_RESULTS]\"\"\"]),\n    format_tools=ToolFormatter(tool_format=\"mistral\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    mm_plugin=get_mm_plugin(name=\"pixtral\", image_token=\"[IMG]\"),\n    template_class=Llama2Template,\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"qwen\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n)\n\n\n# copied from qwen template\nregister_template(\n    name=\"qwen3\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    template_class=ReasoningTemplate,\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"qwen2_audio\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    default_system=\"You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"qwen2_audio\", audio_token=\"<|AUDIO|>\"),\n)\n\n\n# copied from qwen template\nregister_template(\n    name=\"qwen2_omni\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(\n        name=\"qwen2_omni\", audio_token=\"<|AUDIO|>\", image_token=\"<|IMAGE|>\", video_token=\"<|VIDEO|>\"\n    ),\n)\n\n# copied from qwen template\nregister_template(\n    name=\"qwen2_vl\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\"),\n    format_observation=StringFormatter(\n        slots=[\"<|im_start|>user\\n<tool_response>\\n{{content}}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n\"]\n    ),\n    format_tools=ToolFormatter(tool_format=\"qwen\"),\n    default_system=\"You are a helpful assistant.\",\n    stop_words=[\"<|im_end|>\"],\n    replace_eos=True,\n    mm_plugin=get_mm_plugin(name=\"qwen2_vl\", image_token=\"<|image_pad|>\", video_token=\"<|video_pad|>\"),\n)\n\n\nregister_template(\n    name=\"sailor\",\n    format_user=StringFormatter(slots=[\"<|im_start|>question\\n{{content}}<|im_end|>\\n<|im_start|>answer\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    default_system=(\n        \"You are an AI assistant named Sailor created by Sea AI Lab. \"\n        \"Your answer should be friendly, unbiased, faithful, informative and detailed.\"\n    ),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\nregister_template(\n    name=\"seed_coder\",\n    format_user=StringFormatter(\n        slots=[{\"bos_token\"}, \"user\\n{{content}}\", {\"eos_token\"}, {\"bos_token\"}, \"assistant\\n\"]\n    ),\n    format_system=StringFormatter(slots=[{\"bos_token\"}, \"system\\n{{content}}\", {\"eos_token\"}]),\n    default_system=(\n        \"You are an AI programming assistant, utilizing the Seed-Coder model, developed by ByteDance Seed, \"\n        \"and you only answer questions related to computer science. For politically sensitive questions, \"\n        \"security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n\\n\"\n    ),\n)\n\n\n# copied from llama3 template\nregister_template(\n    name=\"skywork_o1\",\n    format_user=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|eot_id|>\"]),\n    format_system=StringFormatter(slots=[\"<|start_header_id|>system<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"]),\n    format_function=FunctionFormatter(slots=[\"{{content}}<|eot_id|>\"], tool_format=\"llama3\"),\n    format_observation=StringFormatter(\n        slots=[\n            (\n                \"<|start_header_id|>ipython<|end_header_id|>\\n\\n{{content}}<|eot_id|>\"\n                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            )\n        ]\n    ),\n    format_tools=ToolFormatter(tool_format=\"llama3\"),\n    format_prefix=EmptyFormatter(slots=[{\"bos_token\"}]),\n    default_system=(\n        \"You are Skywork-o1, a thinking model developed by Skywork AI, specializing in solving complex problems \"\n        \"involving mathematics, coding, and logical reasoning through deep thought. When faced with a user's request, \"\n        \"you first engage in a lengthy and in-depth thinking process to explore possible solutions to the problem. \"\n        \"After completing your thoughts, you then provide a detailed explanation of the solution process \"\n        \"in your response.\"\n    ),\n    stop_words=[\"<|eot_id|>\", \"<|eom_id|>\"],\n)\n\n\nregister_template(\n    name=\"smollm\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\nregister_template(\n    name=\"smollm2\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n    default_system=\"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\",\n)\n\n\nregister_template(\n    name=\"solar\",\n    format_user=StringFormatter(slots=[\"### User:\\n{{content}}\\n\\n### Assistant:\\n\"]),\n    format_system=StringFormatter(slots=[\"### System:\\n{{content}}\\n\\n\"]),\n    efficient_eos=True,\n)\n\n\nregister_template(\n    name=\"starchat\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}<|end|>\\n<|assistant|>\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}<|end|>\\n\"]),\n    stop_words=[\"<|end|>\"],\n)\n\n\nregister_template(\n    name=\"telechat\",\n    format_user=StringFormatter(slots=[\"<_user>{{content}}<_bot>\"]),\n    format_system=StringFormatter(slots=[\"<_system>{{content}}<_end>\"]),\n)\n\n\nregister_template(\n    name=\"telechat2\",\n    format_user=StringFormatter(slots=[\"<_user>{{content}}<_bot>\"]),\n    format_system=StringFormatter(slots=[\"<_system>{{content}}\"]),\n    default_system=(\n        \"ä½ æ˜¯ä¸­å›½ç”µä¿¡æ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡åž‹ï¼Œè‹±æ–‡åæ˜¯TeleChatï¼Œä½ æ˜¯ç”±ä¸­ç”µä¿¡äººå·¥æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸å’Œä¸­å›½ç”µä¿¡äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ï¼ˆTeleAIï¼‰ç ”å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚\"\n    ),\n)\n\n\nregister_template(\n    name=\"vicuna\",\n    format_user=StringFormatter(slots=[\"USER: {{content}} ASSISTANT:\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    replace_jinja_template=True,\n)\n\n\nregister_template(\n    name=\"video_llava\",\n    format_user=StringFormatter(slots=[\"USER: {{content}} ASSISTANT:\"]),\n    default_system=(\n        \"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    ),\n    mm_plugin=get_mm_plugin(name=\"video_llava\", image_token=\"<image>\", video_token=\"<video>\"),\n)\n\n\nregister_template(\n    name=\"xuanyuan\",\n    format_user=StringFormatter(slots=[\"Human: {{content}} Assistant:\"]),\n    default_system=(\n        \"ä»¥ä¸‹æ˜¯ç”¨æˆ·å’Œäººå·¥æ™ºèƒ½åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·ä»¥Humanå¼€å¤´ï¼Œäººå·¥æ™ºèƒ½åŠ©æ‰‹ä»¥Assistantå¼€å¤´ï¼Œ\"\n        \"ä¼šå¯¹äººç±»æå‡ºçš„é—®é¢˜ç»™å‡ºæœ‰å¸®åŠ©ã€é«˜è´¨é‡ã€è¯¦ç»†å’Œç¤¼è²Œçš„å›žç­”ï¼Œå¹¶ä¸”æ€»æ˜¯æ‹’ç»å‚ä¸Žä¸Žä¸é“å¾·ã€\"\n        \"ä¸å®‰å…¨ã€æœ‰äº‰è®®ã€æ”¿æ²»æ•æ„Ÿç­‰ç›¸å…³çš„è¯é¢˜ã€é—®é¢˜å’ŒæŒ‡ç¤ºã€‚\\n\"\n    ),\n)\n\n\nregister_template(\n    name=\"xverse\",\n    format_user=StringFormatter(slots=[\"Human: {{content}}\\n\\nAssistant: \"]),\n)\n\n\nregister_template(\n    name=\"yayi\",\n    format_user=StringFormatter(slots=[{\"token\": \"<|Human|>\"}, \":\\n{{content}}\\n\\n\", {\"token\": \"<|YaYi|>\"}, \":\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\\n\"]),\n    format_system=StringFormatter(slots=[{\"token\": \"<|System|>\"}, \":\\n{{content}}\\n\\n\"]),\n    default_system=(\n        \"You are a helpful, respectful and honest assistant named YaYi \"\n        \"developed by Beijing Wenge Technology Co.,Ltd. \"\n        \"Always answer as helpfully as possible, while being safe.  \"\n        \"Your answers should not include any harmful, unethical, \"\n        \"racist, sexist, toxic, dangerous, or illegal content. \"\n        \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n        \"If a question does not make any sense, or is not factually coherent, \"\n        \"explain why instead of answering something not correct. \"\n        \"If you don't know the answer to a question, please don't share false information.\"\n    ),\n    stop_words=[\"<|End|>\"],\n)\n\n\n# copied from chatml template\nregister_template(\n    name=\"yi\",\n    format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<|im_end|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\n    stop_words=[\"<|im_end|>\"],\n)\n\n\nregister_template(\n    name=\"yi_vl\",\n    format_user=StringFormatter(slots=[\"### Human: {{content}}\\n### Assistant:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\"]),\n    default_system=(\n        \"This is a chat between an inquisitive human and an AI assistant. \"\n        \"Assume the role of the AI assistant. Read all the images carefully, \"\n        \"and respond to the human's questions with informative, helpful, detailed and polite answers. \"\n        \"è¿™æ˜¯ä¸€ä¸ªå¥½å¥‡çš„äººç±»å’Œä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚å‡è®¾ä½ æ‰®æ¼”è¿™ä¸ªAIåŠ©æ‰‹çš„è§’è‰²ã€‚\"\n        \"ä»”ç»†é˜…è¯»æ‰€æœ‰çš„å›¾åƒï¼Œå¹¶å¯¹äººç±»çš„é—®é¢˜åšå‡ºä¿¡æ¯ä¸°å¯Œã€æœ‰å¸®åŠ©ã€è¯¦ç»†çš„å’Œç¤¼è²Œçš„å›žç­”ã€‚\\n\\n\"\n    ),\n    stop_words=[\"###\"],\n    efficient_eos=True,\n    mm_plugin=get_mm_plugin(name=\"llava\", image_token=\"<image>\"),\n)\n\n\nregister_template(\n    name=\"yuan\",\n    format_user=StringFormatter(slots=[\"{{content}}\", {\"token\": \"<sep>\"}]),\n    format_assistant=StringFormatter(slots=[\"{{content}}<eod>\\n\"]),\n    stop_words=[\"<eod>\"],\n)\n\n\nregister_template(\n    name=\"zephyr\",\n    format_user=StringFormatter(slots=[\"<|user|>\\n{{content}}\", {\"eos_token\"}, \"<|assistant|>\\n\"]),\n    format_system=StringFormatter(slots=[\"<|system|>\\n{{content}}\", {\"eos_token\"}]),\n    default_system=\"You are Zephyr, a helpful assistant.\",\n)\n\n\nregister_template(\n    name=\"ziya\",\n    format_user=StringFormatter(slots=[\"<human>:{{content}}\\n<bot>:\"]),\n    format_assistant=StringFormatter(slots=[\"{{content}}\\n\"]),\n)\n",
          "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom collections import OrderedDict, defaultdict\nfrom enum import Enum, unique\nfrom typing import Optional\n\nfrom peft.utils import SAFETENSORS_WEIGHTS_NAME as SAFE_ADAPTER_WEIGHTS_NAME\nfrom peft.utils import WEIGHTS_NAME as ADAPTER_WEIGHTS_NAME\nfrom transformers.utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n\n\nAUDIO_PLACEHOLDER = os.getenv(\"AUDIO_PLACEHOLDER\", \"<audio>\")\n\nCHECKPOINT_NAMES = {\n    SAFE_ADAPTER_WEIGHTS_NAME,\n    ADAPTER_WEIGHTS_NAME,\n    SAFE_WEIGHTS_INDEX_NAME,\n    SAFE_WEIGHTS_NAME,\n    WEIGHTS_INDEX_NAME,\n    WEIGHTS_NAME,\n}\n\nCHOICES = [\"A\", \"B\", \"C\", \"D\"]\n\nDATA_CONFIG = \"dataset_info.json\"\n\nDEFAULT_TEMPLATE = defaultdict(str)\n\nFILEEXT2TYPE = {\n    \"arrow\": \"arrow\",\n    \"csv\": \"csv\",\n    \"json\": \"json\",\n    \"jsonl\": \"json\",\n    \"parquet\": \"parquet\",\n    \"txt\": \"text\",\n}\n\nIGNORE_INDEX = -100\n\nIMAGE_PLACEHOLDER = os.getenv(\"IMAGE_PLACEHOLDER\", \"<image>\")\n\nLAYERNORM_NAMES = {\"norm\", \"ln\"}\n\nLLAMABOARD_CONFIG = \"llamaboard_config.yaml\"\n\nMETHODS = [\"full\", \"freeze\", \"lora\"]\n\nMOD_SUPPORTED_MODELS = {\"bloom\", \"falcon\", \"gemma\", \"llama\", \"mistral\", \"mixtral\", \"phi\", \"starcoder2\"}\n\nMULTIMODAL_SUPPORTED_MODELS = set()\n\nPEFT_METHODS = {\"lora\"}\n\nRUNNING_LOG = \"running_log.txt\"\n\nSUBJECTS = [\"Average\", \"STEM\", \"Social Sciences\", \"Humanities\", \"Other\"]\n\nSUPPORTED_MODELS = OrderedDict()\n\nTRAINER_LOG = \"trainer_log.jsonl\"\n\nTRAINING_ARGS = \"training_args.yaml\"\n\nTRAINING_STAGES = {\n    \"Supervised Fine-Tuning\": \"sft\",\n    \"Reward Modeling\": \"rm\",\n    \"PPO\": \"ppo\",\n    \"DPO\": \"dpo\",\n    \"KTO\": \"kto\",\n    \"Pre-Training\": \"pt\",\n}\n\nSTAGES_USE_PAIR_DATA = {\"rm\", \"dpo\"}\n\nSUPPORTED_CLASS_FOR_S2ATTN = {\"llama\"}\n\nSWANLAB_CONFIG = \"swanlab_public_config.json\"\n\nVIDEO_PLACEHOLDER = os.getenv(\"VIDEO_PLACEHOLDER\", \"<video>\")\n\nV_HEAD_WEIGHTS_NAME = \"value_head.bin\"\n\nV_HEAD_SAFE_WEIGHTS_NAME = \"value_head.safetensors\"\n\n\nclass AttentionFunction(str, Enum):\n    AUTO = \"auto\"\n    DISABLED = \"disabled\"\n    SDPA = \"sdpa\"\n    FA2 = \"fa2\"\n\n\nclass EngineName(str, Enum):\n    HF = \"huggingface\"\n    VLLM = \"vllm\"\n    SGLANG = \"sglang\"\n\n\nclass DownloadSource(str, Enum):\n    DEFAULT = \"hf\"\n    MODELSCOPE = \"ms\"\n    OPENMIND = \"om\"\n\n\n@unique\nclass QuantizationMethod(str, Enum):\n    r\"\"\"Borrowed from `transformers.utils.quantization_config.QuantizationMethod`.\"\"\"\n\n    BNB = \"bnb\"\n    GPTQ = \"gptq\"\n    AWQ = \"awq\"\n    AQLM = \"aqlm\"\n    QUANTO = \"quanto\"\n    EETQ = \"eetq\"\n    HQQ = \"hqq\"\n\n\nclass RopeScaling(str, Enum):\n    LINEAR = \"linear\"\n    DYNAMIC = \"dynamic\"\n    YARN = \"yarn\"\n    LLAMA3 = \"llama3\"\n\n\ndef register_model_group(\n    models: dict[str, dict[DownloadSource, str]],\n    template: Optional[str] = None,\n    multimodal: bool = False,\n) -> None:\n    for name, path in models.items():\n        SUPPORTED_MODELS[name] = path\n        if template is not None and (\n            any(suffix in name for suffix in (\"-Chat\", \"-Distill\", \"-Instruct\")) or multimodal\n        ):\n            DEFAULT_TEMPLATE[name] = template\n\n        if multimodal:\n            MULTIMODAL_SUPPORTED_MODELS.add(name)\n\n\nregister_model_group(\n    models={\n        \"Aya-23-8B-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/aya-23-8B\",\n        },\n        \"Aya-23-35B-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/aya-23-35B\",\n        },\n    },\n    template=\"cohere\",\n)\n\n\nregister_model_group(\n    models={\n        \"Baichuan-7B-Base\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan-7B\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/baichuan-7B\",\n        },\n        \"Baichuan-13B-Base\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan-13B-Base\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan-13B-Base\",\n        },\n        \"Baichuan-13B-Chat\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan-13B-Chat\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan-13B-Chat\",\n        },\n    },\n    template=\"baichuan\",\n)\n\n\nregister_model_group(\n    models={\n        \"Baichuan2-7B-Base\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan2-7B-Base\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan2-7B-Base\",\n        },\n        \"Baichuan2-13B-Base\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan2-13B-Base\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan2-13B-Base\",\n            DownloadSource.OPENMIND: \"Baichuan/Baichuan2_13b_base_pt\",\n        },\n        \"Baichuan2-7B-Chat\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan2-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan2-7B-Chat\",\n            DownloadSource.OPENMIND: \"Baichuan/Baichuan2_7b_chat_pt\",\n        },\n        \"Baichuan2-13B-Chat\": {\n            DownloadSource.DEFAULT: \"baichuan-inc/Baichuan2-13B-Chat\",\n            DownloadSource.MODELSCOPE: \"baichuan-inc/Baichuan2-13B-Chat\",\n            DownloadSource.OPENMIND: \"Baichuan/Baichuan2_13b_chat_pt\",\n        },\n    },\n    template=\"baichuan2\",\n)\n\n\nregister_model_group(\n    models={\n        \"BLOOM-560M\": {\n            DownloadSource.DEFAULT: \"bigscience/bloom-560m\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloom-560m\",\n        },\n        \"BLOOM-3B\": {\n            DownloadSource.DEFAULT: \"bigscience/bloom-3b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloom-3b\",\n        },\n        \"BLOOM-7B1\": {\n            DownloadSource.DEFAULT: \"bigscience/bloom-7b1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloom-7b1\",\n        },\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"BLOOMZ-560M\": {\n            DownloadSource.DEFAULT: \"bigscience/bloomz-560m\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloomz-560m\",\n        },\n        \"BLOOMZ-3B\": {\n            DownloadSource.DEFAULT: \"bigscience/bloomz-3b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloomz-3b\",\n        },\n        \"BLOOMZ-7B1-mt\": {\n            DownloadSource.DEFAULT: \"bigscience/bloomz-7b1-mt\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/bloomz-7b1-mt\",\n        },\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"BlueLM-7B-Base\": {\n            DownloadSource.DEFAULT: \"vivo-ai/BlueLM-7B-Base\",\n            DownloadSource.MODELSCOPE: \"vivo-ai/BlueLM-7B-Base\",\n        },\n        \"BlueLM-7B-Chat\": {\n            DownloadSource.DEFAULT: \"vivo-ai/BlueLM-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"vivo-ai/BlueLM-7B-Chat\",\n        },\n    },\n    template=\"bluelm\",\n)\n\n\nregister_model_group(\n    models={\n        \"Breeze-7B\": {\n            DownloadSource.DEFAULT: \"MediaTek-Research/Breeze-7B-Base-v1_0\",\n        },\n        \"Breeze-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"MediaTek-Research/Breeze-7B-Instruct-v1_0\",\n        },\n    },\n    template=\"breeze\",\n)\n\n\nregister_model_group(\n    models={\n        \"ChatGLM2-6B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/chatglm2-6b\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/chatglm2-6b\",\n        }\n    },\n    template=\"chatglm2\",\n)\n\n\nregister_model_group(\n    models={\n        \"ChatGLM3-6B-Base\": {\n            DownloadSource.DEFAULT: \"THUDM/chatglm3-6b-base\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/chatglm3-6b-base\",\n        },\n        \"ChatGLM3-6B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/chatglm3-6b\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/chatglm3-6b\",\n        },\n    },\n    template=\"chatglm3\",\n)\n\n\nregister_model_group(\n    models={\n        \"Chinese-Llama-2-1.3B\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-llama-2-1.3b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-llama-2-1.3b\",\n        },\n        \"Chinese-Llama-2-7B\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-llama-2-7b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-llama-2-7b\",\n        },\n        \"Chinese-Llama-2-13B\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-llama-2-13b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-llama-2-13b\",\n        },\n        \"Chinese-Alpaca-2-1.3B-Chat\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-alpaca-2-1.3b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-alpaca-2-1.3b\",\n        },\n        \"Chinese-Alpaca-2-7B-Chat\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-alpaca-2-7b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-alpaca-2-7b\",\n        },\n        \"Chinese-Alpaca-2-13B-Chat\": {\n            DownloadSource.DEFAULT: \"hfl/chinese-alpaca-2-13b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/chinese-alpaca-2-13b\",\n        },\n    },\n    template=\"llama2_zh\",\n)\n\n\nregister_model_group(\n    models={\n        \"CodeGeeX4-9B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/codegeex4-all-9b\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/codegeex4-all-9b\",\n        },\n    },\n    template=\"codegeex4\",\n)\n\n\nregister_model_group(\n    models={\n        \"CodeGemma-7B\": {\n            DownloadSource.DEFAULT: \"google/codegemma-7b\",\n        },\n        \"CodeGemma-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/codegemma-7b-it\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/codegemma-7b-it\",\n        },\n        \"CodeGemma-1.1-2B\": {\n            DownloadSource.DEFAULT: \"google/codegemma-1.1-2b\",\n        },\n        \"CodeGemma-1.1-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/codegemma-1.1-7b-it\",\n        },\n    },\n    template=\"gemma\",\n)\n\n\nregister_model_group(\n    models={\n        \"Codestral-22B-v0.1-Chat\": {\n            DownloadSource.DEFAULT: \"mistralai/Codestral-22B-v0.1\",\n            DownloadSource.MODELSCOPE: \"swift/Codestral-22B-v0.1\",\n        },\n    },\n    template=\"mistral\",\n)\n\n\nregister_model_group(\n    models={\n        \"CommandR-35B-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/c4ai-command-r-v01\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/c4ai-command-r-v01\",\n        },\n        \"CommandR-Plus-104B-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/c4ai-command-r-plus\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/c4ai-command-r-plus\",\n        },\n        \"CommandR-35B-4bit-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/c4ai-command-r-v01-4bit\",\n            DownloadSource.MODELSCOPE: \"mirror013/c4ai-command-r-v01-4bit\",\n        },\n        \"CommandR-Plus-104B-4bit-Chat\": {\n            DownloadSource.DEFAULT: \"CohereForAI/c4ai-command-r-plus-4bit\",\n        },\n    },\n    template=\"cohere\",\n)\n\n\nregister_model_group(\n    models={\n        \"DBRX-132B-Base\": {\n            DownloadSource.DEFAULT: \"databricks/dbrx-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/dbrx-base\",\n        },\n        \"DBRX-132B-Instruct\": {\n            DownloadSource.DEFAULT: \"databricks/dbrx-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/dbrx-instruct\",\n        },\n    },\n    template=\"dbrx\",\n)\n\n\nregister_model_group(\n    models={\n        \"DeepSeek-LLM-7B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-llm-7b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-llm-7b-base\",\n        },\n        \"DeepSeek-LLM-67B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-llm-67b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-llm-67b-base\",\n        },\n        \"DeepSeek-LLM-7B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-llm-7b-chat\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-llm-7b-chat\",\n        },\n        \"DeepSeek-LLM-67B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-llm-67b-chat\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-llm-67b-chat\",\n        },\n        \"DeepSeek-Math-7B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-math-7b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-math-7b-base\",\n        },\n        \"DeepSeek-Math-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-math-7b-instruct\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-math-7b-instruct\",\n        },\n        \"DeepSeek-MoE-16B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-moe-16b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-moe-16b-base\",\n        },\n        \"DeepSeek-MoE-16B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-moe-16b-chat\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-moe-16b-chat\",\n        },\n        \"DeepSeek-V2-16B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2-Lite\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2-Lite\",\n        },\n        \"DeepSeek-V2-236B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2\",\n        },\n        \"DeepSeek-V2-16B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2-Lite-Chat\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2-Lite-Chat\",\n        },\n        \"DeepSeek-V2-236B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2-Chat\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2-Chat\",\n        },\n        \"DeepSeek-Coder-V2-16B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\",\n        },\n        \"DeepSeek-Coder-V2-236B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-Coder-V2-Base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-Coder-V2-Base\",\n        },\n        \"DeepSeek-Coder-V2-16B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n        },\n        \"DeepSeek-Coder-V2-236B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-Coder-V2-Instruct\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-Coder-V2-Instruct\",\n        },\n    },\n    template=\"deepseek\",\n)\n\n\nregister_model_group(\n    models={\n        \"DeepSeek-Coder-6.7B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-6.7b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-6.7b-base\",\n        },\n        \"DeepSeek-Coder-7B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-7b-base-v1.5\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-7b-base-v1.5\",\n        },\n        \"DeepSeek-Coder-33B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-33b-base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-33b-base\",\n        },\n        \"DeepSeek-Coder-6.7B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n        },\n        \"DeepSeek-Coder-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n        },\n        \"DeepSeek-Coder-33B-Instruct\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/deepseek-coder-33b-instruct\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/deepseek-coder-33b-instruct\",\n        },\n    },\n    template=\"deepseekcoder\",\n)\n\n\nregister_model_group(\n    models={\n        \"DeepSeek-V2-0628-236B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2-Chat-0628\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2-Chat-0628\",\n        },\n        \"DeepSeek-V2.5-236B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2.5\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2.5\",\n        },\n        \"DeepSeek-V2.5-1210-236B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V2.5-1210\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V2.5-1210\",\n        },\n        \"DeepSeek-V3-671B-Base\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V3-Base\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V3-Base\",\n        },\n        \"DeepSeek-V3-671B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V3\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V3\",\n        },\n        \"DeepSeek-V3-0324-671B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-V3-0324\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-V3-0324\",\n        },\n    },\n    template=\"deepseek3\",\n)\n\n\nregister_model_group(\n    models={\n        \"DeepSeek-R1-1.5B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        },\n        \"DeepSeek-R1-7B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n        },\n        \"DeepSeek-R1-8B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n        },\n        \"DeepSeek-R1-14B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n        },\n        \"DeepSeek-R1-32B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n        },\n        \"DeepSeek-R1-70B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n        },\n        \"DeepSeek-R1-671B-Chat-Zero\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-Zero\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-Zero\",\n        },\n        \"DeepSeek-R1-671B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1\",\n        },\n        \"DeepSeek-R1-0528-8B-Distill\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n        },\n        \"DeepSeek-R1-0528-671B-Chat\": {\n            DownloadSource.DEFAULT: \"deepseek-ai/DeepSeek-R1-0528\",\n            DownloadSource.MODELSCOPE: \"deepseek-ai/DeepSeek-R1-0528\",\n        },\n    },\n    template=\"deepseekr1\",\n)\n\n\nregister_model_group(\n    models={\n        \"EXAONE-3.0-7.8B-Instruct\": {\n            DownloadSource.DEFAULT: \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n        },\n    },\n    template=\"exaone\",\n)\n\n\nregister_model_group(\n    models={\n        \"Falcon-7B\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-7b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/falcon-7b\",\n        },\n        \"Falcon-11B\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-11B\",\n            DownloadSource.MODELSCOPE: \"tiiuae/falcon-11B\",\n        },\n        \"Falcon-40B\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-40b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/falcon-40b\",\n        },\n        \"Falcon-180B\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-180b\",\n            DownloadSource.MODELSCOPE: \"modelscope/falcon-180B\",\n        },\n        \"Falcon-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-7b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/falcon-7b-instruct\",\n        },\n        \"Falcon-40B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-40b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/falcon-40b-instruct\",\n        },\n        \"Falcon-180B-Chat\": {\n            DownloadSource.DEFAULT: \"tiiuae/falcon-180b-chat\",\n            DownloadSource.MODELSCOPE: \"modelscope/falcon-180B-chat\",\n        },\n    },\n    template=\"falcon\",\n)\n\nregister_model_group(\n    models={\n        \"Falcon-H1-0.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-0.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-0.5B-Instruct\",\n        },\n        \"Falcon-H1-0.5B-Base\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-0.5B-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-0.5B-Base\",\n        },\n        \"Falcon-H1-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-1.5B-Instruct\",\n        },\n        \"Falcon-H1-1.5B-Base\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-1.5B-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-1.5B-Base\",\n        },\n        \"Falcon-H1-1.5B-Deep-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-1.5B-Deep-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-1.5B-Deep-Instruct\",\n        },\n        \"Falcon-H1-1.5B-Deep-Base\": {\n            DownloadSource.DEFAULT: \"tiuae/Falcon-H1-1.5B-Deep-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-1.5B-Deep-Base\",\n        },\n        \"Falcon-H1-3B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-3B-Instruct\",\n        },\n        \"Falcon-H1-3B-Base\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-3B-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-3B-Base\",\n        },\n        \"Falcon-H1-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-7B-Instruct\",\n        },\n        \"Falcon-H1-7B-Base\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-7B-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-7B-Base\",\n        },\n        \"Falcon-H1-34B-Instruct\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-34B-Instruct\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-34B-Instruct\",\n        },\n        \"Falcon-H1-34B-Base\": {\n            DownloadSource.DEFAULT: \"tiiuae/Falcon-H1-34B-Base\",\n            DownloadSource.MODELSCOPE: \"tiiuae/Falcon-H1-34B-Base\",\n        },\n        \n    },\n    template=\"falcon_h1\",\n)\n\n\nregister_model_group(\n    models={\n        \"Gemma-2B\": {\n            DownloadSource.DEFAULT: \"google/gemma-2b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gemma-2b\",\n        },\n        \"Gemma-7B\": {\n            DownloadSource.DEFAULT: \"google/gemma-7b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gemma-2b-it\",\n        },\n        \"Gemma-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-2b-it\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gemma-7b\",\n        },\n        \"Gemma-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-7b-it\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gemma-7b-it\",\n        },\n        \"Gemma-1.1-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-1.1-2b-it\",\n        },\n        \"Gemma-1.1-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-1.1-7b-it\",\n        },\n        \"Gemma-2-2B\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-2b\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-2b\",\n        },\n        \"Gemma-2-9B\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-9b\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-9b\",\n        },\n        \"Gemma-2-27B\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-27b\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-27b\",\n        },\n        \"Gemma-2-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-2b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-2b-it\",\n            DownloadSource.OPENMIND: \"LlamaFactory/gemma-2-2b-it\",\n        },\n        \"Gemma-2-9B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-9b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-9b-it\",\n            DownloadSource.OPENMIND: \"LlamaFactory/gemma-2-9b-it\",\n        },\n        \"Gemma-2-27B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-2-27b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-2-27b-it\",\n        },\n        \"Gemma-3-1B\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-1b-pt\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-1b-pt\",\n        },\n        \"Gemma-3-1B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-1b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-1b-it\",\n        },\n        \"MedGemma-27B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/medgemma-27b-text-it\",\n            DownloadSource.MODELSCOPE: \"google/medgemma-27b-text-it\",\n        },\n    },\n    template=\"gemma\",\n)\n\n\nregister_model_group(\n    models={\n        \"Gemma-3-4B\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-4b-pt\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-4b-pt\",\n        },\n        \"Gemma-3-12B\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-12b-pt\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-12b-pt\",\n        },\n        \"Gemma-3-27B\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-27b-pt\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-27b-pt\",\n        },\n        \"Gemma-3-4B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-4b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-4b-it\",\n        },\n        \"Gemma-3-12B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-12b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-12b-it\",\n        },\n        \"Gemma-3-27B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/gemma-3-27b-it\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/gemma-3-27b-it\",\n        },\n        \"MedGemma-4B\": {\n            DownloadSource.DEFAULT: \"google/medgemma-4b-pt\",\n            DownloadSource.MODELSCOPE: \"google/medgemma-4b-pt\",\n        },\n        \"MedGemma-4B-Instruct\": {\n            DownloadSource.DEFAULT: \"google/medgemma-4b-it\",\n            DownloadSource.MODELSCOPE: \"google/medgemma-4b-it\",\n        },\n    },\n    template=\"gemma3\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"GLM-4-9B\": {\n            DownloadSource.DEFAULT: \"THUDM/glm-4-9b\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/glm-4-9b\",\n        },\n        \"GLM-4-9B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/glm-4-9b-chat\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/glm-4-9b-chat\",\n            DownloadSource.OPENMIND: \"LlamaFactory/glm-4-9b-chat\",\n        },\n        \"GLM-4-9B-1M-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/glm-4-9b-chat-1m\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/glm-4-9b-chat-1m\",\n        },\n        \"GLM-4-0414-9B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/GLM-4-9B-0414\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/GLM-4-9B-0414\",\n        },\n        \"GLM-4-0414-32B-Base\": {\n            DownloadSource.DEFAULT: \"THUDM/GLM-4-32B-Base-0414\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/GLM-4-32B-Base-0414\",\n        },\n        \"GLM-4-0414-32B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/GLM-4-32B-0414\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/GLM-4-32B-0414\",\n        },\n    },\n    template=\"glm4\",\n)\n\n\nregister_model_group(\n    models={\n        \"GLM-Z1-0414-9B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/GLM-Z1-9B-0414\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/GLM-Z1-9B-0414\",\n        },\n        \"GLM-Z1-0414-32B-Chat\": {\n            DownloadSource.DEFAULT: \"THUDM/GLM-Z1-32B-0414\",\n            DownloadSource.MODELSCOPE: \"ZhipuAI/GLM-Z1-32B-0414\",\n        },\n    },\n    template=\"glmz1\",\n)\n\n\nregister_model_group(\n    models={\n        \"GPT-2-Small\": {\n            DownloadSource.DEFAULT: \"openai-community/gpt2\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gpt2\",\n        },\n        \"GPT-2-Medium\": {\n            DownloadSource.DEFAULT: \"openai-community/gpt2-medium\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gpt2-medium\",\n        },\n        \"GPT-2-Large\": {\n            DownloadSource.DEFAULT: \"openai-community/gpt2-large\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/gpt2-large\",\n        },\n        \"GPT-2-XL\": {\n            DownloadSource.DEFAULT: \"openai-community/gpt2-xl\",\n            DownloadSource.MODELSCOPE: \"goodbai95/GPT2-xl\",\n        },\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"Granite-3.0-1B-A400M-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-1b-a400m-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-1b-a400m-base\",\n        },\n        \"Granite-3.0-3B-A800M-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-3b-a800m-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-3b-a800m-base\",\n        },\n        \"Granite-3.0-2B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-2b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-2b-base\",\n        },\n        \"Granite-3.0-8B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-8b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-8b-base\",\n        },\n        \"Granite-3.0-1B-A400M-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-1b-a400m-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-1b-a400m-instruct\",\n        },\n        \"Granite-3.0-3B-A800M-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-3b-a800m-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-3b-a800m-instruct\",\n        },\n        \"Granite-3.0-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-2b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-2b-instruct\",\n        },\n        \"Granite-3.0-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.0-8b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.0-8b-instruct\",\n        },\n        \"Granite-3.1-1B-A400M-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-1b-a400m-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-1b-a400m-base\",\n        },\n        \"Granite-3.1-3B-A800M-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-3b-a800m-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-3b-a800m-base\",\n        },\n        \"Granite-3.1-2B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-2b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-2b-base\",\n        },\n        \"Granite-3.1-8B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-8b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-8b-base\",\n        },\n        \"Granite-3.1-1B-A400M-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-1b-a400m-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-1b-a400m-instruct\",\n        },\n        \"Granite-3.1-3B-A800M-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-3b-a800m-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-3b-a800m-instruct\",\n        },\n        \"Granite-3.1-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-2b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-2b-instruct\",\n        },\n        \"Granite-3.1-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.1-8b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.1-8b-instruct\",\n        },\n        \"Granite-3.2-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.2-2b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.2-2b-instruct\",\n        },\n        \"Granite-3.2-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.2-8b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.2-8b-instruct\",\n        },\n        \"Granite-3.3-2B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.3-2b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.3-2b-base\",\n        },\n        \"Granite-3.3-8B-Base\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.3-8b-base\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.3-8b-base\",\n        },\n        \"Granite-3.3-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.3-2b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.3-2b-instruct\",\n        },\n        \"Granite-3.3-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-3.3-8b-instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-3.3-8b-instruct\",\n        },\n    },\n    template=\"granite3\",\n)\n\n\nregister_model_group(\n    models={\n        \"Granite-Vision-3.2-2B\": {\n            DownloadSource.DEFAULT: \"ibm-granite/granite-vision-3.2-2b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/granite-vision-3.2-2b\",\n        },\n    },\n    template=\"granite3_vision\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Hunyuan-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"tencent/Hunyuan-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Hunyuan-7B-Instruct\",\n        },\n    },\n    template=\"hunyuan\",\n)\n\n\nregister_model_group(\n    models={\n        \"Index-1.9B-Base\": {\n            DownloadSource.DEFAULT: \"IndexTeam/Index-1.9B\",\n            DownloadSource.MODELSCOPE: \"IndexTeam/Index-1.9B\",\n        },\n        \"Index-1.9B-Base-Pure\": {\n            DownloadSource.DEFAULT: \"IndexTeam/Index-1.9B-Pure\",\n            DownloadSource.MODELSCOPE: \"IndexTeam/Index-1.9B-Pure\",\n        },\n        \"Index-1.9B-Chat\": {\n            DownloadSource.DEFAULT: \"IndexTeam/Index-1.9B-Chat\",\n            DownloadSource.MODELSCOPE: \"IndexTeam/Index-1.9B-Chat\",\n        },\n        \"Index-1.9B-Character-Chat\": {\n            DownloadSource.DEFAULT: \"IndexTeam/Index-1.9B-Character\",\n            DownloadSource.MODELSCOPE: \"IndexTeam/Index-1.9B-Character\",\n        },\n        \"Index-1.9B-Chat-32K\": {\n            DownloadSource.DEFAULT: \"IndexTeam/Index-1.9B-32K\",\n            DownloadSource.MODELSCOPE: \"IndexTeam/Index-1.9B-32K\",\n        },\n    },\n    template=\"index\",\n)\n\n\nregister_model_group(\n    models={\n        \"InternLM-7B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm-7b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm-7b\",\n        },\n        \"InternLM-20B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm-20b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm-20b\",\n        },\n        \"InternLM-7B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm-chat-7b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm-chat-7b\",\n        },\n        \"InternLM-20B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm-chat-20b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm-chat-20b\",\n        },\n    },\n    template=\"intern\",\n)\n\n\nregister_model_group(\n    models={\n        \"InternLM2-7B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2-7b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2-7b\",\n        },\n        \"InternLM2-20B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2-20b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2-20b\",\n        },\n        \"InternLM2-7B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2-chat-7b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2-chat-7b\",\n        },\n        \"InternLM2-20B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2-chat-20b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2-chat-20b\",\n        },\n        \"InternLM2.5-1.8B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-1_8b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-1_8b\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-1_8b\",\n        },\n        \"InternLM2.5-7B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-7b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-7b\",\n        },\n        \"InternLM2.5-20B\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-20b\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-20b\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-20b\",\n        },\n        \"InternLM2.5-1.8B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-1_8b-chat\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-1_8b-chat\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-1_8b-chat\",\n        },\n        \"InternLM2.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-7b-chat\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-7b-chat\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-7b-chat\",\n        },\n        \"InternLM2.5-7B-1M-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-7b-chat-1m\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-7b-chat-1m\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-7b-chat-1m\",\n        },\n        \"InternLM2.5-20B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm2_5-20b-chat\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm2_5-20b-chat\",\n            DownloadSource.OPENMIND: \"Intern/internlm2_5-20b-chat\",\n        },\n        \"InternLM3-8B-Chat\": {\n            DownloadSource.DEFAULT: \"internlm/internlm3-8b-instruct\",\n            DownloadSource.MODELSCOPE: \"Shanghai_AI_Laboratory/internlm3-8b-instruct\",\n        },\n    },\n    template=\"intern2\",\n)\n\n\nregister_model_group(\n    models={\n        \"InternVL2.5-2B-MPO\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL2_5-2B-MPO-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL2_5-2B-MPO-hf\",\n        },\n        \"InternVL2.5-8B-MPO\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL2_5-8B-MPO-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL2_5-8B-MPO-hf\",\n        },\n        \"InternVL3-1B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-1B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-1B-hf\",\n        },\n        \"InternVL3-2B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-2B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-2B-hf\",\n        },\n        \"InternVL3-8B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-8B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-8B-hf\",\n        },\n        \"InternVL3-14B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-14B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-14B-hf\",\n        },\n        \"InternVL3-38B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-38B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-38B-hf\",\n        },\n        \"InternVL3-78B-hf\": {\n            DownloadSource.DEFAULT: \"OpenGVLab/InternVL3-78B-hf\",\n            DownloadSource.MODELSCOPE: \"OpenGVLab/InternVL3-78B-hf\",\n        },\n    },\n    template=\"intern_vl\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Jamba-v0.1\": {\n            DownloadSource.DEFAULT: \"ai21labs/Jamba-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Jamba-v0.1\",\n        }\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"Kimi-VL-A3B-Instruct\": {\n            DownloadSource.DEFAULT: \"moonshotai/Kimi-VL-A3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"moonshotai/Kimi-VL-A3B-Instruct\",\n        },\n        \"Kimi-VL-A3B-Thinking\": {\n            DownloadSource.DEFAULT: \"moonshotai/Kimi-VL-A3B-Thinking\",\n            DownloadSource.MODELSCOPE: \"moonshotai/Kimi-VL-A3B-Thinking\",\n        },\n    },\n    template=\"kimi_vl\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LingoWhale-8B\": {\n            DownloadSource.DEFAULT: \"deeplang-ai/LingoWhale-8B\",\n            DownloadSource.MODELSCOPE: \"DeepLang/LingoWhale-8B\",\n        }\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"Llama-7B\": {\n            DownloadSource.DEFAULT: \"huggyllama/llama-7b\",\n            DownloadSource.MODELSCOPE: \"skyline2006/llama-7b\",\n        },\n        \"Llama-13B\": {\n            DownloadSource.DEFAULT: \"huggyllama/llama-13b\",\n            DownloadSource.MODELSCOPE: \"skyline2006/llama-13b\",\n        },\n        \"Llama-30B\": {\n            DownloadSource.DEFAULT: \"huggyllama/llama-30b\",\n            DownloadSource.MODELSCOPE: \"skyline2006/llama-30b\",\n        },\n        \"Llama-65B\": {\n            DownloadSource.DEFAULT: \"huggyllama/llama-65b\",\n            DownloadSource.MODELSCOPE: \"skyline2006/llama-65b\",\n        },\n    }\n)\n\n\nregister_model_group(\n    models={\n        \"Llama-2-7B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-7b-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-7b-ms\",\n        },\n        \"Llama-2-13B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-13b-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-13b-ms\",\n        },\n        \"Llama-2-70B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-70b-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-70b-ms\",\n        },\n        \"Llama-2-7B-Chat\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-7b-chat-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-7b-chat-ms\",\n        },\n        \"Llama-2-13B-Chat\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-13b-chat-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-13b-chat-ms\",\n        },\n        \"Llama-2-70B-Chat\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-2-70b-chat-hf\",\n            DownloadSource.MODELSCOPE: \"modelscope/Llama-2-70b-chat-ms\",\n        },\n    },\n    template=\"llama2\",\n)\n\n\nregister_model_group(\n    models={\n        \"Llama-3-8B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3-8B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3-8B\",\n        },\n        \"Llama-3-70B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3-70B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3-70B\",\n        },\n        \"Llama-3-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3-8B-Instruct\",\n        },\n        \"Llama-3-70B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3-70B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3-70B-Instruct\",\n        },\n        \"Llama-3-8B-Chinese-Chat\": {\n            DownloadSource.DEFAULT: \"shenzhi-wang/Llama3-8B-Chinese-Chat\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama3-8B-Chinese-Chat\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Llama3-Chinese-8B-Instruct\",\n        },\n        \"Llama-3-70B-Chinese-Chat\": {\n            DownloadSource.DEFAULT: \"shenzhi-wang/Llama3-70B-Chinese-Chat\",\n        },\n        \"Llama-3.1-8B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-8B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-8B\",\n        },\n        \"Llama-3.1-70B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-70B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-70B\",\n        },\n        \"Llama-3.1-405B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-405B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-405B\",\n        },\n        \"Llama-3.1-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-8B-Instruct\",\n        },\n        \"Llama-3.1-70B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-70B-Instruct\",\n        },\n        \"Llama-3.1-405B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Meta-Llama-3.1-405B-Instruct\",\n        },\n        \"Llama-3.1-8B-Chinese-Chat\": {\n            DownloadSource.DEFAULT: \"shenzhi-wang/Llama3.1-8B-Chinese-Chat\",\n            DownloadSource.MODELSCOPE: \"XD_AI/Llama3.1-8B-Chinese-Chat\",\n        },\n        \"Llama-3.1-70B-Chinese-Chat\": {\n            DownloadSource.DEFAULT: \"shenzhi-wang/Llama3.1-70B-Chinese-Chat\",\n            DownloadSource.MODELSCOPE: \"XD_AI/Llama3.1-70B-Chinese-Chat\",\n        },\n        \"Llama-3.2-1B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-1B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-1B\",\n        },\n        \"Llama-3.2-3B\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-3B\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-3B\",\n        },\n        \"Llama-3.2-1B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-1B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-1B-Instruct\",\n        },\n        \"Llama-3.2-3B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-3B-Instruct\",\n        },\n        \"Llama-3.3-70B-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.3-70B-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.3-70B-Instruct\",\n        },\n    },\n    template=\"llama3\",\n)\n\n\nregister_model_group(\n    models={\n        \"Llama-3.2-11B-Vision\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-11B-Vision\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-11B-Vision\",\n        },\n        \"Llama-3.2-11B-Vision-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-11B-Vision-Instruct\",\n        },\n        \"Llama-3.2-90B-Vision\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-90B-Vision\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-90B-Vision\",\n        },\n        \"Llama-3.2-90B-Vision-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-3.2-90B-Vision-Instruct\",\n        },\n    },\n    template=\"mllama\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Llama-4-Scout-17B-16E\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-4-Scout-17B-16E\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-4-Scout-17B-16E\",\n        },\n        \"Llama-4-Scout-17B-16E-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-4-Scout-17B-16E-Instruct\",\n        },\n        \"Llama-4-Maverick-17B-128E\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-4-Maverick-17B-128E\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-4-Maverick-17B-128E\",\n        },\n        \"Llama-4-Maverick-17B-128E-Instruct\": {\n            DownloadSource.DEFAULT: \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Llama-4-Maverick-17B-128E-Instruct\",\n        },\n    },\n    template=\"llama4\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-1.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-1.5-7b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llava-1.5-7b-hf\",\n        },\n        \"LLaVA-1.5-13B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-1.5-13b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llava-1.5-13b-hf\",\n        },\n    },\n    template=\"llava\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-7B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-v1.6-vicuna-7b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llava-v1.6-vicuna-7b-hf\",\n        },\n        \"LLaVA-NeXT-13B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-v1.6-vicuna-13b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llava-v1.6-vicuna-13b-hf\",\n        },\n    },\n    template=\"llava_next\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-Mistral-7B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-v1.6-mistral-7b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llava-v1.6-mistral-7b-hf\",\n        },\n    },\n    template=\"llava_next_mistral\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-Llama3-8B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llama3-llava-next-8b-hf\",\n            DownloadSource.MODELSCOPE: \"swift/llama3-llava-next-8b-hf\",\n        },\n    },\n    template=\"llava_next_llama3\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-34B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-v1.6-34b-hf\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/llava-v1.6-34b-hf\",\n        },\n    },\n    template=\"llava_next_yi\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-72B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-next-72b-hf\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/llava-next-72b-hf\",\n        },\n        \"LLaVA-NeXT-110B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/llava-next-110b-hf\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/llava-next-110b-hf\",\n        },\n    },\n    template=\"llava_next_qwen\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-Video-7B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n            DownloadSource.MODELSCOPE: \"swift/LLaVA-NeXT-Video-7B-hf\",\n        },\n        \"LLaVA-NeXT-Video-7B-DPO-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/LLaVA-NeXT-Video-7B-DPO-hf\",\n            DownloadSource.MODELSCOPE: \"swift/LLaVA-NeXT-Video-7B-DPO-hf\",\n        },\n    },\n    template=\"llava_next_video\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-Video-7B-32k-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/LLaVA-NeXT-Video-7B-32K-hf\",\n            DownloadSource.MODELSCOPE: \"swift/LLaVA-NeXT-Video-7B-32K-hf\",\n        },\n    },\n    template=\"llava_next_video_mistral\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"LLaVA-NeXT-Video-34B-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/LLaVA-NeXT-Video-34B-hf\",\n            DownloadSource.MODELSCOPE: \"swift/LLaVA-NeXT-Video-34B-hf\",\n        },\n        \"LLaVA-NeXT-Video-34B-DPO-Chat\": {\n            DownloadSource.DEFAULT: \"llava-hf/LLaVA-NeXT-Video-34B-DPO-hf\",\n        },\n    },\n    template=\"llava_next_video_yi\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Marco-o1-Chat\": {\n            DownloadSource.DEFAULT: \"AIDC-AI/Marco-o1\",\n            DownloadSource.MODELSCOPE: \"AIDC-AI/Marco-o1\",\n        },\n    },\n    template=\"marco\",\n)\n\n\nregister_model_group(\n    models={\n        \"MiMo-7B-Base\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-7B-Base\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-7B-Base\",\n        },\n        \"MiMo-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-7B-SFT\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-7B-SFT\",\n        },\n        \"MiMo-7B-Instruct-RL\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-7B-RL\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-7B-RL\",\n        },\n        \"MiMo-7B-RL-ZERO\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-7B-RL-ZERO\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-7B-RL-ZERO\",\n        },\n    },\n    template=\"mimo\",\n)\n\n\nregister_model_group(\n    models={\n        \"MiMo-7B-VL-Instruct\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-VL-7B-SFT\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-VL-7B-SFT\",\n        },\n        \"MiMo-7B-VL-RL\": {\n            DownloadSource.DEFAULT: \"XiaomiMiMo/MiMo-VL-7B-RL\",\n            DownloadSource.MODELSCOPE: \"XiaomiMiMo/MiMo-VL-7B-RL\",\n        },\n    },\n    template=\"mimo_vl\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"MiniCPM-2B-SFT-Chat\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM-2B-sft-bf16\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/miniCPM-bf16\",\n        },\n        \"MiniCPM-2B-DPO-Chat\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM-2B-dpo-bf16\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM-2B-dpo-bf16\",\n        },\n    },\n    template=\"cpm\",\n)\n\n\nregister_model_group(\n    models={\n        \"MiniCPM3-4B-Chat\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM3-4B\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM3-4B\",\n            DownloadSource.OPENMIND: \"LlamaFactory/MiniCPM3-4B\",\n        },\n    },\n    template=\"cpm3\",\n)\n\n\nregister_model_group(\n    models={\n        \"MiniCPM4-0.5B-Chat\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM4-0.5B\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM4-0.5B\",\n        },\n        \"MiniCPM4-8B-Chat\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM4-8B\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM4-8B\",\n        },\n    },\n    template=\"cpm4\",\n)\n\n\nregister_model_group(\n    models={\n        \"MiniCPM-o-2_6\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM-o-2_6\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM-o-2_6\",\n        },\n    },\n    template=\"minicpm_o\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"MiniCPM-V-2_6\": {\n            DownloadSource.DEFAULT: \"openbmb/MiniCPM-V-2_6\",\n            DownloadSource.MODELSCOPE: \"OpenBMB/MiniCPM-V-2_6\",\n        },\n    },\n    template=\"minicpm_v\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Ministral-8B-Instruct-2410\": {\n            DownloadSource.DEFAULT: \"mistralai/Ministral-8B-Instruct-2410\",\n            DownloadSource.MODELSCOPE: \"mistralai/Ministral-8B-Instruct-2410\",\n        },\n        \"Mistral-Nemo-Base-2407\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Nemo-Base-2407\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Mistral-Nemo-Base-2407\",\n        },\n        \"Mistral-Nemo-Instruct-2407\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Nemo-Instruct-2407\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mistral-Nemo-Instruct-2407\",\n        },\n    },\n    template=\"ministral\",\n)\n\n\nregister_model_group(\n    models={\n        \"Mistral-7B-v0.1\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-7B-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mistral-7B-v0.1\",\n        },\n        \"Mistral-7B-v0.2\": {\n            DownloadSource.DEFAULT: \"alpindale/Mistral-7B-v0.2-hf\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mistral-7B-v0.2-hf\",\n        },\n        \"Mistral-7B-v0.3\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-7B-v0.3\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/mistral-7b-v0.3\",\n        },\n        \"Mistral-7B-Instruct-v0.1\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-7B-Instruct-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mistral-7B-Instruct-v0.1\",\n        },\n        \"Mistral-7B-Instruct-v0.2\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-7B-Instruct-v0.2\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mistral-7B-Instruct-v0.2\",\n        },\n        \"Mistral-7B-Instruct-v0.3\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-7B-Instruct-v0.3\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Mistral-7B-Instruct-v0.3\",\n        },\n    },\n    template=\"mistral\",\n)\n\n\nregister_model_group(\n    models={\n        \"Mistral-Small-24B-Base-2501\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Small-24B-Base-2501\",\n            DownloadSource.MODELSCOPE: \"mistralai/Mistral-Small-24B-Base-2501\",\n        },\n        \"Mistral-Small-24B-Instruct-2501\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Small-24B-Instruct-2501\",\n            DownloadSource.MODELSCOPE: \"mistralai/Mistral-Small-24B-Instruct-2501\",\n        },\n    },\n    template=\"mistral_small\",\n)\n\n\nregister_model_group(\n    models={\n        \"Mistral-Small-3.1-24B-Base\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Small-3.1-24B-Base-2503\",\n            DownloadSource.MODELSCOPE: \"mistralai/Mistral-Small-3.1-24B-Base-2503\",\n        },\n        \"Mistral-Small-3.1-24B-Instruct\": {\n            DownloadSource.DEFAULT: \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n            DownloadSource.MODELSCOPE: \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n        },\n    },\n    template=\"mistral_small\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Mixtral-8x7B-v0.1\": {\n            DownloadSource.DEFAULT: \"mistralai/Mixtral-8x7B-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mixtral-8x7B-v0.1\",\n        },\n        \"Mixtral-8x22B-v0.1\": {\n            DownloadSource.DEFAULT: \"mistralai/Mixtral-8x22B-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mixtral-8x22B-v0.1\",\n        },\n        \"Mixtral-8x7B-v0.1-Instruct\": {\n            DownloadSource.DEFAULT: \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mixtral-8x7B-Instruct-v0.1\",\n        },\n        \"Mixtral-8x22B-v0.1-Instruct\": {\n            DownloadSource.DEFAULT: \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Mixtral-8x22B-Instruct-v0.1\",\n        },\n    },\n    template=\"mistral\",\n)\n\n\nregister_model_group(\n    models={\n        \"Moonlight-16B-A3B\": {\n            DownloadSource.DEFAULT: \"moonshotai/Moonlight-16B-A3B\",\n            DownloadSource.MODELSCOPE: \"moonshotai/Moonlight-16B-A3B\",\n        },\n        \"Moonlight-16B-A3B-Instruct\": {\n            DownloadSource.DEFAULT: \"moonshotai/Moonlight-16B-A3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"moonshotai/Moonlight-16B-A3B-Instruct\",\n        },\n    },\n    template=\"moonlight\",\n)\n\n\nregister_model_group(\n    models={\n        \"OLMo-1B\": {\n            DownloadSource.DEFAULT: \"allenai/OLMo-1B-hf\",\n        },\n        \"OLMo-7B\": {\n            DownloadSource.DEFAULT: \"allenai/OLMo-7B-hf\",\n        },\n        \"OLMo-7B-Chat\": {\n            DownloadSource.DEFAULT: \"ssec-uw/OLMo-7B-Instruct-hf\",\n        },\n        \"OLMo-1.7-7B\": {\n            DownloadSource.DEFAULT: \"allenai/OLMo-1.7-7B-hf\",\n        },\n    },\n)\n\n\nregister_model_group(\n    models={\n        \"OpenChat3.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"openchat/openchat-3.5-0106\",\n            DownloadSource.MODELSCOPE: \"xcwzxcwz/openchat-3.5-0106\",\n        }\n    },\n    template=\"openchat\",\n)\n\n\nregister_model_group(\n    models={\n        \"OpenChat3.6-8B-Chat\": {\n            DownloadSource.DEFAULT: \"openchat/openchat-3.6-8b-20240522\",\n        }\n    },\n    template=\"openchat-3.6\",\n)\n\n\nregister_model_group(\n    models={\n        \"OpenCoder-1.5B-Base\": {\n            DownloadSource.DEFAULT: \"infly/OpenCoder-1.5B-Base\",\n            DownloadSource.MODELSCOPE: \"infly/OpenCoder-1.5B-Base\",\n        },\n        \"OpenCoder-8B-Base\": {\n            DownloadSource.DEFAULT: \"infly/OpenCoder-8B-Base\",\n            DownloadSource.MODELSCOPE: \"infly/OpenCoder-8B-Base\",\n        },\n        \"OpenCoder-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"infly/OpenCoder-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"infly/OpenCoder-1.5B-Instruct\",\n        },\n        \"OpenCoder-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"infly/OpenCoder-8B-Instruct\",\n            DownloadSource.MODELSCOPE: \"infly/OpenCoder-8B-Instruct\",\n        },\n    },\n    template=\"opencoder\",\n)\n\n\nregister_model_group(\n    models={\n        \"Orion-14B-Base\": {\n            DownloadSource.DEFAULT: \"OrionStarAI/Orion-14B-Base\",\n            DownloadSource.MODELSCOPE: \"OrionStarAI/Orion-14B-Base\",\n        },\n        \"Orion-14B-Chat\": {\n            DownloadSource.DEFAULT: \"OrionStarAI/Orion-14B-Chat\",\n            DownloadSource.MODELSCOPE: \"OrionStarAI/Orion-14B-Chat\",\n        },\n        \"Orion-14B-Long-Chat\": {\n            DownloadSource.DEFAULT: \"OrionStarAI/Orion-14B-LongChat\",\n            DownloadSource.MODELSCOPE: \"OrionStarAI/Orion-14B-LongChat\",\n        },\n        \"Orion-14B-RAG-Chat\": {\n            DownloadSource.DEFAULT: \"OrionStarAI/Orion-14B-Chat-RAG\",\n            DownloadSource.MODELSCOPE: \"OrionStarAI/Orion-14B-Chat-RAG\",\n        },\n        \"Orion-14B-Plugin-Chat\": {\n            DownloadSource.DEFAULT: \"OrionStarAI/Orion-14B-Chat-Plugin\",\n            DownloadSource.MODELSCOPE: \"OrionStarAI/Orion-14B-Chat-Plugin\",\n        },\n    },\n    template=\"orion\",\n)\n\n\nregister_model_group(\n    models={\n        \"PaliGemma-3B-pt-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma-3b-pt-224\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma-3b-pt-224\",\n        },\n        \"PaliGemma-3B-pt-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma-3b-pt-448\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma-3b-pt-448\",\n        },\n        \"PaliGemma-3B-pt-896\": {\n            DownloadSource.DEFAULT: \"google/paligemma-3b-pt-896\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma-3b-pt-896\",\n        },\n        \"PaliGemma-3B-mix-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma-3b-mix-224\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma-3b-mix-224\",\n        },\n        \"PaliGemma-3B-mix-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma-3b-mix-448\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma-3b-mix-448\",\n        },\n    },\n    template=\"paligemma\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"PaliGemma2-3B-pt-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-3b-pt-224\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-3b-pt-224\",\n        },\n        \"PaliGemma2-3B-pt-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-3b-pt-448\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-3b-pt-448\",\n        },\n        \"PaliGemma2-3B-pt-896\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-3b-pt-896\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-3b-pt-896\",\n        },\n        \"PaliGemma2-10B-pt-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-10b-pt-224\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-10b-pt-224\",\n        },\n        \"PaliGemma2-10B-pt-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-10b-pt-448\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-10b-pt-448\",\n        },\n        \"PaliGemma2-10B-pt-896\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-10b-pt-896\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-10b-pt-896\",\n        },\n        \"PaliGemma2-28B-pt-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-28b-pt-224\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-28b-pt-224\",\n        },\n        \"PaliGemma2-28B-pt-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-28b-pt-448\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-28b-pt-448\",\n        },\n        \"PaliGemma2-28B-pt-896\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-28b-pt-896\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/paligemma2-28b-pt-896\",\n        },\n        \"PaliGemma2-3B-mix-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-3b-mix-224\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-3b-mix-224-bf16\",\n        },\n        \"PaliGemma2-3B-mix-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-3b-mix-448\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-3b-mix-448-bf16\",\n        },\n        \"PaliGemma2-10B-mix-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-10b-mix-224\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-10b-mix-224-bf16\",\n        },\n        \"PaliGemma2-10B-mix-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-10b-mix-448\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-10b-mix-448-bf16\",\n        },\n        \"PaliGemma2-28B-mix-224\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-28b-mix-224\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-28b-mix-224-bf16\",\n        },\n        \"PaliGemma2-28B-mix-448\": {\n            DownloadSource.DEFAULT: \"google/paligemma2-28b-mix-448\",\n            DownloadSource.MODELSCOPE: \"mlx-community/paligemma2-28b-mix-448-bf16\",\n        },\n    },\n    template=\"paligemma\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Phi-1.5-1.3B\": {\n            DownloadSource.DEFAULT: \"microsoft/phi-1_5\",\n            DownloadSource.MODELSCOPE: \"allspace/PHI_1-5\",\n        },\n        \"Phi-2-2.7B\": {\n            DownloadSource.DEFAULT: \"microsoft/phi-2\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/phi-2\",\n        },\n    }\n)\n\n\nregister_model_group(\n    models={\n        \"Phi-3-4B-4k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-mini-4k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-mini-4k-instruct\",\n        },\n        \"Phi-3-4B-128k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-mini-128k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-mini-128k-instruct\",\n        },\n        \"Phi-3-14B-8k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-medium-4k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-medium-4k-instruct\",\n        },\n        \"Phi-3-14B-128k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-medium-128k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-medium-128k-instruct\",\n        },\n        \"Phi-3.5-4B-instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3.5-mini-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3.5-mini-instruct\",\n        },\n        \"Phi-3.5-MoE-42B-A6.6B-instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3.5-MoE-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3.5-MoE-instruct\",\n        },\n    },\n    template=\"phi\",\n)\n\n\nregister_model_group(\n    models={\n        \"Phi-3-7B-8k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-small-8k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-small-8k-instruct\",\n        },\n        \"Phi-3-7B-128k-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/Phi-3-small-128k-instruct\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/Phi-3-small-128k-instruct\",\n        },\n    },\n    template=\"phi_small\",\n)\n\n\nregister_model_group(\n    models={\n        \"Phi-4-14B-Instruct\": {\n            DownloadSource.DEFAULT: \"microsoft/phi-4\",\n            DownloadSource.MODELSCOPE: \"LLM-Research/phi-4\",\n        },\n    },\n    template=\"phi4\",\n)\n\n\nregister_model_group(\n    models={\n        \"Pixtral-12B\": {\n            DownloadSource.DEFAULT: \"mistral-community/pixtral-12b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/pixtral-12b\",\n        }\n    },\n    template=\"pixtral\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen-1.8B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-1_8B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-1_8B\",\n        },\n        \"Qwen-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-7B\",\n        },\n        \"Qwen-14B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-14B\",\n        },\n        \"Qwen-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-72B\",\n        },\n        \"Qwen-1.8B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-1_8B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-1_8B-Chat\",\n        },\n        \"Qwen-7B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-7B-Chat\",\n        },\n        \"Qwen-14B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-14B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-14B-Chat\",\n        },\n        \"Qwen-72B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-72B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-72B-Chat\",\n        },\n        \"Qwen-1.8B-Chat-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-1_8B-Chat-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-1_8B-Chat-Int8\",\n        },\n        \"Qwen-1.8B-Chat-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-1_8B-Chat-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-1_8B-Chat-Int4\",\n        },\n        \"Qwen-7B-Chat-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-7B-Chat-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-7B-Chat-Int8\",\n        },\n        \"Qwen-7B-Chat-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-7B-Chat-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-7B-Chat-Int4\",\n        },\n        \"Qwen-14B-Chat-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-14B-Chat-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-14B-Chat-Int8\",\n        },\n        \"Qwen-14B-Chat-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-14B-Chat-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-14B-Chat-Int4\",\n        },\n        \"Qwen-72B-Chat-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-72B-Chat-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-72B-Chat-Int8\",\n        },\n        \"Qwen-72B-Chat-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen-72B-Chat-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen-72B-Chat-Int4\",\n        },\n    },\n    template=\"qwen\",\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen1.5-0.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-0.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-0.5B\",\n        },\n        \"Qwen1.5-1.8B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-1.8B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-1.8B\",\n        },\n        \"Qwen1.5-4B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-4B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-4B\",\n        },\n        \"Qwen1.5-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-7B\",\n        },\n        \"Qwen1.5-14B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-14B\",\n        },\n        \"Qwen1.5-32B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-32B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-32B\",\n        },\n        \"Qwen1.5-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-72B\",\n        },\n        \"Qwen1.5-110B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-110B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-110B\",\n        },\n        \"Qwen1.5-MoE-A2.7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-MoE-A2.7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-MoE-A2.7B\",\n        },\n        \"Qwen1.5-0.5B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-0.5B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-0.5B-Chat\",\n        },\n        \"Qwen1.5-1.8B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-1.8B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-1.8B-Chat\",\n        },\n        \"Qwen1.5-4B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-4B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-4B-Chat\",\n        },\n        \"Qwen1.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-7B-Chat\",\n        },\n        \"Qwen1.5-14B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-14B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-14B-Chat\",\n        },\n        \"Qwen1.5-32B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-32B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-32B-Chat\",\n        },\n        \"Qwen1.5-72B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-72B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-72B-Chat\",\n        },\n        \"Qwen1.5-110B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-110B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-110B-Chat\",\n        },\n        \"Qwen1.5-MoE-A2.7B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n        },\n        \"Qwen1.5-0.5B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-0.5B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-0.5B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-0.5B-Chat-AWQ\",\n        },\n        \"Qwen1.5-1.8B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-1.8B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-1.8B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-1.8B-Chat-AWQ\",\n        },\n        \"Qwen1.5-4B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-4B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-4B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-4B-Chat-AWQ\",\n        },\n        \"Qwen1.5-7B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-7B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-7B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-7B-Chat-AWQ\",\n        },\n        \"Qwen1.5-14B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-14B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-14B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-14B-Chat-AWQ\",\n        },\n        \"Qwen1.5-32B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-32B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-32B-Chat-AWQ\",\n        },\n        \"Qwen1.5-72B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-72B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-72B-Chat-GPTQ-Int8\",\n        },\n        \"Qwen1.5-72B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-72B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-72B-Chat-AWQ\",\n        },\n        \"Qwen1.5-110B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-110B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-110B-Chat-AWQ\",\n        },\n        \"Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\",\n        },\n        \"CodeQwen1.5-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/CodeQwen1.5-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/CodeQwen1.5-7B\",\n        },\n        \"CodeQwen1.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"Qwen/CodeQwen1.5-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"Qwen/CodeQwen1.5-7B-Chat\",\n        },\n        \"CodeQwen1.5-7B-Chat-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/CodeQwen1.5-7B-Chat-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/CodeQwen1.5-7B-Chat-AWQ\",\n        },\n    },\n    template=\"qwen\",\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen2-0.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-0.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-0.5B\",\n        },\n        \"Qwen2-1.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-1.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-1.5B\",\n        },\n        \"Qwen2-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-7B\",\n        },\n        \"Qwen2-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-72B\",\n        },\n        \"Qwen2-MoE-57B-A14B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-57B-A14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-57B-A14B\",\n        },\n        \"Qwen2-0.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-0.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-0.5B-Instruct\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Qwen2-0.5B-Instruct\",\n        },\n        \"Qwen2-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-1.5B-Instruct\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Qwen2-1.5B-Instruct\",\n        },\n        \"Qwen2-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-7B-Instruct\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Qwen2-7B-Instruct\",\n        },\n        \"Qwen2-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-72B-Instruct\",\n        },\n        \"Qwen2-MoE-57B-A14B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-57B-A14B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-57B-A14B-Instruct\",\n        },\n        \"Qwen2-0.5B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-0.5B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-0.5B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-0.5B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-0.5B-Instruct-AWQ\",\n        },\n        \"Qwen2-1.5B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-1.5B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-1.5B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-1.5B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-1.5B-Instruct-AWQ\",\n        },\n        \"Qwen2-7B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-7B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-7B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-7B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-7B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-7B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-7B-Instruct-AWQ\",\n        },\n        \"Qwen2-72B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-72B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-72B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-72B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-72B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-72B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-72B-Instruct-AWQ\",\n        },\n        \"Qwen2-57B-A14B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-Math-1.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-1.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-1.5B\",\n        },\n        \"Qwen2-Math-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-7B\",\n        },\n        \"Qwen2-Math-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-72B\",\n        },\n        \"Qwen2-Math-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-1.5B-Instruct\",\n        },\n        \"Qwen2-Math-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-7B-Instruct\",\n        },\n        \"Qwen2-Math-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Math-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Math-72B-Instruct\",\n        },\n    },\n    template=\"qwen\",\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen2.5-0.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-0.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-0.5B\",\n        },\n        \"Qwen2.5-1.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-1.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-1.5B\",\n        },\n        \"Qwen2.5-3B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-3B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-3B\",\n        },\n        \"Qwen2.5-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B\",\n        },\n        \"Qwen2.5-14B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B\",\n        },\n        \"Qwen2.5-32B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-32B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-32B\",\n        },\n        \"Qwen2.5-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-72B\",\n        },\n        \"Qwen2.5-0.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-0.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-0.5B-Instruct\",\n        },\n        \"Qwen2.5-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-1.5B-Instruct\",\n        },\n        \"Qwen2.5-3B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-3B-Instruct\",\n        },\n        \"Qwen2.5-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B-Instruct\",\n        },\n        \"Qwen2.5-14B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B-Instruct\",\n        },\n        \"Qwen2.5-32B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-32B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-32B-Instruct\",\n        },\n        \"Qwen2.5-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-72B-Instruct\",\n        },\n        \"Qwen2.5-7B-Instruct-1M\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B-Instruct-1M\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B-Instruct-1M\",\n        },\n        \"Qwen2.5-14B-Instruct-1M\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B-Instruct-1M\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B-Instruct-1M\",\n        },\n        \"Qwen2.5-0.5B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-0.5B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-0.5B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-0.5B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-0.5B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-1.5B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-1.5B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-1.5B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-1.5B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-1.5B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-3B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-3B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-3B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-3B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-3B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-7B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-7B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-7B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-14B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-14B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-14B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-14B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-14B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-32B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-32B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-32B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-32B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-32B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-72B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2.5-72B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2.5-72B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-72B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-72B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-Coder-0.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-0.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-0.5B\",\n        },\n        \"Qwen2.5-Coder-1.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-1.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-1.5B\",\n        },\n        \"Qwen2.5-Coder-3B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-3B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-3B\",\n        },\n        \"Qwen2.5-Coder-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-7B\",\n        },\n        \"Qwen2.5-Coder-14B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-14B\",\n        },\n        \"Qwen2.5-Coder-32B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-32B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-32B\",\n        },\n        \"Qwen2.5-Coder-0.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n        },\n        \"Qwen2.5-Coder-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n        },\n        \"Qwen2.5-Coder-3B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-3B-Instruct\",\n        },\n        \"Qwen2.5-Coder-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n        },\n        \"Qwen2.5-Coder-14B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-14B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-14B-Instruct\",\n        },\n        \"Qwen2.5-Coder-32B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n        },\n        \"Qwen2.5-Math-1.5B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-1.5B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Math-1.5B\",\n        },\n        \"Qwen2.5-Math-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Math-7B\",\n        },\n        \"Qwen2.5-Math-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Math-72B\",\n        },\n        \"Qwen2.5-Math-1.5B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-1.5B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n        },\n        \"Qwen2.5-Math-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n        },\n        \"Qwen2.5-Math-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Math-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Coder-72B-Instruct\",\n        },\n        \"QwQ-32B-Preview-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/QwQ-32B-Preview\",\n            DownloadSource.MODELSCOPE: \"Qwen/QwQ-32B-Preview\",\n        },\n        \"QwQ-32B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/QwQ-32B\",\n            DownloadSource.MODELSCOPE: \"Qwen/QwQ-32B\",\n        },\n    },\n    template=\"qwen\",\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen3-0.6B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-0.6B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-0.6B-Base\",\n        },\n        \"Qwen3-1.7B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-1.7B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-1.7B-Base\",\n        },\n        \"Qwen3-4B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-4B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-4B-Base\",\n        },\n        \"Qwen3-8B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-8B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-8B-Base\",\n        },\n        \"Qwen3-14B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-14B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-14B-Base\",\n        },\n        \"Qwen3-30B-A3B-Base\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-30B-A3B-Base\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-30B-A3B-Base\",\n        },\n        \"Qwen3-0.6B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-0.6B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-0.6B\",\n        },\n        \"Qwen3-1.7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-1.7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-1.7B\",\n        },\n        \"Qwen3-4B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-4B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-4B\",\n        },\n        \"Qwen3-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-8B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-8B\",\n        },\n        \"Qwen3-14B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-14B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-14B\",\n        },\n        \"Qwen3-32B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-32B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-32B\",\n        },\n        \"Qwen3-30B-A3B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-30B-A3B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-30B-A3B\",\n        },\n        \"Qwen3-235B-A22B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-235B-A22B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-235B-A22B\",\n        },\n        \"Qwen3-0.6B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-0.6B-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-0.6B-GPTQ-Int8\",\n        },\n        \"Qwen3-1.7B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-1.7B-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-1.7B-GPTQ-Int8\",\n        },\n        \"Qwen3-4B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-4B-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-4B-AWQ\",\n        },\n        \"Qwen3-8B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-8B-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-8B-AWQ\",\n        },\n        \"Qwen3-14B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-14B-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-14B-AWQ\",\n        },\n        \"Qwen3-32B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-32B-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-32B-AWQ\",\n        },\n        \"Qwen3-30B-A3B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-30B-A3B-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-30B-A3B-GPTQ-Int4\",\n        },\n        \"Qwen3-235B-A22B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen3-235B-A22B-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen3-235B-A22B-GPTQ-Int4\",\n        },\n    },\n    template=\"qwen3\",\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen2-Audio-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Audio-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Audio-7B\",\n        },\n        \"Qwen2-Audio-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-Audio-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-Audio-7B-Instruct\",\n        },\n    },\n    template=\"qwen2_audio\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen2.5-Omni-3B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Omni-3B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Omni-3B\",\n        },\n        \"Qwen2.5-Omni-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Omni-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Omni-7B\",\n        },\n        \"Qwen2.5-Omni-7B-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Omni-7B-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Omni-7B-GPTQ-Int4\",\n        },\n        \"Qwen2.5-Omni-7B-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-Omni-7B-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-Omni-7B-AWQ\",\n        },\n    },\n    template=\"qwen2_omni\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Qwen2-VL-2B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-2B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-2B\",\n        },\n        \"Qwen2-VL-7B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-7B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-7B\",\n        },\n        \"Qwen2-VL-72B\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-72B\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-72B\",\n        },\n        \"Qwen2-VL-2B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-2B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-2B-Instruct\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Qwen2-VL-2B-Instruct\",\n        },\n        \"Qwen2-VL-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-7B-Instruct\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Qwen2-VL-7B-Instruct\",\n        },\n        \"Qwen2-VL-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-72B-Instruct\",\n        },\n        \"Qwen2-VL-2B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-VL-2B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-VL-2B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n        },\n        \"Qwen2-VL-7B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-VL-7B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-VL-7B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-7B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-7B-Instruct-AWQ\",\n        },\n        \"Qwen2-VL-72B-Instruct-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8\",\n        },\n        \"Qwen2-VL-72B-Instruct-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n        },\n        \"Qwen2-VL-72B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2-VL-72B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2-VL-72B-Instruct-AWQ\",\n        },\n        \"QVQ-72B-Preview\": {\n            DownloadSource.DEFAULT: \"Qwen/QVQ-72B-Preview\",\n            DownloadSource.MODELSCOPE: \"Qwen/QVQ-72B-Preview\",\n        },\n        \"Qwen2.5-VL-3B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-3B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-3B-Instruct\",\n        },\n        \"Qwen2.5-VL-7B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-7B-Instruct\",\n        },\n        \"Qwen2.5-VL-32B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-32B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-32B-Instruct\",\n        },\n        \"Qwen2.5-VL-72B-Instruct\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-72B-Instruct\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-72B-Instruct\",\n        },\n        \"Qwen2.5-VL-3B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-VL-7B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\n        },\n        \"Qwen2.5-VL-72B-Instruct-AWQ\": {\n            DownloadSource.DEFAULT: \"Qwen/Qwen2.5-VL-72B-Instruct-AWQ\",\n            DownloadSource.MODELSCOPE: \"Qwen/Qwen2.5-VL-72B-Instruct-AWQ\",\n        },\n    },\n    template=\"qwen2_vl\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Seed-Coder-8B-Base\": {\n            DownloadSource.DEFAULT: \"ByteDance-Seed/Seed-Coder-8B-Base\",\n        },\n        \"Seed-Coder-8B-Instruct\": {\n            DownloadSource.DEFAULT: \"ByteDance-Seed/Seed-Coder-8B-Instruct\",\n        },\n        \"Seed-Coder-8B-Instruct-Reasoning\": {\n            DownloadSource.DEFAULT: \"ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16\",\n        },\n    },\n    template=\"seed_coder\",\n)\n\n\nregister_model_group(\n    models={\n        \"Skywork-13B-Base\": {\n            DownloadSource.DEFAULT: \"Skywork/Skywork-13B-base\",\n            DownloadSource.MODELSCOPE: \"skywork/Skywork-13B-base\",\n        }\n    }\n)\n\n\nregister_model_group(\n    models={\n        \"Skywork-o1-Open-Llama-3.1-8B\": {\n            DownloadSource.DEFAULT: \"Skywork/Skywork-o1-Open-Llama-3.1-8B\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/Skywork-o1-Open-Llama-3.1-8B\",\n        }\n    },\n    template=\"skywork_o1\",\n)\n\n\nregister_model_group(\n    models={\n        \"SmolLM-135M\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-135M\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-135M\",\n        },\n        \"SmolLM-360M\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-360M\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-360M\",\n        },\n        \"SmolLM-1.7B\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-1.7B\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-1.7B\",\n        },\n        \"SmolLM-135M-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-135M-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-135M-Instruct\",\n        },\n        \"SmolLM-360M-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-360M-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-360M-Instruct\",\n        },\n        \"SmolLM-1.7B-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n        },\n    },\n    template=\"smollm\",\n)\n\n\nregister_model_group(\n    models={\n        \"SmolLM2-135M\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-135M\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-135M\",\n        },\n        \"SmolLM2-360M\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-360M\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-360M\",\n        },\n        \"SmolLM2-1.7B\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-1.7B\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-1.7B\",\n        },\n        \"SmolLM2-135M-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n        },\n        \"SmolLM2-360M-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n        },\n        \"SmolLM2-1.7B-Instruct\": {\n            DownloadSource.DEFAULT: \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n            DownloadSource.MODELSCOPE: \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n        },\n    },\n    template=\"smollm2\",\n)\n\n\nregister_model_group(\n    models={\n        \"SOLAR-10.7B-v1.0\": {\n            DownloadSource.DEFAULT: \"upstage/SOLAR-10.7B-v1.0\",\n        },\n        \"SOLAR-10.7B-Instruct-v1.0\": {\n            DownloadSource.DEFAULT: \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/SOLAR-10.7B-Instruct-v1.0\",\n        },\n    },\n    template=\"solar\",\n)\n\n\nregister_model_group(\n    models={\n        \"StarCoder2-3B\": {\n            DownloadSource.DEFAULT: \"bigcode/starcoder2-3b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/starcoder2-3b\",\n        },\n        \"StarCoder2-7B\": {\n            DownloadSource.DEFAULT: \"bigcode/starcoder2-7b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/starcoder2-7b\",\n        },\n        \"StarCoder2-15B\": {\n            DownloadSource.DEFAULT: \"bigcode/starcoder2-15b\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/starcoder2-15b\",\n        },\n    }\n)\n\n\nregister_model_group(\n    models={\n        \"TeleChat-1B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat-1B\",\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat-1B\",\n        },\n        \"TeleChat-7B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/telechat-7B\",\n            DownloadSource.MODELSCOPE: \"TeleAI/telechat-7B\",\n            DownloadSource.OPENMIND: \"TeleAI/TeleChat-7B-pt\",\n        },\n        \"TeleChat-12B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat-12B-v2\",\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat-12B-v2\",\n            DownloadSource.OPENMIND: \"TeleAI/TeleChat-12B-pt\",\n        },\n        \"TeleChat-52B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat-52B\",\n        },\n    },\n    template=\"telechat\",\n)\n\n\nregister_model_group(\n    models={\n        \"TeleChat2-3B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat2-3B\",\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat2-3B\",\n        },\n        \"TeleChat2-7B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat2-7B\",\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat2-7B\",\n        },\n        \"TeleChat2-35B-Chat\": {\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat2-35B-Nov\",\n        },\n        \"TeleChat2-115B-Chat\": {\n            DownloadSource.DEFAULT: \"Tele-AI/TeleChat2-115B\",\n            DownloadSource.MODELSCOPE: \"TeleAI/TeleChat2-115B\",\n        },\n    },\n    template=\"telechat2\",\n)\n\n\nregister_model_group(\n    models={\n        \"Vicuna-v1.5-7B-Chat\": {\n            DownloadSource.DEFAULT: \"lmsys/vicuna-7b-v1.5\",\n            DownloadSource.MODELSCOPE: \"Xorbits/vicuna-7b-v1.5\",\n        },\n        \"Vicuna-v1.5-13B-Chat\": {\n            DownloadSource.DEFAULT: \"lmsys/vicuna-13b-v1.5\",\n            DownloadSource.MODELSCOPE: \"Xorbits/vicuna-13b-v1.5\",\n        },\n    },\n    template=\"vicuna\",\n)\n\n\nregister_model_group(\n    models={\n        \"Video-LLaVA-7B-Chat\": {\n            DownloadSource.DEFAULT: \"LanguageBind/Video-LLaVA-7B-hf\",\n        },\n    },\n    template=\"video_llava\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"XuanYuan-6B\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-6B\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-6B\",\n        },\n        \"XuanYuan-70B\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-70B\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-70B\",\n        },\n        \"XuanYuan2-70B\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan2-70B\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan2-70B\",\n        },\n        \"XuanYuan-6B-Chat\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-6B-Chat\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-6B-Chat\",\n        },\n        \"XuanYuan-70B-Chat\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-70B-Chat\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-70B-Chat\",\n        },\n        \"XuanYuan2-70B-Chat\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan2-70B-Chat\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan2-70B-Chat\",\n        },\n        \"XuanYuan-6B-Chat-8bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-6B-Chat-8bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-6B-Chat-8bit\",\n        },\n        \"XuanYuan-6B-Chat-4bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-6B-Chat-4bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-6B-Chat-4bit\",\n        },\n        \"XuanYuan-70B-Chat-8bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-70B-Chat-8bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-70B-Chat-8bit\",\n        },\n        \"XuanYuan-70B-Chat-4bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan-70B-Chat-4bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan-70B-Chat-4bit\",\n        },\n        \"XuanYuan2-70B-Chat-8bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan2-70B-Chat-8bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan2-70B-Chat-8bit\",\n        },\n        \"XuanYuan2-70B-Chat-4bit\": {\n            DownloadSource.DEFAULT: \"Duxiaoman-DI/XuanYuan2-70B-Chat-4bit\",\n            DownloadSource.MODELSCOPE: \"Duxiaoman-DI/XuanYuan2-70B-Chat-4bit\",\n        },\n    },\n    template=\"xuanyuan\",\n)\n\n\nregister_model_group(\n    models={\n        \"XVERSE-7B\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-7B\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-7B\",\n        },\n        \"XVERSE-13B\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-13B\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-13B\",\n        },\n        \"XVERSE-65B\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-65B\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-65B\",\n        },\n        \"XVERSE-65B-2\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-65B-2\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-65B-2\",\n        },\n        \"XVERSE-7B-Chat\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-7B-Chat\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-7B-Chat\",\n        },\n        \"XVERSE-13B-Chat\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-13B-Chat\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-13B-Chat\",\n        },\n        \"XVERSE-65B-Chat\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-65B-Chat\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-65B-Chat\",\n        },\n        \"XVERSE-MoE-A4.2B\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-MoE-A4.2B\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-MoE-A4.2B\",\n        },\n        \"XVERSE-7B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-7B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-7B-Chat-GPTQ-Int8\",\n        },\n        \"XVERSE-7B-Chat-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-7B-Chat-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-7B-Chat-GPTQ-Int4\",\n        },\n        \"XVERSE-13B-Chat-GPTQ-Int8\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-13B-Chat-GPTQ-Int8\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-13B-Chat-GPTQ-Int8\",\n        },\n        \"XVERSE-13B-Chat-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-13B-Chat-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-13B-Chat-GPTQ-Int4\",\n        },\n        \"XVERSE-65B-Chat-GPTQ-Int4\": {\n            DownloadSource.DEFAULT: \"xverse/XVERSE-65B-Chat-GPTQ-Int4\",\n            DownloadSource.MODELSCOPE: \"xverse/XVERSE-65B-Chat-GPTQ-Int4\",\n        },\n    },\n    template=\"xverse\",\n)\n\n\nregister_model_group(\n    models={\n        \"Yayi-7B\": {\n            DownloadSource.DEFAULT: \"wenge-research/yayi-7b-llama2\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/yayi-7b-llama2\",\n        },\n        \"Yayi-13B\": {\n            DownloadSource.DEFAULT: \"wenge-research/yayi-13b-llama2\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/yayi-13b-llama2\",\n        },\n    },\n    template=\"yayi\",\n)\n\n\nregister_model_group(\n    models={\n        \"Yi-6B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-6B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-6B\",\n        },\n        \"Yi-9B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-9B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-9B\",\n        },\n        \"Yi-34B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-34B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-34B\",\n        },\n        \"Yi-6B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-6B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-6B-Chat\",\n        },\n        \"Yi-34B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-34B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-34B-Chat\",\n        },\n        \"Yi-6B-Chat-8bits\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-6B-Chat-8bits\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-6B-Chat-8bits\",\n        },\n        \"Yi-6B-Chat-4bits\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-6B-Chat-4bits\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-6B-Chat-4bits\",\n        },\n        \"Yi-34B-Chat-8bits\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-34B-Chat-8bits\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-34B-Chat-8bits\",\n        },\n        \"Yi-34B-Chat-4bits\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-34B-Chat-4bits\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-34B-Chat-4bits\",\n        },\n        \"Yi-1.5-6B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-6B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-6B\",\n        },\n        \"Yi-1.5-9B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-9B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-9B\",\n        },\n        \"Yi-1.5-34B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-34B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-34B\",\n        },\n        \"Yi-1.5-6B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-6B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-6B-Chat\",\n            DownloadSource.OPENMIND: \"LlamaFactory/Yi-1.5-6B-Chat\",\n        },\n        \"Yi-1.5-9B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-9B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-9B-Chat\",\n        },\n        \"Yi-1.5-34B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-1.5-34B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-1.5-34B-Chat\",\n        },\n        \"Yi-Coder-1.5B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-Coder-1.5B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-Coder-1.5B\",\n        },\n        \"Yi-Coder-9B\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-Coder-9B\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-Coder-9B\",\n        },\n        \"Yi-Coder-1.5B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-Coder-1.5B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-Coder-1.5B-Chat\",\n        },\n        \"Yi-Coder-9B-Chat\": {\n            DownloadSource.DEFAULT: \"01-ai/Yi-Coder-9B-Chat\",\n            DownloadSource.MODELSCOPE: \"01ai/Yi-Coder-9B-Chat\",\n        },\n    },\n    template=\"yi\",\n)\n\n\nregister_model_group(\n    models={\n        \"Yi-VL-6B-Chat\": {\n            DownloadSource.DEFAULT: \"BUAADreamer/Yi-VL-6B-hf\",\n        },\n        \"Yi-VL-34B-Chat\": {\n            DownloadSource.DEFAULT: \"BUAADreamer/Yi-VL-34B-hf\",\n        },\n    },\n    template=\"yi_vl\",\n    multimodal=True,\n)\n\n\nregister_model_group(\n    models={\n        \"Yuan2-2B-Chat\": {\n            DownloadSource.DEFAULT: \"IEITYuan/Yuan2-2B-hf\",\n            DownloadSource.MODELSCOPE: \"YuanLLM/Yuan2.0-2B-hf\",\n        },\n        \"Yuan2-51B-Chat\": {\n            DownloadSource.DEFAULT: \"IEITYuan/Yuan2-51B-hf\",\n            DownloadSource.MODELSCOPE: \"YuanLLM/Yuan2.0-51B-hf\",\n        },\n        \"Yuan2-102B-Chat\": {\n            DownloadSource.DEFAULT: \"IEITYuan/Yuan2-102B-hf\",\n            DownloadSource.MODELSCOPE: \"YuanLLM/Yuan2.0-102B-hf\",\n        },\n    },\n    template=\"yuan\",\n)\n\n\nregister_model_group(\n    models={\n        \"Zephyr-7B-Alpha-Chat\": {\n            DownloadSource.DEFAULT: \"HuggingFaceH4/zephyr-7b-alpha\",\n            DownloadSource.MODELSCOPE: \"AI-ModelScope/zephyr-7b-alpha\",\n        },\n        \"Zephyr-7B-Beta-Chat\": {\n            DownloadSource.DEFAULT: \"HuggingFaceH4/zephyr-7b-beta\",\n            DownloadSource.MODELSCOPE: \"modelscope/zephyr-7b-beta\",\n        },\n        \"Zephyr-141B-ORPO-Chat\": {\n            DownloadSource.DEFAULT: \"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\",\n        },\n    },\n    template=\"zephyr\",\n)\n",
          "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom ...extras import logging\nfrom ...extras.misc import get_current_device\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel\n\n    from ...hparams import FinetuningArguments, ModelArguments\n\n\nlogger = logging.get_logger(__name__)\n\n\ndef _get_unsloth_kwargs(\n    config: \"PretrainedConfig\",\n    model_name_or_path: str,\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> dict[str, Any]:\n    return {\n        \"model_name\": model_name_or_path,\n        \"max_seq_length\": model_args.model_max_length or 4096,\n        \"dtype\": model_args.compute_dtype,\n        \"load_in_4bit\": model_args.quantization_bit == 4,\n        \"token\": model_args.hf_hub_token,\n        \"full_finetuning\": finetuning_args.finetuning_type == \"full\",\n        \"device_map\": {\"\": get_current_device()},\n        \"rope_scaling\": getattr(config, \"rope_scaling\", None),\n        \"fix_tokenizer\": False,\n        \"trust_remote_code\": model_args.trust_remote_code,\n        \"use_gradient_checkpointing\": \"unsloth\",\n    }\n\n\ndef load_unsloth_pretrained_model(\n    config: \"PretrainedConfig\", model_args: \"ModelArguments\", finetuning_args: \"FinetuningArguments\"\n) -> Optional[\"PreTrainedModel\"]:\n    r\"\"\"Optionally load pretrained model with unsloth. Used in training.\"\"\"\n    from unsloth import FastLanguageModel  # type: ignore\n\n    unsloth_kwargs = _get_unsloth_kwargs(config, model_args.model_name_or_path, model_args, finetuning_args)\n    try:\n        model, _ = FastLanguageModel.from_pretrained(**unsloth_kwargs)\n    except NotImplementedError:\n        logger.warning_rank0(\"Unsloth does not support model type {}.\".format(getattr(config, \"model_type\", None)))\n        model = None\n        model_args.use_unsloth = False\n\n    return model\n\n\ndef get_unsloth_peft_model(\n    model: \"PreTrainedModel\", model_args: \"ModelArguments\", peft_kwargs: dict[str, Any]\n) -> \"PreTrainedModel\":\n    r\"\"\"Get the peft model for the pretrained model with unsloth. Used in training.\"\"\"\n    from unsloth import FastLanguageModel  # type: ignore\n\n    unsloth_peft_kwargs = {\n        \"model\": model,\n        \"max_seq_length\": model_args.model_max_length,\n        \"use_gradient_checkpointing\": \"unsloth\",\n    }\n    return FastLanguageModel.get_peft_model(**peft_kwargs, **unsloth_peft_kwargs)\n\n\ndef load_unsloth_peft_model(\n    config: \"PretrainedConfig\", model_args: \"ModelArguments\", finetuning_args: \"FinetuningArguments\", is_trainable: bool\n) -> \"PreTrainedModel\":\n    r\"\"\"Load peft model with unsloth. Used in both training and inference.\"\"\"\n    from unsloth import FastLanguageModel  # type: ignore\n\n    unsloth_kwargs = _get_unsloth_kwargs(config, model_args.adapter_name_or_path[0], model_args, finetuning_args)\n    try:\n        if not is_trainable:\n            unsloth_kwargs[\"use_gradient_checkpointing\"] = False\n\n        model, _ = FastLanguageModel.from_pretrained(**unsloth_kwargs)\n    except NotImplementedError:\n        raise ValueError(\"Unsloth does not support model type {}.\".format(getattr(config, \"model_type\", None)))\n\n    if not is_trainable:\n        FastLanguageModel.for_inference(model)\n\n    return model\n"
        ],
        "test_patch": "",
        "patch_preview": "From cf5737632240d1b6d2555362aadffc189d0a3537 Mon Sep 17 00:00:00 2001\nFrom: hiyouga <hiyouga@buaa.edu.cn>\nDate: Mon, 23 Jun 2025 09:45:03 +0000\nSubject: [PATCH] add kimi vl 2506\n\n---\n README_zh.md                                  |  1 +\n src/llamafactory/data/template.py             |  7 ++-\n src/llamafactory/extras/constants.py          | 53 ++++++++++---------\n src/llamafactory/model/model_utils/unsloth.py |  5 +-\n 4 files changed, 36 insertions(+), 30 deletions(-)\n\ndiff --git a/README_zh.md "
      },
      "patch": {
        "length": 7812,
        "files_changed": 4,
        "lines_added": 36,
        "lines_deleted": 30,
        "net_change": 6,
        "changed_files": [
          {
            "file": "README_zh.md",
            "added": 1,
            "deleted": 0
          },
          {
            "file": "src/llamafactory/data/template.py",
            "added": 3,
            "deleted": 4
          },
          {
            "file": "src/llamafactory/extras/constants.py",
            "added": 28,
            "deleted": 25
          },
          {
            "file": "src/llamafactory/model/model_utils/unsloth.py",
            "added": 4,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 277,
        "total_lines": 142282,
        "total_bytes": 16915657,
        "python_files": 170,
        "python_lines": 32431,
        "file_extensions": {
          ".md": 6,
          "": 5,
          ".toml": 1,
          ".cff": 1,
          ".txt": 3,
          ".in": 1,
          ".py": 170,
          ".yml": 3,
          ".base": 1,
          ".yaml": 41,
          ".sh": 4,
          ".json": 21,
          ".png": 2,
          ".jpg": 5,
          ".svg": 1,
          ".jsonl": 1,
          ".avi": 1,
          ".mp4": 3,
          ".mp3": 2,
          ".flac": 1,
          ".wav": 1,
          ".zip": 3
        },
        "largest_files": [
          {
            "path": "evaluation/mmlu/mmlu.zip",
            "size": 2336375,
            "lines": 17606,
            "extension": ".zip"
          },
          {
            "path": "evaluation/ceval/ceval.zip",
            "size": 1548171,
            "lines": 11915,
            "extension": ".zip"
          },
          {
            "path": "data/glaive_toolcall_en_demo.json",
            "size": 738925,
            "lines": 9158,
            "extension": ".json"
          },
          {
            "path": "data/glaive_toolcall_zh_demo.json",
            "size": 680684,
            "lines": 9022,
            "extension": ".json"
          },
          {
            "path": "evaluation/cmmlu/cmmlu.zip",
            "size": 1078352,
            "lines": 8020,
            "extension": ".zip"
          },
          {
            "path": "data/dpo_en_demo.json",
            "size": 1575286,
            "lines": 7226,
            "extension": ".json"
          },
          {
            "path": "data/kto_en_demo.json",
            "size": 913519,
            "lines": 5398,
            "extension": ".json"
          },
          {
            "path": "data/dpo_zh_demo.json",
            "size": 853311,
            "lines": 5058,
            "extension": ".json"
          },
          {
            "path": "data/alpaca_en_demo.json",
            "size": 860929,
            "lines": 5002,
            "extension": ".json"
          },
          {
            "path": "data/alpaca_zh_demo.json",
            "size": 636036,
            "lines": 5002,
            "extension": ".json"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 277,
        "files_changed_count": 4,
        "files_changed_ratio": 0.01444043321299639,
        "total_lines_in_repo": 142282,
        "lines_added": 36,
        "lines_deleted": 30,
        "net_lines_changed": 6,
        "lines_changed_ratio": 0.0004638675306785117,
        "pr_body_length": 241,
        "commit_message_length": 97,
        "python_file_count": 170,
        "python_line_count": 32431
      }
    },
    {
      "tar_file_name": "jchanvfx#NodeGraphQt#pull#277",
      "repo_name": "jchanvfx#NodeGraphQt#pull#277",
      "success": false,
      "error": "object of type 'NoneType' has no len()",
      "commit": {
        "sha": "060e61be3e156f5c142b6d669b6b1ffca4ca2679",
        "message": "doc updates",
        "author": {
          "name": "jchan",
          "email": "jjohnnycchan@gmail.com",
          "date": "2022-10-18T23:23:00Z"
        },
        "html_url": "https://github.com/jchanvfx/NodeGraphQt/commit/060e61be3e156f5c142b6d669b6b1ffca4ca2679",
        "api_url": "https://api.github.com/repos/jchanvfx/NodeGraphQt/commits/060e61be3e156f5c142b6d669b6b1ffca4ca2679"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jchanvfx#NodeGraphQt#pull#277",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/jchanvfx#NodeGraphQt#pull#277.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jchanvfx#NodeGraphQt#pull#277/source_code"
      },
      "pr": {
        "number": 277,
        "title": "setup clean up #269",
        "body": null,
        "state": "closed",
        "created_at": "2022-10-19T10:59:40Z",
        "updated_at": "2022-10-19T11:00:44Z",
        "merged_at": "2022-10-19T11:00:25Z",
        "html_url": "https://github.com/jchanvfx/NodeGraphQt/pull/277",
        "user": "jchanvfx",
        "additions": 5,
        "deletions": 8,
        "changed_files": 2,
        "commits": 1
      },
      "swebench": {
        "instance_id": "jchanvfx_NodeGraphQt-277",
        "repo": "/jchanvfx/NodeGraphQt",
        "base_commit": "060e61be3e156f5c142b6d669b6b1ffca4ca2679",
        "problem_statement": {},
        "edit_files": [
          "setup.cfg",
          "setup.py"
        ],
        "oracle_files": [
          "[metadata]\nlicense = MIT License\nlicense_file = LICENSE.md\nlong_description = file: README.md\nlong_description_content_type = text/markdown\ndescription = Node graph framework for PySide2/PyQt5 that can be\n              implemented and re-purposed into applications.\nclassifiers = Operating System :: OS Independent\n              License :: OSI Approved :: MIT License\n              Programming Language :: Python :: 3.6\nurl = https://github.com/jchanvfx/NodeGraphQt\nproject_urls = \n    Documentation = https://jchanvfx.github.io/NodeGraphQt/api/html/index.html\n    Source = https://github.com/jchanvfx/NodeGraphQt/\n    Tracker = https://github.com/jchanvfx/NodeGraphQt/issues\n\n[options]\npackages = find:\ninclude_package_data = True\npython_requires = >=3.6\ninstall_requires =\n    Qt.py>=1.2.0\n\n[options.extras_require]\nPySide2 = PySide2>=5.15\n\n[options.packages.find]\nexclude = examples\n          basic_example.py\n\n[options.package_data]\nNodeGraphQt = widgets/icons/node_base.png\n",
          "#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport setuptools\n\nfrom NodeGraphQt import pkg_info\n\nif __name__ == '__main__':\n    setuptools.setup(\n        name=pkg_info.__module_name__,\n        version=pkg_info.__version__,\n        author=pkg_info.__author__,\n    )\n"
        ],
        "test_patch": "",
        "patch_preview": "From 459009f1f0fcb08ec86f290a5519c58c83f2940c Mon Sep 17 00:00:00 2001\nFrom: jchan <jjohnnycchan@gmail.com>\nDate: Wed, 19 Oct 2022 23:56:49 +1300\nSubject: [PATCH] setup clean up #269\n\n---\n setup.cfg |  3 +++\n setup.py  | 10 ++--------\n 2 files changed, 5 insertions(+), 8 deletions(-)\n\ndiff --git a/setup.cfg b/setup.cfg\nindex 14735f10..da69c535 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -1,4 +1,7 @@\n [metadata]\n+name = NodeGraphQt\n+version=0.4.0\n+author=Johnny Chan\n license = MIT License\n license_"
      },
      "patch": {
        "length": 990,
        "files_changed": 2,
        "lines_added": 5,
        "lines_deleted": 8,
        "net_change": -3,
        "changed_files": [
          {
            "file": "setup.cfg",
            "added": 3,
            "deleted": 0
          },
          {
            "file": "setup.py",
            "added": 2,
            "deleted": 8
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 145,
        "total_lines": 78168,
        "total_bytes": 7749569,
        "python_files": 55,
        "python_lines": 15217,
        "file_extensions": {
          ".md": 2,
          ".txt": 1,
          ".cfg": 1,
          ".in": 1,
          ".py": 55,
          ".rst": 21,
          "": 1,
          ".bat": 1,
          ".png": 29,
          ".gif": 3,
          ".css": 3,
          ".html": 6,
          ".conf": 1,
          ".js": 2,
          ".woff2": 7,
          ".woff": 7,
          ".ttf": 1,
          ".eot": 1,
          ".svg": 1,
          ".json": 1
        },
        "largest_files": [
          {
            "path": "docs/_images/example_subgraph.gif",
            "size": 533446,
            "lines": 5077,
            "extension": ".gif"
          },
          {
            "path": "docs/_themes/sphinx_rtd_theme/static/css/theme.css",
            "size": 138507,
            "lines": 4759,
            "extension": ".css"
          },
          {
            "path": "docs/_images/screenshot.png",
            "size": 527179,
            "lines": 3675,
            "extension": ".png"
          },
          {
            "path": "docs/_themes/sphinx_rtd_theme/static/css/fonts/fontawesome-webfont.ttf",
            "size": 165548,
            "lines": 2983,
            "extension": ".ttf"
          },
          {
            "path": "docs/_themes/sphinx_rtd_theme/static/css/fonts/fontawesome-webfont.eot",
            "size": 165742,
            "lines": 2983,
            "extension": ".eot"
          },
          {
            "path": "docs/_images/app_silhouette_example.png",
            "size": 430491,
            "lines": 2719,
            "extension": ".png"
          },
          {
            "path": "NodeGraphQt/base/graph.py",
            "size": 90062,
            "lines": 2685,
            "extension": ".py"
          },
          {
            "path": "docs/_images/overview.png",
            "size": 392840,
            "lines": 2682,
            "extension": ".png"
          },
          {
            "path": "docs/_themes/sphinx_rtd_theme/static/css/fonts/fontawesome-webfont.svg",
            "size": 444379,
            "lines": 2671,
            "extension": ".svg"
          },
          {
            "path": "docs/_themes/sphinx_rtd_theme/static/css/fonts/lato-bold-italic.woff",
            "size": 323344,
            "lines": 2364,
            "extension": ".woff"
          }
        ]
      }
    },
    {
      "tar_file_name": "jonashaag#klaus#pull#217",
      "repo_name": "jonashaag#klaus#pull#217",
      "success": true,
      "error": null,
      "commit": {
        "sha": "82ef4ab181486e37eb9b9fef08ec4ef81f28e94d",
        "message": "Version 1.3.0",
        "author": {
          "name": "Jonas Haag",
          "email": "jonas@lophus.org",
          "date": "2018-06-19T06:54:09Z"
        },
        "html_url": "https://github.com/jonashaag/klaus/commit/82ef4ab181486e37eb9b9fef08ec4ef81f28e94d",
        "api_url": "https://api.github.com/repos/jonashaag/klaus/commits/82ef4ab181486e37eb9b9fef08ec4ef81f28e94d"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jonashaag#klaus#pull#217",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/jonashaag#klaus#pull#217.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jonashaag#klaus#pull#217/source_code"
      },
      "pr": {
        "number": 217,
        "title": "Ship klaus.1 manpage.",
        "body": "",
        "state": "closed",
        "created_at": "2018-07-29T22:53:04Z",
        "updated_at": "2018-07-30T05:08:41Z",
        "merged_at": "2018-07-30T05:08:41Z",
        "html_url": "https://github.com/jonashaag/klaus/pull/217",
        "user": "jelmer",
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "jonashaag_klaus-217",
        "repo": "/jonashaag/klaus",
        "base_commit": "82ef4ab181486e37eb9b9fef08ec4ef81f28e94d",
        "problem_statement": {},
        "edit_files": [
          "MANIFEST.in"
        ],
        "oracle_files": [
          "recursive-include klaus/static *\nrecursive-include klaus/templates *\n"
        ],
        "test_patch": "",
        "patch_preview": "From 7595cdcd0c3a97b1eb8d0582ef2993f89669267c Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Jelmer=20Vernoo=C4=B3?= <jelmer@jelmer.uk>\nDate: Sun, 29 Jul 2018 23:52:21 +0100\nSubject: [PATCH] Ship klaus.1 manpage.\n\n---\n MANIFEST.in | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/MANIFEST.in b/MANIFEST.in\nindex 8ed8064a..ddeea761 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -1,2 +1,3 @@\n recursive-include klaus/static *\n recursive-include klaus/templates *\n+include klaus.1\n"
      },
      "patch": {
        "length": 475,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 0,
        "net_change": 1,
        "changed_files": [
          {
            "file": "MANIFEST.in",
            "added": 1,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 53,
        "total_lines": 3750,
        "total_bytes": 130528,
        "python_files": 24,
        "python_lines": 2227,
        "file_extensions": {
          ".sh": 1,
          ".txt": 2,
          "": 6,
          ".rst": 2,
          ".in": 1,
          ".py": 24,
          ".1": 1,
          ".htdigest": 1,
          ".png": 1,
          ".css": 2,
          ".js": 1,
          ".html": 11
        },
        "largest_files": [
          {
            "path": "klaus/views.py",
            "size": 15804,
            "lines": 467,
            "extension": ".py"
          },
          {
            "path": "klaus/static/klaus.css",
            "size": 9397,
            "lines": 275,
            "extension": ".css"
          },
          {
            "path": "klaus/repo.py",
            "size": 9869,
            "lines": 246,
            "extension": ".py"
          },
          {
            "path": "klaus/utils.py",
            "size": 7324,
            "lines": 243,
            "extension": ".py"
          },
          {
            "path": "klaus/__init__.py",
            "size": 8146,
            "lines": 188,
            "extension": ".py"
          },
          {
            "path": "CHANGELOG.rst",
            "size": 7382,
            "lines": 185,
            "extension": ".rst"
          },
          {
            "path": "tests/test_make_app.py",
            "size": 5325,
            "lines": 170,
            "extension": ".py"
          },
          {
            "path": "klaus/ctagscache.py",
            "size": 7424,
            "lines": 163,
            "extension": ".py"
          },
          {
            "path": "klaus/templates/view_commit.html",
            "size": 5910,
            "lines": 152,
            "extension": ".html"
          },
          {
            "path": "bin/klaus",
            "size": 4099,
            "lines": 112,
            "extension": ""
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 53,
        "files_changed_count": 1,
        "files_changed_ratio": 0.018867924528301886,
        "total_lines_in_repo": 3750,
        "lines_added": 1,
        "lines_deleted": 0,
        "net_lines_changed": 1,
        "lines_changed_ratio": 0.0002666666666666667,
        "pr_body_length": 0,
        "commit_message_length": 13,
        "python_file_count": 24,
        "python_line_count": 2227
      }
    },
    {
      "tar_file_name": "jwlodek#py_cui#pull#97",
      "repo_name": "jwlodek#py_cui#pull#97",
      "success": true,
      "error": null,
      "commit": {
        "sha": "3f03af05ecc29f04cc07ee6b62c8ac2a76de8a68",
        "message": "Adding recoverpy by @PabloLec to Used By Section of README",
        "author": {
          "name": "jwlodek",
          "email": "jakub.wlodek@stonybrook.edu",
          "date": "2021-03-04T21:26:42Z"
        },
        "html_url": "https://github.com/jwlodek/py_cui/commit/3f03af05ecc29f04cc07ee6b62c8ac2a76de8a68",
        "api_url": "https://api.github.com/repos/jwlodek/py_cui/commits/3f03af05ecc29f04cc07ee6b62c8ac2a76de8a68"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jwlodek#py_cui#pull#97",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/jwlodek#py_cui#pull#97.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/jwlodek#py_cui#pull#97/source_code"
      },
      "pr": {
        "number": 97,
        "title": "Match slider theme to py_cui's overall look",
        "body": "Change slider's default theme to better match other *py_cui*'s widgets. This will turn on the border and title as default.\r\n\r\n- [x] I have read the contribution guidelines\r\n- [x] CI Unit tests pass\r\n- [x] New functions/classes have consistent docstrings\r\n\r\n**What this pull request changes**\r\n\r\n* Changes default theme\r\n* Fixes outdated/wrong documents\r\n\r\nDidn't know *f-string* was something I can't use in *python 3.5*, second commit fixes it. If I recall correctly typing was added past 3.5 but seems like it didn't complain about it - maybe backported?",
        "state": "closed",
        "created_at": "2021-03-07T10:53:02Z",
        "updated_at": "2021-03-07T14:30:03Z",
        "merged_at": "2021-03-07T14:30:03Z",
        "html_url": "https://github.com/jwlodek/py_cui/pull/97",
        "user": "jupiterbjy",
        "additions": 43,
        "deletions": 24,
        "changed_files": 2,
        "commits": 2
      },
      "swebench": {
        "instance_id": "jwlodek_py_cui-97",
        "repo": "/jwlodek/py_cui",
        "base_commit": "3f03af05ecc29f04cc07ee6b62c8ac2a76de8a68",
        "problem_statement": {},
        "edit_files": [
          "examples/controls/slider_demo.py",
          "py_cui/controls/slider.py",
          "py_cui/controls/slider.py"
        ],
        "oracle_files": [
          "#!/usr/bin/env python3\nfrom inspect import cleandoc\nimport itertools\nimport py_cui\n\n\nclass App:\n    character_gen = itertools.cycle((\"X\", \"-\", \"â–ˆ\", \"[\", \"#\"))\n\n    def __init__(self, root_: py_cui.PyCUI):\n        self.root = root_\n\n        # Default configuration\n        self.default = self.root.add_slider(\"Default\", 0, 0, column_span=2, min_val=-50, max_val=50)\n\n        # controls\n        self.title_button = self.root.add_button(\"Toggle title\", 1, 0, command=self.default.toggle_title)\n        self.border_button = self.root.add_button(\"Toggle border\", 1, 1, command=self.default.toggle_border)\n        self.value_button = self.root.add_button(\"Toggle value\", 1, 2, command=self.default.toggle_value)\n        self.character_button = self.root.add_button(\"Cycle char\", 2, 0, command=self.cycle_characters)\n        self.align_button = self.root.add_button(\"Change alignment\", 2, 1, command=self.cycle_height)\n        self.step_slider = self.root.add_slider(\"Step size\", 2, 2, min_val=1, init_val=2, max_val=10)\n        self.spacer = self.root.add_text_block(\"Spacer\", 0, 2)\n\n        # setups\n        self.step_slider.toggle_border()\n        self.step_slider.toggle_title()\n        self.root.set_on_draw_update_func(self.set_step)\n        self.height_cycle = itertools.cycle(\n            (\n                self.default.align_to_top,\n                self.default.align_to_middle,\n                self.default.align_to_bottom,\n            )\n        )\n\n        help_text = \"\"\"\n                    Press a button to make a change.\n                    You can change character freely,\n                    but for demonstration purpose,\n                    I've set it to cycle it here.\n                    \"\"\"\n\n        help_text = cleandoc(help_text)\n        self.spacer.set_text(help_text)\n\n    def cycle_characters(self):\n        self.default.set_bar_char(next(self.character_gen))\n\n    def cycle_height(self):\n        next(self.height_cycle)()\n\n    def set_step(self):\n        self.default.set_slider_step(self.step_slider.get_slider_value())\n\n\nif __name__ == '__main__':\n    root = py_cui.PyCUI(3, 3)\n    root.set_title(\"Slider playground\")\n    # root.set_refresh_timeout(0.1)\n    s = App(root)\n    root.start()\n",
          "import py_cui.ui\nimport py_cui.widgets\nimport py_cui.popups\n\n\nclass SliderImplementation(py_cui.ui.UIImplementation):\n\n    def __init__(self, min_val, max_val, init_val, step, logger):\n        super().__init__(logger)\n\n        self._min_val = min_val\n        self._max_val = max_val\n        self._cur_val = init_val\n        self._step = step\n\n        self._bar_char = \"#\"\n\n        if self._cur_val < self._min_val or self._cur_val > self._max_val:\n            raise py_cui.errors.PyCUIInvalidValue(\n                'initial value must be between {} and {}'\n                .format(self._min_val, self._max_val))\n\n\n    def set_bar_char(self, char):\n        \"\"\"Updates the character used to represent the slider bar\n        \"\"\"\n\n        self._bar_char = char\n\n\n    def update_slider_value(self, offset: int) -> float:\n        \"\"\"\n        Steps up or down the value in offset fashion.\n\n        Parameters\n        ----------\n        offset : int\n            Number of steps to increase or decrease the slider value.\n\n        Returns\n        -------\n        self._cur_val: float\n            Current slider value.\n\n        \"\"\"\n\n        # direction , 1 raise value, -1 lower value\n        self._cur_val += (offset * self._step)\n\n        if self._cur_val <= self._min_val:\n            self._cur_val = self._min_val\n\n        if self._cur_val >= self._max_val:\n            self._cur_val = self._max_val\n\n        return self._cur_val\n\n\n    def get_slider_value(self):\n        \"\"\"return current slider value\n        \"\"\"\n        return self._cur_val\n\n\n    def set_slider_step(self,step):\n        \"\"\"change step value\n        \"\"\"\n        self._step = step\n\n\nclass SliderWidget(py_cui.widgets.Widget, SliderImplementation):\n    \"\"\"Widget for a Slider\n    \"\"\"\n\n    \"\"\"\n    Parameters\n    ----------\n    _min_val : int\n        Lowest value of the slider\n    _max_val: int\n        Highest value of the slider\n    _step : int\n        Increment from low to high value\n    _cur_val:\n        Current value of the slider\n\n    \"\"\"\n\n    def __init__(self, id, title, grid, row, column, row_span, column_span,\n                 padx, pady, logger, min_val, max_val, step, init_val):\n\n        SliderImplementation.__init__(self, min_val, max_val, init_val, step, logger)\n\n        py_cui.widgets.Widget.__init__(self, id, title, grid, row, column,\n                                       row_span, column_span, padx,\n                                       pady, logger, selectable=True)\n\n        self._title_enabled = False\n        self._border_enabled = False\n        self._display_value = True\n        self._alignment = \"mid\"\n        self.set_help_text(\"Focus mode on Slider. Use left/right to adjust value. Esc to exit.\")\n\n\n    def toggle_title(self):\n        \"\"\"Toggles visibility of the widget's name.\n        \"\"\"\n\n        self._title_enabled = not self._title_enabled\n\n\n    def toggle_border(self):\n        \"\"\"Toggles visibility of the widget's border. Enabling this will disable the alignment.\n        \"\"\"\n\n        self._border_enabled = not self._border_enabled\n\n\n    def toggle_value(self):\n        \"\"\"Toggles visibility of the widget's current value in integer.\n        \"\"\"\n\n        self._display_value = not self._display_value\n\n\n    def align_to_top(self):\n        \"\"\"Aligns widget height to top.\n        \"\"\"\n        self._alignment = \"top\"\n\n\n    def align_to_middle(self):\n        \"\"\"Aligns widget height to middle. default configuration.\n        \"\"\"\n        self._alignment = \"mid\"\n\n\n    def align_to_bottom(self):\n        \"\"\"Aligns widget height to bottom.\n        \"\"\"\n        self._alignment = \"btm\"\n\n\n    def _custom_draw_with_border(self, start_y: int, content: str):\n        \"\"\"Custom method made from renderer.draw_border to support alignment for bordered variants.\n\n        Parameters\n        ----------\n        start_y : int\n            border's Y-axis starting coordination\n        content: str\n            string to be drawn inside the border\n        \"\"\"\n\n        renderer = self.get_renderer()\n        ui_element = self\n\n        renderer.set_color_mode(ui_element.get_border_color())\n\n        if ui_element.is_selected():\n            renderer._set_bold()\n            renderer._draw_border_top(ui_element, start_y, False)\n\n            renderer.draw_text(ui_element, content, start_y + 1, selected=True, bordered=True)\n            renderer._set_bold()\n\n            renderer._draw_border_bottom(ui_element, start_y + 2)\n            renderer._unset_bold()\n        else:\n            renderer._draw_border_top(ui_element, start_y, False)\n            renderer.draw_text(ui_element, content, start_y + 1, selected=False, bordered=True)\n            renderer._draw_border_bottom(ui_element, start_y + 2)\n\n        renderer.unset_color_mode(ui_element.get_border_color())\n\n\n    def _generate_bar(self, width: int) -> str:\n        \"\"\"Internal implementation to generate progression bar.\n\n        Parameters\n        ----------\n        width : int\n            Width of bar in character length.\n\n        Returns\n        -------\n        progress: str\n            progressive bar string  with length of width.\n        \"\"\"\n        if self._display_value:\n            min_string = str(self._min_val)\n            value_str = str(int(self._cur_val))\n\n            width -= len(min_string)\n\n            bar = self._bar_char * int((width * (self._cur_val - self._min_val)) / (self._max_val - self._min_val))\n            progress = (self._bar_char * len(min_string) + bar)[: -len(value_str)] + value_str\n        else:\n            progress = self._bar_char * int((width * (self._cur_val - self._min_val)) / (self._max_val - self._min_val))\n\n        return progress\n\n\n    def _draw(self):\n        \"\"\"Override of base class draw function.\n        \"\"\"\n\n        super()._draw()\n        self._renderer.set_color_mode(self._color)\n\n        height, width = self.get_absolute_dimensions()\n        visual_height = (2 if self._border_enabled else 0) + (1 if self._title_enabled else 0)\n\n        if self._alignment == \"top\":\n            text_y_pos = self._start_y\n        elif self._alignment == \"mid\":\n            text_y_pos = self._start_y + ((height - visual_height) // 2)\n        else:\n            text_y_pos = self._start_y + height - visual_height - 1\n\n        if self._title_enabled:\n            self._renderer.draw_text(\n                self, self.get_title(), text_y_pos, selected=self.is_selected(), bordered=False\n            )\n            text_y_pos += 1\n\n        if self._border_enabled:\n            width -= 6\n            self._custom_draw_with_border(text_y_pos, self._generate_bar(width))\n        else:\n            width -= 2\n            self._renderer.draw_text(\n                self, self._generate_bar(width), text_y_pos, selected=self.is_selected(), bordered=False\n            )\n\n        self._renderer.unset_color_mode(self._color)\n\n\n    def _handle_key_press(self, key_pressed):\n        \"\"\"LEFT_ARROW decreases value, RIGHT_ARROW increases.\n\n        Parameters\n        ----------\n        key_pressed : int\n            key code of pressed key\n        \"\"\"\n\n        super()._handle_key_press(key_pressed)\n        if key_pressed == py_cui.keys.KEY_LEFT_ARROW:\n            self.update_slider_value(-1)\n        if key_pressed == py_cui.keys.KEY_RIGHT_ARROW:\n            self.update_slider_value(1)\n\n\nclass SliderPopup(py_cui.popups.Popup, SliderImplementation):\n    pass\n",
          "import py_cui.ui\nimport py_cui.widgets\nimport py_cui.popups\n\n\nclass SliderImplementation(py_cui.ui.UIImplementation):\n\n    def __init__(self, min_val, max_val, init_val, step, logger):\n        super().__init__(logger)\n\n        self._min_val = min_val\n        self._max_val = max_val\n        self._cur_val = init_val\n        self._step = step\n\n        self._bar_char = \"#\"\n\n        if self._cur_val < self._min_val or self._cur_val > self._max_val:\n            raise py_cui.errors.PyCUIInvalidValue(\n                'initial value must be between {} and {}'\n                .format(self._min_val, self._max_val))\n\n\n    def set_bar_char(self, char):\n        \"\"\"Updates the character used to represent the slider bar\n        \"\"\"\n\n        self._bar_char = char\n\n\n    def update_slider_value(self, offset: int) -> float:\n        \"\"\"\n        Steps up or down the value in offset fashion.\n\n        Parameters\n        ----------\n        offset : int\n            Number of steps to increase or decrease the slider value.\n\n        Returns\n        -------\n        self._cur_val: float\n            Current slider value.\n\n        \"\"\"\n\n        # direction , 1 raise value, -1 lower value\n        self._cur_val += (offset * self._step)\n\n        if self._cur_val <= self._min_val:\n            self._cur_val = self._min_val\n\n        if self._cur_val >= self._max_val:\n            self._cur_val = self._max_val\n\n        return self._cur_val\n\n\n    def get_slider_value(self):\n        \"\"\"return current slider value\n        \"\"\"\n        return self._cur_val\n\n\n    def set_slider_step(self,step):\n        \"\"\"change step value\n        \"\"\"\n        self._step = step\n\n\nclass SliderWidget(py_cui.widgets.Widget, SliderImplementation):\n    \"\"\"Widget for a Slider\n    \"\"\"\n\n    \"\"\"\n    Parameters\n    ----------\n    _min_val : int\n        Lowest value of the slider\n    _max_val: int\n        Highest value of the slider\n    _step : int\n        Increment from low to high value\n    _cur_val:\n        Current value of the slider\n\n    \"\"\"\n\n    def __init__(self, id, title, grid, row, column, row_span, column_span,\n                 padx, pady, logger, min_val, max_val, step, init_val):\n\n        SliderImplementation.__init__(self, min_val, max_val, init_val, step, logger)\n\n        py_cui.widgets.Widget.__init__(self, id, title, grid, row, column,\n                                       row_span, column_span, padx,\n                                       pady, logger, selectable=True)\n\n        self._title_enabled = False\n        self._border_enabled = False\n        self._display_value = True\n        self._alignment = \"mid\"\n        self.set_help_text(\"Focus mode on Slider. Use left/right to adjust value. Esc to exit.\")\n\n\n    def toggle_title(self):\n        \"\"\"Toggles visibility of the widget's name.\n        \"\"\"\n\n        self._title_enabled = not self._title_enabled\n\n\n    def toggle_border(self):\n        \"\"\"Toggles visibility of the widget's border. Enabling this will disable the alignment.\n        \"\"\"\n\n        self._border_enabled = not self._border_enabled\n\n\n    def toggle_value(self):\n        \"\"\"Toggles visibility of the widget's current value in integer.\n        \"\"\"\n\n        self._display_value = not self._display_value\n\n\n    def align_to_top(self):\n        \"\"\"Aligns widget height to top.\n        \"\"\"\n        self._alignment = \"top\"\n\n\n    def align_to_middle(self):\n        \"\"\"Aligns widget height to middle. default configuration.\n        \"\"\"\n        self._alignment = \"mid\"\n\n\n    def align_to_bottom(self):\n        \"\"\"Aligns widget height to bottom.\n        \"\"\"\n        self._alignment = \"btm\"\n\n\n    def _custom_draw_with_border(self, start_y: int, content: str):\n        \"\"\"Custom method made from renderer.draw_border to support alignment for bordered variants.\n\n        Parameters\n        ----------\n        start_y : int\n            border's Y-axis starting coordination\n        content: str\n            string to be drawn inside the border\n        \"\"\"\n\n        renderer = self.get_renderer()\n        ui_element = self\n\n        renderer.set_color_mode(ui_element.get_border_color())\n\n        if ui_element.is_selected():\n            renderer._set_bold()\n            renderer._draw_border_top(ui_element, start_y, False)\n\n            renderer.draw_text(ui_element, content, start_y + 1, selected=True, bordered=True)\n            renderer._set_bold()\n\n            renderer._draw_border_bottom(ui_element, start_y + 2)\n            renderer._unset_bold()\n        else:\n            renderer._draw_border_top(ui_element, start_y, False)\n            renderer.draw_text(ui_element, content, start_y + 1, selected=False, bordered=True)\n            renderer._draw_border_bottom(ui_element, start_y + 2)\n\n        renderer.unset_color_mode(ui_element.get_border_color())\n\n\n    def _generate_bar(self, width: int) -> str:\n        \"\"\"Internal implementation to generate progression bar.\n\n        Parameters\n        ----------\n        width : int\n            Width of bar in character length.\n\n        Returns\n        -------\n        progress: str\n            progressive bar string  with length of width.\n        \"\"\"\n        if self._display_value:\n            min_string = str(self._min_val)\n            value_str = str(int(self._cur_val))\n\n            width -= len(min_string)\n\n            bar = self._bar_char * int((width * (self._cur_val - self._min_val)) / (self._max_val - self._min_val))\n            progress = (self._bar_char * len(min_string) + bar)[: -len(value_str)] + value_str\n        else:\n            progress = self._bar_char * int((width * (self._cur_val - self._min_val)) / (self._max_val - self._min_val))\n\n        return progress\n\n\n    def _draw(self):\n        \"\"\"Override of base class draw function.\n        \"\"\"\n\n        super()._draw()\n        self._renderer.set_color_mode(self._color)\n\n        height, width = self.get_absolute_dimensions()\n        visual_height = (2 if self._border_enabled else 0) + (1 if self._title_enabled else 0)\n\n        if self._alignment == \"top\":\n            text_y_pos = self._start_y\n        elif self._alignment == \"mid\":\n            text_y_pos = self._start_y + ((height - visual_height) // 2)\n        else:\n            text_y_pos = self._start_y + height - visual_height - 1\n\n        if self._title_enabled:\n            self._renderer.draw_text(\n                self, self.get_title(), text_y_pos, selected=self.is_selected(), bordered=False\n            )\n            text_y_pos += 1\n\n        if self._border_enabled:\n            width -= 6\n            self._custom_draw_with_border(text_y_pos, self._generate_bar(width))\n        else:\n            width -= 2\n            self._renderer.draw_text(\n                self, self._generate_bar(width), text_y_pos, selected=self.is_selected(), bordered=False\n            )\n\n        self._renderer.unset_color_mode(self._color)\n\n\n    def _handle_key_press(self, key_pressed):\n        \"\"\"LEFT_ARROW decreases value, RIGHT_ARROW increases.\n\n        Parameters\n        ----------\n        key_pressed : int\n            key code of pressed key\n        \"\"\"\n\n        super()._handle_key_press(key_pressed)\n        if key_pressed == py_cui.keys.KEY_LEFT_ARROW:\n            self.update_slider_value(-1)\n        if key_pressed == py_cui.keys.KEY_RIGHT_ARROW:\n            self.update_slider_value(1)\n\n\nclass SliderPopup(py_cui.popups.Popup, SliderImplementation):\n    pass\n"
        ],
        "test_patch": "",
        "patch_preview": "From 581ef2ae3dc4322abcde621a460eebab9736d707 Mon Sep 17 00:00:00 2001\nFrom: jupiterbjy <jupiterbjy@gmail.com>\nDate: Sun, 7 Mar 2021 14:12:05 +0900\nSubject: [PATCH 1/2] Changed default visual of SliderWidget. Updated documents\n and example accordingly.\n\n---\n examples/controls/slider_demo.py |  3 --\n py_cui/controls/slider.py        | 64 +++++++++++++++++++++-----------\n 2 files changed, 43 insertions(+), 24 deletions(-)\n\ndiff --git a/examples/controls/slider_demo.py b/examples/controls/slider_de"
      },
      "patch": {
        "length": 6563,
        "files_changed": 3,
        "lines_added": 44,
        "lines_deleted": 25,
        "net_change": 19,
        "changed_files": [
          {
            "file": "examples/controls/slider_demo.py",
            "added": 0,
            "deleted": 3
          },
          {
            "file": "py_cui/controls/slider.py",
            "added": 43,
            "deleted": 21
          },
          {
            "file": "py_cui/controls/slider.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 83,
        "total_lines": 34329,
        "total_bytes": 2123181,
        "python_files": 43,
        "python_lines": 10608,
        "file_extensions": {
          ".md": 28,
          "": 2,
          ".txt": 2,
          ".py": 43,
          ".yml": 1,
          ".bat": 2,
          ".sh": 2,
          ".gif": 2,
          ".svg": 1
        },
        "largest_files": [
          {
            "path": "docs/assets/pyautogit-demo.gif",
            "size": 852913,
            "lines": 7567,
            "extension": ".gif"
          },
          {
            "path": "docs/assets/py2048-demo.gif",
            "size": 705287,
            "lines": 6559,
            "extension": ".gif"
          },
          {
            "path": "docs/DocstringGenerated/Ui.md",
            "size": 29422,
            "lines": 2009,
            "extension": ".md"
          },
          {
            "path": "py_cui/__init__.py",
            "size": 60494,
            "lines": 1623,
            "extension": ".py"
          },
          {
            "path": "docs/DocstringGenerated/PyCui.md",
            "size": 27248,
            "lines": 1421,
            "extension": ".md"
          },
          {
            "path": "py_cui/ui.py",
            "size": 38692,
            "lines": 1355,
            "extension": ".py"
          },
          {
            "path": "docs/DocstringGenerated/Widgets.md",
            "size": 16106,
            "lines": 1038,
            "extension": ".md"
          },
          {
            "path": "py_cui/widgets.py",
            "size": 29532,
            "lines": 857,
            "extension": ".py"
          },
          {
            "path": "py_cui/dialogs/filedialog.py",
            "size": 30398,
            "lines": 833,
            "extension": ".py"
          },
          {
            "path": "docs/DocstringGenerated/Form.md",
            "size": 11013,
            "lines": 719,
            "extension": ".md"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 83,
        "files_changed_count": 3,
        "files_changed_ratio": 0.03614457831325301,
        "total_lines_in_repo": 34329,
        "lines_added": 44,
        "lines_deleted": 25,
        "net_lines_changed": 19,
        "lines_changed_ratio": 0.0020099624224416673,
        "pr_body_length": 556,
        "commit_message_length": 58,
        "python_file_count": 43,
        "python_line_count": 10608
      }
    },
    {
      "tar_file_name": "kieferk#dfply#pull#44",
      "repo_name": "kieferk#dfply#pull#44",
      "success": true,
      "error": null,
      "commit": {
        "sha": "b1f1accd0b3eea5c4ec1ccb25e3c53b9094fc11b",
        "message": "Merge pull request #37 from TariqAHassan/master\n\nCreate `filter_by()` alias for `mask()`",
        "author": {
          "name": "Kiefer Katovich",
          "email": "kiefer.katovich@gmail.com",
          "date": "2017-11-18T00:21:52Z"
        },
        "html_url": "https://github.com/kieferk/dfply/commit/b1f1accd0b3eea5c4ec1ccb25e3c53b9094fc11b",
        "api_url": "https://api.github.com/repos/kieferk/dfply/commits/b1f1accd0b3eea5c4ec1ccb25e3c53b9094fc11b"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kieferk#dfply#pull#44",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/kieferk#dfply#pull#44.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kieferk#dfply#pull#44/source_code"
      },
      "pr": {
        "number": 44,
        "title": "New verb: pull (extracts a column)",
        "body": "This adds \"`pull`\", the equivalent of dplyr's [`pull`](http://dplyr.tidyverse.org/reference/pull.html).",
        "state": "closed",
        "created_at": "2017-12-14T10:03:52Z",
        "updated_at": "2017-12-16T06:06:33Z",
        "merged_at": "2017-12-16T06:06:33Z",
        "html_url": "https://github.com/kieferk/dfply/pull/44",
        "user": "janfreyberg",
        "additions": 28,
        "deletions": 0,
        "changed_files": 2,
        "commits": 4
      },
      "swebench": {
        "instance_id": "kieferk_dfply-44",
        "repo": "/kieferk/dfply",
        "base_commit": "b1f1accd0b3eea5c4ec1ccb25e3c53b9094fc11b",
        "problem_statement": {},
        "edit_files": [
          "dfply/subset.py",
          "README.md"
        ],
        "oracle_files": [
          "from .base import *\nimport warnings\nimport numpy as np\n\n\n# ------------------------------------------------------------------------------\n# `head` and `tail`\n# ------------------------------------------------------------------------------\n\n@dfpipe\ndef head(df, n=5):\n    return df.head(n)\n\n\n@dfpipe\ndef tail(df, n=5):\n    return df.tail(n)\n\n\n# ------------------------------------------------------------------------------\n# Sampling\n# ------------------------------------------------------------------------------\n\n@dfpipe\ndef sample(df, *args, **kwargs):\n    return df.sample(*args, **kwargs)\n\n\n@pipe\n@group_delegation\n@symbolic_evaluation(eval_as_label=['*'])\ndef distinct(df, *args, **kwargs):\n    if not args:\n        return df.drop_duplicates(**kwargs)\n    return df.drop_duplicates(list(args), **kwargs)\n\n\n@dfpipe\ndef row_slice(df, indices):\n    if isinstance(indices, (tuple, list)):\n        indices = np.array(indices)\n    if isinstance(indices, int):\n        indices = np.array([indices])\n    if isinstance(indices, pd.Series):\n        indices = indices.values\n\n    if indices.dtype == bool:\n        return df.loc[indices, :]\n    else:\n        return df.iloc[indices, :]\n\n\n# ------------------------------------------------------------------------------\n# Filtering/masking\n# ------------------------------------------------------------------------------\n\n@dfpipe\ndef mask(df, *args):\n    mask = pd.Series(np.ones(df.shape[0], dtype=bool))\n    for arg in args:\n        if arg.dtype != bool:\n            raise Exception(\"Arguments must be boolean.\")\n        mask = mask & arg.reset_index(drop=True)\n    return df[mask.values]\n\n\nfilter_by = mask   # alias for mask()\n\n\n@dfpipe\ndef top_n(df, n=None, ascending=True, col=None):\n    if not n:\n        raise ValueError('n must be specified')\n    if not isinstance(col, pd.Series):\n        col = df.columns[-1]\n    else:\n        col = col._name\n    index = df[[col]].copy()\n    index['ranks'] = index[col].rank(ascending=ascending)\n    index = index[index['ranks'] >= index['ranks'].nlargest(n).min()]\n    return df.reindex(index.index)\n",
          "# dfply\n\n### Version: 0.3.1\n\n> Note: Version 0.3.0 is the first big update in awhile, and changes a lot of\nthe \"base\" code. The `pandas-ply` package is no longer being imported. I have coded\nmy own version of the \"symbolic\" objects that I was borrowing from `pandas-ply`. Also,\nI am no longer supporting Python 2, sorry!\n\n> **In v0.3 `groupby` has been renamed to `group_by` to mirror the `dplyr` function.\nIf this breaks your legacy code, one possible fix is to have `from dfply.group import group_by as groupby`\nin your package imports.**\n\nThe `dfply` package makes it possible to do R's `dplyr`-style data manipulation with pipes\nin python on pandas DataFrames.\n\nThis is an alternative to [`pandas-ply`](https://github.com/coursera/pandas-ply)\nand [`dplython`](https://github.com/dodger487/dplython), which both engineer `dplyr`\nsyntax and functionality in python. There are probably more packages that attempt\nto enable `dplyr`-style dataframe manipulation in python, but those are the two I\nam aware of.\n\n`dfply` uses a decorator-based architecture for the piping functionality and\nto \"categorize\" the types of data manipulation functions. The goal of this  \narchitecture is to make `dfply` concise and easily extensible, simply by chaining\ntogether different decorators that each have a distinct effect on the wrapped\nfunction. There is a more in-depth overview of the decorators and how `dfply` can be\ncustomized below.\n\n`dfply` is intended to mimic the functionality of `dplyr`. The syntax\nis the same for the most part, but will vary in some cases as Python is a\nconsiderably different programming language than R.\n\nA good amount of the core functionality of `dplyr` is complete, and the remainder is\nactively being added in. Going forward I hope functionality that is not\ndirectly part of `dplyr` to be incorporated into `dfply` as well. This is not\nintended to be an absolute mimic of `dplyr`, but instead a port of the _ease,\nconvenience and readability_ the `dplyr` package provides for data manipulation\ntasks.\n\n**Expect frequent updates to the package version as features are added and\nany bugs are fixed.**\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [Overview of functions](#overview-of-functions)\n  - [The `>>` and `>>=` pipe operators](#the--and--pipe-operators)\n  - [The `X` DataFrame symbol](#the-x-dataframe-symbol)\n  - [Selecting and dropping](#selecting-and-dropping)\n    - [`select()` and `drop()` functions](#select-and-drop-functions)\n    - [Selection using the inversion `~` operator on symbolic columns](#selection-using-the-inversion--operator-on-symbolic-columns)\n    - [Selection filter functions](#selection-filter-functions)\n  - [Subsetting and filtering](#subsetting-and-filtering)\n    - [`row_slice()`](#row_slice)\n    - [`sample()`](#sample)\n    - [`distinct()`](#distinct)\n    - [`mask()`](#mask)\n  - [DataFrame transformation](#dataframe-transformation)\n    - [`mutate()`](#mutate)\n    - [`transmute()`](#transmute)\n  - [Grouping](#grouping)\n    - [`group_by()` and `ungroup()`](#group_by-and-ungroup)\n  - [Reshaping](#reshaping)\n    - [`arrange()`](#arrange)\n    - [`rename()`](#rename)\n    - [`gather()`](#gather)\n    - [`spread()`](#spread)\n    - [`separate()`](#separate)\n    - [`unite()`](#unite)\n  - [Joining](#joining)\n    - [`inner_join()`](#inner_join)\n    - [`outer_join()` or `full_join()`](#outer_join-or-full_join)\n    - [`left_join()`](#left_join)\n    - [`right_join()`](#right_join)\n    - [`semi_join()`](#semi_join)\n    - [`anti_join()`](#anti_join)\n  - [Set operations](#set-operations)\n    - [`union()`](#union)\n    - [`intersect()`](#intersect)\n    - [`set_diff()`](#set_diff)\n  - [Binding](#binding)\n    - [`bind_rows()`](#bind_rows)\n    - [`bind_cols()`](#bind_cols)\n  - [Summarization](#summarization)\n    - [`summarize()`](#summarize)\n    - [`summarize_each()`](#summarize_each)\n- [Embedded column functions](#embedded-column-functions)\n  - [Window functions](#window-functions)\n    - [`lead()` and `lag()`](#lead-and-lag)\n    - [`between()`](#between)\n    - [`dense_rank()`](#dense_rank)\n    - [`min_rank()`](#min_rank)\n    - [`cumsum()`](#cumsum)\n    - [`cummean()`](#cummean)\n    - [`cummax()`](#cummax)\n    - [`cummin()`](#cummin)\n    - [`cumprod()`](#cumprod)\n  - [Summary functions](#summary-functions)\n    - [`mean()`](#mean)\n    - [`first()`](#first)\n    - [`last()`](#last)\n    - [`nth()`](#nth)\n    - [`n()`](#n)\n    - [`n_distinct()`](#n_distinct)\n    - [`IQR()`](#iqr)\n    - [`colmin()`](#colmin)\n    - [`colmax()`](#colmax)\n    - [`median()`](#median)\n    - [`var()`](#var)\n    - [`sd()`](#sd)\n- [Extending `dfply` with custom functions](#extending-dfply-with-custom-functions)\n  - [Case 1: A custom \"pipe\" function with `@dfpipe`](#case-1-a-custom-pipe-function-with-dfpipe)\n  - [Case 2: A function that works with symbolic objects using `@make_symbolic`](#case-2-a-function-that-works-with-symbolic-objects-using-make_symbolic)\n    - [Without symbolic arguments, `@make_symbolic` functions work like normal functions!](#without-symbolic-arguments-make_symbolic-functions-work-like-normal-functions)\n- [Advanced: understanding base `dfply` decorators](#advanced-understanding-base-dfply-decorators)\n  - [The `Intention` class](#the-intention-class)\n  - [`@pipe`](#pipe)\n  - [`@group_delegation`](#group_delegation)\n  - [`@symbolic_evaluation`](#symbolic_evaluation)\n    - [Controlling `@symbolic_evaluation` with the `eval_symbols` argument](#controlling-symbolic_evaluation-with-the-eval_symbols-argument)\n  - [`@dfpipe`](#dfpipe)\n  - [`@make_symbolic`](#make_symbolic)\n- [Contributing](#contributing)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Overview of functions\n\n### The `>>` and `>>=` pipe operators\n\ndfply works directly on pandas DataFrames, chaining operations on the data with\nthe `>>` operator, or alternatively starting with `>>=` for inplace operations.\n\n```python\nfrom dfply import *\n\ndiamonds >> head(3)\n\n   carat      cut color clarity  depth  table  price     x     y     z\n0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n```\n\nYou can chain piped operations, and of course assign the output to a new\nDataFrame.\n\n```python\nlowprice = diamonds >> head(10) >> tail(3)\n\nlowprice\n\n   carat        cut color clarity  depth  table  price     x     y     z\n7   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n8   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n9   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39\n```\n\nInplace operations are done with the first pipe as `>>=` and subsequent pipes\nas `>>`.\n\n```python\ndiamonds >>= head(10) >> tail(3)\n\ndiamonds\n\n   carat        cut color clarity  depth  table  price     x     y     z\n7   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n8   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n9   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39\n```\n\nWhen using the inplace pipe, the DataFrame is not required on the left hand\nside of the `>>=` pipe and the DataFrame variable is overwritten with the\noutput of the operations.\n\n\n### The `X` DataFrame symbol\n\nThe DataFrame as it is passed through the piping operations is represented\nby the symbol `X`. It records the actions you want to take (represented by\nthe `Intention` class), but does not evaluate them until the appropriate time.\nOperations on the DataFrame are deferred. Selecting\ntwo of the columns, for example, can be done using the symbolic `X` DataFrame\nduring the piping operations.\n\n```python\ndiamonds >> select(X.carat, X.cut) >> head(3)\n\n   carat      cut\n0   0.23    Ideal\n1   0.21  Premium\n2   0.23     Good\n```\n\n\n### Selecting and dropping\n\n#### `select()` and `drop()` functions\n\nThere are two functions for selection, inverse of each other: `select` and\n`drop`. The `select` and `drop` functions accept string labels, integer positions,\nand/or symbolically represented column names (`X.column`). They also accept symbolic \"selection\nfilter\" functions, which will be covered shortly.\n\nThe example below selects \"cut\", \"price\", \"x\", and \"y\" from the diamonds dataset.\n\n```python\ndiamonds >> select(1, X.price, ['x', 'y']) >> head(2)\n\n       cut  price     x     y\n0    Ideal    326  3.95  3.98\n1  Premium    326  3.89  3.84\n```\n\nIf you were instead to use `drop`, you would get back all columns besides those specified.\n\n```python\ndiamonds >> drop(1, X.price, ['x', 'y']) >> head(2)\n\n   carat color clarity  depth  table     z\n0   0.23     E     SI2   61.5   55.0  2.43\n1   0.21     E     SI1   59.8   61.0  2.31\n```\n\n\n#### Selection using the inversion `~` operator on symbolic columns\n\nOne particularly nice thing about `dplyr`'s selection functions is that you can\ndrop columns inside of a select statement by putting a subtraction sign in front,\nlike so: `... %>% select(-col)`. The same can be done in `dfply`, but instead of\nthe subtraction operator you use the tilde `~`.\n\nFor example, let's say I wanted to select any column _except_ carat, color, and\nclarity in my dataframe. One way to do this is to specify those for removal using\nthe `~` operator like so:\n\n\n```python\ndiamonds >> select(~X.carat, ~X.color, ~X.clarity) >> head(2)\n\n       cut  depth  table  price     x     y     z\n0    Ideal   61.5   55.0    326  3.95  3.98  2.43\n1  Premium   59.8   61.0    326  3.89  3.84  2.31\n```\n\nNote that if you are going to use the inversion operator, you _must_ place it\nprior to the symbolic `X` (or a symbolic such as a selection filter function, covered\nnext). For example, using the inversion operator on a list of columns will\nresult in an error:\n\n```python\ndiamonds >> select(~[X.carat, X.color, X.clarity]) >> head(2)\n\nTypeError: bad operand type for unary ~: 'list'\n```\n\n\n#### Selection filter functions\n\nThe vanilla `select` and `drop` functions are useful, but there are a variety of\nselection functions inspired by `dplyr` available to make selecting and dropping\ncolumns a breeze. These functions are intended to be put inside of the `select` and\n`drop` functions, and can be paired with the `~` inverter.\n\nFirst, a quick rundown of the available functions:\n- `starts_with(prefix)`: find columns that start with a string prefix.\n- `ends_with(suffix)`: find columns that end with a string suffix.\n- `contains(substr)`: find columns that contain a substring in their name.\n- `everything()`: all columns.\n- `columns_between(start_col, end_col, inclusive=True)`: find columns between a specified start and end column.\nThe `inclusive` boolean keyword argument indicates whether the end column should be included or not.\n- `columns_to(end_col, inclusive=True)`: get columns up to a specified end column. The `inclusive`\nargument indicates whether the ending column should be included or not.\n- `columns_from(start_col)`: get the columns starting at a specified column.\n\nThe selection filter functions are best explained by example. Let's say I wanted to\nselect only the columns that started with a \"c\":\n\n```python\ndiamonds >> select(starts_with('c')) >> head(2)\n\n   carat      cut color clarity\n0   0.23    Ideal     E     SI2\n1   0.21  Premium     E     SI1\n```\n\nThe selection filter functions are instances of the class `Intention`, just like the\n`X` placeholder, and so I can also use the inversion operator with them. For example,\nI can alternatively select the columns that do not start with \"c\":\n\n```python\ndiamonds >> select(~starts_with('c')) >> head(2)\n\n   depth  table  price     x     y     z\n0   61.5   55.0    326  3.95  3.98  2.43\n1   59.8   61.0    326  3.89  3.84  2.31\n```\n\nThey work the same inside the `drop` function, but with the intention of removal.\nI could, for example, use the `columns_from` selection filter to drop all columns\nfrom \"price\" onwards:\n\n```python\ndiamonds >> drop(columns_from(X.price)) >> head(2)\n\n   carat      cut color clarity  depth  table\n0   0.23    Ideal     E     SI2   61.5   55.0\n1   0.21  Premium     E     SI1   59.8   61.0\n```\n\nAs the example above shows, you can use symbolic column names inside of the\nselection filter function! You can also mix together selection filters and standard\nselections inside of the same `select` or `drop` command.\n\nFor my next trick, I will select the first two columns, the last two columns, and\nthe \"depth\" column using a mixture of selection techniques:\n\n```python\ndiamonds >> select(columns_to(1, inclusive=True), 'depth', columns_from(-2)) >> head(2)\n\n   carat      cut  depth     y     z\n0   0.23    Ideal   61.5  3.98  2.43\n1   0.21  Premium   59.8  3.84  2.31\n```\n\n\n### Subsetting and filtering\n\n#### `row_slice()`\n\nSlices of rows can be selected with the `row_slice()` function. You can pass\nsingle integer indices or a list of indices to select rows as with. This is\ngoing to be the same as using pandas' `.iloc`.\n\n```python\ndiamonds >> row_slice([10,15])\n\n    carat      cut color clarity  depth  table  price     x     y     z\n10   0.30     Good     J     SI1   64.0   55.0    339  4.25  4.28  2.73\n15   0.32  Premium     E      I1   60.9   58.0    345  4.38  4.42  2.68\n```\n\nNote that this can also be used with the `group_by` function, and will operate\nlike a call to `.iloc` on each group. The `group_by` pipe function is\ncovered later, but it essentially works the same as pandas `.groupby` (with a\nfew subtle differences).\n\n```python\ndiamonds >> group_by('cut') >> row_slice(5)\n\n     carat        cut color clarity  depth  table  price     x     y     z\n128   0.91       Fair     H     SI2   64.4   57.0   2763  6.11  6.09  3.93\n20    0.30       Good     I     SI2   63.3   56.0    351  4.26  4.30  2.71\n40    0.33      Ideal     I     SI2   61.2   56.0    403  4.49  4.50  2.75\n26    0.24    Premium     I     VS1   62.5   57.0    355  3.97  3.94  2.47\n21    0.23  Very Good     E     VS2   63.8   55.0    352  3.85  3.92  2.48\n```\n\n\n\n#### `sample()`\n\nThe `sample()` function functions exactly the same as pandas' `.sample()` method\nfor DataFrames. Arguments and keyword arguments will be passed through to the\nDataFrame sample method.\n\n```python\ndiamonds >> sample(frac=0.0001, replace=False)\n\n       carat        cut color clarity  depth  table  price     x     y     z\n19736   1.02      Ideal     E     VS1   62.2   54.0   8303  6.43  6.46  4.01\n37159   0.32    Premium     D     VS2   60.3   60.0    972  4.44  4.42  2.67\n1699    0.72  Very Good     E     VS2   63.8   57.0   3035  5.66  5.69  3.62\n20955   1.71  Very Good     J     VS2   62.6   55.0   9170  7.58  7.65  4.77\n5168    0.91  Very Good     E     SI2   63.0   56.0   3772  6.12  6.16  3.87\n\n\ndiamonds >> sample(n=3, replace=True)\n\n       carat        cut color clarity  depth  table  price     x     y     z\n52892   0.73  Very Good     G     SI1   60.6   59.0   2585  5.83  5.85  3.54\n39454   0.57      Ideal     H     SI2   62.3   56.0   1077  5.31  5.28  3.30\n39751   0.43      Ideal     H    VVS1   62.3   54.0   1094  4.84  4.85  3.02\n```\n\n#### `distinct()`\n\nSelection of unique rows is done with `distinct()`, which similarly passes\narguments and keyword arguments through to the DataFrame's `.drop_duplicates()`\nmethod.\n\n```python\ndiamonds >> distinct(X.color)\n\n    carat        cut color clarity  depth  table  price     x     y     z\n0    0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n3    0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4    0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n7    0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n12   0.22    Premium     F     SI1   60.4   61.0    342  3.88  3.84  2.33\n25   0.23  Very Good     G    VVS2   60.4   58.0    354  3.97  4.01  2.41\n28   0.23  Very Good     D     VS2   60.5   61.0    357  3.96  3.97  2.40\n```\n\n\n#### `mask()`\n\nFiltering rows with logical criteria is done with `mask()`, which accepts\nboolean arrays \"masking out\" False labeled rows and keeping True labeled rows.\nThese are best created with logical statements on symbolic Series objects as\nshown below. Multiple criteria can be supplied as arguments and their intersection\nwill be used as the mask.\n\n```python\ndiamonds >> mask(X.cut == 'Ideal') >> head(4)\n\n    carat    cut color clarity  depth  table  price     x     y     z\n0    0.23  Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n11   0.23  Ideal     J     VS1   62.8   56.0    340  3.93  3.90  2.46\n13   0.31  Ideal     J     SI2   62.2   54.0    344  4.35  4.37  2.71\n16   0.30  Ideal     I     SI2   62.0   54.0    348  4.31  4.34  2.68\n\ndiamonds >> mask(X.cut == 'Ideal', X.color == 'E', X.table < 55, X.price < 500)\n\n       carat    cut color clarity  depth  table  price     x     y     z\n26683   0.33  Ideal     E     SI2   62.2   54.0    427  4.44  4.46  2.77\n32297   0.34  Ideal     E     SI2   62.4   54.0    454  4.49  4.52  2.81\n40928   0.30  Ideal     E     SI1   61.6   54.0    499  4.32  4.35  2.67\n50623   0.30  Ideal     E     SI2   62.1   54.0    401  4.32  4.35  2.69\n50625   0.30  Ideal     E     SI2   62.0   54.0    401  4.33  4.35  2.69\n```\n\nAlternatively, `mask()` can also be called using the alias `filter_by()`:\n\n```python\ndiamonds >> filter_by(X.cut == 'Ideal', X.color == 'E', X.table < 55, X.price < 500)\n\n       carat    cut color clarity  depth  table  price     x     y     z\n26683   0.33  Ideal     E     SI2   62.2   54.0    427  4.44  4.46  2.77\n32297   0.34  Ideal     E     SI2   62.4   54.0    454  4.49  4.52  2.81\n40928   0.30  Ideal     E     SI1   61.6   54.0    499  4.32  4.35  2.67\n50623   0.30  Ideal     E     SI2   62.1   54.0    401  4.32  4.35  2.69\n50625   0.30  Ideal     E     SI2   62.0   54.0    401  4.33  4.35  2.69\n```\n\n### DataFrame transformation\n\n#### `mutate()`\n\nNew variables can be created with the `mutate()` function (named that way to match\n`dplyr`).\n\n```python\ndiamonds >> mutate(x_plus_y=X.x + X.y) >> select(columns_from('x')) >> head(3)\n\n      x     y     z  x_plus_y\n0  3.95  3.98  2.43      7.93\n1  3.89  3.84  2.31      7.73\n2  4.05  4.07  2.31      8.12\n```\n\nMultiple variables can be created in a single call.\n\n```python\ndiamonds >> mutate(x_plus_y=X.x + X.y, y_div_z=(X.y / X.z)) >> select(columns_from('x')) >> head(3)\n\n      x     y     z  x_plus_y   y_div_z\n0  3.95  3.98  2.43      7.93  1.637860\n1  3.89  3.84  2.31      7.73  1.662338\n2  4.05  4.07  2.31      8.12  1.761905\n```\n\n> Note: In Python the new variables created with mutate may not be guaranteed\nto be created in the same order that they are input into the function call, though\nthis may have been changed in Python 3...\n\n\n#### `transmute()`\n\nThe `transmute()` function is a combination of a mutate and a selection of the\ncreated variables.\n\n```python\ndiamonds >> transmute(x_plus_y=X.x + X.y, y_div_z=(X.y / X.z)) >> head(3)\n\n   x_plus_y   y_div_z\n0      7.93  1.637860\n1      7.73  1.662338\n2      8.12  1.761905\n```\n\n\n### Grouping\n\n#### `group_by()` and `ungroup()`\n\nDataFrames are grouped along variables using the `group_by()` function and\nungrouped with the `ungroup()` function. Functions chained after grouping a\nDataFrame are applied by group until returning or ungrouping. Hierarchical/multiindexing\nis automatically removed.\n\n> Note: In the example below, the `lead()` and `lag()` functions are dfply convenience\nwrappers around the pandas `.shift()` Series method.\n\n\n```python\n(diamonds >> group_by(X.cut) >>\n mutate(price_lead=lead(X.price), price_lag=lag(X.price)) >>\n head(2) >> select(X.cut, X.price, X.price_lead, X.price_lag))\n\n          cut  price  price_lead  price_lag\n8        Fair    337      2757.0        NaN\n91       Fair   2757      2759.0      337.0\n2        Good    327       335.0        NaN\n4        Good    335       339.0      327.0\n0       Ideal    326       340.0        NaN\n11      Ideal    340       344.0      326.0\n1     Premium    326       334.0        NaN\n3     Premium    334       342.0      326.0\n5   Very Good    336       336.0        NaN\n6   Very Good    336       337.0      336.0\n```\n\n\n### Reshaping\n\n#### `arrange()`\n\nSorting is done by the `arrange()` function, which wraps around the pandas\n`.sort_values()` DataFrame method. Arguments and keyword arguments are passed\nthrough to that function.\n\n```python\ndiamonds >> arrange(X.table, ascending=False) >> head(5)\n\n       carat   cut color clarity  depth  table  price     x     y     z\n24932   2.01  Fair     F     SI1   58.6   95.0  13387  8.32  8.31  4.87\n50773   0.81  Fair     F     SI2   68.8   79.0   2301  5.26  5.20  3.58\n51342   0.79  Fair     G     SI1   65.3   76.0   2362  5.52  5.13  3.35\n52860   0.50  Fair     E     VS2   79.0   73.0   2579  5.21  5.18  4.09\n49375   0.70  Fair     H     VS1   62.0   73.0   2100  5.65  5.54  3.47\n\n\n(diamonds >> group_by(X.cut) >> arrange(X.price) >>\n head(3) >> ungroup() >> mask(X.carat < 0.23))\n\n    carat      cut color clarity  depth  table  price     x     y     z\n8    0.22     Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n1    0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n12   0.22  Premium     F     SI1   60.4   61.0    342  3.88  3.84  2.33\n```\n\n#### `rename()`\n\nThe `rename()` function will rename columns provided as values to what you set\nas the keys in the keyword arguments. You can indicate columns with symbols or\nwith their labels.\n\n```python\ndiamonds >> rename(CUT=X.cut, COLOR='color') >> head(2)\n\n   carat      CUT COLOR clarity  depth  table  price     x     y     z\n0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n```\n\n#### `gather()`\n\nTransforming between \"wide\" and \"long\" format is a common pattern in data munging.\nThe `gather(key, value, *columns)` function melts the specified columns in your\nDataFrame into two key:value columns.\n\n```python\ndiamonds >> gather('variable', 'value', ['price', 'depth','x','y','z']) >> head(5)\n\n   carat      cut color clarity  table variable  value\n0   0.23    Ideal     E     SI2   55.0    price  326.0\n1   0.21  Premium     E     SI1   61.0    price  326.0\n2   0.23     Good     E     VS1   65.0    price  327.0\n3   0.29  Premium     I     VS2   58.0    price  334.0\n4   0.31     Good     J     SI2   58.0    price  335.0\n```\n\nWithout any columns specified, your entire DataFrame will be transformed into\ntwo key:value pair columns.\n\n```python\ndiamonds >> gather('variable', 'value') >> head(5)\n\n  variable value\n0    carat  0.23\n1    carat  0.21\n2    carat  0.23\n3    carat  0.29\n4    carat  0.31\n```\n\nIf the `add_id` keyword argument is set to true, an id column is added to the\nnew elongated DataFrame that acts as a row id from the original wide DataFrame.\n\n```python\nelongated = diamonds >> gather('variable', 'value', add_id=True)\nelongated >> head(5)\n\n   _ID variable value\n0    0    carat  0.23\n1    1    carat  0.21\n2    2    carat  0.23\n3    3    carat  0.29\n4    4    carat  0.31\n```\n\n#### `spread()`\n\nLikewise, you can transform a \"long\" DataFrame into a \"wide\" format with the\n`spread(key, values)` function. Converting the previously created elongated\nDataFrame for example would be done like so.\n\n```python\nwidened = elongated >> spread(X.variable, X.value)\nwidened >> head(5)\n\n    _ID carat clarity color        cut depth price table     x     y     z\n0     0  0.23     SI2     E      Ideal  61.5   326    55  3.95  3.98  2.43\n1     1  0.21     SI1     E    Premium  59.8   326    61  3.89  3.84  2.31\n2    10   0.3     SI1     J       Good    64   339    55  4.25  4.28  2.73\n3   100  0.75     SI1     D  Very Good  63.2  2760    56   5.8  5.75  3.65\n4  1000  0.75     SI1     D      Ideal  62.3  2898    55  5.83   5.8  3.62\n```\n\nIn this case the `_ID` column comes in handy since it is necessary to not have\nany duplicated identifiers.\n\nIf you have a mixed datatype column in your long-format DataFrame then the\ndefault behavior is for the spread columns to be of type object.\n\n```python\nwidened.dtypes\n\n_ID         int64\ncarat      object\nclarity    object\ncolor      object\ncut        object\ndepth      object\nprice      object\ntable      object\nx          object\ny          object\nz          object\ndtype: object\n```\n\nIf you want to try to convert dtypes when spreading, you can set the `convert`\nkeyword argument in spread to True like so.\n\n```python\nwidened = elongated >> spread(X.variable, X.value, convert=True)\nwidened.dtypes\n\n_ID          int64\ncarat      float64\nclarity     object\ncolor       object\ncut         object\ndepth      float64\nprice        int64\ntable      float64\nx          float64\ny          float64\nz          float64\ndtype: object\n```\n\n#### `separate()`\n\nColumns can be split into multiple columns with the\n`separate(column, into, sep=\"[\\W_]+\", remove=True, convert=False,\nextra='drop', fill='right')` function. `separate()` takes a variety of arguments:\n\n- `column`: the column to split.\n- `into`: the names of the new columns.\n- `sep`: either a regex string or integer positions to split the column on.\n- `remove`: boolean indicating whether to remove the original column.\n- `convert`: boolean indicating whether the new columns should be converted to\nthe appropriate type (same as in `spread` above).\n- `extra`: either `drop`, where split pieces beyond the specified new columns\nare dropped, or `merge`, where the final split piece contains the remainder of\nthe original column.\n- `fill`: either `right`, where `np.nan` values are filled in the right-most\ncolumns for missing pieces, or `left` where `np.nan` values are filled in the\nleft-most columns.\n\n```python\nprint d\n\n         a\n0    1-a-3\n1      1-b\n2  1-c-3-4\n3    9-d-1\n4       10\n\nd >> separate(X.a, ['col1', 'col2'], remove=True, convert=True,\n              extra='drop', fill='right')\n\n   col1 col2\n0     1    a\n1     1    b\n2     1    c\n3     9    d\n4    10  NaN\n\nd >> separate(X.a, ['col1', 'col2'], remove=True, convert=True,\n              extra='drop', fill='left')\n\n   col1 col2\n0   1.0    a\n1   1.0    b\n2   1.0    c\n3   9.0    d\n4   NaN   10\n\nd >> separate(X.a, ['col1', 'col2'], remove=False, convert=True,\n              extra='merge', fill='right')\n\n         a  col1   col2\n0    1-a-3     1    a-3\n1      1-b     1      b\n2  1-c-3-4     1  c-3-4\n3    9-d-1     9    d-1\n4       10    10    NaN\n\nd >> separate(X.a, ['col1', 'col2', 'col3'], sep=[2,4], remove=True, convert=True,\n              extra='merge', fill='right')\n\n  col1 col2 col3\n0   1-   a-    3\n1   1-    b  NaN\n2   1-   c-  3-4\n3   9-   d-    1\n4   10  NaN  NaN\n```\n\n#### `unite()`\n\nThe `unite(colname, *args, sep='_', remove=True, na_action='maintain')` function\ndoes the inverse of `separate()`, joining columns together by a separator. Any\ncolumns that are not strings will be converted to strings. The arguments for\n`unite()` are:\n\n- `colname`: the name of the new joined column.\n- `*args`: list of columns to be joined, which can be strings, symbolic, or\ninteger positions.\n- `sep`: the string separator to join the columns with.\n- `remove`: boolean indicating whether or not to remove the original columns.\n- `na_action`: can be one of `\"maintain\"` (the default), `\"ignore\"`, or\n`\"as_string\"`. The default `\"maintain\"` will make the new column row a `NaN` value\nif any of the original column cells at that row contained `NaN`. `\"ignore\"` will\ntreat any `NaN` value as an empty string during joining. `\"as_string\"` will convert\nany `NaN` value to the string `\"nan\"` prior to joining.\n\n```python\n\nprint d\n\na  b      c\n0  1  a   True\n1  2  b  False\n2  3  c    NaN\n\nd >> unite('united', X.a, 'b', 2, remove=False, na_action='maintain')\n\n   a  b      c     united\n0  1  a   True   1_a_True\n1  2  b  False  2_b_False\n2  3  c    NaN        NaN\n\nd >> unite('united', ['a','b','c'], remove=True, na_action='ignore', sep='*')\n\n      united\n0   1*a*True\n1  2*b*False\n2        3*c\n\nd >> unite('united', d.columns, remove=True, na_action='as_string')\n\n      united\n0   1_a_True\n1  2_b_False\n2    3_c_nan\n```\n\n\n### Joining\n\nCurrently implemented joins are:\n\n1. `inner_join(other, by='column')`\n- `outer_join(other, by='column')` (which works the same as `full_join()`)\n- `right_join(other, by='column')`\n- `left_join(other, by='column')`\n- `semi_join(other, by='column')`\n- `anti_join(other, by='column')`\n\nThe functionality of the join functions are outlined with the toy example\nDataFrames below.\n\n```python\na = pd.DataFrame({\n        'x1':['A','B','C'],\n        'x2':[1,2,3]\n    })\nb = pd.DataFrame({\n    'x1':['A','B','D'],\n    'x3':[True,False,True]\n})\n```\n\n#### `inner_join()`\n\n`inner_join()` joins on values present in both DataFrames' `by` columns.\n\n```python\na >> inner_join(b, by='x1')\n\n  x1  x2     x3\n0  A   1   True\n1  B   2  False\n```\n\n#### `outer_join()` or `full_join()`\n\n`outer_join` merges DataFrame's together on values present in either frame's\n`by` columns.\n\n```python\na >> outer_join(b, by='x1')\n\n  x1   x2     x3\n0  A  1.0   True\n1  B  2.0  False\n2  C  3.0    NaN\n3  D  NaN   True\n```\n\n#### `left_join()`\n\n`left_join` merges on the values present in the left DataFrame's `by` columns.\n\n```python\na >> left_join(b, by='x1')\n\n  x1  x2     x3\n0  A   1   True\n1  B   2  False\n2  C   3    NaN\n```\n\n#### `right_join()`\n\n`right_join` merges on the values present in the right DataFrame's `by` columns.\n\n```python\na >> right_join(b, by='x1')\n\n  x1   x2     x3\n0  A  1.0   True\n1  B  2.0  False\n2  D  NaN   True\n```\n\n#### `semi_join()`\n\n`semi_join()` returns all of the rows in the left DataFrame that have a match\nin the right DataFrame in the `by` columns.\n\n```python\na >> semi_join(b, by='x1')\n\n  x1  x2\n0  A   1\n1  B   2\n```\n\n#### `anti_join()`\n\n`anti_join()` returns all of the rows in the left DataFrame that do not have a\nmatch in the right DataFrame within the `by` columns.\n\n```python\na >> anti_join(b, by='x1')\n\n  x1  x2\n2  C   3\n```\n\n\n### Set operations\n\nThe set operation functions filter a DataFrame based on row comparisons with\nanother DataFrame.\n\nEach of the set operation functions `union()`, `intersect()`, and `set_diff()`\ntake the same arguments:\n\n- `other`: the DataFrame to compare to\n- `index`: a boolean (default `False`) indicating whether to consider the pandas\nindex during comparison.\n- `keep`: string (default `\"first\"`) to be passed through to `.drop_duplicates()`\ncontrolling how to handle duplicate rows.\n\nWith set operations columns are expected to be in the same order in both\nDataFrames.\n\nThe function examples use the following two toy DataFrames.\n\n```python\na = pd.DataFrame({\n        'x1':['A','B','C'],\n        'x2':[1,2,3]\n    })\nc = pd.DataFrame({\n      'x1':['B','C','D'],\n      'x2':[2,3,4]\n})\n```\n\n#### `union()`\n\nThe `union()` function returns rows that appear in either DataFrame.\n\n```python\na >> union(c)\n\n  x1  x2\n0  A   1\n1  B   2\n2  C   3\n2  D   4\n```\n\n#### `intersect()`\n\n`intersect()` returns rows that appear in both DataFrames.\n\n```python\na >> intersect(c)\n\n  x1  x2\n0  B   2\n1  C   3\n```\n\n\n#### `set_diff()`\n\n`set_diff()` returns the rows in the left DataFrame that do not appear in the\nright DataFrame.\n\n```python\na >> set_diff(c)\n\n  x1  x2\n0  A   1\n```\n\n\n### Binding\n\n`dfply` comes with convenience wrappers around `pandas.concat()` for joining\nDataFrames by rows or by columns.\n\nThe toy DataFrames below (`a` and `b`) are the same as the ones used to display\nthe join functions above.\n\n#### `bind_rows()`\n\nThe `bind_rows(other, join='outer', ignore_index=False)` function is an exact\ncall to `pandas.concat([df, other], join=join, ignore_index=ignore_index, axis=0)`,\njoining two DataFrames \"vertically\".\n\n```python\na >> bind_rows(b, join='inner')\n\nx1\n0  A\n1  B\n2  C\n0  A\n1  B\n2  D\n\na >> bind_rows(b, join='outer')\n\n  x1   x2     x3\n0  A  1.0    NaN\n1  B  2.0    NaN\n2  C  3.0    NaN\n0  A  NaN   True\n1  B  NaN  False\n2  D  NaN   True\n```\n\nNote that `bind_rows()` does not reset the index for you!\n\n#### `bind_cols()`\n\nThe `bind_cols(other, join='outer', ignore_index=False)` is likewise just a\ncall to `pandas.concat([df, other], join=join, ignore_index=ignore_index, axis=1)`,\njoining DataFrames \"horizontally\".\n\n```python\na >> bind_cols(b)\n\n  x1  x2 x1     x3\n0  A   1  A   True\n1  B   2  B  False\n2  C   3  D   True\n```\n\nNote that you may well end up with duplicate column labels after binding columns\nas can be seen above.\n\n\n### Summarization\n\nThere are two summarization functions in `dfply` that match `dplr`: `summarize` and\n`summarize_each` (though these functions use the 'z' spelling rather than 's').\n\n#### `summarize()`\n\n`summarize(**kwargs)` takes an arbitrary number of keyword arguments that will\nreturn new columns labeled with the keys that are summary functions of columns\nin the original DataFrame.\n\n```python\ndiamonds >> summarize(price_mean=X.price.mean(), price_std=X.price.std())\n\n    price_mean    price_std\n0  3932.799722  3989.439738\n```\n\n`summarize()` can of course be used with groupings as well.\n\n```python\ndiamonds >> group_by('cut') >> summarize(price_mean=X.price.mean(), price_std=X.price.std())\n\n         cut   price_mean    price_std\n0       Fair  4358.757764  3560.386612\n1       Good  3928.864452  3681.589584\n2      Ideal  3457.541970  3808.401172\n3    Premium  4584.257704  4349.204961\n4  Very Good  3981.759891  3935.862161\n```\n\n#### `summarize_each()`\n\nThe `summarize_each(function_list, *columns)` is a more general summarization\nfunction. It takes a list of summary functions to apply as its first argument and\nthen a list of columns to apply the summary functions to. Columns can be specified\nwith either symbolic, string label, or integer position like in the selection\nfunctions for convenience.\n\n```python\ndiamonds >> summarize_each([np.mean, np.var], X.price, 'depth')\n\n    price_mean     price_var  depth_mean  depth_var\n0  3932.799722  1.591533e+07   61.749405   2.052366\n```\n\n`summarize_each()` works with groupings as well.\n\n```python\ndiamonds >> group_by(X.cut) >> summarize_each([np.mean, np.var], X.price, 4)\n\n         cut   price_mean     price_var  depth_mean  depth_var\n0       Fair  4358.757764  1.266848e+07   64.041677  13.266319\n1       Good  3928.864452  1.355134e+07   62.365879   4.705224\n2      Ideal  3457.541970  1.450325e+07   61.709401   0.516274\n3    Premium  4584.257704  1.891421e+07   61.264673   1.342755\n4  Very Good  3981.759891  1.548973e+07   61.818275   1.900466\n```\n\n\n## Embedded column functions\n\n**UNDER CONSTRUCTION: documentation not complete.**\n\nLike `dplyr`, the `dfply` package provides functions to perform various operations\non pandas Series. These are typically window functions and summarization\nfunctions, and wrap symbolic arguments in function calls.\n\n\n### Window functions\n\nWindow functions perform operations on vectors of values that return a vector\nof the same length.\n\n#### `lead()` and `lag()`\n\nThe `lead(series, n)` function pushes values in a vector upward, adding `NaN`\nvalues in the end positions. Likewise, the `lag(series, n)` function\npushes values downward, inserting `NaN` values in the initial positions. Both\nare calls to pandas `Series.shift()` function under the hood.\n\n```python\n(diamonds >> mutate(price_lead=lead(X.price, 2), price_lag=lag(X.price, 2)) >>\n            select(X.price, -2, -1) >>\n            head(6))\n\n    price  price_lag  price_lead\n 0    326        NaN       327.0\n 1    326        NaN       334.0\n 2    327      326.0       335.0\n 3    334      326.0       336.0\n 4    335      327.0       336.0\n 5    336      334.0       337.0\n```\n\n#### `between()`\n\nThe `between(series, a, b, inclusive=False)` function checks to see if values are\nbetween two given bookend values.\n\n```python\ndiamonds >> select(X.price) >> mutate(price_btwn=between(X.price, 330, 340)) >> head(6)\n\n   price price_btwn\n0    326      False\n1    326      False\n2    327      False\n3    334       True\n4    335       True\n5    336       True\n```\n\n#### `dense_rank()`\n\nThe `dense_rank(series, ascending=True)` function is a wrapper around the `scipy`\nfunction for calculating dense rank.\n\n```python\ndiamonds >> select(X.price) >> mutate(price_drank=dense_rank(X.price)) >> head(6)\n\n   price  price_drank\n0    326          1.0\n1    326          1.0\n2    327          2.0\n3    334          3.0\n4    335          4.0\n5    336          5.0\n```\n\n#### `min_rank()`\n\nLikewise, `min_rank(series, ascending=True)` is a wrapper around the `scipy` ranking\nfunction with min rank specified.\n\n```python\ndiamonds >> select(X.price) >> mutate(price_mrank=min_rank(X.price)) >> head(6)\n\nprice  price_mrank\n0    326          1.0\n1    326          1.0\n2    327          3.0\n3    334          4.0\n4    335          5.0\n5    336          6.0\n```\n\n#### `cumsum()`\n\nThe `cumsum(series)` function calculates a cumulative sum of a column.\n\n```python\ndiamonds >> select(X.price) >> mutate(price_cumsum=cumsum(X.price)) >> head(6)\n\n   price  price_cumsum\n0    326           326\n1    326           652\n2    327           979\n3    334          1313\n4    335          1648\n5    336          1984\n```\n\n#### `cummean()`\n\n`cummean(series)`\n\n```python\ndiamonds >> select(X.price) >> mutate(price_cummean=cummean(X.price)) >> head(6)\n\n   price  price_cummean\n0    326     326.000000\n1    326     326.000000\n2    327     326.333333\n3    334     328.250000\n4    335     329.600000\n5    336     330.666667\n```\n\n#### `cummax()`\n\n`cummax(series)`\n\n```python\ndiamonds >> select(X.price) >> mutate(price_cummax=cummax(X.price)) >> head(6)\n\n   price  price_cummax\n0    326         326.0\n1    326         326.0\n2    327         327.0\n3    334         334.0\n4    335         335.0\n5    336         336.0\n```\n\n#### `cummin()`\n\n`cummin(series)`\n\n```python\ndiamonds >> select(X.price) >> mutate(price_cummin=cummin(X.price)) >> head(6)\n\n   price  price_cummin\n0    326         326.0\n1    326         326.0\n2    327         326.0\n3    334         326.0\n4    335         326.0\n5    336         326.0\n```\n\n#### `cumprod()`\n\n`cumprod(series)`\n\n```python\ndiamonds >> select(X.price) >> mutate(price_cumprod=cumprod(X.price)) >> head(6)\n\n   price     price_cumprod\n0    326               326\n1    326            106276\n2    327          34752252\n3    334       11607252168\n4    335     3888429476280\n5    336  1306512304030080\n```\n\n\n### Summary functions\n\n#### `mean()`\n\n`mean(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_mean=mean(X.price))\n\n         cut   price_mean\n0       Fair  4358.757764\n1       Good  3928.864452\n2      Ideal  3457.541970\n3    Premium  4584.257704\n4  Very Good  3981.759891\n```\n\n#### `first()`\n\n`first(series, order_by=None)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_first=first(X.price))\n\n         cut  price_first\n0       Fair          337\n1       Good          327\n2      Ideal          326\n3    Premium          326\n4  Very Good          336\n```\n\n#### `last()`\n\n`last(series, order_by=None)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_last=last(X.price))\n\n         cut  price_last\n0       Fair        2747\n1       Good        2757\n2      Ideal        2757\n3    Premium        2757\n4  Very Good        2757\n```\n\n#### `nth()`\n\n`nth(series, n, order_by=None)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_penultimate=nth(X.price, -2))\n\n         cut  price_penultimate\n0       Fair               2745\n1       Good               2756\n2      Ideal               2757\n3    Premium               2757\n4  Very Good               2757\n```\n\n#### `n()`\n\n`n(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_n=n(X.price))\n\n         cut  price_n\n0       Fair     1610\n1       Good     4906\n2      Ideal    21551\n3    Premium    13791\n4  Very Good    12082\n```\n\n#### `n_distinct()`\n\n`n_distinct(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_ndistinct=n_distinct(X.price))\n\n         cut  price_ndistinct\n0       Fair             1267\n1       Good             3086\n2      Ideal             7281\n3    Premium             6014\n4  Very Good             5840\n```\n\n#### `IQR()`\n\n`IQR(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_iqr=IQR(X.price))\n\n         cut  price_iqr\n0       Fair    3155.25\n1       Good    3883.00\n2      Ideal    3800.50\n3    Premium    5250.00\n4  Very Good    4460.75\n```\n\n#### `colmin()`\n\n`colmin(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_min=colmin(X.price))\n\n         cut  price_min\n0       Fair        337\n1       Good        327\n2      Ideal        326\n3    Premium        326\n4  Very Good        336\n```\n\n#### `colmax()`\n\n`colmax(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_max=colmax(X.price))\n\n         cut  price_max\n0       Fair      18574\n1       Good      18788\n2      Ideal      18806\n3    Premium      18823\n4  Very Good      18818\n```\n\n#### `median()`\n\n`median(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_median=median(X.price))\n\n         cut  price_median\n0       Fair        3282.0\n1       Good        3050.5\n2      Ideal        1810.0\n3    Premium        3185.0\n4  Very Good        2648.0\n```\n\n#### `var()`\n\n`var(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_var=var(X.price))\n\n         cut     price_var\n0       Fair  1.267635e+07\n1       Good  1.355410e+07\n2      Ideal  1.450392e+07\n3    Premium  1.891558e+07\n4  Very Good  1.549101e+07\n```\n\n#### `sd()`\n\n`sd(series)`\n\n```python\ndiamonds >> groupby(X.cut) >> summarize(price_sd=sd(X.price))\n\n         cut     price_sd\n0       Fair  3560.386612\n1       Good  3681.589584\n2      Ideal  3808.401172\n3    Premium  4349.204961\n4  Very Good  3935.862161\n```\n\n\n## Extending `dfply` with custom functions\n\nThere are a lot of built-in functions, but you are almost certainly going to\nreach a point where you want to use some of your own functions with the `dfply`\npiping syntax. Luckily, `dfply` comes with two handy decorators to make this\nas easy as possible.\n\n> **For a more detailed walkthrough of these two cases, see the [\nbasics-extending-functionality.ipynb](./examples/basics-extending-functionality.ipynb)\njupyter notebook in the examples folder.**\n\n\n### Case 1: A custom \"pipe\" function with `@dfpipe`\n\nYou might want to make a custom function that can be a piece of the pipe chain.\nFor example, say we wanted to write a `dfply` wrapper around a simplified\nversion of `pd.crosstab`. For the most part, you'll only need to do two things\nto make this work:\n1. Make sure that your function's first argument will be the dataframe passed in\nimplicitly by the pipe.\n2. Decorate the function with the `@dfpipe` decorator.\n\nHere is an example of the `dfply`-enabled crosstab function:\n\n```python\n@dfpipe\ndef crosstab(df, index, columns):\n    return pd.crosstab(index, columns)\n```\n\nNormally you could use `pd.crosstab` like so:\n\n```python\npd.crosstab(diamonds.cut, diamonds.color)\n\ncolor         D     E     F     G     H     I    J\ncut                                               \nFair        163   224   312   314   303   175  119\nGood        662   933   909   871   702   522  307\nIdeal      2834  3903  3826  4884  3115  2093  896\nPremium    1603  2337  2331  2924  2360  1428  808\nVery Good  1513  2400  2164  2299  1824  1204  678\n```\n\nThe same result can be achieved now with the custom function in pipe syntax:\n\n```python\ndiamonds >> crosstab(X.cut, X.color)\n\ncolor         D     E     F     G     H     I    J\ncut                                               \nFair        163   224   312   314   303   175  119\nGood        662   933   909   871   702   522  307\nIdeal      2834  3903  3826  4884  3115  2093  896\nPremium    1603  2337  2331  2924  2360  1428  808\nVery Good  1513  2400  2164  2299  1824  1204  678\n```\n\n\n### Case 2: A function that works with symbolic objects using `@make_symbolic`\n\nMany tasks are simpler and do not require the capacity to work as a pipe function. The dfply window functions are the common examples of this: functions that take a Series (or symbolic Series) and return a modified version.\n\n\nLet's say we had a dataframe with dates represented by strings that we wanted to convert to pandas datetime objects using the pd.to_datetime function. Below is a tiny example dataframe with this issue.\n\n```python\nsales = pd.DataFrame(dict(date=['7/10/17','7/11/17','7/12/17','7/13/17','7/14/17'],\n                          sales=[1220, 1592, 908, 1102, 1395]))\n\nsales\n\n      date  sales\n0  7/10/17   1220\n1  7/11/17   1592\n2  7/12/17    908\n3  7/13/17   1102\n4  7/14/17   1395\n```\n\nUsing the `pd.to_datetime` function inside of a call to mutate will unfortunately\nbreak:\n\n```python\nsales >> mutate(pd_date=pd.to_datetime(X.date, infer_datetime_format=True))\n\n...\n\nTypeError: __index__ returned non-int (type Intention)\n```\n\n`dfply` functions are special in that they \"know\" to delay their evaluation until\nthe data is at that point in the chain. `pd.to_datetime` is not such a function,\nand will immediately try to evaluate `X.date`. With a symbolic `Intention` argument\npassed in (`X` is an `Intention` object), the function will fail.\n\n\nInstead, we will need to make a wrapper around `pd.to_datetime`\nthat can handle these symbolic arguments and delay evaluation until the right time.\n\nIt's quite simple: all you need to do is decorate a function with the @make_symbolic decorator:\n\n```python\n@make_symbolic\ndef to_datetime(series, infer_datetime_format=True):\n    return pd.to_datetime(series, infer_datetime_format=infer_datetime_format)\n```\n\nNow the function can be used with symbolic arguments:\n\n```python\nsales >> mutate(pd_date=to_datetime(X.date))\n\n      date  sales    pd_date\n0  7/10/17   1220 2017-07-10\n1  7/11/17   1592 2017-07-11\n2  7/12/17    908 2017-07-12\n3  7/13/17   1102 2017-07-13\n4  7/14/17   1395 2017-07-14\n```\n\n#### Without symbolic arguments, `@make_symbolic` functions work like normal functions!\n\nA particularly nice thing about functions decorated with `@make_symbolic` is that\nthey will operate normally if passed arguments that are not `Intention` symbolic\nobjects.\n\nFor example, you can pass in the series itself and it will return the new\nseries of converted dates:\n\n```python\nto_datetime(sales.date)\n\n0   2017-07-10\n1   2017-07-11\n2   2017-07-12\n3   2017-07-13\n4   2017-07-14\nName: date, dtype: datetime64[ns]\n```\n\n\n## Advanced: understanding base `dfply` decorators\n\nUnder the hood, `dfply` functions work using a collection of different decorators and\nspecial classes. Below the most important ones are detailed. Understanding these\nare important if you are planning on making big additions or changes to the code.\n\n\n### The `Intention` class\n\nPython is not a lazily-evaluated language. Typically, something like this\nwould not work:\n\n```python\ndiamonds >> select(X.carat) >> head(2)\n```\n\nThe `X` is supposed to represent the current state of the data through the\npiping operator chain, and `X.carat` indicates \"select the carat column from\nthe current data at this point in the chain\". But Python will try to evaluate\nwhat `X` is, then what `X.carat` is, then what `select(X.carat)` is, all before\nthe diamonds dataset ever gets evaluated.\n\nThe solution to this is to delay the evaluation until the appropriate time. I will\nnot get into the granular details here (but feel free to check it out for yourself\nin `base.py`). The gist is that things to be delayed are represented by a\nspecial `Intention` class that \"waits\" until it is time to evaluate the stored\ncommands with a given dataframe. This is the core of how `dplyr` data manipulation\nsyntax is made possible in `dfply`.\n\n(Thanks to the creators of the `dplython` and `pandas-ply` for trailblazing a lot\nof this before I made this package.)\n\n\n### `@pipe`\n\nThe primary decorator that enables chaining functions with the `>>` operator\nis `@pipe`. For functions to work with the piping syntax they must be decorated\nwith `@pipe`.\n\nAny function decorated with `@pipe` implicitly receives a single first argument\nexpected to be a pandas DataFrame. This is the DataFrame being passed through\nthe pipe. For example, `mutate` and `select` have function specifications\n`mutate(df, **kwargs)` and `select(df, *args, **kwargs)`, but when used\ndo not require the user to insert the DataFrame as an argument.\n\n```python\n# the DataFrame is implicitly passed as the first argument\ndiamonds >> mutate(new_var=X.price + X.depth) >> select(X.new_var)\n```\n\nIf you create a new function decorated by `@pipe`, the function definition\nshould contain an initial argument that represents the DataFrame being passed\nthrough the piping operations.\n\n```python\n@pipe\ndef myfunc(df, *args, **kwargs):\n  # code\n```\n\n### `@group_delegation`\n\nIn order to delegate a function across specified groupings (assigned by the\n`group_by()` function), decorate the function with the `@group_delegation`\ndecorator. This decorator will query the DataFrame for assigned groupings and\napply the function to those groups individually.\n\nGroupings are assigned by `dfply` as an attribute `._grouped_by` to the DataFrame\nproceeding through the piped functions. `@group_delegation` checks for the\nattribute and applies the function by group if groups exist. Any hierarchical\nindexing is removed by the decorator as well.\n\nDecoration by `@group_delegation` should come after (internal) to the `@pipe`\ndecorator to function as intended.\n\n```python\n@pipe\n@group_delegation\ndef myfunc(df, *args, **kwargs):\n  # code\n```\n\n### `@symbolic_evaluation`\n\nEvaluation of any `Intention`-class symbolic object (such as `X`) is\nhandled by the `@symbolic_evaluation` function. For example, when calling\n`mutate(new_price = X.price * 2.5)` the `X.price` symbolic representation of\nthe price column in the DataFrame will be evaluated to the actual Series\nby this decorator.\n\nThe `@symbolic_evaluation` decorator can have functionality modified by\noptional keyword arguments:\n\n\n#### Controlling `@symbolic_evaluation` with the `eval_symbols` argument\n\n```python\n@symbolic_evaluation(eval_symbols=False)\ndef my_function(df, arg1, arg2):\n    ...\n```\n\nIf the `eval_symbols` argument is `True`, all symbolics will be evaluated\nwith the passed-in dataframe. If `False` or `None`, there will be no attempt\nto evaluate symbolics.\n\nA list can also be passed in. The list can contain a mix of positional integers\nand string keywords, which reference positional arguments and keyworded arguments\nrespectively. This targets which arguments or keyword arguments to try and\nevaluate specifically:\n\n\n```python\n# This indicates that arg1, arg2, and kw1 should be targeted for symbolic\n# evaluation, but not the other arguments.\n# Note that positional indexes reference arguments AFTER the passed-in dataframe.\n# For example, 0 refers to arg1, not df.\n@symbolic_evaluation(eval_symbols=[0,1,'kw1'])\ndef my_function(df, arg1, arg2, arg3, kw1=True, kw2=False):\n    ...\n```\n\nIn reality, you are unlikely to need this behavior unless you really want to\nprevent `dfply` from trying to evaluate symbolic arguments. Remember that if\nan argument is not symbolic it will be evaluated as normal, so there shouldn't\nbe much harm leaving it at default other than a little bit of computational overhead.\n\n\n### `@dfpipe`\n\nMost new or custom functions for dfply will be decorated with the pattern:\n\n```python\n@pipe\n@group_delegation\n@symbolic_evaluation\ndef myfunc(df, *args, **kwargs):\n  # code\n```\n\nBecause of this, the decorator `@dfpipe` is defined as exactly this combination\nof decorators for your convenience. The above decoration pattern for the function\ncan be simply written as:\n\n```python\n@dfpipe\ndef myfunc(df, *args, **kwargs):\n  # code\n```\n\nThis allows you to easily create new functions that can be chained together\nwith pipes, respect grouping, and evaluate symbolic DataFrames and Series\ncorrectly.\n\n\n### `@make_symbolic`\n\nSometimes, like in the window and summary functions that operate on series,\nit is necessary to defer the evaluation of a function. For example, in the\ncode below:\n\n```python\ndiamonds >> summarize(price_third=nth(X.price, 3))\n```\n\nThe `nth()` function would typically be evaluated before `summarize()` and the\nsymbolic argument would not be evaluated at the right time.\n\nThe `@make_symbolic` decorator can be placed above functions to convert them\ninto symbolic functions that will wait to evaluate. Again, this is used\nprimarily for functions that are embedded inside the function call within\nthe piping syntax.\n\nThe `nth()` code, for example, is below:\n\n\n```python\n@make_symbolic\ndef nth(series, n, order_by=None):\n    if order_by is not None:\n        series = order_series_by(series, order_by)\n    try:\n        return series.iloc[n]\n    except:\n        return np.nan\n```\n\nFunctions you write that you want to be able to embed as an argument\ncan use the `@make_symbolic` to wait until they have access to the DataFrame\nto evaluate.\n\n\n\n## Contributing\n\nBy all means please feel free to comment or contribute to the package. The more\npeople adding code the better. If you submit an issue, pull request, or ask for\nsomething to be added I will do my best to respond promptly.\n\nThe TODO list (now located in the \"Projects\" section of the repo) has an\nongoing list of things that still need to be resolved and features to be added.\n\nIf you submit a pull request with features or bugfixes, please target the\n\"develop\" branch rather than the \"master\" branch.\n"
        ],
        "test_patch": "",
        "patch_preview": "From fcda703a0443c353ad2f5cc8a429cbca0c70fdd1 Mon Sep 17 00:00:00 2001\nFrom: janfreyberg <jan.freyberg@gmail.com>\nDate: Thu, 14 Dec 2017 09:52:54 +0000\nSubject: [PATCH 1/2] Add \"pull\", a relatively new dplyr verb\n\nPull retrieves a column. With no arguments it retrieves the right-most\ncolumn, otherwise columns can be specified with either integer or\nstrings.\n---\n dfply/subset.py | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/dfply/subset.py b/dfply/subset.py\nindex 5668f65..8f01713 1006"
      },
      "patch": {
        "length": 2269,
        "files_changed": 2,
        "lines_added": 28,
        "lines_deleted": 0,
        "net_change": 28,
        "changed_files": [
          {
            "file": "dfply/subset.py",
            "added": 6,
            "deleted": 0
          },
          {
            "file": "README.md",
            "added": 22,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 40,
        "total_lines": 65920,
        "total_bytes": 2943206,
        "python_files": 27,
        "python_lines": 3776,
        "file_extensions": {
          ".md": 2,
          ".txt": 6,
          ".in": 1,
          ".py": 27,
          ".csv": 1,
          ".ipynb": 2,
          "": 1
        },
        "largest_files": [
          {
            "path": "dfply/data/diamonds.csv",
            "size": 2571905,
            "lines": 53941,
            "extension": ".csv"
          },
          {
            "path": "examples/dfply-example-gallery.ipynb",
            "size": 130595,
            "lines": 4693,
            "extension": ".ipynb"
          },
          {
            "path": "README.md",
            "size": 53232,
            "lines": 1784,
            "extension": ".md"
          },
          {
            "path": "examples/basics-extending-functionality.ipynb",
            "size": 25270,
            "lines": 880,
            "extension": ".ipynb"
          },
          {
            "path": "LICENSE.md",
            "size": 35141,
            "lines": 674,
            "extension": ".md"
          },
          {
            "path": "test/test_summary_functions.py",
            "size": 12845,
            "lines": 343,
            "extension": ".py"
          },
          {
            "path": "dfply/base.py",
            "size": 10936,
            "lines": 334,
            "extension": ".py"
          },
          {
            "path": "dfply/reshape.py",
            "size": 12150,
            "lines": 330,
            "extension": ".py"
          },
          {
            "path": "dfply/join.py",
            "size": 9540,
            "lines": 299,
            "extension": ".py"
          },
          {
            "path": "test/test_select.py",
            "size": 9470,
            "lines": 264,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 40,
        "files_changed_count": 2,
        "files_changed_ratio": 0.05,
        "total_lines_in_repo": 65920,
        "lines_added": 28,
        "lines_deleted": 0,
        "net_lines_changed": 28,
        "lines_changed_ratio": 0.00042475728155339805,
        "pr_body_length": 103,
        "commit_message_length": 88,
        "python_file_count": 27,
        "python_line_count": 3776
      }
    },
    {
      "tar_file_name": "kronenthaler#mod-pbxproj#pull#250",
      "repo_name": "kronenthaler#mod-pbxproj#pull#250",
      "success": true,
      "error": null,
      "commit": {
        "sha": "440ce1ac11af74c022b2e56d018947f4c5de36d1",
        "message": "chore: release 2.6.0",
        "author": {
          "name": "kronenthaler",
          "email": "kronenthaler@gmail.com",
          "date": "2019-09-29T11:34:08Z"
        },
        "html_url": "https://github.com/kronenthaler/mod-pbxproj/commit/440ce1ac11af74c022b2e56d018947f4c5de36d1",
        "api_url": "https://api.github.com/repos/kronenthaler/mod-pbxproj/commits/440ce1ac11af74c022b2e56d018947f4c5de36d1"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kronenthaler#mod-pbxproj#pull#250",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/kronenthaler#mod-pbxproj#pull#250.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kronenthaler#mod-pbxproj#pull#250/source_code"
      },
      "pr": {
        "number": 250,
        "title": "Update: add .entitlements file type",
        "body": "",
        "state": "closed",
        "created_at": "2019-10-22T07:50:50Z",
        "updated_at": "2019-10-22T11:14:37Z",
        "merged_at": "2019-10-22T11:14:37Z",
        "html_url": "https://github.com/kronenthaler/mod-pbxproj/pull/250",
        "user": "sunsetroads",
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "kronenthaler_mod-pbxproj-250",
        "repo": "/kronenthaler/mod-pbxproj",
        "base_commit": "440ce1ac11af74c022b2e56d018947f4c5de36d1",
        "problem_statement": {},
        "edit_files": [
          "pbxproj/pbxextensions/ProjectFiles.py"
        ],
        "oracle_files": [
          "from pbxproj.pbxsections import *\nfrom pbxproj import PBXList\n\n\nclass TreeType:\n    ABSOLUTE = u'<absolute>'\n    GROUP = u'<group>'\n    BUILT_PRODUCTS_DIR = u'BUILT_PRODUCTS_DIR'\n    DEVELOPER_DIR = u'DEVELOPER_DIR'\n    SDKROOT = u'SDKROOT'\n    SOURCE_ROOT = u'SOURCE_ROOT'\n\n    @classmethod\n    def options(cls):\n        return [TreeType.SOURCE_ROOT, TreeType.SDKROOT, TreeType.GROUP, TreeType.ABSOLUTE,\n                TreeType.DEVELOPER_DIR, TreeType.BUILT_PRODUCTS_DIR]\n\n\nclass HeaderScope:\n    PUBLIC = u'Public'\n    PRIVATE = u'Private'\n    PROJECT = u''\n\n\nclass FileOptions:\n    \"\"\"\n    Wrapper class for all file parameters required at the moment of adding a file to the project.\n    \"\"\"\n    def __init__(self, create_build_files=True, weak=False, ignore_unknown_type=False, embed_framework=True,\n                 code_sign_on_copy=True, header_scope=HeaderScope.PROJECT):\n        \"\"\"\n        Creates an object specifying options to be considered during the file creation into the project.\n\n        :param create_build_files: Creates any necessary PBXBuildFile section when adding the file\n        :param weak: When adding a framework set it as a weak reference\n        :param ignore_unknown_type: Stop insertion if the file type is unknown (Default is false)\n        :param embed_framework: When adding a framework sets the embed section\n        :param code_sign_on_copy: When embedding a framework, sets the code sign attribute\n        :param header_scope: When adding a header file, adds the header as HeaderScope.PROJECT (default),\n            HeaderScope.PRIVATE or HeaderScope.PUBLIC.\n        \"\"\"\n        self.create_build_files = create_build_files\n        self.weak = weak\n        self.ignore_unknown_type = ignore_unknown_type\n        self.embed_framework = embed_framework\n        self.code_sign_on_copy = code_sign_on_copy\n        self.header_scope = header_scope\n\n    def get_attributes(self, file_ref, build_phase):\n        if file_ref.get_file_type() != u'wrapper.framework' and file_ref.get_file_type() != u'sourcecode.c.h':\n            return None\n\n        attributes = None\n        if build_phase.isa == u'PBXFrameworksBuildPhase':\n            attributes = [u'Weak'] if self.weak else None\n\n        if build_phase.isa == u'PBXCopyFilesBuildPhase' and \\\n                file_ref.sourceTree != TreeType.SDKROOT and \\\n                self.code_sign_on_copy:\n            if attributes is None:\n                attributes = []\n            attributes += [u'CodeSignOnCopy', u'RemoveHeadersOnCopy']\n\n        if build_phase.isa == u'PBXHeadersBuildPhase':\n            attributes = [self.header_scope] if self.header_scope != HeaderScope.PROJECT else None\n\n        return attributes\n\n\nclass ProjectFiles:\n    _FILE_TYPES = {\n        u'': (u'text', u'PBXResourcesBuildPhase'),\n        u'.a': (u'archive.ar', u'PBXFrameworksBuildPhase'),\n        u'.app': (u'wrapper.application', None),\n        u'.s': (u'sourcecode.asm', u'PBXSourcesBuildPhase'),\n        u'.c': (u'sourcecode.c.c', u'PBXSourcesBuildPhase'),\n        u'.cpp': (u'sourcecode.cpp.cpp', u'PBXSourcesBuildPhase'),\n        u'.framework': (u'wrapper.framework', u'PBXFrameworksBuildPhase'),\n        u'.h': (u'sourcecode.c.h', u'PBXHeadersBuildPhase'),\n        u'.hpp': (u'sourcecode.c.h', u'PBXHeadersBuildPhase'),\n        u'.pch': (u'sourcecode.c.h', u'PBXHeadersBuildPhase'),\n        u'.d': (u'sourcecode.dtrace', u'PBXSourcesBuildPhase'),\n        u'.def': (u'text', u'PBXResourcesBuildPhase'),\n        u'.swift': (u'sourcecode.swift', u'PBXSourcesBuildPhase'),\n        u'.icns': (u'image.icns', u'PBXResourcesBuildPhase'),\n        u'.m': (u'sourcecode.c.objc', u'PBXSourcesBuildPhase'),\n        u'.j': (u'sourcecode.c.objc', u'PBXSourcesBuildPhase'),\n        u'.mm': (u'sourcecode.cpp.objcpp', u'PBXSourcesBuildPhase'),\n        u'.nib': (u'wrapper.nib', u'PBXResourcesBuildPhase'),\n        u'.plist': (u'text.plist.xml', u'PBXResourcesBuildPhase'),\n        u'.json': (u'text.json', u'PBXResourcesBuildPhase'),\n        u'.png': (u'image.png', u'PBXResourcesBuildPhase'),\n        u'.jpg': (u'image.jpg', u'PBXResourcesBuildPhase'),\n        u'.rtf': (u'text.rtf', u'PBXResourcesBuildPhase'),\n        u'.tiff': (u'image.tiff', u'PBXResourcesBuildPhase'),\n        u'.txt': (u'text', u'PBXResourcesBuildPhase'),\n        u'.xcodeproj': (u'wrapper.pb-project', None),\n        u'.xib': (u'file.xib', u'PBXResourcesBuildPhase'),\n        u'.strings': (u'text.plist.strings', u'PBXResourcesBuildPhase'),\n        u'.bundle': (u'wrapper.plug-in', u'PBXResourcesBuildPhase'),\n        u'.dylib': (u'compiled.mach-o.dylib', u'PBXFrameworksBuildPhase'),\n        u'.xcdatamodeld': (u'wrapper.xcdatamodel', u'PBXSourcesBuildPhase'),\n        u'.xcassets': (u'folder.assetcatalog', u'PBXResourcesBuildPhase'),\n        u'.xcconfig': (u'sourcecode.xcconfig', u'PBXSourcesBuildPhase'),\n        u'.tbd': (u'sourcecode.text-based-dylib-definition', u'PBXFrameworksBuildPhase'),\n        u'.bin': (u'archive.macbinary', u'PBXResourcesBuildPhase'),\n        u'.mlmodel':(u'file.mlmodel', u'PBXSourcesBuildPhase'),\n        u'.html':(u'text.html', u'PBXResourcesBuildPhase'),\n    }\n    _SPECIAL_FOLDERS = [\n        u'.bundle',\n        u'.framework',\n        u'.xcodeproj',\n        u'.xcassets',\n        u'.xcdatamodeld',\n        u'.storyboardc'\n    ]\n\n    def __init__(self):\n        raise EnvironmentError('This class cannot be instantiated directly, use XcodeProject instead')\n\n    def add_file(self, path, parent=None, tree=TreeType.SOURCE_ROOT, target_name=None, force=True, file_options=FileOptions()):\n        \"\"\"\n        Adds a file to the project, taking care of the type of the file and creating additional structures depending on\n        the file type. For instance, frameworks will be linked, embedded and search paths will be adjusted automatically.\n        Header file will be added to the headers sections, but not compiled, whereas the source files will be added to\n        the compilation phase.\n        :param path: Path to the file to be added\n        :param parent: Parent group to be added under\n        :param tree: Tree where the path is relative to\n        :param target_name: Target name or list of target names where the file should be added (none for every target)\n        :param force: Add the file without checking if the file already exists\n        :param file_options: FileOptions object to be used during the addition of the file to the project.\n        :return: a list of elements that were added to the project successfully as PBXBuildFile objects\n        \"\"\"\n        results = []\n        # if it's not forced to add the file stop if the file already exists.\n        if not force:\n            for section in self.objects.get_sections():\n                for obj in self.objects.get_objects_in_section(section):\n                    if u'path' in obj and ProjectFiles._path_leaf(path) == ProjectFiles._path_leaf(obj.path):\n                        return []\n\n        file_ref, abs_path, path, tree, expected_build_phase = self._add_file_reference(path, parent, tree, force,\n                                                                                        file_options)\n        if path is None or tree is None:\n            return None\n\n        # no need to create the build_files, done\n        if not file_options.create_build_files:\n            return results\n\n        # create build_files for the targets\n        results.extend(self._create_build_files(file_ref, target_name, expected_build_phase, file_options))\n\n        # special case for the frameworks and libraries to update the search paths\n        if tree != TreeType.SOURCE_ROOT or abs_path is None:\n            return results\n\n        # the path is absolute and it's outside the scope of the project for linking purposes\n        library_path = os.path.join(u'$(SRCROOT)', os.path.split(file_ref.path)[0])\n        if os.path.isfile(abs_path):\n            self.add_library_search_paths([library_path], recursive=False)\n        else:\n            self.add_framework_search_paths([library_path, u'$(inherited)'], recursive=False)\n\n        return results\n\n    def add_project(self, path, parent=None, tree=TreeType.GROUP, target_name=None, force=True, file_options=FileOptions()):\n        \"\"\"\n        Adds another Xcode project into this project. Allows to use the products generated from the given project into\n        this project during compilation time. Optional: it can add the products into the different sections: frameworks\n        and bundles.\n\n        :param path: Path to the .xcodeproj file\n        :param parent: Parent group to be added under\n        :param tree: Tree where the path is relative to\n        :param target_name: Target name or list of target names where the file should be added (none for every target)\n        :param force: Add the file without checking if the file already exists\n        :param file_options: FileOptions object to be used during the addition of the file to the project.\n        :return: list of PBXReferenceProxy objects that can be used to create the PBXBuildFile phases.\n        \"\"\"\n        results = []\n        # if it's not forced to add the file stop if the file already exists.\n        if not force:\n            for section in self.objects.get_sections():\n                for obj in self.objects.get_objects_in_section(section):\n                    if u'path' in obj and ProjectFiles._path_leaf(path) == ProjectFiles._path_leaf(obj.path):\n                        return []\n\n        file_ref, _, path, tree, expected_build_phase = self._add_file_reference(path, parent, tree, force,\n                                                                                 file_options)\n        if path is None or tree is None:\n            return None\n\n        # load project and add the things\n        child_project = self.__class__.load(os.path.join(path, u'project.pbxproj'))\n        child_products = child_project.get_build_phases_by_name(u'PBXNativeTarget')\n\n        # create an special group without parent (ref proxies)\n        products_group = PBXGroup.create(name=u'Products', children=[])\n        self.objects[products_group.get_id()] = products_group\n\n        for child_product in child_products:\n            product_file_ref = child_project.objects[child_product.productReference]\n\n            # create the container proxies\n            container_proxy = PBXContainerItemProxy.create(file_ref, child_product)\n            self.objects[container_proxy.get_id()] = container_proxy\n\n            # create the reference proxies\n            reference_proxy = PBXReferenceProxy.create(product_file_ref, container_proxy)\n            self.objects[reference_proxy.get_id()] = reference_proxy\n\n            # add reference proxy to the product group\n            products_group.add_child(reference_proxy)\n\n            # append the result\n            results.append(reference_proxy)\n\n            if file_options.create_build_files:\n                _, expected_build_phase = self._determine_file_type(reference_proxy, file_options.ignore_unknown_type)\n                self._create_build_files(reference_proxy, target_name, expected_build_phase, file_options)\n\n        # add new PBXFileReference and PBXGroup to the PBXProject object\n        project_object = self.objects.get_objects_in_section(u'PBXProject')[0]\n        project_ref = PBXGenericObject(project_object).parse({\n            u'ProductGroup': products_group.get_id(),\n            u'ProjectRef': file_ref.get_id()\n        })\n\n        if u'projectReferences' not in project_object:\n            project_object[u'projectReferences'] = PBXList()\n\n        project_object.projectReferences.append(project_ref)\n\n        return results\n\n    def get_file_by_id(self, file_id):\n        \"\"\"\n        Gets the PBXFileReference to the given id\n        :param file_id: Identifier of the PBXFileReference to be retrieved.\n        :return: A PBXFileReference if the id is found, None otherwise.\n        \"\"\"\n        file_ref = self.objects[file_id]\n        if not isinstance(file_ref, PBXFileReference):\n            return None\n        return file_ref\n\n    def get_files_by_name(self, name, parent=None):\n        \"\"\"\n        Gets all the files references that have the given name, under the specified parent PBXGroup object or\n        PBXGroup id.\n        :param name: name of the file to be retrieved\n        :param parent: PBXGroup that should be used to narrow the search or None to retrieve files from all project\n        :return: List of all PBXFileReference that match the name and parent criteria.\n        \"\"\"\n        if parent is not None:\n            parent = self._get_parent_group(parent)\n\n        files = []\n        for file_ref in self.objects.get_objects_in_section(u'PBXFileReference'):\n            if file_ref.get_name() == name and (parent is None or parent.has_child(file_ref.get_id())):\n                files.append(file_ref)\n\n        return files\n\n    def get_files_by_path(self, path, tree=TreeType.SOURCE_ROOT):\n        \"\"\"\n        Gets the files under the given tree type that match the given path.\n        :param path: Path to the file relative to the tree root\n        :param tree: Tree type to look for the path. By default the SOURCE_ROOT\n        :return: List of all PBXFileReference that match the path and tree criteria.\n        \"\"\"\n        files = []\n        for file_ref in self.objects.get_objects_in_section(u'PBXFileReference'):\n            if file_ref.path == path and file_ref.sourceTree == tree:\n                files.append(file_ref)\n\n        return files\n\n    def remove_file_by_id(self, file_id, target_name=None):\n        \"\"\"\n        Removes the file id from given target name. If no target name is given, the file is removed\n        from all targets\n        :param file_id: identifier of the file to be removed\n        :param target_name: Target name or list of target names where the file should be removed from (none for every\n            target)\n        :return: True if the file id was removed. False if the file was not removed.\n        \"\"\"\n\n        file_ref = self.get_file_by_id(file_id)\n        if file_ref is None:\n            return False\n\n        for target in self.objects.get_targets(target_name):\n            for build_phase_id in target.buildPhases:\n                build_phase = self.objects[build_phase_id]\n\n                for build_file_id in build_phase.files:\n                    build_file = self.objects[build_file_id]\n\n                    if build_file.fileRef == file_ref.get_id():\n                        # remove the build file from the phase\n                        build_phase.remove_build_file(build_file)\n\n                # if the build_phase is empty remove it too, unless it's a shell script.\n                if build_phase.files.__len__() == 0 and build_phase.isa != u'PBXShellScriptBuildPhase':\n                    # remove the build phase from the target\n                    target.remove_build_phase(build_phase)\n\n        # remove it iff it's removed from all targets or no build file reference it\n        for build_file in self.objects.get_objects_in_section(u'PBXBuildFile'):\n            if build_file.fileRef == file_ref.get_id():\n                return True\n\n        # remove the file from any groups if there is no reference from any target\n        for group in self.objects.get_objects_in_section(u'PBXGroup'):\n            if file_ref.get_id() in group.children:\n                group.remove_child(file_ref)\n\n        # the file is not referenced in any build file, remove it\n        del self.objects[file_ref.get_id()]\n        return True\n\n    def remove_files_by_path(self, path, tree=TreeType.SOURCE_ROOT, target_name=None):\n        \"\"\"\n        Removes all files for the given path under the same tree\n        :param path: Path to the file relative to the tree root\n        :param tree: Tree type to look for the path. By default the SOURCE_ROOT\n        :param target_name: Target name or list of target names where the file should be removed from (none for every\n            target)\n        :return: True if all the files were removed without problems. False if at least one file failed.\n        \"\"\"\n        files = self.get_files_by_path(path, tree)\n        result = 0\n        total = files.__len__()\n        for file_ref in files:\n            if self.remove_file_by_id(file_ref.get_id(), target_name=target_name):\n                result += 1\n\n        return result != 0 and result == total\n\n    def add_folder(self, path, parent=None, excludes=None, recursive=True, create_groups=True, target_name=None,\n                   file_options=FileOptions()):\n        \"\"\"\n        Given a directory, it will create the equivalent group structure and add all files in the process.\n        If groups matching the logical path already exist, it will use them instead of creating a new one. Same\n        apply for file within a group, if the file name already exists it will be ignored.\n\n        :param path: OS path to the directory to be added.\n        :param parent: Parent group to be added under\n        :param excludes: list of regexs to ignore\n        :param recursive: add folders recursively or stop in the first level\n        :param create_groups: add folders recursively as groups or references\n        :param target_name: Target name or list of target names where the file should be added (none for every target)\n        :param file_options: FileOptions object to be used during the addition of the file to the project.\n        :return: a list of elements that were added to the project successfully as PBXBuildFile objects\n        \"\"\"\n        if not os.path.isdir(path):\n            return None\n\n        if not excludes:\n            excludes = []\n\n        results = []\n\n        # add the top folder as a group, make it the new parent\n        path = os.path.abspath(path)\n        if not create_groups and os.path.splitext(path)[1] not in ProjectFiles._SPECIAL_FOLDERS:\n            return self.add_file(path, parent, target_name=target_name, force=False, file_options=file_options)\n\n        parent = self.get_or_create_group(os.path.split(path)[1], path, parent)\n\n        # iterate over the objects in the directory\n        for child in os.listdir(path):\n            # exclude dirs or files matching any of the expressions\n            if [pattern for pattern in excludes if re.match(pattern, child)]:\n                continue\n\n            full_path = os.path.join(path, child)\n            children = []\n            if os.path.isfile(full_path) or os.path.splitext(child)[1] in ProjectFiles._SPECIAL_FOLDERS or \\\n                    not create_groups:\n                # check if the file exists already, if not add it\n                children = self.add_file(full_path, parent, target_name=target_name, force=False,\n                                         file_options=file_options)\n            else:\n                # if recursive is true, go deeper, otherwise create the group here.\n                if recursive:\n                    children = self.add_folder(full_path, parent, excludes, recursive, target_name=target_name,\n                                               file_options=file_options)\n                else:\n                    self.get_or_create_group(child, child, parent)\n\n            results.extend(children)\n\n        return results\n\n    # miscellaneous functions, candidates to be extracted and decouple implementation\n\n    def _add_file_reference(self, path, parent, tree, force, file_options):\n        # decide the proper tree and path to add\n        abs_path, path, tree = ProjectFiles._get_path_and_tree(self._source_root, path, tree)\n        if path is None or tree is None:\n            return None, abs_path, path, tree, None\n\n        # create a PBXFileReference for the new file\n        file_ref = PBXFileReference.create(path, tree)\n\n        # determine the type of the new file:\n        file_type, expected_build_phase = ProjectFiles._determine_file_type(file_ref, file_options.ignore_unknown_type)\n\n        # set the file type on the file ref add the files\n        file_ref.set_last_known_file_type(file_type)\n        self.objects[file_ref.get_id()] = file_ref\n\n        # determine the parent and add it to it\n        self._get_parent_group(parent).add_child(file_ref)\n\n        return file_ref, abs_path, path, tree, expected_build_phase\n\n    def _create_build_files(self, file_ref, target_name, expected_build_phase, file_options):\n        results = []\n        for target in self.objects.get_targets(target_name):\n            # determine if there is a suitable build phase created\n            build_phases = target.get_or_create_build_phase(expected_build_phase)\n\n            # if it's a framework and it needs to be embedded\n            if file_options.embed_framework and expected_build_phase == u'PBXFrameworksBuildPhase' and \\\n                    file_ref.get_file_type() == u'wrapper.framework':\n                embed_phase = target.get_or_create_build_phase(u'PBXCopyFilesBuildPhase',\n                                                               search_parameters={'dstSubfolderSpec': '10'},\n                                                               create_parameters=(PBXCopyFilesBuildPhaseNames.EMBEDDED_FRAMEWORKS,))\n                # add runpath search flag\n                self.add_flags(XCBuildConfigurationFlags.LD_RUNPATH_SEARCH_PATHS,\n                               u'$(inherited) @executable_path/Frameworks', target_name)\n                build_phases.extend(embed_phase)\n\n            # create the build file and add it to the phase\n            for target_build_phase in build_phases:\n                build_file = PBXBuildFile.create(file_ref, file_options.get_attributes(file_ref, target_build_phase))\n                self.objects[build_file.get_id()] = build_file\n                target_build_phase.add_build_file(build_file)\n\n                results.append(build_file)\n\n        return results\n\n    @classmethod\n    def _determine_file_type(cls, file_ref, unknown_type_allowed):\n        ext = os.path.splitext(file_ref.get_name())[1]\n        if os.path.isdir(os.path.abspath(file_ref.path)) and ext not in ProjectFiles._SPECIAL_FOLDERS:\n            file_type = 'folder'\n            build_phase = u'PBXResourcesBuildPhase'\n        else:\n            file_type, build_phase = ProjectFiles._FILE_TYPES.get(ext, (None, u'PBXResourcesBuildPhase'))\n\n        if not unknown_type_allowed and file_type is None:\n            raise ValueError(\n                u'Unknown file extension: {0}. Please add the extension and Xcode type to ProjectFiles._FILE_TYPES' \\\n                    .format(os.path.splitext(file_ref.get_name())[1]))\n\n        return file_type, build_phase\n\n    @classmethod\n    def _path_leaf(cls, path):\n        head, tail = os.path.split(path)\n        return tail or os.path.basename(head)\n\n    @classmethod\n    def _get_path_and_tree(cls, source_root, path, tree):\n        # returns the absolute path, the relative path and the tree\n        abs_path = None\n        if os.path.isabs(path):\n            abs_path = path\n\n            if not os.path.exists(path):\n                return None, None, None\n\n            if tree == TreeType.SOURCE_ROOT:\n                path = os.path.relpath(path, source_root)\n            else:\n                tree = TreeType.ABSOLUTE\n\n        return abs_path, path, tree\n"
        ],
        "test_patch": "",
        "patch_preview": "From 106291bc9d8938d1f1e398ba859b3f13697b64cc Mon Sep 17 00:00:00 2001\nFrom: znvy <ningvvy@gmail.com>\nDate: Fri, 18 Oct 2019 16:15:21 +0800\nSubject: [PATCH] Update: add .entitlements file type\n\n---\n pbxproj/pbxextensions/ProjectFiles.py | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/pbxproj/pbxextensions/ProjectFiles.py b/pbxproj/pbxextensions/ProjectFiles.py\nindex ad26628b..ce0a4015 100644\n--- a/pbxproj/pbxextensions/ProjectFiles.py\n+++ b/pbxproj/pbxextensions/ProjectFiles.py\n@@ -106,6 +10"
      },
      "patch": {
        "length": 859,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 0,
        "net_change": 1,
        "changed_files": [
          {
            "file": "pbxproj/pbxextensions/ProjectFiles.py",
            "added": 1,
            "deleted": 0
          }
        ]
      },
      "issue_comments": [
        {
          "id": 544845915,
          "body": "\n[![Coverage Status](https://coveralls.io/builds/26461187/badge)](https://coveralls.io/builds/26461187)\n\nCoverage remained the same at 96.344% when pulling **106291bc9d8938d1f1e398ba859b3f13697b64cc on znvy:master** into **440ce1ac11af74c022b2e56d018947f4c5de36d1 on kronenthaler:master**.\n",
          "user": "coveralls",
          "created_at": "2019-10-22T07:53:49Z",
          "html_url": "https://github.com/kronenthaler/mod-pbxproj/pull/250#issuecomment-544845915"
        }
      ],
      "issue_comments_count": 1,
      "code_statistics": {
        "total_files": 88,
        "total_lines": 12708,
        "total_bytes": 671411,
        "python_files": 71,
        "python_lines": 4752,
        "file_extensions": {
          ".txt": 1,
          ".md": 2,
          ".py": 71,
          ".pbxproj": 11,
          ".a": 1,
          ".m": 2
        },
        "largest_files": [
          {
            "path": "mod_pbxproj/tests/samples/demo4.pbxproj",
            "size": 243388,
            "lines": 2900,
            "extension": ".pbxproj"
          },
          {
            "path": "mod_pbxproj/tests/samples/demo2.pbxproj",
            "size": 38474,
            "lines": 797,
            "extension": ".pbxproj"
          },
          {
            "path": "mod_pbxproj/tests/samples/demo.pbxproj",
            "size": 34565,
            "lines": 680,
            "extension": ".pbxproj"
          },
          {
            "path": "tests/samplescli/project.pbxproj",
            "size": 25969,
            "lines": 626,
            "extension": ".pbxproj"
          },
          {
            "path": "mod_pbxproj/tests/samples/demo3.pbxproj",
            "size": 21582,
            "lines": 560,
            "extension": ".pbxproj"
          },
          {
            "path": "pbxproj/pbxextensions/ProjectFiles.py",
            "size": 23267,
            "lines": 486,
            "extension": ".py"
          },
          {
            "path": "tests/pbxextensions/TestProjectFiles.py",
            "size": 21415,
            "lines": 440,
            "extension": ".py"
          },
          {
            "path": "mod_pbxproj/tests/samples/embedded-framework.pbxproj",
            "size": 17134,
            "lines": 439,
            "extension": ".pbxproj"
          },
          {
            "path": "mod_pbxproj/tests/samples/metal-image-processing.pbxproj",
            "size": 19068,
            "lines": 419,
            "extension": ".pbxproj"
          },
          {
            "path": "tests/samplescli/dependency.xcodeproj/project.pbxproj",
            "size": 14344,
            "lines": 407,
            "extension": ".pbxproj"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 88,
        "files_changed_count": 1,
        "files_changed_ratio": 0.011363636363636364,
        "total_lines_in_repo": 12708,
        "lines_added": 1,
        "lines_deleted": 0,
        "net_lines_changed": 1,
        "lines_changed_ratio": 7.869058860560277e-05,
        "pr_body_length": 0,
        "commit_message_length": 20,
        "python_file_count": 71,
        "python_line_count": 4752
      }
    },
    {
      "tar_file_name": "kushaldas#pym#pull#128",
      "repo_name": "kushaldas#pym#pull#128",
      "success": true,
      "error": null,
      "commit": {
        "sha": "c74f8f27ac7e0db024c0e9ff09391f4f99303000",
        "message": "Merge pull request #139 from smarvin/patch-1\n\nUpdate thebeginning.rst",
        "author": {
          "name": "Kushal Das",
          "email": "mail@kushaldas.in",
          "date": "2021-02-17T03:46:48Z"
        },
        "html_url": "https://github.com/kushaldas/pym/commit/c74f8f27ac7e0db024c0e9ff09391f4f99303000",
        "api_url": "https://api.github.com/repos/kushaldas/pym/commits/c74f8f27ac7e0db024c0e9ff09391f4f99303000"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kushaldas#pym#pull#128",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/kushaldas#pym#pull#128.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kushaldas#pym#pull#128/source_code"
      },
      "pr": {
        "number": 128,
        "title": "Fixed the code in Matrix Multiplication in the file docs/datastructure.rst",
        "body": "The matrix multiplication example in [here](https://github.com/kushaldas/pym/blob/master/docs/datastructure.rst) gives wrong output:\r\nSo changed the code from \r\n```\r\nc.append([a[i][j] * b[j][i] for j in range(0, n)])\r\n```\r\n**to**\r\n\r\n```\r\nc.append([ sum([a[i][k] * b[k][j] for k in range(0, n)]) for j in range(0,n) ])\r\n```\r\nand also the output of the code snippet",
        "state": "open",
        "created_at": "2018-07-20T11:21:09Z",
        "updated_at": "2021-02-17T03:56:45Z",
        "merged_at": null,
        "html_url": "https://github.com/kushaldas/pym/pull/128",
        "user": "RaviTejaKomma",
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "kushaldas_pym-128",
        "repo": "/kushaldas/pym",
        "base_commit": "c74f8f27ac7e0db024c0e9ff09391f4f99303000",
        "problem_statement": {},
        "edit_files": [
          "docs/datastructure.rst"
        ],
        "oracle_files": [
          "\n\n===============\nData Structures\n===============\n\nPython has a few built-in data structures. If you are wondering what a data structure is, it is nothing but a way to store data and having particular methods to retrieve or manipulate it. We already encountered lists before; now we will go in some depth.\n\n.. index:: List\n\nLists\n=====\n\n::\n\n    >>> a = [23, 45, 1, -3434, 43624356, 234]\n    >>> a.append(45)\n    >>> a\n    [23, 45, 1, -3434, 43624356, 234, 45]\n\nAt first we created a list *a*. Then to add *45* at the end of the list we call the *a.append(45)* method. You can see that *45* is added at the end of the list. Sometimes it is necessary to insert data at any place within the list, for that we have *insert()* method.\n\n::\n\n    >>> a.insert(0, 1) # 1 added at the 0th position of the list\n    >>> a\n    [1, 23, 45, 1, -3434, 43624356, 234, 45]\n    >>> a.insert(0, 111)\n    >>> a\n    [111, 1, 23, 45, 1, -3434, 43624356, 234, 45]\n\n.. index:: count\n\n*count(s)* will return you the number of times *s* is in the list. Here we are going to check how many times *45* is there in the list.\n\n::\n\n    >>> a.count(45)\n    2\n\nIf you want to remove any particular value from the list you have to use the *remove()* method.\n\n.. index:: remove\n\n::\n\n    >>> a.remove(234)\n    >>> a\n    [111, 1, 23, 45, 1, -3434, 43624356, 45]\n\nNow to reverse the whole list\n\n.. index:: reverse\n\n::\n\n    >>> a.reverse()\n    >>> a\n    [45, 43624356, -3434, 1, 45, 23, 1, 111]\n\nWe can store anything in the list, so first we are going to add another list  *b* in  *a*; then we will learn how to add the values of  *b* into  *a*.\n\n.. index:: extend, append\n\n::\n\n    >>> b = [45, 56, 90]\n    >>> a.append(b)\n    >>> a\n    [45, 43624356, -3434, 1, 45, 23, 1, 111, [45, 56, 90]]\n    >>> a[-1]\n    [45, 56, 90]\n    >>> a.extend(b)  # To add the values of b not the b itself\n    >>> a\n    [45, 43624356, -3434, 1, 45, 23, 1, 111, [45, 56, 90], 45, 56, 90]\n    >>> a[-1]\n    90\n\n.. index:: sort\n\nAbove you can see how we used the *a.extend()* method to extend the list. To sort any list we have *sort()* method. The *sort()* method will only work if elements in the list are comparable. We will remove the list b from the list and then sort. \n\n::\n\n    >>> a.remove(b)\n    >>> a\n    [45, 43624356, -3434, 1, 45, 23, 1, 111, 45, 56, 90]\n    >>> a.sort()\n    >>> a\n    [-3434, 1, 1, 23, 45, 45, 45, 56, 90, 111, 43624356]\n\n\n.. note:: Remember that `sort` method does not return the sorted list, instead the list itself will be sorted. This is done to keep performance of the code in mind. More details can be found `here <https://docs.python.org/3/faq/design.html?highlight=walrus#why-doesn-t-list-sort-return-the-sorted-list>`_.\n\nYou can also delete an element at any particular position of the list using the del keyword.\n\n::\n\n    >>> del a[-1]\n    >>> a\n    [-3434, 1, 1, 23, 45, 45, 45, 56, 90, 111]\n\nUsing lists as stack and queue\n==============================\n\nStacks are often known as LIFO (Last In First Out) structure. It means the data will enter into it at the end, and the last data will come out first. The easiest example can be of couple of marbles in an one side closed pipe. So if you want to take the marbles out of it you have to do that from the end where you inserted the last marble. To achieve the same in code\n\n::\n\n    >>> a = [1, 2, 3, 4, 5, 6]\n    >>> a\n    [1, 2, 3, 4, 5, 6]\n    >>> a.pop()\n    6\n    >>> a.pop()\n    5\n    >>> a.pop()\n    4\n    >>> a.pop()\n    3\n    >>> a\n    [1, 2]\n    >>> a.append(34)\n    >>> a\n    [1, 2, 34]\n\nWe learned a new method above *pop()*. *pop(i)* will take out the ith data from the list.\n\nIn our daily life we have to encounter queues many times, like at ticket counters or in the library or in the billing section of any supermarket. Queue is the data structure where you can append more data at the end and take out data from the beginning. That is why it is known as FIFO (First In First Out).\n\n::\n\n    >>> a = [1, 2, 3, 4, 5]\n    >>> a.append(1)\n    >>> a\n    [1, 2, 3, 4, 5, 1]\n    >>> a.pop(0)\n    1\n    >>> a.pop(0)\n    2\n    >>> a\n    [3, 4, 5, 1]\n\nTo take out the first element of the list we are using *a.pop(0)*.\n\n.. index:: List comprehension\n\nList Comprehensions\n===================\n\nList comprehensions provide a concise way to create lists. Each list comprehension consists of an expression followed by a for clause, then zero or more for or if clauses. The result will be a list resulting from evaluating the expression in the context of the for and if clauses which follow it.\n\nFor example if we want to make a list out of the square values of another list, then\n\n::\n\n    >>> a = [1, 2, 3]\n    >>> [x ** 2 for x in a]\n    [1, 4, 9]\n    >>> z = [x + 1 for x in [x ** 2 for x in a]]\n    >>> z\n    [2, 5, 10]\n\nAbove in the second case we used two list comprehensions in a same line.\n\n\nA few special functions related to lists\n=========================================\n\n`sum` takes a list as argument and sums all the values inside of it.\n\n::\n\n    >>> numbers = [1, 2, 3, 4, 5]\n    >>> sum(numbers)\n    15\n\n`min` and `max` tells us about the minimum and maximum value from a list.\n\n::\n\n    >>> min(numbers)\n    1\n    >>> max(numbers)\n    5\n\nThere are two other functions related to boolean value checking. `any` and `all`. `any` tells us if any of the value in the list if `True`.\n`all` tells us if all the values in the list are `True` or not.\n\n::\n\n    >>> numbers = [1, 2, 3, 4, 5, 0]\n    >>> any(numbers)\n    True\n    >>> all(numbers)\n    False\n\nHere `all` returned `False` because we have 0 in the list.\n\n.. index:: Tuple\n\nTuples\n======\n\nTuples are data separated by commas.\n\n::\n\n    >>> a = 'Fedora', 'Debian', 'Kubuntu', 'Pardus'\n    >>> a\n    ('Fedora', 'Debian', 'Kubuntu', 'Pardus')\n    >>> a[1]\n    'Debian'\n    >>> for x in a:\n    ...     print(x, end=' ')\n    ...\n    Fedora Debian Kubuntu Pardus\n\nYou can also unpack values of any tuple into variables, like\n\n::\n\n    >>> divmod(15,2)\n    (7, 1)\n    >>> x, y = divmod(15,2)\n    >>> x\n    7\n    >>> y\n    1\n\nTuples are immutable, meaning that you can not del/add/edit any value inside the tuple. Here is another example\n\n::\n\n    >>> a = (1, 2, 3, 4)\n    >>> del a[0]\n    Traceback (most recent call last):\n    File \"<stdin>\", line 1, in <module>\n    TypeError: 'tuple' object doesn't support item deletion\n\nAs you can see above, Python gives an error when we try to delete a value in the tuple.\n\nTo create a tuple which contains only one value, type a trailing comma.\n\n::\n\n    >>> a = (123)\n    >>> a\n    123\n    >>> type(a)\n    <class 'int'>\n    >>> a = (123, ) #Look at the trailing comma\n    >>> a\n    (123,)\n    >>> type(a)\n    <class 'tuple'>\n\nUsing the built-in function *type()* you can know the data type of any variable. Remember the *len()* function we used to find the length of any sequence?\n\n::\n\n    >>> type(len)\n    <class 'builtin_function_or_method'>\n\n.. index:: Set\n\nSets\n====\n\nSets are another type of data structure with no duplicate items. We can apply mathematical set operations on sets.\n\n::\n\n    >>> a = set('abcthabcjwethddda')\n    >>> a\n    {'a', 'c', 'b', 'e', 'd', 'h', 'j', 't', 'w'}\n\nAnd some examples of the set operations\n\n::\n\n    >>> a = set('abracadabra')\n    >>> b = set('alacazam')\n    >>> a                                  # unique letters in a\n    {'a', 'r', 'b', 'c', 'd'}\n    >>> a - b                              # letters in a but not in b\n    {'r', 'd', 'b'}\n    >>> a | b                              # letters in either a or b\n    {'a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'}\n    >>> a & b                              # letters in both a and b\n    {'a', 'c'}\n    >>> a ^ b                              # letters in a or b but not both\n    {'r', 'd', 'b', 'm', 'z', 'l'}\n\nTo add or pop values from a set\n\n::\n\n    >>> a\n    {'a', 'c', 'b', 'e', 'd', 'h', 'j', 'q', 't', 'w'}\n    >>> a.add('p')\n    >>> a\n    {'a', 'c', 'b', 'e', 'd', 'h', 'j', 'q', 'p', 't', 'w'}\n\n.. index:: Dictionary\n\nDictionaries\n============\n\nDictionaries are unordered set of *key: value* pairs where keys are unique. We declare dictionaries using {} braces. We use dictionaries to store data for any particular key and then retrieve them.\n\n::\n\n    >>> data = {'kushal':'Fedora', 'kart_':'Debian', 'Jace':'Mac'}\n    >>> data\n    {'kushal': 'Fedora', 'Jace': 'Mac', 'kart_': 'Debian'}\n    >>> data['kart_']\n    'Debian'\n\nWe can add more data to it by simply\n\n::\n\n    >>> data['parthan'] = 'Ubuntu'\n    >>> data\n    {'kushal': 'Fedora', 'Jace': 'Mac', 'kart_': 'Debian', 'parthan': 'Ubuntu'}\n\nTo delete any particular *key:value* pair\n\n::\n\n    >>> del data['kushal']\n    >>> data\n    {'Jace': 'Mac', 'kart_': 'Debian', 'parthan': 'Ubuntu'}\n\nTo check if any *key* is there in the dictionary or not you can use *in* keyword.\n\n::\n\n    >>> 'Soumya' in data\n    False\n\nYou must remember that no mutable object can be a *key*, that means you can not use a *list* as a *key*.\n\n*dict()* can create dictionaries from tuples of *key,value* pair.\n\n::\n\n    >>> dict((('Indian','Delhi'),('Bangladesh','Dhaka')))\n    {'Indian': 'Delhi', 'Bangladesh': 'Dhaka'}\n\nMany times it happens that we want to add more data to a value in a dictionary and if the key does not exists then we add some default value. You can do this efficiently using *dict.setdefault(key, default)*.\n::\n\n    >>> data = {}\n    >>> data.setdefault('names', []).append('Ruby')\n    >>> data\n    {'names': ['Ruby']}\n    >>> data.setdefault('names', []).append('Python')\n    >>> data\n    {'names': ['Ruby', 'Python']}\n    >>> data.setdefault('names', []).append('C')\n    >>> data\n    {'names': ['Ruby', 'Python', 'C']}\n\nWhen we try to get value for a key which does not exists we get *KeyError*. We can use *dict.get(key, default)* to get a default value when they key does not exists before.\n\n::\n\n    >>> data['foo']\n    Traceback (most recent call last):\n    File \"<stdin>\", line 1, in <module>\n    KeyError: 'foo'\n    >>> data.get('foo', 0)\n    0\n\n\nLooping over a dictionary\n--------------------------\n\nIf you just do a `for` loop over a dictionary, it will provide you all the available keys in the dictionary.\n\n::\n\n    >>> data\n    {'Kushal': 'Fedora', 'Jace': 'Mac', 'kart_': 'Debian', 'parthan': 'Ubuntu'}\n    >>> for x in data:\n    ...     print(f\"Key = {x}\")\n    ...\n    Kushal\n    Jace\n    kart_\n    parthan\n\n\n.. index:: items\n\nIf you want to loop through a dict use *items()* method.\n\n::\n\n    >>> data\n    {'Kushal': 'Fedora', 'Jace': 'Mac', 'kart_': 'Debian', 'parthan': 'Ubuntu'}\n    >>> for x, y in data.items():\n    ...     print(\"%s uses %s\" % (x, y))\n    ...\n    Kushal uses Fedora\n    Jace uses Mac\n    kart_ uses Debian\n    parthan uses Ubuntu\n\nFrom Python 3.7 dictionaries maintain the insertion order of the items. While\nlooping over *items()* method, you will get the key/value combination based on\nthe insertion order of those items.\n\n.. index:: enumerate\n\nIf you want to loop through a list (or any sequence) and get iteration number at the same time you have to use *enumerate()*.\n\n::\n\n    >>> for i, j in enumerate(['a', 'b', 'c']):\n    ...     print(i, j)\n    ...\n    0 a\n    1 b\n    2 c\n\nIf you want to start the sequence from a different number, you can can do that using *start* value in the enumerate\nfunction.\n\n::\n\n    >>> for i, j in enumerate(['a', 'b', 'c'], start=1):\n    ...   print(i, j)\n    ...\n    1 a\n    2 b\n    3 c\n\n\n\nYou may also need to iterate through two sequences same time, for that use *zip()* function.\n\n::\n\n    >>> a = ['Pradeepto', 'Kushal']\n    >>> b = ['OpenSUSE', 'Fedora']\n    >>> for x, y in zip(a, b):\n    ...     print(\"%s uses %s\" % (x, y))\n    ...\n    Pradeepto uses OpenSUSE\n    Kushal uses Fedora\n\nstudents.py\n===========\n\nIn this example , you have to take number of students as input , then ask marks for three subjects as 'Physics', 'Maths', 'History', if the total marks for any student is less 120 then print he failed, or else say passed.\n\n::\n\n    #!/usr/bin/env python3\n    n = int(input(\"Enter the number of students:\"))\n    data = {} # here we will store the data\n    languages = ('Physics', 'Maths', 'History') #all languages\n    for i in range(0, n): #for the n number of students\n        name = input('Enter the name of the student %d: ' % (i + 1)) #Get the name of the student\n        marks = []\n        for x in languages:\n            marks.append(int(input('Enter marks of %s: ' % x))) #Get the marks for  languages\n        data[name] = marks\n    for x, y in data.items():\n        total =  sum(y)\n        print(\"%s 's  total marks %d\" % (x, total))\n        if total < 120:\n            print(\"%s failed :(\" % x)\n        else:\n            print(\"%s passed :)\" % x)\n\nThe output\n\n::\n\n    $ ./students.py\n    Enter the number of students:2\n    Enter the name of the student 1: Babai\n    Enter marks of Physics: 12\n    Enter marks of Maths: 45\n    Enter marks of History: 40\n    Enter the name of the student 2: Tesla\n    Enter marks of Physics: 99\n    Enter marks of Maths: 98\n    Enter marks of History: 99\n    Babai 's  total marks 97\n    Babai failed :(\n    Tesla 's  total marks 296\n    Tesla passed :)\n\nAssignment problems\n===================\n\nRemember that in Python, variables are just names pointing to the value. In the\nfollowing example, both `x` and `y` points to same value, means when `x`\nchanges, `y` also changes.\n\n::\n\n    >>> x = [1, 2, 3]\n    >>> y = x\n    >>> x.append(20)\n    >>> print(x)\n    [1, 2, 3, 20]\n    >>> print(y)\n    [1, 2, 3, 20]\n\nIf you want a full copy of the data assigned to a new variable, call `obj.copy()` method.\n\nThis also happens when you pass them into functions. For example, in the below\nfunction, we are passing a list, and appending new numbers into it.  This also\nchanges the variable outside of the function.\n\n::\n\n    numbers = [1, 2, 4]\n\n    def modify(numbers):\n        numbers.append(42)\n    \n    modify(numbers)\n    print(numbers)\n    [1, 2, 4, 42]\n\n\n.. note:: Please go through the documentation for the mentioned data types in docs.python.org, at least have a look\n          at the different methods available on them. For most of the day to day work, we use these data types in our Python code. So, it is important to understand them well. Before you move into next chapter, read the help documentation if nothing else. For example: `help(dict)` or `help(list)`.\n\nCreate a calculator\n====================\n\nHere is a small problem for you. You will have to use list, and dictionary to\ncreate a tool, which will take input like `(* (+ 3 4) 2)` and return the answer\nlike `14`. The four valid operators are `+`, `-`,  `/*`. Every operator will\nneed two operands to work on. Another input `(* 2 3)` and the output is `6`.\n\n"
        ],
        "test_patch": "",
        "patch_preview": "From 8a02f3db9d91983da745d3ec722765629d356266 Mon Sep 17 00:00:00 2001\nFrom: RaviTejaKomma <ravieee929374s@gmail.com>\nDate: Fri, 20 Jul 2018 16:00:57 +0530\nSubject: [PATCH] fixed the code in matrix multiplication in\n docs/datastructure.rst line number: 442\n\nchanged the output of matrix multiplication example in docs/datastructure.rst\n---\n docs/datastructure.rst | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a/docs/datastructure.rst b/docs/datastructure.rst\nindex b941fe"
      },
      "patch": {
        "length": 1614,
        "files_changed": 1,
        "lines_added": 4,
        "lines_deleted": 4,
        "net_change": 0,
        "changed_files": [
          {
            "file": "docs/datastructure.rst",
            "added": 4,
            "deleted": 4
          }
        ]
      },
      "issue_comments": [
        {
          "id": 407754103,
          "body": "Hello Kushaldas. I have squashed the commits to a single commit. Thanks :)",
          "user": "RaviTejaKomma",
          "created_at": "2018-07-25T13:28:54Z",
          "html_url": "https://github.com/kushaldas/pym/pull/128#issuecomment-407754103"
        },
        {
          "id": 407407454,
          "body": "Please squash the commit.",
          "user": "kushaldas",
          "created_at": "2018-07-24T13:31:26Z",
          "html_url": "https://github.com/kushaldas/pym/pull/128#issuecomment-407407454"
        }
      ],
      "issue_comments_count": 2,
      "code_statistics": {
        "total_files": 113,
        "total_lines": 146085,
        "total_bytes": 14397283,
        "python_files": 42,
        "python_lines": 938,
        "file_extensions": {
          "": 4,
          ".txt": 4,
          ".cfg": 1,
          ".rst": 30,
          ".sh": 1,
          ".py": 42,
          ".gif": 14,
          ".png": 9,
          ".html": 4,
          ".pyc": 1,
          ".conf": 1,
          ".css_t": 1,
          ".css": 1
        },
        "largest_files": [
          {
            "path": "docs/img/cpx.gif",
            "size": 6259914,
            "lines": 62875,
            "extension": ".gif"
          },
          {
            "path": "docs/img/how_to_get_device_simulator.gif",
            "size": 3250115,
            "lines": 29484,
            "extension": ".gif"
          },
          {
            "path": "docs/img/installing_devicesimulator.gif",
            "size": 1693903,
            "lines": 16434,
            "extension": ".gif"
          },
          {
            "path": "docs/img/mu_running_code.gif",
            "size": 541787,
            "lines": 5214,
            "extension": ".gif"
          },
          {
            "path": "docs/img/red_light.gif",
            "size": 449398,
            "lines": 4581,
            "extension": ".gif"
          },
          {
            "path": "docs/img/mu_starting.gif",
            "size": 347666,
            "lines": 3210,
            "extension": ".gif"
          },
          {
            "path": "docs/img/checkname_pyper.gif",
            "size": 256800,
            "lines": 2721,
            "extension": ".gif"
          },
          {
            "path": "docs/img/rgb.gif",
            "size": 157773,
            "lines": 1837,
            "extension": ".gif"
          },
          {
            "path": "docs/img/theidemo.gif",
            "size": 148496,
            "lines": 1525,
            "extension": ".gif"
          },
          {
            "path": "docs/img/textinput_pyper.gif",
            "size": 169723,
            "lines": 1480,
            "extension": ".gif"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 113,
        "files_changed_count": 1,
        "files_changed_ratio": 0.008849557522123894,
        "total_lines_in_repo": 146085,
        "lines_added": 4,
        "lines_deleted": 4,
        "net_lines_changed": 0,
        "lines_changed_ratio": 5.476263819009481e-05,
        "pr_body_length": 363,
        "commit_message_length": 69,
        "python_file_count": 42,
        "python_line_count": 938
      }
    },
    {
      "tar_file_name": "kvesteri#sqlalchemy-continuum#pull#321",
      "repo_name": "kvesteri#sqlalchemy-continuum#pull#321",
      "success": true,
      "error": null,
      "commit": {
        "sha": "ccd87070baee40546f3289135e655ecf79b48cf8",
        "message": "Fix code example to use `append`",
        "author": {
          "name": "Andrew Sayman",
          "email": "andrew.sayman@gmail.com",
          "date": "2023-09-12T19:11:20Z"
        },
        "html_url": "https://github.com/sqlalchemy-continuum/sqlalchemy-continuum/commit/ccd87070baee40546f3289135e655ecf79b48cf8",
        "api_url": "https://api.github.com/repos/sqlalchemy-continuum/sqlalchemy-continuum/commits/ccd87070baee40546f3289135e655ecf79b48cf8"
      },
      "repository": {
        "full_name": "Unknown/Unknown",
        "name": "Unknown",
        "owner": "Unknown",
        "html_url": "https://github.com/Unknown/Unknown",
        "api_url": null
      },
      "local_paths": {
        "commit_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kvesteri#sqlalchemy-continuum#pull#321",
        "tar_file": "/data/xubing/oss_swe/swebench_sample_40k/kvesteri#sqlalchemy-continuum#pull#321.tar",
        "source_code_dir": "/data/xubing/oss_swe/swebench_sample_40k_commit_raw_code/kvesteri#sqlalchemy-continuum#pull#321/source_code"
      },
      "pr": {
        "number": 321,
        "title": "Update table_builder.py",
        "body": "Apply fix specified in https://github.com/kvesteri/sqlalchemy-continuum/issues/285#issuecomment-1229426028",
        "state": "open",
        "created_at": "2022-11-29T10:06:56Z",
        "updated_at": "2024-03-14T01:27:48Z",
        "merged_at": null,
        "html_url": "https://github.com/kvesteri/sqlalchemy-continuum/pull/321",
        "user": "ew318",
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "commits": 1
      },
      "swebench": {
        "instance_id": "kvesteri_sqlalchemy-continuum-321",
        "repo": "/kvesteri/sqlalchemy-continuum",
        "base_commit": "ccd87070baee40546f3289135e655ecf79b48cf8",
        "problem_statement": {},
        "edit_files": [
          "sqlalchemy_continuum/table_builder.py"
        ],
        "oracle_files": [
          "import sqlalchemy as sa\nfrom sqlalchemy_utils import get_column_key\n\n\nclass ColumnReflector(object):\n    def __init__(self, manager, parent_table, model=None):\n        self.parent_table = parent_table\n        self.model = model\n        self.manager = manager\n\n    def option(self, name):\n        try:\n            return self.manager.option(self.model, name)\n        except TypeError:\n            return self.manager.options[name]\n\n    def reflect_column(self, column):\n        \"\"\"\n        Make a copy of parent table column and some alterations to it.\n\n        :param column: SQLAlchemy Column object of parent table\n        \"\"\"\n        # Make a copy of the column so that it does not point to wrong table.\n        column_copy = column._copy()\n        column_copy.unique = False\n        column_copy.onupdate = None\n        if column_copy.autoincrement:\n            column_copy.autoincrement = False\n        if column_copy.name == self.option('transaction_column_name'):\n            column_copy.nullable = False\n\n        if not column_copy.primary_key:\n            column_copy.nullable = True\n\n        # Find the right column key\n        if self.model is not None:\n            for key, value in sa.inspect(self.model).columns.items():\n                if value is column:\n                    column_copy.key = key\n        return column_copy\n\n    @property\n    def operation_type_column(self):\n        \"\"\"\n        Return the operation type column. By default the name of this column\n        is 'operation_type'.\n        \"\"\"\n        return sa.Column(\n            self.option('operation_type_column_name'),\n            sa.SmallInteger,\n            nullable=False,\n            index=True\n        )\n\n    @property\n    def transaction_column(self):\n        \"\"\"\n        Returns transaction column. By default the name of this column is\n        'transaction_id'.\n        \"\"\"\n        return sa.Column(\n            self.option('transaction_column_name'),\n            sa.BigInteger,\n            primary_key=True,\n            index=True,\n            autoincrement=False  # This is needed for MySQL\n        )\n\n    @property\n    def end_transaction_column(self):\n        \"\"\"\n        Returns end_transaction column. By default the name of this column is\n        'end_transaction_id'.\n        \"\"\"\n        return sa.Column(\n            self.option('end_transaction_column_name'),\n            sa.BigInteger,\n            index=True\n        )\n\n    @property\n    def reflected_parent_columns(self):\n        for column in self.parent_table.c:\n            if (\n                self.model and\n                self.manager.is_excluded_column(self.model, column)\n            ):\n                continue\n            reflected_column = self.reflect_column(column)\n            yield reflected_column\n\n    def __iter__(self):\n        for column in self.reflected_parent_columns:\n            yield column\n\n        # Only yield internal version columns if parent model is not using\n        # single table inheritance\n        if not self.model or not sa.inspect(self.model).single:\n            yield self.transaction_column\n            if self.option('strategy') == 'validity':\n                yield self.end_transaction_column\n            yield self.operation_type_column\n\n\nclass TableBuilder(object):\n    \"\"\"\n    TableBuilder handles the building of version tables based on parent\n    table's structure and versioning configuration options.\n    \"\"\"\n    def __init__(\n        self,\n        versioning_manager,\n        parent_table,\n        model=None\n    ):\n        self.manager = versioning_manager\n        self.parent_table = parent_table\n        self.model = model\n\n    def option(self, name):\n        try:\n            return self.manager.option(self.model, name)\n        except TypeError:\n            return self.manager.options[name]\n\n    @property\n    def table_name(self):\n        \"\"\"\n        Returns the version table name for current parent table.\n        \"\"\"\n        return self.option('table_name') % self.parent_table.name\n\n    @property\n    def columns(self):\n        return list(\n            column for column in\n            ColumnReflector(self.manager, self.parent_table, self.model)\n        )\n\n    def __call__(self, extends=None):\n        \"\"\"\n        Builds version table.\n        \"\"\"\n        columns = self.columns if extends is None else []\n        self.manager.plugins.after_build_version_table_columns(self, columns)\n        return sa.schema.Table(\n            extends.name if extends is not None else self.table_name,\n            self.parent_table.metadata,\n            *columns,\n            schema=self.parent_table.schema,\n            extend_existing=extends is not None\n        )\n"
        ],
        "test_patch": "",
        "patch_preview": "From aafca028db71406700c3106ac777a0587a9ea313 Mon Sep 17 00:00:00 2001\nFrom: Emily Waters <31249687+ew318@users.noreply.github.com>\nDate: Tue, 29 Nov 2022 10:06:26 +0000\nSubject: [PATCH] Update table_builder.py\n\nApply fix specified in https://github.com/kvesteri/sqlalchemy-continuum/issues/285#issuecomment-1229426028\n---\n sqlalchemy_continuum/table_builder.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/sqlalchemy_continuum/table_builder.py b/sqlalchemy_continuum/table_bui"
      },
      "patch": {
        "length": 983,
        "files_changed": 1,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_change": 0,
        "changed_files": [
          {
            "file": "sqlalchemy_continuum/table_builder.py",
            "added": 1,
            "deleted": 1
          }
        ]
      },
      "issue_comments": [],
      "issue_comments_count": 0,
      "code_statistics": {
        "total_files": 120,
        "total_lines": 13066,
        "total_bytes": 406430,
        "python_files": 99,
        "python_lines": 10681,
        "file_extensions": {
          ".ini": 1,
          "": 2,
          ".py": 99,
          ".in": 1,
          ".rst": 16,
          ".bat": 1
        },
        "largest_files": [
          {
            "path": "CHANGES.rst",
            "size": 17608,
            "lines": 701,
            "extension": ".rst"
          },
          {
            "path": "sqlalchemy_continuum/dialects/postgresql.py",
            "size": 15473,
            "lines": 526,
            "extension": ".py"
          },
          {
            "path": "tests/relationships/test_many_to_many_relations.py",
            "size": 14738,
            "lines": 464,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/manager.py",
            "size": 15939,
            "lines": 455,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/utils.py",
            "size": 12249,
            "lines": 452,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/relationship_builder.py",
            "size": 12703,
            "lines": 363,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/plugins/activity.py",
            "size": 10607,
            "lines": 343,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/unit_of_work.py",
            "size": 10939,
            "lines": 326,
            "extension": ".py"
          },
          {
            "path": "sqlalchemy_continuum/model_builder.py",
            "size": 9815,
            "lines": 288,
            "extension": ".py"
          },
          {
            "path": "tests/plugins/test_flask.py",
            "size": 8227,
            "lines": 266,
            "extension": ".py"
          }
        ]
      },
      "difficulty_metrics": {
        "total_files_in_repo": 120,
        "files_changed_count": 1,
        "files_changed_ratio": 0.008333333333333333,
        "total_lines_in_repo": 13066,
        "lines_added": 1,
        "lines_deleted": 1,
        "net_lines_changed": 0,
        "lines_changed_ratio": 0.00015306903413439462,
        "pr_body_length": 106,
        "commit_message_length": 32,
        "python_file_count": 99,
        "python_line_count": 10681
      }
    }
  ]
}